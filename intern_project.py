# -*- coding: utf-8 -*-
"""intern project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WeqtSQtr_roH_0yav9EPxc2kTdOAHgI4
"""

pip install nltk --upgrade

pip install vaderSentiment

pip install numpy --upgrade

pip install scipy --upgrade

import nltk
nltk.download()

import numpy as np

#VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.

from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

"""
sid = SentimentIntensityAnalyzer()
for sentence in sentences:
    ss = sid.polarity_scores(sentence)
    for k in sorted(ss):
      print('{0}: {1}, '.format(k, ss[k]), end='')
    print()
"""

from nltk.tokenize import sent_tokenize, word_tokenize,line_tokenize

f = open("2012LTinsultsLKML.tsv.txt", "r")
all_text = f.read()
print(all_text)

#for i in word_tokenize(all_text):
 # print(i)
print(word_tokenize(all_text))

t_hash = "#"
for i in line_tokenize(all_text):
  if t_hash not in i:
    print(i)

# preprocessing the text to remove the unwanted part and making the whole text in lowercase 
import re
def pre_process(text):
    
    # lowercase
    text=text.lower()
    
    #remove tags
    text=re.sub("<!--?.*?-->","",text)
    """
    # remove special characters and digits
    text=re.sub("(\\d|\\W)+"," ",text)
    """
    return text

def get_index(file):
  new_index = 0
  for line in file:
    new_index = new_index +1
  return(new_index)

# need to do stemming and Lemmatization

from nltk.stem import PorterStemmer

ps = PorterStemmer()

sid = SentimentIntensityAnalyzer()

def get_sentiment(line):
  ss = sid.polarity_scores(line)
  
  count = 0
  output = np.zeros(4)
  for k in ss:
    count = count + 1
    if count % 4 == 0 :
      output[3] = ss[k]
      print('{0}: {1}, '.format(k, ss[k]), end='')
      #print(k)
    
    elif count % 3 == 0:
      output[2] = ss[k]
      print('{0}: {1}, '.format(k, ss[k]), end='')
      #print(k)
    elif count%2 == 0:
      output[1] = ss[k]
      print('{0}: {1}, '.format(k, ss[k]), end='')
      #print(k)
    else:
      output[0] = ss[k]
      print('{0}: {1}, '.format(k, ss[k]), end='')
      #print(k)
  #print(ss[k])
  #print('{0}: {1}, '.format(k, ss[k]), end='')
  
  return output
      
    #print('{0}: {1}, '.format(k, ss[k]), end='')

    #print(ss[k])
  #print()
  #print(new_line)

from nltk.corpus import stopwords


stop_words = set(stopwords.words("english"))

webpage = "http" # since the comments rows in the file have a weblink available we extract the rows using this

new_fout = open("comments.txt","w")
sentiment_file = open("sentiment_value.txt","w")


index = 0

from __future__ import print_function
with open ("2012LTinsultsLKML.tsv.txt") as fin, open("positive_tokens.txt","w+") as fout:
    for line in fin:
      if t_hash not in line[0:2]:
        
        tokens = word_tokenize(line)
        
        if webpage in tokens:
          print(line[64:],end = '',file = new_fout)
          new_line = pre_process(line[64:]) # line changed to temp_sentence
          
          index = index + 1 
          out_sen = []
          out_sen = get_sentiment(line[64:])
          new_tokens = word_tokenize(new_line) 
                    
          filtered_sentence = []
          
          for w in new_tokens:
            if w not in stop_words:
              better_words = ps.stem(w)
              #print(w, " : ", better_words)
              filtered_sentence.append(better_words)
              #print(' '.join(new_tokens), end='\n', file=fout)
          
          print(filtered_sentence, end='\n', file=fout)
          
          print(out_sen,end = '\n',file = sentiment_file)

print (index)

"""
# delete this cell after use

vals = open("sentiment_value.txt","r")
values = vals.read()
print(values)
"""

c_file = open("all_files.txt","r")

all_index = get_index(c_file)
c_file = open("all_files.txt","r")
f_matrix = np.zeros((all_index,5))
l_count = 0
for line in c_file:
  f_matrix[l_count][0:4] = get_sentiment(line)
  l_count = l_count + 1

print(f_matrix[200:300])

print(l_count)

"""
for sentence in new_fout:
  ss = sid.polarity_scores(sentence)
  for k in sorted(ss):
      print('{0}: {1}, '.format(k, ss[k]), end='')
  print()
"""

from nltk.corpus import words
print (len(words.words()))

# We will now implement TF IDF to obtain the features in a numeric form 

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

new_file = open("positive_tokens.txt", "r")
X = vectorizer.fit_transform(new_file)

feature_file = open("positive_features.txt","w")
print(vectorizer.get_feature_names(),end = '\n',file = feature_file)

print(vectorizer.get_feature_names().index('idiot'))

print(X.toarray().shape)

#named entity recognition

#====================================================================================================================================================================================================

pos_file = open("build_noname.txt","r")

pos_index = get_index(pos_file)

pos_file = open("build_noname.txt","r")
pos_sentiment = np.ones((pos_index,5))
pos_count = 0
for line in pos_file:
  pos_sentiment[pos_count,:][0:4] = get_sentiment(line)
  pos_count = pos_count + 1  
print(pos_sentiment.shape)  

pos_val_file = open("pos_val_file.txt","w+")
for item in pos_sentiment:
  pos_val_file.write("%s\n" % item)

print(pos_sentiment[0:,4])

X = [[0, 0], [1, 1]]
     
print(X)     
Y = np.zeros((2,2))
Y[0][0] = 5
print(Y)

new_feature_matrix = np.concatenate((f_matrix, pos_sentiment))
print(f_matrix.shape)
print(pos_sentiment.shape)
print(new_feature_matrix)

labels = new_feature_matrix[0:,4]
final_features = new_feature_matrix[0:,0:4]
print(final_features)
print(labels)
from sklearn import svm

clf = svm.SVC(gamma='scale')
clf.fit(final_features[0:1000],labels[0:1000])

def pred_sentiment(file,no_lines):
  
  sentiment = np.zeros((no_lines,4))
  my_index = 0
  for line in file:
    sentiment[my_index] = get_sentiment(line)
    my_index = my_index+1
  return(sentiment)

def get_insults(file):
  
  t_hash = "#"
  webpage = "http"
  
  index = 0
  
  with open("2015insults_only.txt","w+") as file_out:
    for line in file:
      if t_hash not in line[0:2]:
        tokens = word_tokenize(line)
        if webpage in tokens:
          print(line[64:],end = '',file = file_out)

with open("2015LTinsultsLKML.tsv.txt","r") as temp_insults:
  get_insults(temp_insults)

with open("2015insults_only.txt","r") as test_insults:
  no_lines = get_index(test_insults)
with open("2015insults_only.txt","r") as test_insults:
  sent_test = pred_sentiment(test_insults,no_lines)

label_test = np.zeros((no_lines,1))
#print(sent_test)

print(sent_test.shape)
print(clf.score(sent_test,label_test))
print(clf.score(final_features[1000:1600],labels[1000:1600]))

print(final_features[1101,0:])

x = []
x = clf.predict(final_features[1000:1300])
print(x)
count = 0
for i in x:
  if i==0:
    print(count+692)
  count = count+1
phillipj		Kudos for getting these changes out in the open üëç   Really appreciate the effort you have put into the `README.md`s. The current lack of docs has been painful for a ansible rookie now and then.  Just wanted to give you a quick thumbs up so far, and do a more thorough review later.
joaocgreis		I've used this to deploy a Ubuntu server (for ChakraCore nightlies) and to generate my ssh config. This is amazing work and I believe our work going forward should be based on this as much as possible. I'd hate to see this bit rot in a PR.  Would it be easy to clean our current `setup` directory and leave only what is not included here? I would be happy to land this and remove what is no longer necessary in `setup`. We could add a note in the README (both this and `setup/README.md` perhaps) stating our intention to transition everything. For now, the `setup/windows` directory can be simply left there, there is no conflict nor overlap.  About the commit messages, since this is a major rewrite, I would be happy with squashing big parts (or all) of it. I do not care about authorship or integrity of my commit in this case (it's just 1 char!). 
gibfahn		>I'd hate to see this bit rot in a PR.  +100, this PR is awesome, and it's on my list of things to look at, but it'll be tough to review (at least for me). Also, I assume that the real proof will come from actually using it to provision and update machines.  Given that it's entirely additive, could we not just merge this in without removing any of the current stuff, and start trying it out for machines (perhaps on test machines that we have more than one of), and see how it goes? If we run into issues we should be able to iterate on what's already there right? Then we can start removing the old information once we know the refactored equivalent is stable.
jbergstroem		Thanks for your comments! Let me try and reduce a list of 'needs to be done' and work on those. I intentionally split up the folders so they could live alongside to have cover for things like windows and aix, but the main goal should obviously be having it support our entire infrastructure. We're already pretty close.  I guess a few aspects of this may or may not be "how to solve things in ansible" ‚Äì for instance baking our own inventory parser or having plugins that extend the config. If there are better patterns, I'm all open for suggestions.  It's been a lot of fun building this but I fell a bit short of time at the end; but I will obviously finish it as well as keep maintaining it.
gibfahn		@jbergstroem I was trying to connect to [test-requireio-osx1010-x64-1](https://ci.nodejs.org/computer/test-requireio-osx1010-x64-1/), and discovered I didn't have the ssh key in my `.ssh/config`. I reran the `write-ssh-config` playbook, but that didn't add the key.   Is this just something that needs to be added (or am I doing something wrong)?
jbergstroem		> @gibfahn Is this just something that needs to be added (or am I doing something wrong)?  Did you first add the worker to `inventory.yml`? If so, perhaps file the diff below.
jbergstroem		After a quick "kick off" for the collab summit we agreed to land the refactor in a subfolder so we can start filing PR's against it.
jbergstroem		One option to solve the credentials issue would be to create a small proxy script that adds this credential. This creates another trust issue since we then lack logic to control when it should fire, but that might be the better trade. 
Starefossen		Just making sure, and you are probably aware, that there exists an GitHub OAuth API scope which only grants access to the [Status API](https://developer.github.com/v3/repos/statuses/) ‚Äì namely the [`repo:status` scope](https://developer.github.com/v3/oauth/#scopes). 
jbergstroem		@Starefossen yep, that's what we're using but its still leakable in its current form. 
DavidTPate		The credentials is where it really starts to get difficult, I haven't seen a good way yet. If you look at Travis CI for example. When dealing with encrypted things (such as credentials or keys) it just doesn't provide them with PRs that aren't from the same repository.  You could totally limit the impact of the credentials being discovered by limiting the scope of the keys (and you want to do this regardless). But the ideal case would be to not have the keys leaked in the first place. What typically happens with most SaSS products that do this right now is that their status is reported by a service that they manage which has open access. It's not exactly ideal since anyone could update the status, but it gets us to the point where we have kept our credentials safe.  The last part would be limiting access to the service for updating statuses. I'm not familiar with the infrastructure, but if a web service can be created which simply updates the status for PRs and either have access limited by CIDRs, Routing, or some other method that would get us pretty much there. 
jbergstroem		@DavidTPate that's roughly what I suggested with my 'proxy' script. The problem is still that anyone can call it if they know what they're doing. The layer of security by obscurity is tricky to get rid of when you allow people to modify source code. 
thefourtheye		Given that our CI is mostly red these days, if we could somehow show the list of failing tests and their corresponding environments, it would be awesome. 
DavidTPate		Yeah, someone who knows what they are doing would still be able to manipulate it it would just have a tougher barrier to get to that point. It's definitely a tough problem.   The only way that I can think to really do this and limit exposure would be to have some credential generated for each build that allows exactly one call to update the build status for each job. 
jbergstroem		Thing is. If you know what you're doing you can privilege escalate other stuff (or `rm -rf`) as well. I think this is more about finding "good enough" security, then trust that people that start jobs actually glance over a PR before submitting it for execution. 
DavidTPate		@jbergstroem Yeah, that seems to be the case to me, there just doesn't seem to be a good way to completely secure something like this and "good enough" is a great start. 
orangemocha		Is there any way that we can distinguish Jenkins' success from unstable (only flaky tests failed)? I am concerned that if people start relying on the status checks to vet their PRs, that we'll lose visibility on flaky tests. Reporting the list of failed flaky tests back to GitHub would be ideal. 
jbergstroem		@orangemocha at github, not really -- we've got in progress, success or failed. What I've done though is added a note in the text mentioning how many flaky tests were run. 
jbergstroem		After giving it some thought I'm thinking we should do what @rvagg has been suggesting; 1. have a hook in `node-test-pull-request` that pings a server that starts polling 2. poll `node-test-commit` for slaves 3. poll each slave for updates until the parent is closed  polling sucks but this completely avoids any security-related issues and makes it more portable, meaning others can benefit from our work. 
Starefossen		Makes sense. Nothing wrong in taking the secure route here. 
DavidTPate		That sounds like a good solution, polling does suck but it seems like a very acceptable tradeoff here. 
Starefossen		@jbergstroem what is the status (pun not intended) here? I will have more time the next few weeks to help out with this if needed. 
jbergstroem		@Starefossen great news! Haven't started with this yet. Let coordinate something. 
Starefossen		Great! Is the polling service still the plan? I have played around with the `node-test-pull-request` REST API and it looks like we can get all the status we need from that single endpoint without having to poll the individual `node-test-commit` jobs.  https://ci.nodejs.org/job/node-test-pull-request/932/api/json  ``` json   "subBuilds": [     {       "abort": false,       "build": {         "subBuilds": [           {             "abort": false,             "build": {              },             "buildNumber": 1395,             "duration": "16 min",             "icon": "blue.png",             "jobName": "node-test-commit-arm",             "parentBuildNumber": 1359,             "parentJobName": "node-test-commit",             "phaseName": "Tests",             "result": "SUCCESS", ```  Just need to know how this endpoint behaves during a build and we should be good to go, my guess is that `"building": true` while it is building. 
jbergstroem		While at it, I think we should make something more generic. My thoughts are currently something in style with: - create jobs with endpoints. a job would represent a job at jenkins. also, understand the notion of sub-jobs. - create an api endpoint which receives a post for job notifications. This could be from github or jenkins (jenkins in our case, every time a job is created) - poll the specific job about connected slaves - store and update states;   - what's going on right now?   - is github successfully updated about it? - once a slave finishes:   - report back to gh, finish or pass   - store all information since things like flaky tests might be available at github in a later version.  We could also have a generic poller -- wouldn't be my preferred route though. 
Starefossen		Not sure I got the first `create jobs` part, but otherwise this is my understanding on how this service could be implemented:  ``` pseudocode # post jenkins build status to github pull request algorithm jenkins-github-status is   input: Integer job_id          Integer pr_id   output: Void    # save sub-build result between loop intervals   cache ‚Üê new Map()    do     job ‚Üê jenkins.getJob(job_id)      for build in job.subBuilds do       # only update GitHub if status has changed since last loop       if cache[build.jobName] ‚â† build.result         cache[build.jobName] ‚Üê build.result         github.postStatus(pr_id, build.jobName, build.result)       end if                      end for                      while                           job.building == true         end do                         return  end algorithm ```  This is obviously a simplification as you can not post a status to GitHub without having the shasum of one of the commits in the pull request, and since our builds take 16+ minutes to complete there should probably be a 60 seconds delay between the while loops intervals etc. 
jbergstroem		Generally looks good. Few comments; - jenkins has an guesstimation for job length; we can use that as part of our "polling interval frequency algoritm" - the job definition probably needs to be thought through; it can contain things like what input is expected to launch a poller against a job (looking up through shasum might not be impossible since that information is available in the node-test-commit sub-task) 
Starefossen		> - jenkins has an guesstimation for job length; we can use that as part of our "polling interval frequency algoritm"  Good suggestion!  > - the job definition probably needs to be thought through; it can contain things like what input is expected to launch a poller against a job (looking up through shasum might not be impossible since that information is available in the node-test-commit sub-task)  From the (current) `node-test-pull-request` action parameters we can query the GitHub API to get the commits for the pull request under testing (`TARGET_GITHUB_ORG` + `TARGET_REPO_NAME` + `PR_ID`) assuming the service has access to that repo of course.  We can also use the `POST_STATUS_TO_PR` parameter to control whether the service should post statuses to GitHub or not.   ``` json   "actions": [     {       "parameters": [         {           "name": "TARGET_GITHUB_ORG",           "value": "nodejs"         },         {           "name": "TARGET_REPO_NAME",           "value": "node"         },         {           "name": "PR_ID",           "value": "4116"         },         {           "name": "POST_STATUS_TO_PR",           "value": true ``` 
jbergstroem		Yes, but we need to unfold this into the workers at node-test-commit. At that level we'll have sha1 as well. Just saying that it'd be pretty easy to find a job based on sha1 (what we would get from github if we chose that route) since in most cases there'll only be one test-commit running the same sha.  Not saying using sha is the way to go here; I just see this util useful for more people than us. 
jbergstroem		The main problem with a hook from jenkins is that we'd have to share a secret; similar to the constraints of the current solution. Polling would remedy that, but so would sha1 from github as well; for instance having a hook receive input from github on new pr's or comments on a pr, checking pr id and/or together with link in comment. 
Fishrock123		As a note, [nodejs-github-bot](https://github.com/nodejs-github-bot/github-bot) now posts GitHub statuses. The live bot hasn't been updated yet, but it should first roll out for readable-stream, nodejs.org either at the same time or later. ([pr#15](https://github.com/nodejs-github-bot/github-bot/pull/15)).  I'll try look into how we might do this, but any help on the build end would be great.  ##   Some **important** notes:  Possible GitHub statuses: success, pending, failure, error. Additional "description" info can also be provided. Statuses also have a url parameter to link directly to the build.  We currently do it by PR for travis, but fully by commit only is totally possible. So we could either do the linking via PR id or by sha. 
jbergstroem		@fishrock123 I have a few suggestions/ideas; will post them shortly. 
Fishrock123		> Is there any way that we can distinguish Jenkins' success from unstable (only flaky tests failed)? I am concerned that if people start relying on the status checks to vet their PRs, that we'll lose visibility on flaky tests. Reporting the list of failed flaky tests back to GitHub would be ideal.  I contacted GitHub support about this, and their suggestion was to report back flaky tests as a separate status. I'm not really sure that is possible to separate out of jenkins easily though?  (Actually maybe I'm overthinking it and it isn't that hard..) 
Fishrock123		If it's green we can just report double green (or just a single green) status. If it is flay we change/add one that notes that flaky tests failed. 
jbergstroem		My plan of attack is to report each sub-worker of node-test-pr with results (if skip or fail; mention fails, skips and total) as well as introducing linter results and doing new code for commit messages. 
Fishrock123		@jbergstroem do you plan on doing directly from the CI, or just providing hooks to the bot? 9I sorta prefer the latter because then we can do a lot of it in JS..) 
jbergstroem		@Fishrock123 No, we can't do it from CI. We need to have the bot poll both github [pubsubhub events] and CI (api) to match what's being run, then poll each sub-job and post pending/ok/fail. I'm a bit in transit this week but are putting together a document that should outline what I see needs being done. 
mhdawson		@orangemocha, @misterdjules  FYI in case you want to help out getting these running on windows/smartos 
misterdjules		@mhdawson Thank you very much for the heads up, very much appreciated!  How can I get access to the SmartOS machines that are used to run these tests?  Also, I thought we were trying to avoid floating patches on top of V8 as much as possible. Do we have a list of V8 patches that are currently floating on top of each node branch? 
jbergstroem		As I outlined in the other issue, we need to reprovision a bigger vm to test on SmartOS. I can look at doing that in the coming week. As for windows, I reckon @joaocgreis would be the best guy to decide which machine we can use (or if we have to reprovision). 
misterdjules		@jbergstroem Excellent, I'll let you handle this, feel free to ping me if you need anything. Thank you! 
bnoordhuis		> We also only test on x64 which we believe is ok as we think the risk of our floating patches only affecting 32 bit is low. If this proves not to be the case we can adjust.  I'd be happier if we tested both, even if we only do it initially on e.g. x86 and x86_64 Linux.  V8 is largely platform agnostic but it's very much not architecture agnostic.  I'm very happy with the progress we're making, by the way! 
mhdawson		@jbergstroem is there a x86 32 bit machine that is big enough to run the tests ? 
mhdawson		@misterdjules  "trying to avoid floating" is the key part.   While we trying to keep the list small I think its quite likely we will have some small number at any one point. 
jbergstroem		@mhdawson There's PAE and swap. Reckon both would work. 
misterdjules		@mhdawson Is there a list of these floating patches somewhere so that we know at least how close we are from that goal? 
mhdawson		@misterdjules I don't have the lis, @ofrobots might comment as he put together a recent PR for 5.0.  I don't think we have a process for capturing them in a list but I have recent a recent one related to the test-tick-process on mac so I know there still are some.  I also think that even if we reach that goal we'll likely to have temporary ones as we wait for changes to percolate into new releases (and in the LTS releases were we won't take new v8 levels) 
mhdawson		@jbergstroem sorry I'm not quite following.  I searched on the ci page for machines with PAE or swap in the name and did not find anything but maybe  I'm just blind.  
misterdjules		@mhdawson   > I don't have the list, @ofrobots might comment as he put together a recent PR for 5.0. I don't think we have a process for capturing them in a list but I have recent a recent one related to the test-tick-process on mac so I know there still are some.  Sounds good. We had [such a list in joyent/node](https://github.com/nodejs/node-v0.x-archive/wiki/V8-upgrade-process/753a42dbe181c1afd4a7ff73fa28899101d938dd) and at that time I thought it was helpful. The fact that it was maintained manually was not ideal, but just having that info publicly available for everyone interested is I think valuable. 
ofrobots		@misterdjules @mhdawson At the moment we don't have a list of floating patches. I manually look at this history of the `deps/v8` directory.  I do agree that a list would be nice, however, the V8 team is talking to the LTS WG to figure out if V8 can unfreeze old branches so that Node could use those directly instead of floating patches. That might be an even nicer solution than keeping a manually updated list. 
refack		A possiblly related case where a patch made the V8 CI jobs fail, while the node jobs pass: https://github.com/nodejs/node/pull/14913. IMHO simply adding `node-test-commit-v8-linux/nodes=rhel72-s390x,v8test=v8test` (it's an 8 minute job) to the `node-test-commit` matrix will cover these cases.
mhdawson		@refack node-test-commit-v8-linux/nodes=rhel72-s390x was already there in the job, I just had not updated the matrix in the first comment in this issue. See https://ci.nodejs.org/job/node-test-commit-v8-linux/ to see the 4 platforms covered.
refack		@mhdawson Ack, but it's not in the `node-test-commit` cascade, so what might have happened is that a PR landed on `node/master` that passes `node-test-commit` but breaks `node-test-commit-v8-linux`. (my assumption is that `D8` doesn't compile, and it's not part of the "regular" node build tree): ```cpp ../src/d8.cc:229:12: error: 'platform_' was not declared in this scope      return platform_->GetTracingController(); ```
mhdawson		@refack got it.  I thought you were asking that it be added.  Is it that you were suggesting that we add the v8 tests to the node-test-commit cascade ? That is different from the topic of this issue but might be an idea to explore.  We've not done that in the past because of how long it runs, but we might be able to add it for zlinux since it runs fast.  
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
rvagg		As mentioned in https://github.com/nodejs/build/issues/108#issuecomment-343143008 I've started work on a shared-openssl job for 1.1.0g based in a docker container. I've been tinkering with node-test-commit-linux-fips and think we should just roll it into the same set of jobs along with other shared library builds and also a Debug build. Basically we'd use a pool of Ubuntu 16.04 containers that are running on 2 or 3 beefier machines (with `JOBS` relatively high so they can soak up unused capacity where possible) that can run all of these things. node-test-commit-linux-fips currently downloads and compiles the FIPS OpenSSL but I think we should just bake this into the container along with other shared libraries. Then our configs are in Dockerfiles inside our Ansible scripts and we can update everything there and we don't get the extra build overhead for what is identical on each run.
rvagg		merged for now, can tweak in additional PRs
rvagg		oh, this includes the "sharedlibs" Ubuntu container that can run OpenSSL 1.1.0 and OpenSSL-FIPS tests too, currently playing with this here: https://ci.nodejs.org/view/All/job/node-test-commit-linux-linked/
refack		ü•á +1 from me  cc @nodejs/build (silly GitHub)
refack		It's markdown so we can add a comment `<!-- sorted by GitHub handle -->`
maclover7		updated @gibfahn, not sure why I thought it was sorted by first name üòÜ 
gibfahn		>updated @gibfahn, not sure why I thought it was sorted by first name üòÜ  Probably because we put the names first üò¨ 
gibfahn		@nodejs/build any objections? If not we should consider this settled.
gibfahn		Landed in https://github.com/nodejs/build/commit/c1f91cc4e9620bd6ef39bb7074c402766e50c7da
MylesBorins		Do we have something like [fail2ban](http://www.fail2ban.org/wiki/index.php/Main_Page) set up on the server?  I've had fantastic success minimizing bruteforce attacks with it 
jbergstroem		@TheAlphaNerd nope. adding all its variations on all of our hosts is somewhat tedious. feel free to PR, but changing port would be the simplest way to avoid bruteforce stuff. 
MylesBorins		I just took a quick perusal... please correct me if I am wrong here - build/setup is a set of ansible scripts used for creating each of the platforms we are testing on? - each platform has its own playbook - adding a meta service such as fail2ban would require adding it to each service   - testing as well 
jbergstroem		@TheAlphaNerd correct. not sure how well abstracted the firewall implementations are either (ipfw on freebsd, pf on solaris, etc). 
Starefossen		If insecure password auth is disabled, as it should be, what is the benefit by moving to an non-standard port? Just wondering. Is the brute force attempts a DDoS concern? Obviously they will never be successful in brute forcing RSA SSH keys. 
bnoordhuis		Your sshd logs not getting flooded is one. 
jbergstroem		@Starefossen: at one of our boxes when i logged in earlier today:  > There were 1047183 failed login attempts since the last successful login.  The question for me is "why not". A poorly configured client sounds like the least of our issues. 
jbergstroem		Here's another one:  > Last failed login: Mon Nov  9 11:18:15 UTC 2015 from a.b.c.d on ssh:notty > There were 14542 failed login attempts since the last successful login.  (that's 10 hours) 
Starefossen		Yes, that is quite a lot of failed login attempts. I guess "why not". 
imjacobclark		This should be pretty standard to do in the ansible-playbook, pseudo code where the host ssh config lives in `/etc/ssh/sshd_config`.  ansible-vars:   ``` ansible ubuntu_common_ssh_port: 9999 ```  ansible-playbook:  ``` - name: Change ssh port   lineinfile: dest=/etc/ssh/sshd_config               regexp="^Port\s"               line="Port {{ common_ssh_port }}"               state=present   notify: Restart ssh ```  Is this something I could pick up?   Are there any docs over the best way to get everything working locally to test any changes? Or is it just a case of working through Ansible on a vanilla VM, provision and test that way? 
jbergstroem		You would have to check for 9999 or 22 as well when connecting, right (otherwise you wouldn't know if the port needs to change)?  I usually test on local ansible with bhyve (or xhyve) vm's.  
imjacobclark		Yes that's correct, I guess if you know which resource you're connecting to, you would know the port number too, prevents 22 spam/bruit force attacks as a minimum prevention.  
jbergstroem		`^Port` might not work; default configs usually does `# Port`; How about `^(#\s)?Port\s`? 
imjacobclark		Ah yes quite right. I'll pick this up! 
jbergstroem		I'm still not sure how to catch connection errors from ansible's `ansible_ssh_port` (just `ssh_port` in ansible 2.0 which we will be targetting) so a connection fail to 12345 would make us try 22 and run the ssh job though. 
Trott		@nodejs/build @jbergstroem Seems do-able but just needs a committed person to make it happen maybe?
maclover7		Closing for now as something that would be nice to have, but seems to not currently be within our means. If someone wants to tackle this, please feel free to reopen.
mhdawson		Discussed in latest (and I think in earlier meeting), no objections.  Gibson to raise PR explaining approach.
rvagg		added `JOBS`, PTAL @jbergstroem  
jbergstroem		LGTM with optional readability nit. 
rvagg		@jbergstroem is this OK? 
jbergstroem		@rvagg sorry - should have elaborated. JENKINS_PATH gets passed to the binary [through the init script](https://github.com/nodejs/build/blob/master/setup/centos5/resources/jenkins.initd#L25). My suggestion was to move your path juggling to the JENKINS_PATH env since thats already used for the same purpose. I guess we're down to semantics so feel free to commit whatever's working for you. 
jbergstroem		LGTM 
bnoordhuis		https://github.com/nodejs/node/issues/12719#issuecomment-304941983 is relevant, error when building the documentation with `make doc`: ``` /Users/iojs/build/ws/tools/doc/node_modules/marked/lib/marked.js:1062       while (this.next().type !== 'list_item_end') {                         ^  TypeError: Cannot read property 'type' of undefined ```
mhdawson		Strangely make doc seemed to fail before this PR landed and passed afterwareds https://github.com/nodejs/node/pull/13274
mhdawson		It failed in generating errors.md which was the one changed by that PR.
mhdawson		@jasongin can you check if last nights builds were ok ?   I'm  wondering if the PR I mentioned fixed the doc issue.
gibfahn		Header packages are there in the latest nightly (assuming this is the right one):  https://nodejs.org/download/nightly/v9.0.0-nightly20170531102e1aa4e3/  ![image](https://cloud.githubusercontent.com/assets/15943089/26688778/86b49dca-46eb-11e7-9de4-e8dafa41bf41.png) 
jasongin		Yes, the latest nightly build looks good. Closing.
rvagg		fixicated 
therebelrobot		I would love to help out, though I'm not sure where all this would be done... a digitalocean instance maybe? Or could we set up a task in travisCI? 
chrisdickinson		If I can feed the [static analysis](https://github.com/chrisdickinson/estoc) output into a queryable database, we should be able to get a list of "affected modules" given a changelist fairly cheaply. If we further filter that against a minimum number of downloads or dependents, we can at least get automated smoke tests.   I'd love to help out with this effort. 
rvagg		@therebelrobot open question how it actually gets done but I suspect we'd be best to use our own CI resources, so at the moment it might look like a combination of Jenkins and build servers somewhere. DigitalOcean and/or Rackspace for Linux, Rackspace for possible Windows testing (might be best to defer that one!). 
rvagg		oh, if anybody wants to make a start on this then I can provide server resources on one of our free accounts, if it's legitimately for this effort and won't be wasted resource. 
therebelrobot		That makes sense. I would love to take a look at it, though I may not be the best to take point on it, given how much experience I have on automated regression testing (which is little). 
jbergstroem		@rvagg how about a new job in jenkins we run as part of the RC/release procedure? We could check in a list packages (or have it as part of documentation that we can parse somehow) that we pass to the build and run. `npm install` would return non-zero exit codes so we'd at least get failures as a start. Adding more functionality to make output more visible could be a next step. 
Fishrock123		Subbing to this issue, the big question I have is: is there any way we could know / try to run any arbitrary module's tests?  I suppose we could try a bunch of the most common options? 
ceejbot		Pre-select a group of modules, top 50 downloaded from npm to start, and choose only modules where `npm install && npm test` works to run the tests. You could seed the VM with a few common resources (figure out what the modules need by looking at their `.travis.yml` files) or to simplify, start with modules that need nothing.  
jbergstroem		I think selecting 10 packages ‚Äì pure js and native ‚Äì that doesn't have dependencies such as mysql and lives in the top 100 would be a good enough start.   Edit: ..with the added benefit of being able to trust their test suites enough to have it part of our verification. 
rvagg		I started work on this here: https://github.com/rvagg/iojs-smoke-tests  Uses Docker to isolate so it's limited to Linux atm. The procedure could also work on a dedicated machine too so perhaps if we wanted to multi-OS this we could. For now though, the tricky bit is in making the test runs informative and non-flakey. 
mhdawson		We already do this in our internal (IBM) builds for a list of modules and @jasnell was working to pull this over into the Node builds.  It was originally in perl so he was going to port it over to run under Node.  Right now we run in 2 flavors.  1) just do the install,  2) do the install and run built in tests for the module.  The second is a subset of the first as its not always easy to automatically identify and run the built in tests.    One thought we had along the way would be if there was guidance from the Node community to module developers with respect to how to include tests so that they could be easily run.   More recently we've also broken it out into 2 different types of runs.  One which fixes the module versions so that we can tell if its the runtime that causes regressions.  A second runs with the latest versions of the modules but a fixed runtime level so that we can identify regressions in the modules themselves.   
jasnell		The node port on this is still in plan, it just ended up being pushed to a lower priority for me. Can push it up the stack if need be. On Jun 1, 2015 6:26 AM, "Michael Dawson" notifications@github.com wrote:  > We already do this in our internal (IBM) builds for a list of modules and > @jasnell https://github.com/jasnell was working to pull this over into > the Node builds. It was originally in perl so he was going to port it over > to run under Node. Right now we run in 2 flavors. 1) just do the install, > 2) do the install and run built in tests for the module. The second is a > subset of the first as its not always easy to automatically identify and > run the built in tests. One thought we had along the way would be if there > was guidance from the Node community to module developers with respect to > how to include tests so that they could be easily run. >  > More recently we've also broken it out into 2 different types of runs. One > which fixes the module versions so that we can tell if its the runtime that > causes regressions. A second runs with the latest versions of the modules > but a fixed runtime level so that we can identify regressions in the > modules themselves. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/82#issuecomment-107462885. 
cjihrig		> One thought we had along the way would be if there was guidance from the Node community to module developers with respect to how to include tests so that they could be easily run.  I'm not sure if this is what you mean, but (as previously mentioned in this thread) `npm install && npm test` should be the only steps required for most modules. 
mhdawson		@cjihrig, I've not looked into the details myself but from what I understand not all modules include the right scripts so that just works.   @cgavrila can you provide some of the specifics, possibly with an example of one where it does not work.  
Fishrock123		> I've not looked into the details myself but from what I understand not all modules include the right scripts so that just works.  Right, the suggestion is to just start with the ones that do. 
CGavrila		In reply to @mhdawson's reply, a couple of comments before.  For simple installation testing, I think what we have is quite solid. It takes a list of npm modules and iteratively does `npm install module`, `npm install module2`, cleans up after itself and then displays statistics.  The way we do the testing is something like:  ``` npm install some_module cd node_modules/some_module npm install npm test ```  However, there are a couple of modules which don't come with tests packaged when you do `npm install module`, even though they do have tests in their repositories. One example of that would be socket.io.  So what we do is something more like:  ``` npm install some_module git clone git_repo_of_some_module cd git_repo_of_some_module git checkout last_available_tag  // so that we get the latest released version and not some dev. version npm install npm test ```  I think this sort of approach as a principle is sane, but the implementation we have is not bulletproof, primarily because there can be inconsistencies with the versions and other stuff which I haven't quite covered here. For example, sometimes it's very difficult to tell if a module installation failed or succeeded because exit codes are not reliable. You can easily get false positives or false negatives since there are modules which don't install the 'standard' way.  What we currently have is written in Python, and went from being a ~30 line script to ~700 lines at the moment, as these sort of issues arose.   Depending on what you want to test exactly, you may find that this is really easy to overthink, as I have found out myself.  
Fishrock123		> However, there are a couple of modules which don't come with tests packaged when you do npm install module, even though they do have tests in their repositories. One example of that would be socket.io.  Good point, I know of a lot of express submodules that do not bundle tests.. :/  All of them should have repo info in package.json though.  Also, we shouldn't have to clone them, we can probably just grab github tarballs of the repos. 
jasnell		Another key challenge is that several of the modules require additional dependencies that aren't necessarily part of the install. Testing the redis module, for instance, requires a redis server. It's certainly not a difficult problem to solve if we know in advance what additional resources the modules are going to require but it does make fully automating the process more difficult.   The other challenge is the fact that the test output is not standardized. While many do end up producing consistent output, there are quite a few that do not. In order to appropriately determine the status of the tests when we're done, unless we're looking for a simple pass/fail, we would need to either (a) get all the modules updated to produce consistent test output or (b) implement one off parsers to read the output for specific modules. As @CGavrila says, it's easy to overthink it tho.  The tl/dr version is this: 1. Not all the modules support npm test 2. Not all the modules that do support npm test are fully self-contained 3. Not all the modules that do support npm test generate consistent output 
Starefossen		I'm not entirely sure how a more consistent output test output like tap would make much change in this instance? When some package's test suite fails, a developer will most likely have to inspect the failing test anyway. Also, most of them provides a fairly understandable message to pinpoint at which point the test failed.  Also, since the packages are selected manually in head of time, I would say it is fairly easy to set up a Docker environment which will be able to spin up any required external services before running the test. I would imagine a [`docker-compose.yml`](https://github.com/docker/compose) file for each package would do the trick. 
jasnell		As I said, it's not a huge problem ;) going with a strictly pass/fail approach that simply presents the relevant output without attempting to dig in "more intelligently" is likely ideal. And if we are being selective about which items to test, then yes, we can set up the necessary environment in advance. I'm not saying we shouldn't do it this way, I'm just drawing out what the constraints are. We should avoid attempting to do anything too clever here. On Jun 1, 2015 1:01 PM, "Hans Kristian Flaatten" notifications@github.com wrote:  > I'm not entirely sure how a more consistent output test output like tap > would make much change in this instance? When some package's test suite > fails, a developer will most likely have to inspect the failing test > anyway. Also, most of them provides a fairly understandable message to > pinpoint at which point the test failed. >  > Also, since the packages are selected manually in head of time, I would > say it is fairly easy to set up a Docker environment which will be able to > spin up any required external services before running the test. I would > imagine a docker-compose.yml https://github.com/docker/compose file for > each package would do the trick. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/82#issuecomment-107690128. 
retrohacker		Late to the party. FWIW @othiym23 made this suggestion almost a year ago. Does it still stand as best practice?  https://github.com/nodesource/distributions/pull/12#issuecomment-56770431 
rvagg		I used https://github.com/rvagg/iojs-smoke-tests on the last release, @ceejbot contributed a bunch of additional popular packages and it's been helpful but there's still some work to be done to make it more useful. 
retrohacker		I was going to take a look at some of this for the docker-iojs working group. I'll report back here with what I find :smile: 
jbergstroem		Now lives in the [citgm repository](http://github.com/nodejs/citgm/) and is run on ci.nodejs.org! :sunny:  
jasnell		Awesome! On Nov 26, 2015 5:22 AM, "Johan Bergstr√∂m" notifications@github.com wrote:  > Now lives in the citgm repository https://github.com/nodejs/citgm/ and > is run on ci.nodejs.org! [image: :sunny:] >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/82#issuecomment-159910835. 
MylesBorins		AFAIK the smoker job is not fully working. Should we open another issue? 
jbergstroem		Superseded by https://github.com/nodejs/build/issues/367. 
retrohacker		The concern was raised during the first WG meeting about running untrusted code on a PR. This was in reference to the iojs codebase though.  That being said, with the website repo, I don't see a reason why we couldn't offload the work to TravisCI or Codeship. 
jbergstroem		I like the approach suggested by @rvagg in our last meeting wrt having trusted collaborators for full runs within our own ci. 
retrohacker		Would that be overkill for a website repository? 
jbergstroem		Sorry, I somehow missed that this was only about `website`. Yeah, overkill indeed.  
retrohacker		`website` does not have any unit tests and we do not have access to the settings for that repo. If we did propose this to the website WG, what would be the scope of the build team's involvement? - [ ] Give build team access to website repo to configure and maintain CI. - [ ] Setup CI server for website repo. - [ ] CI ensures gulp task completes without error (no guarantee what it generated is correct). - [ ] Develop unit tests for website repo. - [ ] Deploy upon a successful CI test after merging into master. 
rvagg		you'd better file an issue over at iojs/website to get buy-in for this, no point in pushing forward if they don't care 
kenperkins		Good point @rvagg, added https://github.com/iojs/website/issues/231 
mikeal		This is a "nice-to-have" but at the top of our list right now is just to trigger the gulp build on each commit/re-deploy so we can stop checking in "rebuild" commits all the time :) 
kenperkins		@mikeal I wasn't capturing this necessarily for "right now" but just to make sure it's in an issue. Do we have an issue for auto build on commit? 
jbergstroem		Arguing that https://github.com/nodejs/build/issues/236 is more relevant and contains more info. Also, auto-testing PR's are ways off. Closing. 
gibfahn		@gdams has used pip on AIX before, he's going to try getting it working on a temporary machine so we can work out the process.
edelsohn		I have Python pip installed on my AIX systems. I was able to bootstrap with get-pip.py.  There is a typo / bug in the sysconfigdata built on AIX for some Python releases.
gdams		I got pip working on aix.   ```bash curl https://bootstrap.pypa.io/get-pip.py | python export PATH=/opt/freeware/bin:$PATH ```
jbergstroem		great news! can we get this in as part of the ansible playbook
gibfahn		@jbergstroem So, my plan is:   1. Install pip manually on one of the test machines and document in [setup/aix61/manualBootstrap.txt](https://github.com/nodejs/build/blob/master/setup/aix61/manualBootstrap.txt).  2. If nothing is broken after a week, install on the other test machine and ask you to install on the release machine.  3. As part of the Ansible work on AIX (see https://github.com/nodejs/build/issues/551) add pip to the Ansible scripts.
gibfahn		I've installed pip on `power8-nodejs3.osuosl.org` (which I'm pretty sure is `test-osuosl-aix61-ppc64_be-1`). Pip itself seems to be working fine, my issue with curl is documented in #551.  What I did:  ```bash curl -O https://bootstrap.pypa.io/get-pip.py  scp get-pip.py test-osuosl-aix61-ppc64_be-1:/home/gib  ssh test-osuosl-aix61-ppc64_be-1 bash python /home/gib/get-pip.py ln -s /opt/freeware/bin/pip /usr/bin/pip pip ```
sxa555		I thought I'd added this comment yesterday but apparently not - might be better to symlink /opt/freeware/bin/pip to /usr/bin/pip to avoid having to tweak the path - that would make it consistent with how a lot of the other freeware tools get installed.
gibfahn		Makes sense, I'll update the script (and the docs)
gibfahn		I've now installed pip on `power8-nodejs2.osuosl.org` (which I'm pretty sure is `test-osuosl-aix61-ppc64_be-2`). Pip itself seems to be working fine, but I had to change the LIBPATH fix, as what works for curl doesn't work for python.  What I did:  ```bash ssh test-osuosl-aix61-ppc64_be-2 bash LIBPATH=/usr/lib curl https://bootstrap.pypa.io/get-pip.py | python ln -s /opt/freeware/bin/pip /usr/bin/pip pip # You should get the pip options ```  @mhdawson can you install `pip` on the release AIX machine? @jbergstroem does that make sense as the next step? I don't think there have been any issues with the machine that's had `pip` on it for the last three weeks.
gibfahn		PR to update the LIBPATH instructions: https://github.com/nodejs/build/pull/629 
gibfahn		Updating the release machine will be covered in https://github.com/nodejs/build/issues/630, so this can be closed.
jbergstroem		To elaborate on the jenkins folder; this is the storage distribution:  ``` bash $ /var/lib/jenkins# du -d1 -h . 60K ./secrets 8.0K    ./userContent 1.2M    ./logs 684K    ./nodes 12M ./fingerprints 872K    ./config-history 168G    ./jobs 1.3M    ./updates 2.0M    ./users 8.0K    ./init.groovy.d 128M    ./plugins 168G    . ```  The job folders looks slightly different depending on what plugin we call the through (axis, multijob, etc) but the gist of a job would look something like this:  ``` bash $ /var/lib/jenkins/jobs/node-test-commit-linux/configurations/axis-nodes# ls -al centos5-32/builds/1834/ total 2396 drwxr-xr-x    3 jenkins jenkins    4096 Jan 16 01:40 . drwxr-x--- 1829 jenkins jenkins   32768 Jan 18 22:03 .. -rw-r--r--    1 jenkins jenkins  671595 Jan 16 01:40 build.xml -rw-r--r--    1 jenkins jenkins     734 Jan 16 01:31 changelog.xml -rw-r--r--    1 jenkins jenkins    1616 Jan 16 01:40 injectedEnvVars.txt -rw-r--r--    1 jenkins jenkins 1725938 Jan 16 01:40 log drwxr-xr-x    2 jenkins jenkins    4096 Jan 16 01:40 tap-master-files ```  I'll experiment a bit with exports, but I wouldn't have a problem with excluding everything sans `$job/config.xml` (which is the config for that job) as part of the backup job. 
jbergstroem		#### Some kind of actionlist/todo/work in progress/god knows  Outstanding: - [x] create backup keys/users - [x] decide how to handle old jobs in jenkins before we write backup scripts  ##### cold storage things: - [x] /home/jenkins/old jenkins.tar.xz which lives at new ci host - [x] /var/log/nginx/old which contains old nginx access logs for nodejs  ##### key parts of rsnapshot.conf  ``` ini link_dest   1  interval    daily       7 interval    weekly      4 interval    monthly     12  rsync_long_args --timeout=300 --delete --numeric-ids --no-relative --delete-excluded --no-owner --no-group ssh_args    -F /path/to/ssh-config-with-hosts-and-keys  backup_script   /path/to/below/database-script.sh                       db/ backup          user@www-host:/var/log/nginx/nodejs.org-access.log*     logs/nodejs backup          user@www-host:/var/log/nginx/iojs.org-access.log*       logs/iojs backup          user@www-host:/var/log/nginx/libuv.org-access.log*      logs/iojs ```  ##### database backup script  ``` bash #!/bin/bash  ARGS="-F /path/to/ssh-config-for-db" DB_ARGS="--single-transaction --extended-insert=FALSE -u backup_user" DATE=$(date '+%Y%m%d@%M%S')  ssh ${ARGS} ${host} "mysqldump ${DB_ARGS} ${database} | xz -c" > benchmark-${DATE}.tar.xz ``` 
jbergstroem		Quick update; I've got a server running on joyent where I've now copied the cold storage stuff. The readme currently contains:  #### nodejs backup directory  This folder contains both 'cold storage' files/folders as well as periodically synced content (generated by [rsnapshot](http://rsnapshot.org)).  Try to keep the list of items up to date since this likely still stay manual.  ##### Table of contents  | filename | description | | --- | --- | | archive/ | a folder meant for cold storage; ie no periodic updates | | archive/nodejs-logs | contains compressed logfiles from the old nodejs server | | archive/old-jenkins.tar.xz | a full copy of jenkins pre-security-vulnerability | 
mhdawson		+1 for rsync for backups, I like it and use it at home.    One suggestion is that we might want to have 2 copies with different providers.  Once we have it setup for 1 it should be trivial to add the other.   That way if we have issues with one provider we - can still carry on doing backups with the other provider - can still restore from the other provider  I'm assuming that storage in the vms (ex joyent) is not backed-up/guaranteed  to be available.   
jbergstroem		@mhdawson i'm all for it. Not sure which vm provider to use; 500G+ is starting to be expensive (I guess that's when providers want you to go to object storage). 
rvagg		Additional directories on nodejs.org we need to save: - /home/dist/nodejs/ (binaries, vital, most important thing we can save!) - /home/dist/iojs/ (binaries) - /home/dist/metrics/ (questionable, they are generated from the log files but can take a day or more to generate, nearly 10G atm) - /home/nodejs/old/ (historical stuff from Joyent server, worth keeping, won't ever change though) - /home/libuv/www/dist/  I'm also a fan of just bundling up and storing /etc/ since it's so small and often contains stuff that you wish you kept but didn't realise wasn't stored anywhere else. 
rvagg		also note the `libuv` -> `iojs` in the config you posted @jbergstroem   ``` backup          user@www-host:/var/log/nginx/nodejs.org-access.log*     logs/nodejs backup          user@www-host:/var/log/nginx/iojs.org-access.log*       logs/iojs backup          user@www-host:/var/log/nginx/libuv.org-access.log*      logs/iojs ``` 
jbergstroem		@rvagg correct indeed -- poor copy paste. I'll post the final config here before the actual rsync. I reckon we'll want to keep access logs indefinitely. 
mhdawson		Some data from discussion with jbergstroem 1) a softlayer vm with 1TB storage 1CPU and 1GB ram is only $82/month + $45 to take traffic from 250G to 1T per month 2) a softlayer dedicate machine with 2 500GB disks is $180/month, probably also additional $ for traffic over 250G 3) softlayer network storage with low i/o speed is $150/month for 1T  I'm thinking 1) might be a reasonable option for one of the mirrors. 
rvagg		@jbergstroem I'm +1 on your suggested approach and am keen for you to move forward in the way you see appropriate, I'd like to have a look after the fact to sanity check that we're getting what we need backed up. I have no opinion on _where_ it should be backed up to except that it'd probably be good if we didn't do it in DO and probably not in Rackspace either cause of our existing high usage level. So Softlayer or Joyent or .. whatever, great. Let me know where I can help along the way! 
jbergstroem		Thanks, @rvagg. I'll file a PR with the config once I move closer. And yeah, review is much appreciated.  Also, just so I don't forget: - [x] exclude jenkins workspaces (currently 60G) from rsync 
jbergstroem		Fyi - while testing/running a rsync with exclude [`rsync --exclude "workspace*" -avz -e ssh foo@bar:/var/lib/jenkins .`] on our current setup (33 days of jobs) ended up consuming 53Gb. Once I get all variations of build folders (there's a few) we'll be slightly smaller. 
jbergstroem		@rvagg one thing that' id appreciate some help with would be doing some kind of inventory of what we want to back up (and possibly how long -- I can for instance see how we'd never want to have release tarballs as part of a 1y backup rotation) for the www-host.   edit: I'm a fool. Somehow missed that you've already done this above. 
jbergstroem		@rvagg does this mean we don't care about cloudfuse stuff? 
rvagg		I think it best to consider cloudfuse a failed experiment. I'm not happy with the reliability of the fuse mount nor the fact that it's a pure object-store that can't deal with fs metadata so that all gets lost. 
jbergstroem		Moar: - [x] run "delete-jobs" as part of a (successful) rsync from rsnapshot - [x] back up the new jenkins release-master - [x] iptables rules 
jbergstroem		Just an update -- will file a PR after I've sorted reloading jenkins after the cleanup job (needs github credentials). Here's the latest readme which should cover what we're doing at the moment:  ---  # nodejs backup directory  This folder contains both 'cold storage' files/folders as well as periodically synced content [generated by rsnapshot].  Try to keep below up to date.  ### servers that are actively backed up - ci.nodejs.org: the main jenkins server. full backup of /var/lib/jenkins (rotated) - ci-release.nodejs.org: release jenkins server. full backup of /var/lib/jenkins (rotated) - iojs-softlayer-benchmark: benchmark server. mysql dump (rotated) - iojs-www: www server for nodejs.org/iojs.org.   - /home/dist/iojs: all artifacts for iojs.org (mirror, no rotation/deletion)   - /home/dist/nodejs: all artifacts for nodejs.org (mirror, no rotation/deletion)   - /home/libuv/www/dist: all artifacts for libuv.org (mirror, no rotation/deletion)   - /var/log/nginx: all logs for nodejs, iojs and libuv (rotated)  ### table of contents  | filename | description | | --- | --- | | archive/ | archived content | | archive/nodejs-logs | contains compressed logfiles from the old nodejs server | | archive/old-jenkins.tar.xz | a full copy of jenkins pre-security-vulnerability | | mirror/ | backed up content indended for non-rotation | | periodic/ | periodically updated storage managed by rsnapshot | 
MylesBorins		I also think it is really worth bringing up the question if we should be running automated tests against examples in the docs.  If we broke that out into it's own testing suite we could have a CI job to run `only` those tests, and just tack that CI job onto the current stack.  This could end up being very useful for LTS 
jasnell		Having a docs-only CI that ran only the potentially affected tests would be ideal. Having it clearly documented that there are, in fact, tests that depend on the docs is even more ideal 
chrisdickinson		I'd like to second what @TheAlphaNerd suggested ‚Äî instead of pulling out code examples from docs, generating tests from them, and running them, we should just turn those examples into real tests.   Doc CI is a thing that should happen separately and should probably address different concerns than "ensure the addon API works as expected." 
jbergstroem		I think we should do both. Having a doc-specific test run could focus on other areas of testing. 
orangemocha		fwiw, node-test-commit has a `pure_docs_change` option under NODES_SUBSET. Right now no tests are run. It was meant as a placeholder for docs tests. 
kunalspathak		@joaocgreis  - Please help review.
kunalspathak		Thanks @thefourtheye  for reviewing.
joaocgreis		Landed in https://github.com/nodejs/build/commit/44354cbf30faa4163a7ffba0da552a79b53ca1c7 and https://github.com/nodejs/build/commit/7b9d9f9b145cf7477047fae8b1b0f1b0bf879840 .
Fishrock123		@TheAlphaNerd what's the benefit to using citgm for thus over `make test-npm`? 
zkat		As of https://github.com/nodejs/node/pull/4960 and https://github.com/nodejs/node/pull/4958, `make test-npm` should pass on non-windows platforms without any modifications. Note that there's some discussion in #4958 about false negatives due to progress bar config, so track that.  
MylesBorins		@Fishrock123 I more meant that the way citgm is setup we could modify the job to call `make test-npm` instead of citgm and we'd have a CI job ready to run. Just the way it is designed is easy to modify  edit: The idea being cloning the ci job into a new job specifically for testing npm 
MylesBorins		I just went ahead and made it...   https://ci.nodejs.org/job/thealphanerd-npm/ 
MylesBorins		I'm going to re-examine this as a possibility next week... /cc @nodejs/npm   edit: likely only using osx + windows as a target for now 
gibfahn		@TheAlphaNerd https://github.com/nodejs/node/pull/7867 should mean that `make test-npm` and `vcbuild.bat nosign test-npm` generate TAP files (called `test-npm.tap`) in the node directory. 
MylesBorins		@gibfahn the problems we were having in CI were unrelated to the output. Simply getting the suite to pass was not happening. that being said, tap output is a great start 
maclover7		Very similar to #234 -- should they be combined or closed?
gibfahn		We have [gibfahn-test-npm](https://ci.nodejs.org/view/All/job/gibfahn-test-npm) and [gibfahn-test-npm-win](https://ci.nodejs.org/job/gibfahn-test-npm-win), which pass on my PR branch (https://github.com/nodejs/node/pull/11540). Once that lands I'll take my name off the jobs and give collaborators access.  Unfortunately I've never seen a green CI run on Linux (macOS is pretty good).
gibfahn		This is done:  https://ci.nodejs.org/view/All/job/node-test-npm/ https://ci.nodejs.org/view/All/job/node-test-npm-win/
jbergstroem		I'm not sure if you're asking about giving him access to the `test-` key (ref: ongoing) or just adding his key to test-osuosl-aix*? I'm ok with the second option seeing how you vouch for him. 
mhdawson		It is the second - add his key to those machines.  His request was access to the release machine as well as we'll need his support there as well.  I guess the more consistent way would be to give him access when needed to that machine but we should at least agree here that it can happen when necessary.   
mhdawson		@nodejs/build any objections ?  Otherwise I'll consider approved for access to the test machines by @jbergstroem's ok. 
mhdawson		David's key are installed on test machines so closing. 
phillipj		Thanks for excellent reviews, just what I needed! 
jbergstroem		You accidentally checked in `ansible-playbook.retry` 
phillipj		Just pushed a commit removing some unwanted quotes. I thought _every_ variable should be quoted, that didn't work as expected. Tried this playbook on the server running the bot in production ATM, but couldn't get the bot started afterwards. Suddenly realised a strange directory had been made in `/home`:  ``` bash /home# ls drwxr-xr-x 4 iojs   root   4096 Aug 25 13:41 "iojs" drwxr-xr-x 5 iojs   iojs   4096 Aug 25 13:47 iojs ``` 
jbergstroem		@phillipj you should quote the full string, not the variable. That way it doesn't get escaped. 
jbergstroem		Ok, if you've tested that quotes work this is LGTM. Thanks for enduring my nitpick :) Great to have you on board!  
phillipj		My pleasure, now I almost know how ansible works and your nitpicks was perfect for doing things right‚Ñ¢.  I'll squash 14 commits so far, but let you decide when the time is right to merge -- maybe land your big ansible refactoring first for all I know. 
jbergstroem		Squash it and see if anyone else in the build group wants to LGTM, then merge at will! 
mhdawson		LGTM 
rvagg		grr, a persistent bug that I apparently haven't squashed! something to do with the way dist-indexer caches values and how it handles lookup failures, I _think_. I'll remove the cache and run it again but that's not going to solve the bug obviously. 
springmeyer		Thanks, that fixed it! Also noticing it fixed the 1.1.x -> 1.8.x versions which were reporting 42 but should be reporting 43 (now fixed). 
ChALkeR		This is blocked https://github.com/nodejs/build/issues/359, as I understand it. 
maclover7		Going to close this in favor of #359, more discussion there.
rvagg		Sorry, the reason you couldn't log in is probably because we're now having to use user `odroid` to log in rather than `root`, inventory.cfg has now been updated to reflect this.  So in this situation, where you get "Read-only file system", you can reboot and cross your fingers that the file system check that's done during boot to rectify this does a successful job. _Or_, you can do it yourself to make sure since you can still log in.  This is what I've just done:  * Log in (using `odroid`) * `touch foo` to confirm that the file system is in "Read-only" * Run `sudo e2fsck -y /dev/mmcblk0p2` (root device confirmed using `df`)  This is essentially what's done on boot with a dirty filesystem, by running it yourself you can catch any problems that may cause a boot to fail and prompt for what to do on the console‚Äîwhich we can't do and would rather avoid. When you have a file system in read-only then it's safe (mostly) to run this and make fixes.  The machine is running again. I've logged back in and done a `sudo rm -rf /home/iojs/build/workspace` just to start from a fresh workspace cause it may have been compromised during whatever the failure was that happened. It's been brought back [online](https://ci.nodejs.org/computer/test-mininodes-ubuntu1604-arm64_odroid_c2-2/) now to.
joaocgreis		For reference, the ChakraCore changes had been in https://github.com/nodejs/build/pull/547 , I didn't land them because I wanted to keep it more dynamic, but it's been there for a while and there's no harm in landing either.
joaocgreis		@rvagg looking at https://github.com/nodejs/build/pull/547/files , `user.yaml` has two entries: dist and staging directories. Isn't this PR missing the staging one?
rvagg		fixed the problem in user.yml that @joaocgreis pointed out and merged it
joaocgreis		A lock file was left behind by git, probably because of a job that was aborted while git was running (all evidence points to https://ci.nodejs.org/job/node-compile-windows/11399/).  - `test-rackspace-win2012r2-x64-10` was cleaned by @refack (Ref https://github.com/nodejs/build/issues/858) - I just cleaned `test-rackspace-win2012r2-x64-5`   I believe that's all, please reopen if I missed something.
vielmetti		@rvagg - is this still an issue, getting build machines for ARMv8 builds? If so I have some ideas. Let me know the current state of affairs and how things have changed in the past 6 mo.
addaleax		/cc @mhdawson @jbergstroem 
bnoordhuis		I build V8 locally with gcc and it's a bit of a pain.  At times you have to build with `make GYPFLAGS="-Dclang=0"` which then becomes `make GYPFLAGS="-Dclang=0 -Dwerror='' -Dv8_enable_gdbjit=1 ..."` because `make werror=no gbdjit=on ...` doesn't work when you override GYPFLAGS. 
Trott		@addaleax Is this still an issue? Should it remain open?
addaleax		I *think* I remember what caused me to open this issue‚Ä¶ I think it would be nice to have, but it‚Äôs not all that important either. (It would have enabled us to catch a non-trivial bug before we released it, though.)  I‚Äôm closing this as there‚Äôs been no movement, but that shouldn‚Äôt stop anybody from re-opening.
orangemocha		I am going to be on vacation from June 20th to June 30th, so I won't be able to attend this one. Perhaps @mhdawson or @jbergstroem can facilitate?  Note that the meeting is scheduled for June 28th. I posted the issue with more advance notice than usual 
mhdawson		I'm happy to facilitate, @jbergstroem unless you want to do it I'll go ahead and create the hangouts etc. 
jbergstroem		I'm not quite sure on my whereabouts either. Can hopefully attend but I will know next week. 
mhdawson		Added hangouts/youtube links 
jbergstroem		I won't make it. Sorry. I'll try to update issues tomorrow with updates. 
williamkapke		Hey @nodejs/build - I hope everyone got a chance to review #404 for the meeting today. It is the foundation of many tasks I'm passionately wanting to pursue. 
mhdawson		@williamkapke we are pretty small group in the WG meeting this week as quite a few people are away on vacation.  I can see the specific action you'd like is whether the build WG can include the work to manage the bots.   @jbergstroem what's your take on that based on the discussions you've had so far ? I feel like we don't want to just defer until the next meeting with William is waiting on a answer to know how to proceed.  I'm not sure when everybody is back from vacation but maybe we can try to find a time to discuss this specific issue.  I'll try to ping people next week to see if we have enough people back to find a time to set something up. 
mhdawson		PR for meeting minutes: https://github.com/nodejs/build/pull/443 
williamkapke		Thank you for the update @mhdawson. I am a big fan of taking _a little of something_ rather _than all of nothing_. So I'm hoping just this WG can start with: 1) decide on whether the WG feels it is within their domain (or is even interested) in taking on this responsibility 2) a run down of thoughts on the "What are the next steps needed?" section I posted here: https://github.com/nodejs/build/issues/404#issuecomment-225752703  I am, unfortunately, a 1 man effort on the org automation parts and it isn't a flashy project- so it isn't a priority for many. So, I very much appreciate the effort to move this forward.  My hope was to complete some of the org automation tasks and try to schedule some talks later this year to show them off- but I don't think that timeline will workout for that. Instead, I think I may need to try to use the time to collaborate face-to-face with anyone that will help.  I will be at CascadiaFest, Node Summit, Node Interactive EU and US + Collaborator Summits, NodeConf EU... and more if anyone is interested. 
mhdawson		Minutes landed here: https://github.com/nodejs/build/pull/443 
mhdawson		@williamkapke have talked to @jbergstroem and he had a few ideas to pursue before getting back to you, don't think it will be too much longer. 
williamkapke		@mhdawson Thank you for the update! 
jbergstroem		The plan is to get @phillipj on board and have him set it up! 
mhdawson		Ok I think I can close this now.  
rvagg		yes, I think remove the first one though, let the direct go through from http to https and we'll always just link to the https version when we refer to it publicly anyway 
jbergstroem		+1 to https only. Redirects like these will stay in a config forever; the less rules the better 
joaocgreis		Thanks, updated 
orangemocha		LGTM 
joaocgreis		@rvagg what more needs to be done here to update the server config? 
rvagg		Sorry! Someone with access to the web server just needed to drop this in /etc/nginx/sites-available/ and reload nginx. I've done that now, https://nodejs.org/windows-environment is active. 
joaocgreis		Thanks! 
jbergstroem		@joaocgreis (you should have access with the infra key) 
bnoordhuis		That's a really nice gesture, thanks!  My two cents: anything that would speed up the CI would be great because building the project and running the test suite can be pretty agonizing now.  Something that would be able to run the benchmarks reliably after each commit would be pretty awesome as well.  That said, @rvagg should really be the one to comment, he has the most insight into the CI infrastructure. 
rmg		Rackspace has "bare-metal" servers. That could be very useful for benchmarking. 
kenperkins		For context, they're fully [on-demand bare-metal servers](http://www.rackspace.com/cloud/servers/onmetal) with the same CRUD API as all of our cloud servers, available through [`pkgcloud`](https://www.npmjs.org/package/pkgcloud) 
kenperkins		This is not actionable, and I'm now part of the committee (representing Rackspace), so we can deal with it as appropriate moving forward. As such, I'm going to close this issue. 
joaocgreis		The Jenkins Git plugin clears the workspace only after checkout. I remember that was the only choice we had, but now I see before and after checkout options, probably added in some Jenkins update.  Cleaning both before and after should do no harm, we should experiment first and add it to every job.
gibfahn		We could just clean before, is there a reason to clean after checkout?
targos		Sometimes a checkout can leave untracked files behind (not sure exactly what are the conditions)
refack		Just for argument's sake why clean? wholesale cleaning that is. Won't the old artifacts speed up building? Maybe only clean for releases? Maybe do custom cleaning?  > Sometimes a checkout can leave untracked files behind (not sure exactly what are the conditions)  AFAIK if a newly added file does exist in the checkout target branch, git will not delete it, just untrack it.
targos		> Won't the old artifacts speed up building?  We have ccache for that
gibfahn		>Sometimes a checkout can leave untracked files behind (not sure exactly what are the conditions)  I guess if something was not in `.gitignore` before checkout, but is in `.gitignore` after, then it wouldn't be deleted unless you did both, so fair enough.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
orangemocha		Still happening after master is fixed. @rvagg @jbergstroem this is now blocking CI  https://jenkins-iojs.nodesource.com/job/node-test-commit-linux/66/nodes=ubuntu1504-64/console 
rvagg		I believe the 15.04 problem is simply about permissions, Jenkins seems to have been run as root at some point, making the directories and files owned by root, then running as iojs it doesn't have permission to update anything. A systemd config problem perhaps @jbergstroem? I've manually deleted the workspace and restarted using start.sh as iojs and will try again. 
orangemocha		Perhaps we should remove 15.04 from the matrix while this issue gets resolved? 
rvagg		nah, looking good now: https://jenkins-iojs.nodesource.com/job/node-test-commit-linux/68/nodes=ubuntu1504-64/console 
orangemocha		Yay! I think we can proclaim the CI unblocked... 
rvagg		not quite, linter machines are borked and failing 
orangemocha		good thing the linter job is not blocking the rest of the job run. 
orangemocha		@rvagg can I close this issue or do you need to track any follow-ups related to the 15.04 permission issue? 
rvagg		will follow up later with @jbergstroem  
rvagg		I believe this is done now with it deploying to http://new-nodejs.iojs.org/ until we get control of DNS. The build and deploy procedure is identical to iojs.org so copy that or let us know if you want it changed. Assumes gulp and all that. 
snostorm		:+1: thanks, missed this update 
snostorm		I was curious if this is actually wired up? Looking at the build/docker repos it is a little hard to find any reference (I could be missing something.)  I just pushed a test to master of `nodejs/new.nodejs.org` which includes a reference to the same gulp build command and a placeholder `public` directory to test it out. I guess in about 10m I'll know :) 
rvagg		FYI the script is set up to run the same sequence of commands as for iojs.org and publish to http://new-nodejs.iojs.org/ but it looks like the command set has changed?  This is what it runs to generate the static content:  ``` docker pull iojs:latest docker run \   --rm \   -v /home/iojs/new.nodejs.org.github/:/website/ \   -v /home/iojs/.npm:/npm/ \   iojs:latest \   bash -c " \     addgroup iojs --gid 1000 && \     adduser iojs --uid 1000 --gid 1000 --gecos iojs --disabled-password && \     su iojs -c ' \       npm config set loglevel http && \       npm config set cache /npm/ && \       cd /website/ && \       npm install --cache-min 9999 --production && \       node_modules/.bin/gulp build \     ' \   "  rsync -avz --delete --exclude .git /home/iojs/new.nodejs.org.github/public/ /home/iojs/new-nodejs/ ``` 
snostorm		I actually added some dummy placeholder commands before to test out the flow, so the following should be cleanly exiting when the docker flow executes:  ```    npm install --cache-min 9999 --production && \    node_modules/.bin/gulp build ```  Committed to the repo (for testing purposes) is a `public/index.html` so the rsync command should be finding content to copy over.  But we still get the 403 Forbidden.  Can you possibly point me in the right direction for where this Docker code lives? (I'll dig again and update if I find it.) I am also happy to work in the server environment assuming it is reasonable to provide me some access. It could be helpful for me in the future to be able to do some hands on debugging/setup work when we have some of these build-website workflow concerns. Although ensuring process and separation of concerns is also good for other reasons. 
snostorm		@rvagg and @nodejs/build in the coming week or so it would be nice to begin seeing the latest master of the new.nodejs.org site up on the web for easier review. Still blocked by the mysteries listed here.  (If we want to try and align a new site launch around Node v4.0.0 this is a higher priority -- if not, never mind for now -- though in all honesty, I'm not sure if we'll have the site ready by then.) 
mikeal		We all know that isn't the right command for the new website right? It's build.js now :)  Also, how is the forward to /en setup? 
rvagg		http://new-nodejs.iojs.org/  It was a simple permissions thing, sorry!  Do I need to replace `node_modules/.bin/gulp build` with `./build.js` for this? 
snostorm		> We all know that isn't the right command for the new website right? It's build.js now :)  Yes ;)  But that's why I added the dummy commands.  The theory is the docker script should be executing the dummy function right it now, cleanly exits, then proceed with the `rsync` of `./public` which contains a proof-of-concept `index.html` which should publish.  If that works I'll quickly rewire the dummy gulp command to execute our `build.js` script. 
snostorm		> Do I need to replace node_modules/.bin/gulp build with ./build.js for this?  Yes/no. Ideally, yes. Right now that will build to a different target than `public`. Let me check. 
snostorm		> Also, how is the forward to /en setup?  In the iojs.org site we use a simple static HTML file to trigger a redirect to `/en` but we should probably make that an Apache/Ngnix driven alias for SEO, etc.  ``` <html><head><title>IU Webmaster redirect</title><META http-equiv=refresh content="0;URL=en/index.html"> ... ```  > Yes/no. Ideally, yes. Right now that will build to a different target than public. Let me check.  I'll hijack the gulp script for now. It adds an extra weird step but keeps the two projects in sync process wise. I'll fix it up tonight to publish a copy to `./public` versus `./build`. 
Trott		This _seems_ like it should be closed. Please re-open or comment if I'm mistaken. 
jbergstroem		LGTM 
orangemocha		I am working on the second item: "Ensure test CI covers the converged repo/branches". 
gibfahn		That's odd, the release team still have access according to the permissions, and you're definitely [still in the release team](https://github.com/orgs/nodejs/teams/release). @nodejs/release is anyone else having this issue?  ![image](https://cloud.githubusercontent.com/assets/15943089/25923479/a8029d8c-35d5-11e7-94e6-905e0449315d.png) 
gibfahn		The Github Auth plugin is currently on `0.26`, and there's a `0.27` available, no idea if that's relevant to anything.  ![image](https://cloud.githubusercontent.com/assets/15943089/25923593/20984986-35d6-11e7-8ebc-1f058194de57.png) 
jbergstroem		@jasnell has spesjul permissions on `ci` at least (not part of a group), so this might be a github acl thing?
cjihrig		>  is anyone else having this issue?  The release Jenkins seems to be working for me. It looks like @jasnell was able to get in and start a job.
rvagg		@jasnell not a solution per se but perhaps try navigating straight to https://ci-release.nodejs.org/job/iojs+release/ and see if you get anywhere, that shouldn't require "Overall" I think
maclover7		ping @jasnell -- can this be closed?
jbergstroem		Syntax btw (for above rsync script): ```console # tail -n +1 /etc/systemd/system/rsyncmirror.* ==> /etc/systemd/system/rsyncmirror.service <== [Unit] Description=runs a rsync mirror against nodejs main server  [Service] Type=oneshot ExecStart=/usr/bin/rsync -e "ssh" -a -q --exclude "doc" --exclude "docs" --exclude "metrics" --exclude "tools" --exclude ".*" --exclude "rsync-old-nodejs.org.sh" --exclude "www" --delete root@host:/home/dist/ /home/dist/  ==> /etc/systemd/system/rsyncmirror.timer <== [Unit] Description=run the rsync mirror job every 15 minutes  [Timer] OnCalendar=*:0/15 ```
gibfahn		Makes sense to me if it means we can reuse scripts. 
jbergstroem		> @gibfahn said: > Makes sense to me if it means we can reuse scripts.  I guess the same applies for a checked in cron job (but os-specific things could be different).  I just really liked the tools to control timed scripts; getting full log history and confirmation that it ran successfully. It takes a bit more polish to get cron jobs to that shape.
joaocgreis		Syntax as listed by @jbergstroem above does not make the timer active on startup, so a `reboot` disables the timer. An `[Install]` section is required, and the timer must be explicitly enabled with `systemctl enable`.   Updated syntax for reference: ```ini # tail -n +1 /etc/systemd/system/rsyncmirror.* ==> /etc/systemd/system/rsyncmirror.service <== [Unit] Description=runs a rsync mirror against nodejs main server  [Service] Type=oneshot ExecStart=/usr/bin/rsync -e "ssh" -a -q --exclude "www" --exclude "doc" --exclude "metrics" --exclude "tools" --exclude ".*" --exclude "rsync-old-nodejs.org.sh" --delete root@direct.nodejs.org:/home/dist/ /home/dist/ ExecStartPost=/usr/bin/rsync -e "ssh" -a -q --delete root@direct.nodejs.org:/home/www/ /home/www/  ==> /etc/systemd/system/rsyncmirror.timer <== [Unit] Description=run the rsync mirror job every 15 minutes  [Timer] OnBootSec=1min OnUnitActiveSec=15min  [Install] WantedBy=timers.target ```  Then to enable the timer: ```console # systemctl enable rsyncmirror.timer Created symlink from /etc/systemd/system/timers.target.wants/rsyncmirror.timer to /etc/systemd/system/rsyncmirror.timer. ```
gibfahn		Is the plan to make everyone in `diagnostics` a member of `diagnostics-admins`, or only a subset? If the latter, you should probably list the initial members here.  LGTM anyway.
mhdawson		I'm not sure who would need to be part of the diagnostics-admin, need feedback from them.  Once I add one the team can then self-managed as per the process.  I was not planning on adding add, just a subset.
mhdawson		Starting to rough in job here: https://ci.nodejs.org/view/x%20-%20Diagnostics/job/node-inspect-continuous-integration/
gibfahn		@mhdawson see https://github.com/nodejs/node-report/issues/18#issuecomment-280234054, I've re-evaluated, and I think the hassle citgm adds isn't worth the benefit for single module testing. I'd like to make it simpler, but for now I think it's probably better to just not use it at all.  _**EDIT:**_ This isn't a dig at citgm, the complexity it adds is vital for testing multiple modules.  Also for these jobs (`node-inspect` and `node-report`) I think we need a way to trigger builds on `v4`, `v6`, `v7`, and master with one job (which will hopefully soon be triggered on new PRs automatically). I think the easiest way might be a flow which triggers one job 4 times (with different node branch parameters), but I'm not sure.  It'd be helpful to have access to the Jenkins jobs to see how things are implemented. However we do it, we should try to be consistent across all the node modules we `npm test` (at least `citgm`, `node-report`, `node-inspect`).
mhdawson		If you have time to work on your suggestions for the jobs, since you are part of the build WG I'm happy to give you access to one or more.  Do you want to start with node-report ?   
mhdawson		Should be no surprises since they are a clone of the CITGM job you worked on, and they could use more optimization improvement.
gibfahn		>Do you want to start with node-report ?  SGTM
rvagg		Sounds OK but please keep `diagnostics-admins` access fairly exclusive as it gives you pretty broad control over our test infra and the ability to get away with nefarious activity while mostly covering your tracks. Also make sure they can't edit any of the other jobs, only diagnostics ones! I assume the list is pretty small here anyway? Maybe only a couple of people?
gibfahn		>Sounds OK but please keep diagnostics-admins access fairly exclusive as it gives you pretty broad control over our test infra and the ability to get away with nefarious activity while mostly covering your tracks.  Has anyone looked at using the new `Jenkinsfile` options (equivalent of `.travis.yml` files) in Jenkins 2 to see whether we could put our configurations in there? If we could then it wouldn't be necessary for people to have access to the CI, they could just submit PRs to update the `Jenkinsfile` configs.
mhdawson		@rvagg configuration is in line with: https://github.com/nodejs/build/blob/master/doc/process/jenkins_job_configuation_access.md.  It limits config to specific jobs for the WG/team to the corresponding "admins" team.   In terms of who gets added to the "admins" team I start with a small set but its delegated as per that process to the team itself.
mhdawson		@gibfahn you should now be able to view/configure https://ci.nodejs.org/view/post-mortem/job/nodereport-continuous-integration/configure.  
Trott		`node-inspect` + tests are now in Node.js core. Closing. Feel free to re-open if there's something I'm missing. Thanks!
gibfahn		+1 to retiring fedora 22 if it's been EOL for 6 months. Does that mean it won't be supported in 8 (so it will be supported until v6 EOL)?
jbergstroem		Since this is still floating I'm not sure how we'll handle 'official' support: https://github.com/nodejs/node/pull/8922
jbergstroem		Also, I don't think we are specific in terms of os'es we support or don't other than what we build against. 
jbergstroem		I have two vm's on rackspace running; worked fine with the current refactor. Attaching them to jenkins tomorrow!
jbergstroem		build just passed: https://ci.nodejs.org/job/node-test-commit-linux/nodes=fedora25-x64/6938/  Does anyone object against retiring 22? No document to update in main repo, so to speak..
jbergstroem		> @jbergstroe said: > Does anyone object against retiring 22? No document to update in main repo, so to speak..  Actually, let me check with libuv first..
jbergstroem		ping @saghul, @bnoordhuis, @indutny, ?
saghul		No objections from me!
bnoordhuis		Likewise.
jbergstroem		Cool. I'll go ahead and retire fedora 22 shortly then.
maclover7		22 has been removed from ci.nodejs.org as far as I can see, 25 has been added, so I think this is done now. Going to give this a close, please reopen if necessary.
jbergstroem		üôä
MylesBorins		fixed
gdams		I would change to  "The test process must be executable with only the commands `git clone github.com/$url && npm install && npm test`" because as I understand it you are only planning to support vanilla github clones (e.g no submodules) 
MylesBorins		@GeorgeAdams95 this is not the case as we get a tarball from github, we do not git clone. 
gdams		okay then: "The test process must be executable with only the commands `npm install https://github.com/x/y/release.tar.gz && npm test`" 
MylesBorins		changed the copy to `The test process must be executable with only the commands npm install && npm test using the extracted tarball mentioned above` 
mhdawson		@TheAlphaNerd and I discussed and the current version has a +1 from me. 
gibfahn		This LGTM. I'd have thought it should live in the CitGM repo, but no strong opinions.  *EDIT:* Second sentence was in answer to:  >  and then figure out where they should live.
MylesBorins		@gibfahn the policy could live there. I think we just need to document it
mhdawson		I guess the question is what more do we need to close on this ?  
gibfahn		Maybe a couple more reviews? @nodejs/citgm 
mhdawson		@nodejs/citgm @nodejs/build please comment.  Myles do you want to put together a PR for this ?  I'm also happy to do it.  I figure it should go into the CITGM readme. 
gdams		LGTM, and yeah +1 for this living in the citgm repo
gibfahn		CITGM PR: https://github.com/nodejs/citgm/pull/310 
gibfahn		CitGM PR has landed
joaocgreis		@mscdex that is a known issue. https://github.com/nodejs/node/pull/7798 has landed today, if there are no further issues then tonight the files will be back. 
mscdex		Ah ok, thanks. 
joaocgreis		They're back: https://nodejs.org/download/nightly/v7.0.0-nightly2016080329e49fc286/ 
MylesBorins		+1 
Starefossen		+1 üëè  
Trott		Thanks. This would be great. 
phillipj		+1 
mhdawson		+1 
jbergstroem		The build WG decided to vote @Trott into our (not so) secret society! Glad to have you onboard. 
jbergstroem		We're now just using joyent deploys for linting instead. 
jbergstroem		(because we can't run network tests) 
jbergstroem		Closing since its irrelevant, but we should document this somewhere somehow at some time. 
MylesBorins		Fixed
mhdawson		@nodejs/build 
MylesBorins		LGTM 
joaocgreis		Some problem with my name in the 3 places it is used (encoding?)  Everything else LGTM  
mhdawson		Ok cut/paste Joao's name from previous minutes and landing 
mhdawson		Landed as aaead264814230900e443f6ac11fd3d95e9bbac3 
mikeal		If someone could give me access to the raw logs I could write a script to process the unique IP data. 
Fishrock123		See https://nodejs.org/metrics/#countries?  Anything beyond that gets a bit more tricky. It becomes a bit too easy to potentially pinpoint specific users. 
Fishrock123		i.e. per region in a country was not considered acceptable due to the above at the time.  Edit: That is, to be posted publicly. 
mikeal		Right, I don't want the actual IP data, I want the number of unique IPs in a given time range. daily, weekly, and monthly is probably ideal. 
Fishrock123		cc @rvagg 
rvagg		@mikeal since this is all stripped fairly early in the processing chain it's going to have to be a special job to generate them separately, and we can't allow the logs to be public either obviously.  If you're interested, the stripping happens here: https://github.com/nodejs/build/blob/master/setup/www/tools/metrics/download-counts.sh#L36-L37  And unfortunately, collating this across ~1 file per day (not exact day boundaries and not guaranteed to be exactly 1 per day) is very awkward so this is not a simple task. It is an interesting one, however, so I've got it on my list now to tinker with some time. You'll have to be patient!  _(sorry for the delay, I remember replying to this issue when it first came up but I don't see a reply here so I obviously screwed that up!)_ 
maclover7		Seems like this is something that would require working around how the system is currently setup, going to close for now -- please feel free to reopen if this is something we still want to do.
mhdawson		LGTM 
gibfahn		LGTM, I can put his key on the machines.
gibfahn		Added a new label `access request`, as we seem to get a lot of these
gibfahn		Added @targos's key to the `freebsd` user on `test-digitalocean-freebsd11-x64-1` (`45.55.90.237`).
fhinkel		@targos Can you find out what compiler version we use there?
gibfahn		I think it's:  ``` freebsd@test-digitalocean-freebsd11-x64-1:~ $ cc --version FreeBSD clang version 3.8.0 (tags/RELEASE_380/final 262564) (based on LLVM 3.8.0) Target: x86_64-unknown-freebsd11.0 Thread model: posix InstalledDir: /usr/bin ```
fhinkel		Thanks!
targos		The issue has been fixed. You can revoke my access. Thanks!
gibfahn		Key removed.
ChALkeR		Wait, what? Do I get this right that we got CSRF protection disabled? 
jbergstroem		The setting got introduced somewhere in the 1.x series and defaulted to off. I'm currently in transit but will be back tonight; will likely test and implement it then. 
jbergstroem		This is now enabled on both deploys. It might end up being problematic for rest access (upstream says). Ping @Starefossen, @nodejs/build. If you find any issues, let me know. 
jbergstroem		Closing since it's resolved, but post updates here should issues arise. 
evanlucas		Just an FYI, my cli tool to fetch/submit jenkins jobs for our CI (https://github.com/evanlucas/nodejs-ci-ctl) is still working properly even with this enabled. 
ChALkeR		@jbergstroem   > The setting got introduced somewhere in the 1.x series and defaulted to off.  Sigh. Jenkins‚Ä¶  Good that it's enabled now, thanks! 
ChALkeR		@evanlucas What API does it use? 
evanlucas		Just the Jenkins REST API 
gibfahn		>can we merge all the TAP reports and put upstream (similar to citgm-smoker)?  https://github.com/nodejs/build/issues/802  citgm-smoker uses Junit not TAP.
MylesBorins		@jbergstroem did some work on getting transforming the node test suite to output Junit ... the junit plugin for Jenkins is waaaaay better 
jbergstroem		The compiler packages dependencies looks specific; is that necessary? 
mhdawson		The level of the compiler which you get on Fedora by default is not recent enough so I needed to specify at least a 4.8 version.  If you know there is a better way to specify at least a 4.8 version which will still match that dependency I can give it a try. 
mhdawson		@jbergstroem any suggestions ?  
jbergstroem		@mhdawson from the looks of it, would referencing `gcc.ppc64` from the updates repo work?  ``` bash $ yum info gcc.ppc64 Installed Packages Name        : gcc Arch        : ppc64 Version     : 4.8.3 Release     : 7.fc20 Size        : 36 M Repo        : installed From repo   : updates ``` 
mhdawson		I'm heading off for vacation but I'll try it out when I get back. 
mhdawson		@jbergstroem   Finally go back to looking at this.  So I think using gcc.ppc64 would work. I did a yum install on another machine and it installed the right compiler.    However, I think that would then make the config ppc specific.  I was thinking I might rename fedora20 to fedor20ppc but since I don't see any others like that I thought I'd ask if we have dealt with this issue before. 
jbergstroem		Perhaps override the gcc package for fedora somehow. I can have a look (or someone with more ansible experience could suggest how!). 
mhdawson		I had figured out how to conditionally install gcc.ppc64 and g++.ppc64 (see second commit) however, after seeing the ansible output and additional testing it seems we don't need that as 4.8 seems to get installed on fedora20 by default when just gcc and g++ are specified so I removed that part and the result is generic.  I think we should be good to go  
jbergstroem		LGTM. Great that we could avoid ppc-specific gcc stuff. 
mhdawson		Agreed  Landed as https://github.com/nodejs/build/commit/55a2a398f7d635b56a0c532857eedb8e9dd9a224 
maclover7		ping -- has this been done?
maclover7		Ref #868, looks like no Ansible playbook yet for benchmark machines. I am more than happy to help mentor and get this done if that helps at all, I've been doing a lot recently with the Ansible scripts :)
mhdawson		@maclover7 it would be great if you could help get the ansible scripts created.  In the end I manually added wrk so closing this issue.
gibfahn		So this is related to https://github.com/libuv/libuv/pull/1412#issuecomment-314837122 right?  If you look at the [libuv job](https://ci.nodejs.org/job/libuv-test-commit-linux/365/), you'll see that we have a large number of platforms. Are you asking whether it's installed on all of them?  ![image](https://user-images.githubusercontent.com/15943089/28153452-20c93082-67ae-11e7-8afb-bb59e9d009e2.png)  You can see the ansible scripts we use to set up machines [here](https://github.com/nodejs/build/tree/master/ansible), you may be able to tell from that. Also if you give me a command to run I can test it on the machines.
maclover7		Can this be closed?
gibfahn		I think so, @CurlyMoo let us know if you disagree.
CurlyMoo		If these modules won't become available then this topic isn't any use indeed.
gibfahn		>If these modules won't become available then this topic isn't any use indeed.  @CurlyMoo do you still have a use for these modules? If so then we should reopen. Assuming they're installable via the standard package managers then there's no reason we shouldn't include them in our Ansible config.
CurlyMoo		I want to use it to unittest some networking code.
bnoordhuis		You need root privileges (CAP_NET_ADMIN, technically speaking) to bring up a TAP interface so I don't know if this is going to work.
CurlyMoo		In order to properly test networking code, a dummy networking device would be great. So one option would be to enable one by default at boot time.
maclover7		Going to close this for now, since there doesn't seem to be anything immediately actionable -- please reopen if I'm wrong
jbergstroem		SGTM if it scratches an itch!
refack		Ping? I would use it quite often. If it could be set to just re-populate the form and leave the "Submit" for manual clicking that would be great for `node-stress-single-test`. But that's a "nice-to-have".
gibfahn		>If it could be set to just re-populate the form and leave the "Submit" for manual clicking that would be great for node-stress-single-test. But that's a "nice-to-have".  Not quite sure what you mean, but when you press rebuild it pre-populates the parameters with whatever you set when you initially ran the build. That's kinda the whole point üòú 
refack		> If it could be set to just re-populate the form and leave the "Submit" for manual clicking that would be great for node-stress-single-test. But that's a "nice-to-have". > Not quite sure what you mean, but when you press rebuild it pre-populates the parameters with whatever you set when you initially ran the build. That's kinda the whole point üòú  I mean fill the form but not submit it. e.g. for `node-stress-single-test` you could start a job on one RUN_LABEL, then hit rebuild, change RUN_LABEL, and submit a second job... But anyway, just activate it for the gen-pop.
gibfahn		>start a job on one RUN_LABEL, then hit rebuild, change RUN_LABEL, and submit a second job...  Yeah, that's how it works
gdams		SGTM
gibfahn		![image](https://user-images.githubusercontent.com/15943089/28408024-c87cdf9e-6d68-11e7-9696-717f4f5a6e12.png)  So this plugin is already installed, but just not enabled.  >[Rebuilder](http://wiki.jenkins-ci.org/display/JENKINS/Rebuild+Plugin) This plugin is for rebuilding a job using the same parameters. 1.25  I'm not entirely sure why it's called `Rebuilder` instead of `Rebuild Plugin`, but the link is the same.  I've enabled it, if anyone knows why it was disabled (and thinks it should continue to be disabled) please let me know.   
gibfahn		It won't actually be enabled until Jenkins is next restarted. I'm not sure how we normally do that, am I okay to just click the `Restart Jenkins once no jobs are running` button?
refack		I found this: ![image](https://user-images.githubusercontent.com/96947/28494216-b1c18e1a-6ef5-11e7-9d33-06701945afa8.png) Seems like it reruns the build with the same parameters, but "auto-submits"
gibfahn		Restarted and it's there, see:  # Rebuild  Rebuilds that job (like hitting `Build with Parameters` and then pre-filling all the parameters with the same values as that build.  ![image](https://user-images.githubusercontent.com/15943089/28496560-00f8c52e-6fa1-11e7-89bd-4b788195bf9c.png)  # Rebuild last  Like clicking on the last build and clicking `Rebuild`.  ![image](https://user-images.githubusercontent.com/15943089/28496537-4d7e636e-6fa0-11e7-979a-3c45d605b07c.png)  Note that you get the chance to change parameters before you run.  cc/ @nodejs/collaborators , I found this saves a lot of time when running jobs.
refack		Regarding restart, I just found its avalible to unauthenticated users ![image](https://user-images.githubusercontent.com/96947/28500906-0538da86-6f9f-11e7-89e6-aeba71291e5d.png) Is that a security issue?
Trott		@refack üò± 
bnoordhuis		https://ci.nodejs.org/job/node-test-pull-request/9310/ - "Started by anonymous user"  Although I'm not sure if it actually does anything.  The sub-jobs are done near instantaneously but with reported running times in the tens of minutes, except for an ARM job which seems stuck on something.
gibfahn		I think this is the (fixed in next release) bug report: https://issues.jenkins-ci.org/browse/JENKINS-36333  I don't think this has anything to do with the Rebuild Plugin, the `Resume Build` button was I think there before. I could be wrong though.  _**EDIT:**_ Looking at this [feature request](https://issues.jenkins-ci.org/browse/JENKINS-30505), which I think is the one that added `Resume Build`, it looks like all it does is rebuild the sub-jobs that failed, so it's not a huge issue.  Unfortunately we won't be able to update to the newer version of Jenkins until all our machines use Java 8.
rvagg		btw I installed this really early on but it was removed because of this security problem, somehow we've lost this institutional knowledge and I'm not really sure how we retain it across people changes. Interesting data point nonetheless. Sorry I missed this issue and the fact that it was back again!
refack		IMHO the only issue with this is DOS, and that might be mitigated by that you can only `Resume` each job once.
gibfahn		>btw I installed this really early on but it was removed because of this security problem  @rvagg which security problem, am I wrong in saying:  >I don't think this has anything to do with the Rebuild Plugin, the `Resume Build` button was I think there before. >https://issues.jenkins-ci.org/browse/JENKINS-36333  ?
rvagg		probably needs readme .. 
cjihrig		Thoughts on using the same coding style as node core? 
rvagg		running on iojs-www in /etc/crontab:  ``` 0 18    * * *   dist    node /home/dist/nightly-builder/ --type nightly --token redacted 0 19    * * *   dist    node /home/dist/nightly-builder/ --type next-nightly --token redacted ```  this server is running as UTC-4 (unfortunately) 
rvagg		> Thoughts on using the same coding style as node core?  ![1cda5be0f1bc1ac709847305a1565c72__tumblr_inline_n0510cwkrd1r3yh7o](https://cloud.githubusercontent.com/assets/495647/7101502/f010845a-e09f-11e4-94d6-4eacc26b0e04.gif) 
cjihrig		Sorry :fearful:  
Fishrock123		Seems fine. Some style meh, but no big deal I suppose. Mostly just leftover nits. 
rvagg		@maclover7 were we able to restrict this to just the jenkins-workspace hosts? Otherwise it's going to be annoying whenever we add and remove IP addresses from the list. This happens every other week.  The IP addresses are here: https://github.com/nodejs/build/blob/master/ansible/inventory.yml, see the two jenkins-workspace hosts near the bottom, hopefully we can restrict it to just those. Otherwise you're going to have to add everything under the `test` section except for the `requireio` hosts which will come from the single ip address that resolves from `vagg-arm.nodejs.org`.
maclover7		> were we able to restrict this to just the jenkins-workspace hosts?  Yeah, the only machines running `post-build-status-update` and working with the bot should be jenkins-workspace machines as defined [here](https://github.com/nodejs/build/blob/master/jenkins/pipelines/post-build-status-update.jenkinsfile#L10).
phillipj		Thanks! Opened https://github.com/nodejs/build/pull/985 to provide those two current IPs to the bot.
rvagg		Did a üëé cause I'm not going to get enough notice whether it goes ahead or not and I'm not a fan of needless 6am wakeups!
joaocgreis		cc @nodejs/build 
jbergstroem		Can't join either. Sorry!
joaocgreis		The only issue with WG agenda is https://github.com/nodejs/build/issues/711 , I believe we can make progress through github if needed. I suggest we skip this week and schedule for May 30, 3 weeks from today.
jbergstroem		A summary of the summit would be nice to hear!
piccoloaiutante		I think that @mhdawson could have some notes about it.
mhdawson		I'll be writing up a short summary of the summit later today and will share in some form.
piccoloaiutante		@mhdawson let us know when your short summary is ready. I'll close this issue in the meantime.
gibfahn		Are you talking about the [`Aggregated Test Result`](https://ci.nodejs.org/job/node-test-commit/lastCompletedBuild/aggregatedTestReport/)? If so then yeah, those results disappear if one of the options is disabled on one of the subjobs.   Output is:  >Fingerprinting not enabled on this build. Test aggregation requires fingerprinting.
gibfahn		I'd suggest we need to be enabling fingerprinting like so:  ![image](https://user-images.githubusercontent.com/15943089/28343580-a5980ef4-6c50-11e7-83ea-a2f97dae2f3d.png) 
gibfahn		On a related note, is there a reason we're using the `Jenkins Text Finder` plugin? The TAP plugin parses the results and marks the build as `UNSTABLE` if tests fail anyway. 
rvagg		no objection from me if you want to give that a go @gibfahn, are you in jenkins-admins? re Jenkins Text Finder, I don't know who or why it's in there unfortunately.
gibfahn		>are you in jenkins-admins  Yes. I'll have a look.
mhdawson		The aggregate list sounds really useful :)  I'll look forward to trying it out.
gibfahn		I'll test it out in readable-stream-continuous-integration https://github.com/nodejs/build/issues/782 first.
MylesBorins		any traction on this? Having to dig into raw tap is a pretty bad ux
gibfahn		Enabling fingerprinting doesn't seem to actually work, at least not reliably. This irritates a lot of people, it's just waiting for someone to work out what the magic Jenkins formula is.  Alternatively we could just use `tap2junit`
MylesBorins		Do we have any insight into improving this? I noticed it was randomly working on a machine today
rvagg		This one? https://nodejs.org/metrics/summaries/version.png  ![screenshot from 2016-11-19 11-14-20](https://cloud.githubusercontent.com/assets/495647/20451035/5627f502-ae49-11e6-829f-8c0cbe12c7f3.png)  Just need to zoom in I guess. Unfortunately the green is the same as 0.12!  If someone wants to optimise the list of colours then head over to https://github.com/nodejs/build/blob/master/setup/www/tools/metrics/plot.gp#L12-L19 and put in a PR. I'm far from a colour expert and just grabbed someone else's list that I thought looked good on a graph.  To test your work, grab that file (plot.gp) then run this:  ``` $ for t in country version arch os total; do curl -sLO https://nodejs.org/metrics/summaries/${t}.csv; done $ gnuplot -e "inputdir='./'; outputdir='./'" plot.gp  ```  and you'll end up with .png files for each of them. Help from @nodejs/website experts would be neat. 
mweagle		Handy tool for color selection: https://color.adobe.com/ 
maclover7		Is this okay to close?
maclover7		It seems you like you can see the downloads okay, the colors are just a little messy. Going to close for now, but if someone wants to update the color to something different please reopen.
gibfahn		cc/ @mhdawson 
mhdawson		Is there any context as to why it was needed and is no longer needed ? 
refack		> Is there any context as to why it was needed and is no longer needed ?  Sometime in the past V8 needed it for some testing tools. They have moved all their tooling to git. The main issue is that the script installs `httpd` which is an optional feature even if `svn` was required, and is an unnecessary "wide load". https://github.com/nodejs/build/blob/18cdefb192a91b339873d239d17cb1e6967b03c0/ansible/README.md#L160
refack		I run this command on both machines: ```bash rpm -vv -e subversion httpd serf apr apr-util apr-util-ldap openldap openldap-devel neon file-libs pcre file krb5-libs ```
rvagg		Just added config for http://dist.libuv.org/ which we are now hosting off the main web server, it has a user account `libuv` that core members of that team have access to and a `dist/` directory that is exposed via nginx. We're also getting a wildcard ssl cert for *.libuv.org and I'll update the config and secrets repo when we have that in place. 
jbergstroem		I can't see the relevant nginx config for libuv/dist stuff. Is that coming once ssl certs are in place? 
gibfahn		Will land if there are no objections over the next couple of days.
rvagg		only 1 failure from release-related bugs, that makefile thing  I'm trying to think through how this could work and I just see many yaks to shave, perhaps you, or someone else could suggest a workflow that would make this doable and useful 
Fishrock123		Makefile and vcbuild.bat would have been easier to narrow down like this.  I'm really just thinking of things that run the release procedures but throw out the builds. 
jbergstroem		Does our release jobs cover this or are we missing something? (ping @rvagg) 
rvagg		no, it creates release builds but they are not tests, the OP is about doing full release builds and testing that they install/unpack & work and I don't know how we can achieve that everywhere without making a huge mess. 
maclover7		ping -- is this still needed?
gibfahn		>ping -- is this still needed?  I think this is still needed, but it'd require quite a lot of changes.
mhdawson		We now test the downloads nightly to validate the packages are ok, the next step might be to do extend this to allow us to run the test suite as well. 
gibfahn		Landed in https://github.com/nodejs/build/commit/388499d15ff2666a2cc7cb820ac58511e4a96a42
MylesBorins		Not a fan of gitter myself.  For clarification, I assume this is working group discussion?   Unless all node wg's decide to go under a single slack, which may not be a terrible idea, I personally think IRC is the best option. 
mikeal		So, there's no project or foundation-wide policy around this. However, one of the requirements of all projects and WGs is that they are "transparent, participatory, and effective."  To that end systems which introduce significant barriers to entry might be flagged as anti-participatory or anti-transparent. However, you can balance those issues if the tool is effective and you can ensure transparency and participatoryness (not a word) in other ways.  A good example are the meetings many WGs and TCs have. They are not as transparent or participatory as GitHub but are used in such a way as decisions are not made on them without first being adequately serviced on GitHub issues. They then function as "final call" for dissent so that contributions are not held up indefinitely (so that the project remains _effective_). 
mhdawson		I suffer from the problem that I don't have irc open enough either because  updates cause my machine to reboot or the browser crashes or whatever.  Slack (and I think gitter as well) on the other hand sends me an email with a summary of what I've missed and in particular when my name is called out.  I  find that really helps me not being too late with a reply or missing the question completely.  So my first take would be to vote against irc.  Having said that I'm always ready to learn something new if that resolves the problems.   In terms of irccloud I see this "Stay connected for 2 hours" in the free version I'm not sure it would achieve "always on" 
jbergstroem		@mhdawson didn't know irccloud changed their pricing structure which would then imply that we're pushing the burden of payment to a user. that's not good. [ircanywhere](http://ircanywhere.com) is an alternative. Not sure either would solve the 'ping me on email if i'm missing out' though. 
mikeal		wow, @mhdawson just wrote up exactly my current workflow around all these tools I can't seem to use :) 
bnoordhuis		I feel that as an open source project we should prefer open source alternatives over closed source ones, if for no other reason than that it's good to stay neutral and to have control over our data.  IRC has the additional benefit that you can be fairly anonymous.  That's important to some people.  As to logs, I can add the channel to slurp, our IRC bot. 
jbergstroem		@bnoordhuis see [pr](https://github.com/piscisaureus/slurp/pull/5).   @mikeal, @mhdawson created this: https://github.com/ircanywhere/ircanywhere/issues/276 
orangemocha		What issues did you experience with Gitter? Apparently I have been lucky, but it has always worked seamlessly for me so I have to say I am pretty happy with it. It is also open and free and uses GitHub for authentication, which is nice.  IRC without always-on would be a step back IMO. It would be great if we can figure out a way to make it work for everybody, without requiring people to jump through too many hoops. But if we had to pay for it/irccloud, would it win in a side-by-side comparison with Slack? 
rvagg		I'm a very happy IRCCloud user, I can strongly recommend it ($5 per month). I'm always on. Always have access to logs of discussions that happen when I'm not at the computer, it doesn't take up as many resources as my gitter tab does, and it's snappy (particularly compared to gitter). And the mobile client is good too.  
DavidTPate		An alternative to Slack might be [Rocket.Chat](https://rocket.chat/) it's an open-source alternative. They support things like GitHub authentication which I know the whole authentication scheme for slack is less than ideal for an Open Source collaborative project. I haven't used it much, but tried it out a few times and it seems pretty cool. Would need to research the history retention and such. 
jbergstroem		Another benefit for using IRC is that a lot of nodejs working group members already hang out in node.js or io.js, giving us a nice overlap if they have build-related questions (which as been more frequent as of late) 
mhdawson		Is there an irc channel in use ? I tried node_build today and looked like nobody else was connected 
orangemocha		@mhdawson it's `node-build` 
Starefossen		Just for reference (if anyone happens to stumble upon this) I think most members of the Build WG have transitioned over to the `#node-build` IRC channel on Freenode. 
jbergstroem		Closing this. As mentioned above, feel free to join us at `#node-build` on irc.freenode.net! 
orangemocha		For things like our jobs that test and land pull requests, it's certainly useless. Computing the delta in failures between different PRs, and across multiple branches doesn't add a lot of value. The failure delta might make more sense in more traditional type of CI jobs that run periodically on the same branch. In that category we currently have only node-daily-master. But if the side effect is that a job needs to block on other runs just to report the results, I am definitely in favor of removing that feature. 
orangemocha		It looks like we are going to have to modify the TAP plugin: https://issues.jenkins-ci.org/browse/JENKINS-29650 
orangemocha		Also relevant: https://issues.jenkins-ci.org/browse/JENKINS-9913 
jbergstroem		@orangemocha how do you suggest we proceed here? Doesn't look like upstream jenkins/tap plugin is moving forward anytime soon. 
orangemocha		- [ ] Read https://issues.jenkins-ci.org/browse/JENKINS-9913 well to see if there is a better solution than the following steps - [ ] Learn how to build Jenkins plugins - [ ] Build the Jenkins [TAP plugin](https://github.com/jenkinsci/tap-plugin), give it a different ID so that our build can be used alternatively to the existing one - [ ] Try uploading our built plugin at https://jenkins-iojs.nodesource.com/pluginManager/advanced (Upload Plugin). - [ ] Test a our plugin with a clone of of one of the test jobs, e.g. node-test-commit-unix - [ ] Fix checkpoint issue by modifying plug-in code. Hopefully something like https://github.com/jenkinsci/slack-plugin/commit/832e7b50028ca086dd49311ce454e7a19c9a3af9   Test again in our Jenkins - [ ] Provided it works, update all our node-test-commit-\* jobs to use the new plugin instead of the TAP plugin, making sure all configuration for it is the same - [ ] Open a pull request to https://github.com/jenkinsci/tap-plugin with the fix - [ ] When the fix gets accepted and released, revert to the official plug-in 
Trott		Closing as this has been inactive for over a year and I believe we intend to move off the TAP plugin. Feel free to re-open or comment if I am mistaken to close this. 
gibfahn		Moving to next week sounds good.  üëé to indicate that I wouldn't be able to make the meeting, not that I disapprove of the issue btw.
joaocgreis		Moving or skipping both sound good to me.  @nodejs/build 
refack		I'm home, so I'm ok with whatever...
rvagg		OK, I guess it's off then! That's fine. I'm available next week if we want to shift but if it's not in the calendar I may not remember!
gdams		I'm good with either week
gibfahn		@mhdawson can you update the calendar entry? Once that's done we can close this in favour of https://github.com/nodejs/build/issues/902.
mhdawson		Closing as meeting was held.
mhdawson		@nodejs/build any comments ?  
rvagg		Not yet, will try and take a look soon @mhdawson, just a bit snowed under atm. Also, fwiw, your PR subject, description and the commit details all say that it's a "strawman for discussion" but not about what, I had to go to the actual changes to figure out what this was even about. Basic details that appear in GH notifications would be nice to help with prioritisation. 
orangemocha		Left a few minor comments. It also needs a reworded commit message. LGTM once those things are addressed. 
mhdawson		Updated to address @orangemocha's commends 
orangemocha		LGTM 
joaocgreis		LGTM 
jbergstroem		Lots of nits but mostly LGTM! 
mhdawson		added commit to address @jbergstroem comments, incorporate all except for limiting use cases to collaborators only as I think we've left the door open to non-collaborators in some of the cases covered (with more precautions such as re-imaging etc.) 
mhdawson		@jbergstroem let me know if you have any other comments otherwise I'll plan to land early next week. 
jbergstroem		@mhdawson nah looks good lets get it in. 
mhdawson		Landed as 0936e417b7aac392e8b7ba1c39e81a20532b9df3 
refack		Cross-ref https://github.com/nodejs/nodejs.org/issues/1382 It seems like the ARM6 binaries were misnamed so they probably overwrote the ARM7 ones. 
joaocgreis		This looks like a duplicate of https://github.com/nodejs/build/issues/829 .  This is not related to https://github.com/nodejs/nodejs.org/issues/1382, that issue is because there were no ARMv6 machines available to build the release.
rvagg		Yeah, still on my plate, had to compile a custom gcc for our armv7 machines to get binaries on them. They are currently using gcc 4.8 from the raspbian repositories (they are not raspberry pi machines either) and that's an armv6 compiler. Current status is that I have a 4.9 compiled on a spare server on Scaleway but haven't had anywhere to store it for future provisioning, since we now have the static served directory on ci.nodejs.org that problem is solved so it's just a matter of putting the pieces together. This is a job someone else in build/test could do I suppose, I'd just have to give you a rundown on what I've done so far and what still needs to be done.
gibfahn		There were no objections to this at the meeting. Next steps are for someone to write up the meeting template and PR it to the build WG, and then we'll start doing this for future meetings.  @jbergstroem I can do the PR if you want (I'm already doing the minutes).
rvagg		I have a request re meetings too -- can we consider adjusting our time for it? DST has made it go back to 6am for me and that sucks so deeply (for me at least). I don't want to exclude folks but perhaps we could do a spreadsheet similar to what the CTC uses to find good meeting times and work out if there is a better time of day? @mhdawson do you have time to facilitate that? Perhaps copy the CTC one and leave you and I on it and have others fill it in. Alternatively perhaps we just expand the CTC one and include Build, I don't know if there's anything particularly sensitive there, what do you think @trott?
Trott		> I don't know if there's anything particularly sensitive there, what do you think @trott?  I wouldn't object, but I guess if you're asking my preference: separate spreadsheet. I don't know that sharing the link (and thus the data in the spreadsheet) is a violation of anyone's privacy, but it *feels* wrong to me. So just make a new one. But if you want to instead just add a Build tab, I'm not going to stop you.
Trott		Also: I've not been terribly active in the WG. I'd like to maintain some elevated privileges in Jenkins to take problematic hosts offline and to be able to log in to the machines to terminate stalled tests and stuff like that. If I can somehow do that without having the guilt of missing pretty much every WG meeting and not really participating, that would be awesome. On the other hand, if the two are tied closely together--like, you don't want someone who's not paying attention to Build WG matters to be able to mess things up--I get it.
piccoloaiutante		@gibfahn if you want i can go ahead with the PR for template.   @rvagg yes 6AM sucks a lot, lets try to find a better timeframe. 
mhdawson		Opened issue here to work on new time: https://github.com/nodejs/build/issues/698
maclover7		ping -- can this be closed?
gibfahn		Yep
mikeal		Yup.  Also, how do I get the .ics url for this?
gibfahn		If you follow [the link](https://calendar.google.com/calendar/embed?src=nodejs.org_nr77ama8p7d7f9ajrpnu506c98%40group.calendar.google.com) there's a `+google calendar` button.  ![image](https://cloud.githubusercontent.com/assets/15943089/25362061/9dd45a50-2949-11e7-94b0-8b18ab42e3d9.png)  If you click on the `ICS` button in the calendar settings after adding it, you get this:  https://calendar.google.com/calendar/ical/nodejs.org_nr77ama8p7d7f9ajrpnu506c98%40group.calendar.google.com/public/basic.ics  I feel like there's probably an easier way...
rvagg		added `$` to the regex and also added one for https://nodejs.org/calendar.ics to take you to the .ics file, is that going to be helpful?
rvagg		deployed, we now have those two URLs active
nschonni		May be related to this from the downloads page https://scan.coverity.com/download ``` WARNING: Linux users on kernel version 4.8.x and newer will need to apply a sysctl parameter to support our binaries. Without this parameter, our binaries will not work on your platform. This is a known issue which we will address with a future release.  # sysctl vsyscall=emulate ```
jbergstroem		I just updated the toolchain and are trying a rebuild with some minor adjustments. I've also added `deps/nghttp2` and `deps/node-inspect` to exclusion patterns.  Finally, how do we handle people that wants to see the result? As of now, it is invite only but there's two invites from april that haven't been approved üòû  -- I don't have any objections to accepting all invites seeing how its relatively trivial to reproduce the results. Opinions?  Edit: looks like its working; coverity says it's analyzing the new results. I will close this later today and open a new issue about access at some stage.
jbergstroem		..and it finished. We're back to working state.
fhemberger		@refack Fixed both URLs.  Both renamed files are already online.
refack		LGTM (I'm not write allowed in this repo, so I don't know if it's enough for you)
rvagg		This is now deployed, but there was a problem on the two lines, @fhemberger I don't quite know what it is but there's some special space character at the begining of both lines, maybe a unicode whitespace of some kind. nginx borked at it and died. Gave us a good chance to exercise our failover solution though and it worked fine!
rvagg		See https://github.com/nodejs/build/commit/f347666a3a1db760356aa31177ff5521272904a5 FYI, note the weird diff for those two lines.
fhemberger		Uh, sorry for that! Used the GitHub web editor for this PR ‚Ä¶ glad it worked out, thanks!
TimothyGu		Would it not be possible to label all machines manually? I assume the Jenkins would skip the currently offline machines while looking for a machine with such a label?
refack		> Would it not be possible to label all machines manually?  Possible but laborious (there are currently 150 machines), and error prone... I'm looking into patching the PlatformLabel plugin...
gibfahn		>Would it not be possible to label all machines manually?   We also don't currently automate the adding of a newly provisioned machine to Jenkins. If we had a script for that, we could add the labels there.  >I assume the Jenkins would skip the currently offline machines while looking for a machine with such a label?  Correct
refack		> We also don't currently automate the adding of a newly provisioned machine to Jenkins. If we had a script for that, we could add the labels there.  There's a chicken&egg problem, we need to register the machine in J to get the secret, and we need to secret to ansible a machine. Maybe we need: 1. add machine to inventory 1. `bootstrap.playbook` (also useful for updates) 2. inventory -> jenkins -> secret list 3. `setupJenkinsWorker.playbook`
gibfahn		>we need to register the machine in J to get the secret  This is the bit that needs automating. Hopefully there's an api equivalent to: - Log into ci.nodejs.org - Click the `Create machine` button - Download secret.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
rvagg		forgot about #679 which is essentially the same. I've closed that but there's some notes there that are relevant to this
rvagg		- `Host key verification failed.` on `iojs-www` .. so prime with a host key?? 
rvagg		- Add `id_rsa` for staging 
Trott		This hasn't been updated in over a year, and I _think_ these things are all in ansible now. Closing, but feel free to re-open of course if I'm mistaken. 
piscisaureus		:+1:  
jbergstroem		@tracker1 I think one challenge will be fully supporting musl as a libc replacement. I'm not against it but have very little knowledge about it. It doesn't look like its in too bad a shape. I'm keen on setting up a system based on musl, so I might give this a go down the road, but don't count on me for the moment. 
tracker1		@jbergstroem understandable... :-)  Really wish I had more time (and a more intimate working relationship with C/C++). 
springmeyer		> Also, would be nice to see tests for building common binary modules (sqlite3, expat and a few others).  As the maintainer of node-sqlite3, :+1:   > I'm keen on setting up a system based on musl, so I might give this a go down the road, but don't count on me for the moment.  Same here. I'm also interested in musl. 
mhart		The failures in the tests (there are only two) are due to assumptions in the tests themselves. One being that the platform has glibc (which Alpine isn't), the other being that the `ps` command supports the `-p` flag (which Alpine doesn't).  You can read more here: https://github.com/iojs/docker-iojs/issues/44#issuecomment-95969892 
Starefossen		I am putting this on the agenda for #300  
jbergstroem		I don't think anyone objects to get this part of the ci, but work needs to be done. Lets get an outline going for whats required - feel free to add: - we lack a vm provider that can run alpine (unless we automate docker stuff) - we lack ansible scripts to set a host up 
Starefossen		Regarding version of Alpine linux it looks like the Docker Library guys are using `alpine:3.3` for some of their official images on Docker Hub.  https://github.com/docker-library/python/blob/master/3.5/alpine/Dockerfile#L1 https://github.com/docker-library/redis/blob/master/3.0/alpine/Dockerfile#L1 
jbergstroem		@Starefossen how often is stable updated? How does that affect toolchain, etc? (thinking if they have a LTS mindset) 
Starefossen		I had a chat with one of the Alpine Linux maintainers today. This is what they told me: - They have Node.js in main repository - Releases are supported for 2 years since initial release with security backports or critical bugfixes - edge is our rolling release, where latest releases land - official alpine image pulls latest stable release by default - breaking changes are usually unrelated to high level stuff like Node.js - suggests running tests for maintenance releases on latest stable, and development on edge 
mhart		@Starefossen some important things to note about the Node.js package in the aports repository are: - They compile with shared zlib (`--shared-zlib`) - They compile with shared libuv (`--shared-libuv`) - They compile with shared openssl (`--shared-openssl`) (this recently caused a [compatibility issue](http://bugs.alpinelinux.org/issues/4999)) - They compile without snapshot (`--without-snapshot`) - I'm not sure how to get Node.js 5.x if you're not on edge?  These may not be an issue but they're worth being aware of. They've certainly been a lot better at keeping up to date with releases since 4.x  (latest build file is here: http://git.alpinelinux.org/cgit/aports/tree/main/nodejs/APKBUILD?id=18fd7609f0ec2df89c873e788880119a703496a1) 
jbergstroem		@mhart the intention is to build node, so not being on edge shouldn't be a problem. Meeting the dependencies (if using shared libraries) might though. 
MylesBorins		Having just done the ssl update I know there are 4 floating patch's on top of the release 
mhart		@jbergstroem yeah, sorry I kinda got off track there ‚Äì Node.js certainly builds on Alpine without requiring you to use shared libraries, was just pointing out that that's what you get with the package @Starefossen was referring to ‚Äì but it's more relevant to users of the package (or any official endorsement of it) rather than building on Alpine from scratch. 
rvagg		Can we get someone to pass on that `--without-snapshot` shouldn't be used anymore, it's no longer a (potential) security concern since v4+ and with more recent releases (where we fixed a bug that was causing it to always actually be without-snapshots ...) the startup speed improvement is non-trivial. I would think that this would be particularly beneficial to the kinds of applications where Alpine is used, i.e. presumably leaning toward the small and nimble service side of the spectrum, hopefully employing the fast-kill-and-restart paradigm on errors.  @ncopa perhaps? 
jbergstroem		@rvagg might be an issue, see https://github.com/nodejs/node/issues/4212. 
ncopa		@mhart we [fixed](http://git.alpinelinux.org/cgit/aports/commit/main/openssl?id=f49f8361fbff134c6c45089e09a64d4ad2bfdbba) openssl in alpine linux (edge) and will probably backport that fix for alpine 3.3-stable.  @rvagg I am looking at removing that `--without-snapshot`. It seems to require gold linker? why? 
bnoordhuis		V8 uses it to speed up the link step (ld-gold is multi-threaded, regular ld is not).  I think you can disable it by setting `'linux_use_gold_flags%': 0` in common.gypi. 
ncopa		i can install gold too.  But I have other issue now, mksnapshot is run during build. I need to pax-mark the binary to disable PaX' [MPROTECT](https://pax.grsecurity.net/docs/mprotect.txt) as JITs does not work with it enabled.  So I need to: 1) build mksnapshot 2) pax mark it 3) build the rest  Currently I am trying to figure out how to only build mksnapshot. 
bnoordhuis		`make -C out mksnapshot BUILDTYPE=Release` - replace Release with Debug for a build with a ton of extra checks. 
ncopa		That worked. many thanks! `--without-snapshot` is [removed](http://git.alpinelinux.org/cgit/aports/commit/?id=1bd5e2b0e6536df1727331cd2e6b4d57403d3450). 
mhart		@ncopa can you explain why you need to paxmark mksnapshot? (as in, what happens if you don't?) I've never had a problem on Alpine compiling the node binary in a single step and then just paxmark'ing it (the node binary). 
jbergstroem		@ncopa you can check how we pax-mark in the ebuild I've done for gentoo: https://github.com/jbergstroem/gentoo/blob/verbump/net-libs/nodejs-5.4.1/net-libs/nodejs/nodejs-5.5.0.ebuild#L115 
ncopa		@mhart the PaX patched kernel we use in Alpine will not permit JIT compiled code to execute. `mksnapshot` will crash if you don't disable those memory protections. This only affects when you use Grsecurity/PaX kernel.  As mentioned, it now builds just fine.  I have also submitted a couple of pull requests to fix the test suite: https://github.com/nodejs/node/pull/5099 https://github.com/nodejs/node/pull/5056  AFAIK all issues should now be resolved. What is now needed to move forward? 
mhart		@ncopa ‚Äì ah, ok, I failed to mention I was compiling on Docker, so yeah, haven't seen this. 
shouze		Ok guys... I didn't mentioned this discussion when I've opened this PR earlier today https://github.com/nodejs/docker-node/pull/107.  :+1: @ncopa BTW for nodejs/node#5099 & nodejs/node#5056.  @ncopa: - Do [this patch](http://git.alpinelinux.org/cgit/aports/tree/main/nodejs/issue-4221.patch?id=1bd5e2b0e6536df1727331cd2e6b4d57403d3450) still relevant (even if we don't compile with `--shared-openssl`)? - Why compiling also with `--shared-zlib` & `--shared-libuv`?  
shouze		@ncopa `--with-intl=small-icu` && `--download=all` configure options are not in the [APKBUILD](http://git.alpinelinux.org/cgit/aports/tree/main/nodejs/APKBUILD?id=1bd5e2b0e6536df1727331cd2e6b4d57403d3450) but @Starefossen and/or @rvagg can probably confirm that every official node binary is distributed with small-icu. 
jbergstroem		@shouze if icu is already apk'ed you could build against a system version of it. 
shouze		@jbergstroem yup, but I'm not sure, it's not the usual way node is distributed (small icu, can be completed with a `npm install full-icu`, openssl not shared). 
jbergstroem		@shouze if you already have a full icu from system there's no need to do anything else. 
shouze		@jbergstroem you make a point, in fact I'm thinking about official bin you can download from node homepage so yes - of course - all static is the rule. 
maclover7		ping -- is this still an issue?
MylesBorins		I think we are good for now
rvagg		also added a change from iojs to nodejs for nightly-builder, not going to add an option to make it switchable since I don't see io.js releases on our horizon 
jbergstroem		> @refack said: > 1. How long does it keep job logs?  Currently 7 days as defined by our backup job (also: disk at 85% full)  > 2. Can you extend the cookie expiration, it seems like I need to login everytime I use it...  Can't find a setting for it. Sorry.
refack		Thanks!
gibfahn		>Can you extend the cookie expiration, it seems like I need to login everytime I use it...  Maybe this:  http://stackoverflow.com/questions/26407541/increase-the-jenkins-login-timeout
rmg		:+1:  
gibfahn		Definitely +1 on quotes, otherwise it looks like it's missing a `citation needed` tag.  I'm fine with having a single line about what each company represents, but if we're doing this we should make sure we go back to our existing donors and ask what they'd like to see (so it doesn't look like we're favouring one company).
mhdawson		If it just said "a world leader" instead of "the", I'd be ok without the quotes and happier overall.  My preference is that we include whatever link that the provider wants, and in the page for that link they can sing their own praises, but that the text on our page is relatively neutral.  The hard case would be if we have two contributors both claiming to be "the world leaders". 
joaocgreis		I believe a link would allow for companies to provide more information, without creating an opportunity for conflicting descriptions in this page. But if the general feeling is that the tag-lines provide better marketing value here, I'm also ok with that.
refack		LGTM
menepie		Was this one settled? Having a link to provide more information would help indeed. 
gibfahn		>Was this one settled? Having a link to provide more information would help indeed.  Nope, it'll be settled when this Pull Request is landed (closed).
rvagg		This is on hold for now, see https://github.com/nodejs/build/issues/829#issuecomment-337391099, need to figure out how to get proper libc++6 compat from this.
maclover7		@rvagg Is this outdated now?
rvagg		ugh, sadly yes
jbergstroem		No opinion really; feel free.  The main thing I wanted to do with the 'default' view was to keep it as small and relevant for most people as possible.  
mhdawson		Ok added, thanks. 
gibfahn		I started to set up a couple of jobs:  [`llnode-pipeline`](https://ci.nodejs.org/view/post-mortem/job/llnode-pipeline/) triggers [`llnode-continuous-integration`](https://ci.nodejs.org/view/post-mortem/job/llnode-continuous-integration/) with the different versions of node you want to test.   I haven't run them (sure to be lots of bugs), and I assume the Linux ones will fail. I've given @nodejs/post-mortem-admins and @bnoordhuis access, so feel free to modify.  We'll need to install llnode on ubuntu, so that'll require a PR to the [`ansible/`](https://github.com/nodejs/build/tree/master/ansible) scripts.  I'll sort the pipeline out tomorrow. I tried testing on my local machine, but I'll have to get past the configure error first:  ```bash ‚ûú  llnode git:(pr-109) ‚ùØ npm i  > llnode@1.5.1 preinstall /Users/gib/wrk/com/llnode > node scripts/configure.js  Build dir is: /Users/gib/wrk/com/llnode xcode-select: error: tool 'xcodebuild' requires Xcode, but active developer directory '/Library/Developer/CommandLineTools' is a command line tools instance Unable to locate lldb binary. llnode installation failed.  ‚ûú  llnode git:(pr-109) ‚ùØ which lldb                                                                                                                                                        ~/wrk/com/llnode /usr/bin/lldb  ‚ûú  llnode git:(pr-109) ‚ùØ lldb --version                                                                                                                                                    ~/wrk/com/llnode lldb-370.0.42   Swift-3.1 ```
gibfahn		Note that there's also [`llnode`](https://ci.nodejs.org/view/post-mortem/job/llnode/), which it looks like @rvagg was setting up in November 2016. Looks like it's using docker, but I _think_ it's a little out of date now. Happy to move to this style if it makes more sense (gave @nodejs/post-mortem-admins access to that too).  <details><summary>Scripts from the <code>llnode</code> job:</summary>  ```bash ## TESTING WITH NODE.JS v7 ----------------------------------------------  docker run -i --rm --security-opt=seccomp:unconfined llnode-base:xenial /bin/bash -c '  curl -sL https://deb.nodesource.com/setup_6.x | bash - apt-get install nodejs -y  echo + node -p process.versions node -p process.versions  cd /home/iojs && su iojs -c "  echo + git clone https://github.com/'${github_user}'/'${github_repo}'.git llnode git clone https://github.com/'${github_user}'/'${github_repo}'.git llnode echo + cd llnode cd llnode echo + git checkout '${github_ref}' -f git checkout '${github_ref}' -f echo + git clone https://chromium.googlesource.com/external/gyp.git tools/gyp git clone https://chromium.googlesource.com/external/gyp.git tools/gyp echo + npm install npm install echo + make _travis make _travis  "  ' ```  ```bash ## TESTING WITH NODE.JS v7 ----------------------------------------------  docker run -i --rm --security-opt=seccomp:unconfined llnode-base:xenial /bin/bash -c '  curl -sL https://deb.nodesource.com/setup_7.x | bash - apt-get install nodejs -y  echo + node -p process.versions node -p process.versions  cd /home/iojs && su iojs -c "  echo + git clone https://github.com/'${github_user}'/'${github_repo}'.git llnode git clone https://github.com/'${github_user}'/'${github_repo}'.git llnode echo + cd llnode cd llnode echo + git checkout '${github_ref}' -f git checkout '${github_ref}' -f echo + git clone https://chromium.googlesource.com/external/gyp.git tools/gyp git clone https://chromium.googlesource.com/external/gyp.git tools/gyp echo + npm install npm install echo + make _travis make _travis  "  ' ```  </details>
bnoordhuis		Thanks Gibson, nice work.  Maiden run: https://ci.nodejs.org/view/post-mortem/job/llnode-pipeline/1/ - doesn't seem to work yet though but it makes up for that in speed (23 ms!)  Naive question perhaps but how do I go about debugging that?  Or do I just need to wait a little longer?
rnchamberlain		@gibfahn many thanks for setting this up. Just curious, why the extra 'pipeline' build? For node-report we have just a single build.
gibfahn		>Just curious, why the extra 'pipeline' build? For node-report we have just a single build.  If you want to trigger the build for Node 4, 6, 8 then you have to run the build three times. I'm lazy so I just have a pipeline that triggers things.  >Maiden run: https://ci.nodejs.org/view/post-mortem/job/llnode-pipeline/1/ - doesn't seem to work yet though but it makes up for that in speed (23 ms!)  Yep, I need to sort out the pipeline. I'll try to get to that in a bit.
hhellyer		@gibfahn There is a travis build here https://travis-ci.org/nodejs/llnode - we just accepted a PR to make it point at the right place which has caused me to notice it's broken. I'm working on fixes:  https://github.com/nodejs/llnode/pull/110 https://github.com/nodejs/llnode/pull/111  (Disclaimer, I didn't setup travis so I'm not sure how it relates to everything else.) 
bnoordhuis		@hhellyer Travis doesn't have access to the organization (disallowed, it requests private access) so I don't think that's ultimately going to work.
mhdawson		It is better to test with the Node.js community CI as it also gives us the option of full platform coverage.
bnoordhuis		@gibfahn Were you able to make progress on this?  (Can't check myself, CI is currently under lock-down.)
gibfahn		Okay, I've [gotten the pipeline to work](https://ci.nodejs.org/view/All/job/llnode-pipeline/11/console). It looks a bit mental but I'm not aware of an easier way of creating an arbitrary number of jobs.  If you're just running against one version of Node just run [llnode-continuous-integration](https://ci.nodejs.org/view/All/job/llnode-continuous-integration/). If you want to run this on a PR (for example), then using the pipeline should be easier.  Next steps are to actually fix the job itself. That's where it might be more of a @bnoordhuis or @nodejs/post-mortem issue.
gibfahn		I note that the only members of @nodejs/post-mortem-admins are @mhdawson and @rnchamberlain. I think we should expand the membership to include people who are interested in setting up and maintaining the Jenkins jobs.  I'd suggest @indutny @bnoordhuis and @hhellyer , but I haven't been very involved in `llnode` so there may well be others I've missed.
bnoordhuis		@gibfahn I just tried to start https://ci.nodejs.org/view/post-mortem/job/llnode-pipeline/ but it says "master is offline."  > I'd suggest @indutny @bnoordhuis and @hhellyer , but I haven't been very involved in llnode so there may well be others I've missed.  Yes, that should cover it.
joaocgreis		"master is offline" is because of https://github.com/nodejs/build/issues/825#issuecomment-322359191 , I can fix it.
joaocgreis		Done, did it by bringing master back online. @rvagg @gibfahn apparently, pipeline jobs do not have a "Preference of node" or "Restrict where this project can be run" options, so it'll only run on master. So, unless I missed something, we'll need master online to run pipeline jobs.
bnoordhuis		Thanks @joaocgreis, I can confirm that it's working now.
joyeecheung		@gibfahn The Jenkins job is currently broken, I think it's probably because the build flow of llnode has changed (although the test suite itself was a bit flaky). I've got the Travis working in https://github.com/nodejs/llnode/pull/144, a fix to Jenkins would probably involve getting the scripts up-to-date similar to what's done in https://github.com/joyeecheung/llnode/blob/ae83fe8384fb37f6039b4fe2c9f07c3e852727e9/.travis.yml (we might want a makefile target for Jenkins).  Can I have the permission to configure the Jenkins job? I would like to make it work with Node core PRs as well. (refs: https://github.com/nodejs/post-mortem/issues/50#issuecomment-348050389)
mhdawson		@joyeecheung is this the one that you need access to: https://ci.nodejs.org/view/post-mortem/job/llnode-pipeline/  ?
gibfahn		I gave nodejs/build permission to access [llnode-pipeline](ci.nodejs.org/view/post-mortem/job/llnode-pipeline) and [llnode-continuous-integration](https://ci.nodejs.org/job/llnode-continuous-integration/), which should be enough.
jbergstroem		LGTM 
watilde		Thanks for the following up! LGTMüò∫ 
phillipj		@jbergstroem you got the time to run this in production? 
jbergstroem		> @phillipj said: > @jbergstroem you got the time to run this in production?  Done. 
piccoloaiutante		@mhdawson should I add mine even if i'm not part of the WG?
gibfahn		>@mhdawson should I add mine even if i'm not part of the WG?  Yes!
mhdawson		@piccoloaiutante just to add to @gibfahn response.  Absolutely we want to take into account all regular attendees.
mhdawson		Hmm, no easy choice from the spreadsheet.  We may need to look at having 2 different times. 
mhdawson		adding to agenda for this weeks meeting.
mhdawson		Ok agreed on alternating between 20 and 22 UTC on every 3rd Tuesday.  I just updated the Foundation calendar.  Let me know if it does not match what's expected.  Closing.
jbergstroem		Have you seen this again? Not me. 
jbergstroem		quick question: how do we then automate installing alpine/similar on top of docker? is it part of the same build script? 
Vanuan		@jbergstroem distribution installation is always a part of Dockerfiles. No ansible scripts needed. Here's a pull request that passes locally, but fails due to resource constraints: https://github.com/nodejs/docker-node/pull/156 
Vanuan		The problematic part is how to publish binaries from docker to https://nodejs.org/dist/ archives. 
Vanuan		So there are 4 dockerfiles needed: 1. Dockerfile that include build tools to build distributed binaries for each version (release delivery  to https://nodejs.org/dist/). 2. Dockerfile that include build tools to build distributed binaries for the latest version (continuous delivery to  https://nodejs.org/dist/). 3. Dockerfile that include build tools and test tools to run tests (continuous integration) 4. Dockerfile that include binaries from https://nodejs.org/dist/ and deploy them to Docker Hub (continuous deployment)  1 & 2 could be the same dockerfile, but with a different build context (different source trees). 3 is different in that it includes build and run scripts for tests. 4 is different in that it's mostly a packaging and deployment thing and should be used by end user 
Vanuan		Number 4 in that list is this: https://github.com/nodejs/docker-node/pull/156 But it should be triggered either regularly by the publish hook to https://nodejs.org/dist/  Number 3 is what this issue is for.  Number 1 & 2 should be handled in https://github.com/nodejs/build/issues/75 
Vanuan		Number 3 (this issue) is presumably being fixed by #437 
mhdawson		I think this issue can be closed as the motivation was to get alpine runs into the ci and it looks like they are there. 
gibfahn		@nodejs/build please review.
refack		/cc @gdams @joaocgreis @gibfahn 
rvagg		lgtm
rvagg		+1, @phillipj has been a huge help for the website WG and has proven his personal investment in Node over all that time. 
mhdawson		+1 from me 
phillipj		Thanks for the invite, I would be honored! It would be nice to get the bot up-n-running on the project's infra, and probably tinker with nginx once in a while for nodejs.org & friends. 
MylesBorins		+1 
jbergstroem		Unless anyone has any objections we should vote and welcome @phillipj as part of the next meeting. 
rvagg		+1 although I'd like to invite _all_ of @phillipj, not just that particular part of him  _grr, editing the above comment made my joke non-funny, spoil sport_ 
jbergstroem		@phillipj joined with https://github.com/nodejs/build/pull/455 
joaocgreis		Looks like Pi1 needs some manual cleanup ([job 266](https://jenkins-iojs.nodesource.com/job/node-test-commit-arm/266/nodes=pi1-raspbian-wheezy/console)).  LGTM 
jbergstroem		LGTM 
rvagg		The C2's are hosted at https://www.mininodes.com/, we email David Tischler to deal with them on the hardware front when required.  This machine locked up last week when I was doing an OS update and needed a forced restart by David. I guess the filesystem got corrupted in the process. I've done an e2fsck on it and restarted and it's back up again with a read/write filesystem. Let me know if you notice this again because it could indicate a corrupt disk (I think these guys are eMMC rather than MicroSD so they _should_ be closer in reliability to SSDs).
rvagg		_(nodejs_build_test folks should have access to that machine to do what I just did fwiw, this doesn't require any special privs)_
rvagg		ok I guess, stretching my nginx fu a little too much so other reviews would be appreciated here 
Starefossen		LGTM 
jbergstroem		Just a comment: Yes, we're raising the shared cache to 376M (~1.5m unique ip's) but since we have a cache timeout of 24h at least 200M (~800k) seems to be too low. We could always lower the timeout but we've got nothing better to spend memory on anyway. 
jbergstroem		Ok, some post-review commits here I've found while testing it out locally. I've begun to deploy this in production and it looks good; load is going down (just slightly though). 
jbergstroem		FYI; we just plowed through 376M @24h. I'm testing 512M @12h at the moment. 
mhdawson		@gireeshpunathil once you figure out what is going on please get either @refack or @gibfahn to help make the change on the machine that was disabled/reenable.
refack		That server I bonkers. 1) It's failing with a perfectly good test (`parallel/test-repl-envvars` passes as is on `test-osuosl-aix61-ppc64_be-1` 2) gives compilation errors (https://ci.nodejs.org/job/node-test-commit-aix/10098/nodes=aix61-ppc64/console) Please don't bring on-line until it's fixed
refack		P.S. I even rebooted, and it didn't help.
mhdawson		@refack I've been told its not really expected that AIX machines are rebooted often like you would with linux machines. It is the recommendation of our AIX contact that we only do it when absolutely necessary...  Likely not an issue but just to ask that we don't do it too often.
mhdawson		One, other thing to consider is if the rest of the tests pass, and its going to take a while to figure out what the issue is with this specific test, one option is to exclude the test.  Even though we have the other machine we should keep an eye on whether we end up with a big backlog because we are down to a single machine.
mhdawson		@gibfahn I think @gireeshpunathil may already have access to the AIX machines but if not can you facilitate that.
mhdawson		Ran the test manually on the machine and it passed:  ``` bash-4.3$  CONFIG_FLAGS="--dest-cpu=ppc64" CC=gcc tools/test.py parallel/test-repl-envvars [00:00|% 100|+   1|-   0]: Done ```
mhdawson		Trying in the CI to see if it still fails when run that way: https://ci.nodejs.org/job/node-test-commit-aix/10102/nodes=aix61-ppc64/console
mhdawson		Seems to have failed in CI. Going to kill and restart the jenkins agent in case it is related to how it was started 
mhdawson		Passed in this run: https://ci.nodejs.org/job/node-test-commit-aix/nodes=aix61-ppc64/10103/console.  My only guess may be that it might somehow be related to how the jenkins agent is started.  What I used was (and have used in the past): ``` sh /etc/rc.d/rc2.d/S20Jenkins started as the root user from a bash shell ``` And to be more precise I usually cd into etc/rc.d/rc2.d and then run sh S20jenkins start  I wonder if there is something subtly different when it gets started automatically on reboot.   
mhdawson		Going to leave enabled.  The only difference between the passing and previous failing run was me killing and restarting the agent so I'm hoping that is what caused the passing run and means the jobs will continue to pass on the machine as well.
refack		https://ci.nodejs.org/computer/test-osuosl-aix61-ppc64_be-2/builds Looking good üëç  ![image](https://user-images.githubusercontent.com/96947/32549186-8205978c-c456-11e7-90a7-19bcf64f854e.png) 
refack		Closing as this seems resolved.  #### Summary: * Restarting the Jenkins agent can solve ghosts * Should avoid rebooting AIX hosts
gibfahn		SGTM, I'll put your key on the machines (DM me).
mhdawson		+1
mhdawson		Add Myles key.
MylesBorins		fwiw getting an `ESRCH` error in parallel/test-child-process-detached when trying to kill child process in v6.x-staging  ``` [ESRCH] No process or process group can be found corresponding to that specified by pid. ```
MylesBorins		all done!  issue was a spurious common.noop in a fixture
rvagg		Yikes, that's properly borked. On it, no promises on speed though. 
rvagg		Did a minor server upgrade and restarted Jenkins, I think the Jenkins restart was all that was required. I triggered a full 8.2.1 @ https://ci-release.nodejs.org/job/iojs+release/1887/ and it looks like it's going through just fine, armv6 is building properly currently too.  If you want to promote just the armv6 binaries for 8.2.1 then let me know and I'll clean out the rest from staging.
bnoordhuis		Wouldn't it be less effort to start with the io.js next branch?  It bundles V8 4.3.61.21. 
jbergstroem		Awesome! If possible, it'd be great to hook it up similar to what we've done with the other machines and ansible. 
mhdawson		My first step was to get some builds working in the environment that I'm familiar with (our internal jenkins) using io.js next.    I have them building/testing after applying what looked to be the minimal set of patches to add ppc as a supported platform for Node  (excluding the ones for AIX which I'll add in later) but I have about 6 crypto related failures on ppc that I don't see on x.  My next step is to figure out if they are because I'm missing some additional patches or something else.  Once I get that sorted I'll start looking at getting the machines added to the converged build infrastructure and adding ppc as a platform that the builds for next run on.   
mhdawson		Going to close this since I have the required access 
rmg		The build order issue is reported as fixed in the latest Jenkins (1.634, released Oct. 18).. I'll post an update when I confirm whether or not it fixes the problem on my own Jenkins server. 
rmg		Seemed to do the trick on my server, so we should either upgrade or downgrade.  BTW, Jenkins changelog points to https://issues.jenkins-ci.org/browse/JENKINS-30899 as the issue tracking this. 
orangemocha		Thanks @rmg! @nodejs/jenkins-admins  , any concerns with upgrading? Are backups up to date? 
jbergstroem		New version deployed. 
joaocgreis		Looks like `cctest.tap` was not added to the Windows job as it probably should have been when it was added to others. It can be easily added, but only when cctest passes on Windows, to avoid breaking all runs.
refack		ping https://github.com/nodejs/node/pull/14111
kunalspathak		Updated jenkins job : https://ci.nodejs.org/job/node-test-binary-windows/jobConfigHistory/showDiffFiles?timestamp1=2017-06-25_20-05-00&timestamp2=2017-07-21_12-36-26
rvagg		Initial list might be worth taking from libuv: https://github.com/libuv/libuv/commit/be0e24c1e85ed27c03df4e500299ceedac57bab3 (/h1 @cjihrig).  Also note FTR that this needs to go back on `ctc-agenda` when we have something, the CTC is waiting on us here. 
claudiorodriguez		Not part of the workgroup but I thought it might be useful to share that Mozilla also similarly does Tier 1-3: https://developer.mozilla.org/en-US/docs/Supported_build_configurations and has some nice explanations to go with them. 
rvagg		Nice, I particularly like the way they assign individuals responsible for tier-3 platforms, we can be doing that with the IBM platforms for instance and if anyone shows up super keen to maintain something obscure then we can give them a :thumbsup: as long as their name gets to go next to it and they remain responsive enough to fix stuff. 
geek		@rvagg looks good.   The list should include SmartOS as an officially supported tier 1 platform with @misterdjules and myself supporting it. 
misterdjules		I'm not part of the working group, but I would like to add that the list of platforms supported by libuv is not necessarily a good starting point for the Node.js project with regards to the Solaris and SmartOS platforms.  The libuv project currently documents only Solaris (and not SmartOS) as a supported platform. The reason for not using the name SmartOS and for not making it a tier 1 platform, even if tests are run by the CI infrastructure on SmartOS and [they all pass](https://ci.nodejs.org/view/libuv/job/libuv-test-commit-smartos/119/), might be that there's no _official_ collaborator to the libuv project who committed to support that platform full time, even though I've been responding quickly to any issue that arose in the past.  For the Node.js project, node "sunos" binaries are actually built on SmartOS, and as a result [don't run on Solaris](https://gist.github.com/misterdjules/eae9ec70dc1d91fb8dd1). So they are effectively supported only on SmartOS and should really be "smartos" binaries. Finally, there are at least two _official_ collaborators in the Node.js project who can support the SmartOS platform full-time (@geek and me).  As a result I would expect the SmartOS platform (named "SmartOS", not "SunOS", "Solaris" or with any other name) to be present in the list at tier 1 level. 
bnoordhuis		> The list should include SmartOS as an officially supported tier 1 platform with @misterdjules and myself supporting it.  I think that is stretching it a bit.  IBM has a magnitude more people working on node.js and I don't think we claim that AIX/zOS/PPC are tier 1.  And no offense to you personally but I don't see many code fixes or interaction on the bug tracker vis-a-vis smartos-related issues from you.  To claim tier 1 status for a platform, its supporters should be both active and proactive; I think it's fair to say that neither you nor @misterdjules really fit that description.  (I'm not attacking or berating you, just stating my perceptions.  If you think I'm wrong, please say so.) 
mhdawson		Just to throw in my 2 cents in, I like the Mozilla 1-3 but I would like to see the IBM platforms getting to Tier 2 as I think we do better than Tier 3 implies.  There is hardware in the CI for these platforms and pretty much all tests are running on them as well.  
misterdjules		> > The list should include SmartOS as an officially supported tier 1 platform with @misterdjules and myself supporting it. >  > I think that is stretching it a bit. IBM has a magnitude more people working on node.js and I don't think we claim that AIX/zOS/PPC are tier 1.  I don't think comparing the number of people employed by some of the companies that drive the development of some platforms on which node runs is a good way to approach this problem.  It seems that the reason why some of the maintainers of the AIX/zOS/PPC platforms may not be considering these platforms as part of tier 1 _yet_ is that they've been added relatively recently to the CI platform compared to other platforms such as SmartOS, and they still have [more tests failing and/or flaky than most other platforms](https://github.com/nodejs/node/blob/master/test/parallel/parallel.status#L22-L38).  In comparison, tests for the SmartOS platform have been running for years and now run with a number of flaky tests that is comparable to other platforms, thanks among other things to the great work done by @Trott, @jbergstroem, @santigimeno, the @nodejs/platform-solaris team and others.  > And no offense to you personally but I don't see many code fixes or interaction on the bug tracker vis-a-vis smartos-related issues from you.  @geek joined Joyent only two weeks ago, and thus just started to have the time resources to focus on SmartOS since then, so it's not a surprise you haven't seen him active on that platform _yet_. But Joyent hiring him is a clear sign of its commitment to maintaining Node.js on SmartOS.  > To claim tier 1 status for a platform, its supporters should be both active and proactive; I think it's fair to say that neither you nor @misterdjules really fit that description.  I've been active _and_ proactive in supporting the SmartOS platform, in nodejs/node, libuv/libuv but also in other projects like nvm. When not directly involved in fixing issues that are specific to SmartOS, I've provided support and resources to the build team for issues related to the SmartOS platform on a regular basis. Several other collaborators that I mentioned above have done the same.  This is why the level of support for the SmartOS platform in the Node.js project matches both libuv's and Mozilla's definition of a Tier 1 platform.  If that level of support is not considered enough, then we should work together to bring it to a level that satisfies the expectations. An example of such an effort is https://github.com/libuv/libuv/issues/1036, where I'm trying to improve the communication channels between the libuv collaborators team and maintainers of the SmartOS platform. 
mhdawson		I think that only AIX has more flaky tests (which we are working on). PPC and linuxOne run consistently clean.   
jbergstroem		Here's a initial stab at a "supported platform" document: https://gist.github.com/jbergstroem/8a4a5b6a602aacc96eb8a171698c324c 
Trott		- What's the justification for SmartOS being Experimental rather than Tier 2? (I see lots of discussion about whether or not its Tier 1 above, but if it's not even Tier 2, a clear statement of the gap would be helpful. I'm not invested in SmartOS or anything, and I don't really much care where it ends up, but I am surprised to see it as Experimental given the descriptions of the different tiers in the doc. Perhaps I am misunderstanding something somewhere?) - Perhaps related: Does "There needs to be at least one individual..." refer to an individual in the project or an individual in the Build WG? 
jbergstroem		> @Trott said: > What's the justification for SmartOS being Experimental rather than Tier 2?  No one from the build group has compiled or tested Node.js on Smartos 15 and 16. 
Trott		> No one from the build group has compiled or tested Node.js on Smartos 15 and 16.  Ah! I see. Never mind. Makes sense to me. 
jbergstroem		The idea is to have a document on the table for next weeks (Week of  September 26th) CTC meeting. 
misterdjules		@jbergstroem   > All Tier 1 releases should be available through the nodejs.org/download page.  Does that mean that node binaries built on SmartOS 14 (or any other platform version) would not be available from nodejs.org? 
jbergstroem		@misterdjules: I'll remove that line. Was there while drafting, doesn't belong. 
bnoordhuis		Isn't "macOS >= 10.7" a little white lie?  We only test 10.10, don't we? 
bnoordhuis		Also:  > Node.js officially supports to be built against shared representations of what can be found in deps/ as long as the version of the shared library is at least equal or newer to the one found in deps.  Since we regularly float patches, I don't think we can really make that claim.  I'd mellow it to 'best effort' status. 
jbergstroem		> @bnoordhuis said: > Isn't "macOS >= 10.7" a little white lie? We only test 10.10, don't we?  Yes, you're right. We're hopefully closing in on having this environment up and running -- I guess that won't make it in a week though. It would indeed be a shame to enforce 10.10 seeing how 10.7 is lowest req for v8 5.2 (right? -- recall chatting about that a few months back) and 10.8 for v8 5.4.  Would that imply that we just drop support for <10.10 or add >10.7 < 10.10 as a lower Tier?  > @bnoordhuis said: > Since we regularly float patches, I don't think we can really make that claim. I'd mellow it to 'best effort' status.  The last times we've floated (sans v8) - for instance http_parser - could have been avoided. I think we should be more careful with floating. You're correct in this not being the place for that discussion though. C-ares is a bit unfortunate too, but I've successfully linked and tested against the upstream version, windows being the exception.  Unless other people chip in I'll update the wording before we put it in front of CTC. 
bnoordhuis		> Would that imply that we just drop support for <10.10 or add >10.7 < 10.10 as a lower Tier?  If we can add 10.8 and 10.9 to the CI, I'd be okay with promoting it to tier 1.  I think IBM internally tests from 10.8 and up but I don't know the details.  No opinion on 10.7.  If that's not an option, we could demote <10.10 to experimental / best effort status. 
rvagg		This is what https://www.netmarketshare.com/ reckon the breakdown is for last month:  ![screenshot 2016-09-21 21 45 44](https://cloud.githubusercontent.com/assets/495647/18709712/ca88f3f2-8044-11e6-87c4-38ea29584abb.png)  Nobody deploys server code to OSX, I suspect that the primary way that OSX is a distribution target for us is now via Electron, although I'm not sure how to factor that in to our deliberations here!  The dropoff below 10.10 is pretty sharp, below 10.9 it becomes even more decisive. At 10.9 we're at nearly 1/2 of what Windows Vista still holds and 10.8 is a 5th of Vista.  Given those numbers I think I'd be comfortable relegating <10.10 to a lower category and when we get our new OSX testing infra we can make a call on how far below 10.10 we go but it's hard to see the point in even going to 10.9 with those numbers.  The only strong opinion I hold here, however, is that I don't really want us to spread ourselves too thin, which argues for keeping things as tight as we can justify without pushing too many potential consumers to the margins. 
richardlau		>  I think IBM internally tests from 10.8 and up but I don't know the details.  Correct. Minimum supported operating systems for IBM built binaries for v6 can be found on the table at the bottom of https://developer.ibm.com/node/sdk/#v6 
sam-github		@richardlau does that mean IBM only supports OS X 10.8? I don't see a "or later" footnote. 
sam-github		@jbergstroem what are the X and Y in the GNU/Linux Tier 2/3 colums? WIP? 
richardlau		@sam-github It means that's the version of OS X in our internal CI that ran tests on the binary.  
jbergstroem		@sam-github: the X/Y will be filled in by @mhdawson shortly. 
mhdawson		Yes as @jbergstroem says I was just not sure what the X and Y were off hand.  I just need to pull then from what ubuntu and RHEL 7.2 use. 
jbergstroem		I've updated the proposal to reflect the changes in macOS support as well as taking a step back from shared builds. 
bnoordhuis		Linked gist LGTM. 
mhdawson		We may need to break out the PPC/BE ones a bit as this is the info for the machines/OS levels we currently build/test on for s390 and PPC.  s390: kernel>=3.10, glibc 2.17 PPC LE: kernel>=3.13.0, glibc 2.19 PPC BE: kernel>=4.2.0, glibc 2.19 
rvagg		nevermind, it decided to work just as I submitted this
chorrell		If this is a machine on the Joyent Public Cloud you would not be able to modify the zone configuration. However, /etc/resolv.conf should only be populated at provision time so any modifications you make should persist after a reboot. Also note that depending on which data center you provision to I think the 4.2.2.2 entry might not always be added. I'm not 100% on that though. It's probably easier to just ensure you have the Google DNS servers and no other entries.  
joaocgreis		@chorrell that's what I needed to know, thank you! PR it is then. 
joaocgreis		Fixed by 18b3608f45a11dabff8bcf6f6558c706528c73f1. 
joaocgreis		+1! Michele has been participating and helpful for some months now, it's well about time we add him!
mhdawson		+1 from me as well.
gibfahn		@nodejs/build this is currently our only agenda item, so it'd be good if we could weigh in on this.
gibfahn		+1 from me (to be explicit)
mhdawson		Agreed in meeting, welcome. :)
gibfahn		@jbergstroem can you arrange the onboarding?
jbergstroem		If @joaocgreis can be part of this that would be cool as well -- @piccoloaiutante I assume you will be more active in the Windows category of things?  Also, great to have you on board!
gibfahn		>@piccoloaiutante I assume you will be more active in the Windows category of things?  From what I understood @piccoloaiutante is cross-platform user (main machine is a mac etc.), he's just been doing lots of Windows stuff because that's where there were things to be done. But it definitely makes sense to have @joaocgreis involved anyway.
jbergstroem		Then I'll have first dibs :) I'll set it up
piccoloaiutante		Thank guys for the on boarding. Let say that my background is on windows but I've been using other operating systems for a while. Also for me it's easy to get new VM on my laptop on whatever windows version we want as I have and MSDN subscription. 
joaocgreis		@jbergstroem happy to join, let me know when!
jbergstroem		Can you guys do tomorrow or Wednesday? Your later pm, my not-as-late pm?
piccoloaiutante		@jbergstroem Wednesday works for me, if it's not too long we ca do it after dinner around 9 PM CET if that is better for you.
jbergstroem		@piccoloaiutante 9pm CET is ok for me. @joaocgreis ?
joaocgreis		Works for me as well. I'm assuming you mean CEST, that is 7 PM UTC, 9 PM in Italy, 3 PM in Santiago.
piccoloaiutante		@joaocgreis yes i meant CEST :)
jbergstroem		SGTM!
gibfahn		@piccoloaiutante you're in the [build Github team](https://github.com/orgs/nodejs/teams/build/members), but not in [the README](https://github.com/nodejs/build#people). I assume you've been onboarded, so could you raise a PR that `Fixes:` this one adding yourself to the README?
gibfahn		ping @piccoloaiutante 
mhdawson		@jbergstroem can you take a look at this since I don't have access to the release machines.  It is blocking the nightlies and the 6.x and 4.x updates.
jbergstroem		```console bash-4.3# chown -R iojs:staff .git bash-4.3#  ```
mhdawson		That seems to have done the trick, thanks.
gibfahn		+1  I'd also note that we've given him access to AIX machines in the past https://github.com/nodejs/build/issues/616 https://github.com/nodejs/build/issues/661
joaocgreis		+1 for individual machines access. @gdams I believe you'd be welcome in the Build WG if you want to join, and keep access after this is done!  Also ok with infra access, but please make sure to revoke access when the machines go live (by updating the password, if I'm not missing anything). We must be much more careful at this level. cc @rvagg @jbergstroem 
joaocgreis		To be clear, @mhdawson your intention is to share the credentials on `infra/macstadium.md`, right?
gdams		> I believe you'd be welcome in the Build WG if you want to join, and keep access after this is done!  @joaocgreis thanks for that! I am hoping to help set this up and then help setup a KeyBox instance for SSH key management so being in the build WG would be great!
mhdawson		@joaocgreis yes that is what I was thinking.  But of course wanted to see if other people on the build WG thought that would be ok or not.  If that is not ok then the fallback would be to just give him access to the machines themselves, but that will take more work as the firewall etc needs to be setup first.
gdams		@mhdawson shall I create another issue for adding me to the build WG? That way everyone can vote 
mhdawson		@joaocgreis would you like to create the issue to add gdams to the build WG ?   Otherwise I can go it, just thinking that's better than self nomination.
gibfahn		>@joaocgreis would you like to create the issue to add gdams to the build WG ? Otherwise I can go it, just thinking that's better than self nomination.  I think if @gdams raises a PR to add himself to the README then that's fine (and people can use the PR reviews to comment).
joaocgreis		@mhdawson I'm ok with giving him only the macstadium password and reset it after he's done.  About joining, I don't think self nomination is a problem because we'd have to analyse the issue in exactly the same way. I'd have been happy to do it though, but I see I'm too late.
mhdawson		Seems like there are no objections but I'd feel more comfortable with at +1 from @jbergstroem  or @rvagg as well.
rvagg		Whoops! Yes, sorry, I very approve. I don't know if it'll help but perhaps we should schedule another day where we work collaboratively on this? My fear is that this vSphere stuff is such obscure specialised knowledge that only one person on the team (or not even on the team in this case I guess) will be the holder of that knowledge. It'd be great if we knew that more than one person knew how to wrangle this stuff for the future. 
gdams		@rvagg let's see if https://github.com/nodejs/build/pull/860 gets approved and then I can hopefully work with you on putting together the new system?
mhdawson		I've got a time set next week with @gdams to ramp him up on what's in place already, outline key next steps and give me the access needed to get started.  At that point would be useful to have a working session if we can find a time everbody can make it. 
gibfahn		@mhdawson did your onramp with @gdams happen? Would definitely be good to have a working session.
mhdawson		Yes, hooked him up maybe 2 weeks ago.  I'll follow up with him tomorrow/Monday and then we can see were we are.
refack		I'm assuming this has concluded
mhdawson		I think this should stay open until its complete and we have updated to revoke his access.
gdams		@mhdawson can we close this? I have the same level of access via my build WG membership 
mhdawson		Yup, closing
bnoordhuis		I think `./configure && make install` is all you really need to do, although most people install node from ports.  It used to be that you had to install libexecinfo first but in FreeBSD 11 it's part of the base system, I think. 
eljefedelrodeodeljefe		Okay. I think the Vagrant image I was using ran FreeBSD 10, that would explain it. My question would aim primarily on prerequisites, which seem to be more missing in a blank box in comparison to Linux and OS X. I will dig into it through running `./configure && make install` then. Thanks! 
santigimeno		In `FreeBSD 10` I had also to set the `CC` env variable to the desired compiler. 
jbergstroem		@eljefedelrodeodeljefe `gmake` is the only prereq on FreeBSD 9 and up. As @santigimeno mentioned, depending on your environment you might want to pass `CC=clang CXX=clang++` if you want to choose compiler, but I recall cc/cxx symlinks to clang. 
jbergstroem		Known limitation in some scenarios. We will solve this by avoid using: 1. triton at joyent (loopback and broadcast not available for use) 2. freebsd jails  We currently have one freebsd runner at digital ocean and one at joyent. I will retire the joyent one and replace it with a new setup at DO. 
jbergstroem		This shouldn't be an issue any longer -- the two remaining freebsd bots are not run in triton or jails. 
saghul		Will test and report back, thanks Johan! 
mhdawson		FYI for new OSX ansible scripts @gdams 
maclover7		cc @gdams, should this be issue be closed as part of the new OSX playbook?
maclover7		Since we've moved to the new OSX Ansible playbooks, going on the hunch this has been solved. Please reopen if this appears again.
mhdawson		Issues seemed to be jobs were trying to use /workspace instead of /home/iojs/workspace.  Configuration of iojs-softlayer-benchmark seemed to be missing setting for:  ``` Remote root directory ```  Updated to match those of other slaves to be /home/iojs/build and moved existing workspace to that location.  Had to restart jenkins agent on the machine and then all seemed ok. 
mhdawson		Closing as resolved, but wanted to doc here in case we see other similar issues 
gabrielschulhof		@mcollina, right?
refack		LGTM (I would like to take a shot at implementing)
mcollina		@gabrielschulhof yes.  Thanks @refack!
gibfahn		I can help get you guys set up (creating Jenkin jobs etc), and then @refack can help with configuration stuff.
gibfahn		So I see that there is a bunch of n-api stuff already, see this [CI Tab with a bunch of jobs](https://ci.nodejs.org/view/x%20-%20Abi%20stable%20module%20API/). I note that the access permissions are configured for [`nodejs/abi-stable-node-admins`](https://github.com/orgs/nodejs/teams/abi-stable-node-admins/members) and `nodejs/abi-stable-node`, the latter doesn't exist any more.  So I'm guessing that the effort to add CI already started but stalled. Is that correct?  Assuming this is correct, I'd say we should: - Rename `abi-stable-node-admins` to `n-api-admins` to be compatible with `n-api` (maybe @mhdawson could do this as he's an org owner and a member of both teams). - Delete the existing jobs, which are based on `node-test-commit` - Copy an existing "pull node, npm install, npm test" project, like `node-report` or `citgm`. - Give `n-api` run access, and `n-api-admins` configure access (and also @refack so he can help out).  @nodejs/n-api can someone confirm that this is correct?
gabrielschulhof		That tab was for the abi-stable-node fork while we were preparing the initial N-API PR. Now that N-API is in, I don't think we need it anymore. @mhdawson knows more, but that's my general impression.  OTOH, node-addon-api is a separate npm package which will remain separate, so we will continue to need separate CI for it.
mhdawson		There are some jobs in there that are still needed and that we are working on.  The naming of the admins is because the team was renamed, since Sampson has been able to edit its possible that it is respecting both names  I'll delete the jobs we don't need 
mhdawson		I still see the team which is called:  abi-stable-node-admins which matches what the job is configured for.
mhdawson		The name also matches the repo abi-stable-node, and host jobs that test modules that test both modules that use the  node-addon-api and which use the N-API directly.
mhdawson		I'll create a new job which is configured like the others and add refack to the admins team so that he can help configure.
mhdawson		The other team though was renamed, at one point so I guess the admins one should match and it has less visible impact.
mhdawson		Reconfigured existing jobs Added this job as a clone of the node-report one Added refack as n-api-admins so he can configure job.  
mhdawson		@refack let us know if you need anything else.
refack		> @refack let us know if you need anything else.  Will do. Thanks!
mhdawson		Thank you for volunteering :).  I really like to have as much CI as possible and having somebody to help build it out is great.
mhdawson		oops missed pasting in the link to the new job. Its: https://ci.nodejs.org/view/x%20-%20Abi%20stable%20module%20API/job/node-test-node-addon-api/
maclover7		https://ci.nodejs.org/view/x%20-%20Abi%20stable%20module%20API/job/node-test-node-addon-api/ has been created per above, going to close for now since this seems largely solved (and the job is green!)
jbergstroem		Problem was that the slave was restarted through `start.sh`  instead of the init script. Fixed shortly thereafter. 
rmg		The multiple copies of the readme and host variables is clearly intentional.. But I don't know the reason.   Other than my own curiosity, LGTM.  
rvagg		@rmg yeah, there's lots of duplication here and I'd love someone to help combine it into a single "setup all ubuntu hosts" directory with a single ansible script that just does things a little different for them all. I really just took the easiest path, which was a `cp -a` to get this all working. There's small variations on each of these.  The host variables need to be different because each one has a secret that's unique. 
rmg		@rvagg merge as is and mark the improvement as low hanging fruit for any new contributors who happen to be good at DRYing up Ansible scripts? 
rvagg		yeah, sorry, got lost in so many other things, and I already have some ansible updates locally that I need to push! 
Fishrock123		@nodejs/build if you really want to come we can probably still squeeze you in but time is running low.
jbergstroem		I had high hopes for being able to squeeze in two days but it looks like I won't be able to make it :( 
jbergstroem		Summit over, closing.
joaocgreis		If you mean the "Rebuilder" plugin, it's there but not enabled. The compelling reason comes from the MultiJob plugin, and that one we need for everything. 
jbergstroem		I disabled the plugin 2 days ago. Reopen if you need it. 
jbergstroem		Using github for releases would provide s3 storage but that still leaves nightlies out in the cold. 
rvagg		having run in to space constraints on the (only) server we're running iojs.org on this week I'm thinking that this is perhaps even more urgent to get sorted out. S3 would be great but we'd need $$ for that. I'm not as familiar with the rackspace alternative(s) but I'm guessing that the costs could be covered under our sponsorship there.  This also goes hand-in-hand with HA for iojs.org, we need to rearchitect the whole deal for the website and hosting. 
kenperkins		I'm thinking I want to do a cluster (2 minimum) behind a load balancer, with block storage volumes for a local cache of builds and authoritative storage in cloud files. 
rvagg		:thumbsup: sounds ideal to me, not something we can do on DO atm so I guess Rackspace is the likely new home for this but I know very little about both the block storage or the cloud files service there so you'll have to drive that architecture. 
rosskukulinski		While @kenperkins knows Rackspace's products backwards and forwards, I'd be happy to lend a hand with architecture design review and/or development if you'd like @rvagg.  I've been using Rackspace in production for 2+ years now, so I should be just dangerous enough. 
rvagg		brilliant! make sure you stay tuned in to this then @rosskukulinski 
rosskukulinski		Will do @rvagg. I'm in the same tz as @kenperkins so I'll try to make it to the next build WG. 
jbergstroem		We're now using cloudflare to front our downloads. I wouldn't call it storage (rather cache), but it at least offers redundancy and availability should we briefly go offline. We should still explore s3 for proper storage though 
rvagg		We should .. and tbh I don't think we've actually got full redundancy with cloudflare yet‚Äîwe have it configured so that it does a pass-through for tarballs and installers so we can log them, the intention was for the configuration to also serve them direct from CF if the server isn't reachable but I don't think that's working in practice. I'm obviously not keen to take the server down to test it out but we need to work it out. 
jbergstroem		I just saw those settings. Must be a better way.. 
rvagg		the better way is to use the log file upload feature of enterprise CF, that's been offered and we just need to experiment with it. 
DavidTPate		With CloudFront + S3 what you would want to do for this is setup an S3 bucket for the downloads which can be either Private (and accessible only through CloudFront) or Public (there's other policies too, but for downloads they don't matter as much). You would then set it up to use the S3 bucket as your origin and then be able to use CloudFront as the delivery mechanism.  To get the download counts you would want to turn on Logs for CloudFront and then use some code (or a service) to analyze those to get statistics. 
Trott		@nodejs/build @kenperkins Should this remain open?
mhdawson		Adding   140.211.168.35/Slave test-osuosl-ubuntu14-ppc64_le-3, initialized through new openstack and ansible configuration complete.  Just need to add to firewall to enable. 
jbergstroem		These have been added to the firewall. 
mhdawson		Also added 140.211.168.37/test-osuosl-ubuntu14-ppc64_le-4  (Johan also added to firewall) 
mhdawson		So far node and v8 tests on the new machines look good. 
mhdawson		Ok now starting to add the permanent machines so will disable the test ones to start.  Disabled  https://ci.nodejs.org/computer/test-osuosl-ubuntu14-ppc64_le-3/ https://ci.nodejs.org/computer/test-osuosl-ubuntu14-ppc64_le-4/ 
mhdawson		These machines have now been deleted. Closing 
rvagg		whatever, I just find it hard to keep track of where "sysconfig" is across the various OS' and distros, /etc/sysconfig, /etc/defaults, /opt/foo/bar/what. 
jbergstroem		Valid point. That's probably something that should live in README.md though 
jbergstroem		Filing this as nitpick since we're not supposed to edit any files anyway. 
gibfahn		SGTM
gibfahn		@Trott would you mind PRing the update to the inventory if you've got things working?  I understand from IRC that you've got the IP and the keys, you just need the user (which I assume will be `root`).
Trott		> I understand from IRC that you've got the IP and the keys, you just need the user (which I assume will be root).  `root` was my guess too but alas no...(or I have something else set up wrong)
gibfahn		>root was my guess too but alas no...(or I have something else set up wrong)  How did you get the IP address?  @jbergstroem in general how do we get the IP of a machine?
rvagg		@Trott it's not straightforward but this _should_ get you in:  ``` Host jump-nodejs-arm   User jump   HostName vagg-arm.nodejs.org   Port 2222   IdentityFile ~/.ssh/nodejs_build_test Host test-requireio-osx1010-x64-1   HostName 192.168.2.210   ProxyCommand ssh jump-nodejs-arm nc -q0 %h %p   User iojs   IdentityFile ~/.ssh/nodejs_build_test ```
rvagg		sorry, that's for your .ssh/config
jbergstroem		@gibfahn all machines should live in our inventory. This one doesn't.
gibfahn		>@gibfahn all machines should live in our inventory. This one doesn't.  Yeah, I meant how do we get the IP to add to the inventory. Using one of these from the [Script Console](https://ci.nodejs.org/computer/test-requireio-osx1010-x64-1/script) does seem to work, but of course it gave `192.168.2.210`, which I assumed was wrong (didn't think of the jump box).   ```groovy println InetAddress.localHost.canonicalHostName // or println InetAddress.localHost.hostAddress ```
gibfahn		Followon question, at the moment the generated `~/.ssh/config` entries that use the jumpbox are of the form:  ``` Host test-requireio_louiscntr-debian7-arm_pi2-1    HostName 192.168.2.68   IdentityFile ~/.ssh/nodejs_build_test   User pi   ProxyCommand ssh -i ~/.ssh/nodejs_build_test -W %h:%p -p 2222 jump@vagg-arm.nodejs.org ```  The one @rvagg just posted seems like a nicer way to write it, could we convert the inventory ansible script to write configs like that? I'm happy to do the work if it's possible.
Trott		Thanks, @rvagg. That worked.
rvagg		@gibfahn I don't mind either way. The one you pasted doesn't require the parent Host entry though so perhaps it's nicer.
rvagg		@Starefossen is the source to this somewhere on github? Can you point us in the right direction to be able to make changes? Unless you have time to make such a change of course.  Also @joaocgreis can you update us on the status of the azure machines? Is this just a reality we need to accept and adapt to somehow? Do you have any plans for new things to try? 
Starefossen		Yes, the source code for the Jenkins Monitor is up at https://github.com/Starefossen/jenkins-monitor. Would need to refactor how the state of the individual nodes are stored in redis to account for scoring. 
joaocgreis		@rvagg me and @jbergstroem already made a few attempts at fixing the Azure slaves by adjusting keep alive ping timeouts. I looked at it more carefully today, and still haven't given up.   The tricky thing with timeouts is that jenkins has to be restarted for changes to take effect, but after the restart everything works well. The problem happens in the time window after the Azure load balancer cuts the connection because of inactivity and before jenkins notices it and restarts the slave. Not sure if both the master and the slave must detect that the connection was cut though.  If this proves to be a dead end, I have no other clues yet. 
Starefossen		I have deployed a new version of Jenkins Monitor with a simple scoring algorithm. Current configuration is based on @jbergstroem suggestion ‚Äì 5 minutes interval and 3 consecutive offline before notifying. This means there will be a 15 minutes delay before a notification is sent out and hopefully this will reduce the number of "false" positives. 
jbergstroem		Much better as a result of above comment. I consider this fixed -- closing. 
mhdawson		Landed as 0ef7fddb3d655033b47b713885936ae8b12fcd32
orangemocha		A few more points for discussion: - Getting hardware resources for OSX - Cutting our own builds of Jenkins - Setting up Jenkins secondary masters 
mhdawson		I'm on holiday so won't be able to make this one. 
orangemocha		This meeting is confirmed for this Tuesday.  
jbergstroem		(I'll be joining) 
orangemocha		Meeting notes are here: https://docs.google.com/document/d/1zScaSAUZiGrbhEk9mFxA8l3isoDq34p3J8TwNnpG5xg/edit 
orangemocha		On the issue of ARM hardware support, we resolved to ping the hardware WG and see if they already have any data or can help otherwise. @rvagg do you want to take this one?  I'll follow up to these other issues: - Getting resources for OSX - Cutting our own builds of Jenkins - Setting up Jenkins secondary masters 
rvagg		you could trigger a build with one of the subgroups, like https://jenkins-iojs.nodesource.com/job/iojs+pr+linux/, on the positive side you'll get faster feedback, on the negative you won't get full coverage.  We could add a "no lint" parameter at the top level if necessary. 
jbergstroem		Another alternative would be what we brought up in the last build meeting -- continue building regardless of lint output. 
rvagg		until we really have resource problems I don't mind; I'll go undo that fast-fail restriction now 
joaocgreis		The linter was moved to run in parallel, so I believe this can be closed. @Fishrock123 please reopen if I'm wrong! 
Trott		Hopefully there can be a general Build (and maybe some Test) WG folks who get notified whenever node-daily-master fails. 
rvagg		Set up aliases in https://github.com/nodejs/email/tree/master/iojs.org for each job or type of job and get people to PR themselves into that if they want to be included. 
Trott		Should we close this and address it if/when someone asks for a specific email notification?
maclover7		Some of the jobs have email notifiers installed, like the ppc/aix jobs and are fairly easy to add via the job config page in Jenkins, going to close for now
rvagg		:+1: I'm going to try working this into the raspbian setup scripts I'm working on right now, I have to set up all these new Pi's so am taking the opportunity to clean some stuff up 
jbergstroem		Perhaps abstract the config slightly since it will be a useful combo on deployments where we launch jenkins from sysvinit as well? 
rvagg		I'm finding that it's just not necessary to add sysvinit when you're also doing monit because monit will bring the process up when it does its first check anyway, the Pi's are set up this way and happily restart when I reboot them 
jbergstroem		Ok, cool.  
jbergstroem		FWIW, exploring putting `mon`  (instead of monit) into our sysvinit script over at #292. 
joaocgreis		I don't think this is still needed, with `refactor-the-world`.
jbergstroem		@joaocgreis correct, it's aleady in there.
gibfahn		I guess the plan would be to confirm that all our machines are running Java 8 and then update.
jbergstroem		```console $ ssh ci java -version               java version "1.8.0_131" Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) ```
gibfahn		@jbergstroem sure, but what about the individual hosts? For example I'm pretty sure AIX is using Java 7 (see [here](https://github.com/nodejs/build/blob/master/setup/aix61/manualBootstrap.md#gettext-java-make)).  For background, we upgraded our Jenkins instance, and a bunch of our slaves were suddenly unable to connect, so we had to revert.
jbergstroem		@gibfahn I didn't get that even agents now require java 8. What kind of crazy is this :( 
jbergstroem		So, anyone with any ideas on how we move forward here? We are out of LTS meaning any security bugs (expect them) will hit us pretty hard.
bnoordhuis		There is a Java 8 JDK for AIX ([link](https://developer.ibm.com/javasdk/downloads/sdk8/)), same for FreeBSD.  The only problem might be SmartOS (which frankly I couldn't care less about.)
geek		SmartOS has Java 8 support https://gist.github.com/dekobon/305883fb6d776b0c9fc1  
jbergstroem		There's a few older systems like centos5 which doesn't pack it either. If this is only a matter of adding more package source repos then it might not be as bad then.
gibfahn		Discussed in the meeting (minutes: https://github.com/nodejs/build/pull/845), consensus was that we need to deal with this. I'll put some concrete steps in the first comment.
joaocgreis		Here is a first version of the job: https://ci.nodejs.org/view/All/job/check-java-version/2/ (Build WG only) feel free to edit as needed. I left Windows out because all Windows machines already have Java 8.
rvagg		I improvified it: cross-platform bash execution, use the parent-of-parent to figure out how `java` is executed (not using `which java` which may be incorrect). Changing the `grep` of the version line to be OK with OpenJDK and other types of JRE.  https://ci.nodejs.org/view/All/job/check-java-version/13/ is the latest run. A quick visual inspection suggests that discounting the Pi's, we're more than 1/2 way there.
gibfahn		Did someone delete the job? I'm not seeing one called Java.  >use the parent-of-parent to figure out how `java` is executed (not using `which java` which may be incorrect).  I was going to suggest we make `grandparent java != which java` an error, unless we're aware of some specific exceptions it'd be nice if the default Java (in the path) was always the one used to run Jenkins.
rvagg		@gibfahn log in to ci.nodejs.org before clicking the link above, I believe it's been made private, which is probably a good idea.
gibfahn		Okay, updated it to be unstable (yellow) if Java is at least 8 but Jenkins doesn't use the one in the path.
gibfahn		So I was looking at this and I realised that it doesn't get the full java path in a lot of cases, because it's printing the `comm` part of the `ps` field, which is usually just the `basename` of the command (so it just returns `java`).  I changed it from:  ```bash JAVA_CMD=$(ps -p $HUDSON_PID -o comm=) ```  to  ```bash JAVA_CMD=$(ps -p $HUDSON_PID -o args= | awk '{print $1}') ```
gibfahn		Updated the job to handle windows (although how good that windows handling is is questionable, @refack @joaocgreis feel free to double-check/correct).  https://ci.nodejs.org/view/All/job/check-java-version/37/  This should be pretty much accurate.
refack		https://ci.nodejs.org/view/All/job/check-java-version/37/label=test-rackspace-win2012r2-x64-9/console Looks sane ‚úîÔ∏è 
joaocgreis		Looks good. Two notes: - I recommend having `set -e` on all jobs (`set -x` is already there). When it makes a script fail we usually want it to. - We explicitly don't want to run Jenkins as a server on Windows. There were issues with it in the past, though I suspect they might have been fixed by the recent stdio changes.
gibfahn		>I recommend having set -e on all jobs (set -x is already there). When it makes a script fail we usually want it to.  Jenkins runs with `-ex` by default doesn't it? Seems to looking at how the script gets called in the console output:  ``` [check-java-version] $ /bin/sh -xe /tmp/hudson1653446783271619934.sh ```  The `set -x` was originally a `set +x`, it's just there so I can flip it for debugging.
gibfahn		>We explicitly don't want to run Jenkins as a server on Windows. There were issues with it in the past, though I suspect they might have been fixed by the recent stdio changes.  Okay, good to know. What issues were those? We converted to running as a service and haven't had any issues with it (unless we didn't realise they were related).  Does the `jenkins.bat` file get run automatically run on reboot? I've actually found the service to be a bit of a pain, it has to be run as an administrator, and it has to be restarted if you change passwords.
joaocgreis		> Jenkins runs with `-ex` by default doesn't it?  True. I tend to forget that because I always add the `#!/bin/bash -ex` line to make the script run bash and not dash. The current script looks good as is.  > Okay, good to know. What issues were those? We converted to running as a service and haven't had any issues with it (unless we didn't realise they were related).  I didn't investigate it at the time, just converted the job to not use a service. I remember something because input/output was not available, thus I suspect it was fixed by https://github.com/nodejs/node/pull/11863 . Good to know it works as a service now, did you somehow try to verify if all tests are still running as expected? We have many tests that skip, some we want them to skip on CI (Windows unsupported, things like that), but I suspect some may start skipping because of changes like that, when they should really run. This makes CI very brittle in my opinion, but is the balance we have to make users able to easily test locally, and I don't have a good idea for fixing it.  The CI machines use [autologon](https://github.com/nodejs/build/blob/69aeda3a434f16a6c2b3cbd9f8074f7faad6ebf7/setup/windows/resources/Enable-Autologon.ps1) and [run `jenkins.bat` on startup](https://github.com/nodejs/build/blob/69aeda3a434f16a6c2b3cbd9f8074f7faad6ebf7/setup/windows/resources/Jenkins-Shortcuts.ps1#L4). It doesn't work perfectly every time, sometimes autologon fails and Jenkins only starts when we manually login to the machine, but has been good enough.
gibfahn		>True. I tend to forget that because I always add the #!/bin/bash -ex line to make the script run bash and not dash.   Fair enough, I actually removed the shebang because it doesn't work with Git Bash.  >Good to know it works as a service now, did you somehow try to verify if all tests are still running as expected?  Internally (IBM) we use it as a service, and we do full CI runs, without ignoring flaky tests. Haven't seen any issues with that. I should check whether things are being skipped though, that's a good point!
refack		Can we stop calling it `Git Bash`, it's `MSYS Bash`. Git for Windows is just the package that deploys it...
gibfahn		>Can we stop calling it Git Bash, it's MSYS Bash. Git for Windows is just the package that deploys it...  <img width="658" alt="image" src="https://user-images.githubusercontent.com/15943089/30138752-f07d8ccc-9361-11e7-9a26-44f46dece57e.png">  https://git-for-windows.github.io/  It's the version of bash that comes with Git for Windows. As they changed the name from Msysgit to Git for Windows (see [StackOverflow](https://stackoverflow.com/a/784743/2799191)), no-one knows what `MSYS Bash` is. I prefer Git Bash, everyone knows what that means.
refack		Fine. But it's still a misnomer. <sub>P.S. They only changed the name of the Git package from MSYSGit to Git for Windows. It's still MSYS Bash üë¥ </sub>
rvagg		Hey, so we really need to upgrade Jenkins, we're behind on security updates.  I've just been through and manually updated a whole heap of machines to Java 8 and we're now mostly green now @ https://ci.nodejs.org/view/All/job/check-java-version/2/ and https://ci-release.nodejs.org/job/check-java-version/2/  I have skipped the Ansible bit for now just to get this done cause some of it is kind of complicated. This is what I've done:  * ~~~Ubuntu 12 and 14 Intel are now using the webupd8team/java PPA, done with `add-apt-repository ppa:webupd8team/java`, there'd be a fancy Ansible way of doing it I suppose). Installed via the `oracle-java8-installer` package which downloads the installer from Oracle's site and asks you to accept their license. Java 7 removed.~~~ _(see #964)_ * ~~~Debian 8 is also using webupd8team/java, that's done with `apt-get install software-properties-common && add-apt-repository "deb http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main"` and the `oracle-java8-installer` package again. Java 7 removed.~~~ _(see #964)_ * SmartOS 14 had Java 8 available so it was just a `pkg` update & upgrade and install plus removal of Java 7. Unfortunately the service manifest needed editing because it's hardwired to the Java 7 path, that's all in Ansible in build/ansible/roles/jenkins-worker/templates/jenkins_manifest.xml.j2 although I guess this `{{ java_path[os] }}` does it automatically? * Raspberry Pi's, `oracle-java8-jdk` is in Raspbian repos for Wheezy and Jessie so it was more straightforward to swap them out. I have this in my local Raspberry Pi Ansible config, not sure if I've pushed it or not, will check later. * Ubuntu 14 ARM64 were a pain. I had to manually download the tarball for Java 8 from Oracle and put it in /usr/local/lib/ and symlink it in /usr/local/bin/java. I'd not bother automating these as I think we're going to be retiring them pretty soon. * CentOS5 doesn't have it available so I had to download the Java 8 .rpm from Oracle and install it that way. Which is easier than a tarball but still a pain. Downloading can be automated despite the need to accept the license on the Oracle site. Use `curl -L -b "oraclelicense=a" http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.rpm -O` (with path to file that you need copied from download site). * A couple of other CentOS machines needed upgrading, the inconsistency is a bit strange but perhaps it's older machines where we had Java 7 in the Ansible config. It's easy from CentOS 6 onward though and our new config does it.  I believe I did some of the others previously and I'm not sure what I captured in Ansible, sorry!  The outstanding ones are the IBM ones, so @gibfahn & @mhdawson would you be able to take care of these?  * test-marist-zos13-s390x-1 * test-marist-zos13-s390x-2 * test-osuosl-aix61-ppc64_be-1 * test-osuosl-aix61-ppc64_be-2 * test-osuosl-ubuntu14-ppc64_be-1 * test-osuosl-ubuntu14-ppc64_be-2 * test-osuosl-ubuntu14-ppc64_le-1 * test-osuosl-ubuntu14-ppc64_le-2 * release-osuosl-aix61-ppc64_be-1 * release-osuosl-ubuntu14-ppc64_be-1 * release-osuosl-ubuntu14-ppc64_le-1  We also have a failing release-joyent-smartos13-x64-1 in the mix too but I believe that's a legacy of pre-v4 and can be removed, need to confirm though.
gibfahn		@gdams did the upgrades for our internal machines for this, so he'd probably be well placed to make the same changes in ci.nodejs.org.
gdams		yeah I'm happy to take a look at this!
rvagg		@gdams anything to report on this? Looks like we're going to have to move faster on this now.
rvagg		I've manually updated the benchmark server so now we just have these left:  * test-marist-zos13-s390x-1 * test-marist-zos13-s390x-2 * test-osuosl-aix61-ppc64_be-1 * test-osuosl-aix61-ppc64_be-2 * test-osuosl-ubuntu14-ppc64_be-1 * test-osuosl-ubuntu14-ppc64_be-2 * test-osuosl-ubuntu14-ppc64_le-1 * test-osuosl-ubuntu14-ppc64_le-2  (and the release ones)
Starefossen		Great work, Johan! If you need a candidate to test the on-boarding on I would happily volunteer. I could use a formal introduction to our Ansible / Jenkins Slave setup. 
jbergstroem		@Starefossen cool! This is based on the session I did with @phillipj but I'm happy to run you through as well. Chat me up in private (IRC or smth) and we'll set up a time. 
Trott		I'm happy to be a test subject for this too, if that's helpful. 
jbergstroem		Thanks! Super cool to have one on ones with all of you :) 
phillipj		The things covered here was pretty much ideal. Most of it was new to me and I've got ~zero  ansible experience, but did cover [getting started with ansible](http://docs.ansible.com/ansible/intro_getting_started.html) beforehand so I had some idea what inventory and playbooks were, which was helpful to follow along with the examples shown from the build repo.  Bottom line great onboarding by @jbergstroem üëç  
rvagg		LRGTM ("really"), perhaps you could land this and then we could have one of our new onboardees file another PR to add some meat into here that we could get out of our heads and into a doc. There's a few things in here that are best suited to 1:1 explanation but a lot of it could just be captured in text. 
joaocgreis		LGTM! Perhaps some things that could be added is mentions to the web server, the GH bot, and how the release files go from the release machines to public. Some of this is still a bit dark to me and I guess newcomers would appreciate the overview. 
jbergstroem		@joaocgreis thanks for the feedback! I'll revise. 
mhdawson		LTGM from me as well.  I'll echo @rvagg's comment that this would be a good template to flesh out into document(s) to help us all remember the parts we don't do day to day. 
jbergstroem		@mhdawson My idea was to expand before landing. I don't aim to replace README.md or similar, but lest se how we go. 
refack		Could we talk a bit the option of adding people to the WG without direct ssh access, but more geared towards helping the Automating SIG? (Also about having some DevOps sandbox machines, and even a sandbox Jenkins)?
gibfahn		@refack will raise issue.
gibfahn		@joyeecheung and @maclover7, as prospective members it'd be great if you could attend, assuming you have time. I know it's a bit last minute.  We should probably extend the invitation to anyone in nodejs/automation who's willing (join if you'd like). However I'd like to make sure everyone in build is okay with that before inviting, so we can discuss at this meeting and plan for next time if no objections.
refack		> We should probably extend the invitation to anyone in nodejs/automation who's willing (join if you'd like). However I'd like to make sure everyone in build is okay with that before inviting, so we can discuss at this meeting and plan for next time if no objections.  IMHO we should first talk in the current format about changing the format...
rvagg		@nodejs/automation ping, anyone's welcome to join this meeting in ~45 mins and we can discuss how Build can best support Automation efforts and what an optimal relationship might be between the two groups.
rvagg		I might be late to this, I have a quick but important meeting scheduled shortly beforehand
evenstensberg		@rvagg Could I get an invite? 
gibfahn		@ev1stensberg someone should post a link to the hangout a couple of minutes before it starts.  Speaking of which, @mhdawson are you hosting today?
mhdawson		Link for participants: https://hangouts.google.com/hangouts/_/ytl/44AbiRBv8QKglTqxiGwCAdqq-6PIaYtRhjoWTkD1n7E=?eid=100598160817214911030
maclover7		not sure if meeting has started yet, but is not streaming on youtube
refack		We're "cat herding"
refack		on now 
gibfahn		See https://github.com/nodejs/build/issues/951#issuecomment-339951524, we're trying to collate all the Pi issues into one place.  Resolution is in there as well, obviously that's just a short term fix.
mhdawson		AIX release machine has base configuration and entry added here to the release ci:  https://ci-release.nodejs.org/computer/release-osuosl-aix61-ppc64_be-1/  and the slave shows as connected.  I believe the next step is to add the keys required to push to the download site and replace the test key with the release machine ssh key @jbergstroem. 
jbergstroem		~~What's the ip of `release-osuosl-aix61-ppc64_be-1`?~~ found it  Added release keys and access to release host. 
mhdawson		Ok created a separate release job, and validated binaries were built/published ok.    Now added to standard release job.  Will check tomorrow to make sure the nightlies are ok tonight and include AIX. 
mhdawson		AIX is showing up in nightlies 
mhdawson		AIX was included in 6.7.0 and looks ok so closing. 
bnoordhuis		Tagged https://github.com/nodejs/build/issues/199.  Would be good to make some progress on that. 
Starefossen		Is there something that can be shared regarding the Jenkins downtime for this meeting? 
jbergstroem		@Starefossen sure. 
orangemocha		Confirming this meeting for tomorrow. I edited the issue details with a link to the hangout and updated agenda. Please check the event time in your local time, this week a few times zones are changing: http://www.timeanddate.com/worldclock/fixedtime.html?msg=Node.js+Foundation+Build+WG+Meeting&iso=20151110T20&p1=%3A 
orangemocha		If there is time left, I also have a question about how to get download statistics for recent Node releases. 
orangemocha		Join the hangout here: https://plus.google.com/hangouts/_/hoaevent/AP36tYcaIOj1fJv9HokV-qDHWu__7MWpsFp1p09T21hjkpRH3tQKHw?hl=en&authuser=1 
rmg		@orangemocha can we get a link to the hangout itself? The links all point to the public view. 
orangemocha		@rmg : this one should work: https://docs.google.com/document/d/1zTS3dc--ziFdRJrqQqzH6el0pir0EgQBq2cCCUT7VxU/edit 
jbergstroem		done & dusted. 
cjihrig		@rvagg I'm not sure if it matters, but I've done the past few libuv releases. Do I need to be listed in the `libuv_users` portion of this file?
rvagg		Yes @cjihrig, good catch, if I add you is that libuv list correct or does it need additional adjustments?
cjihrig		If this is just for releasers, then yes, that is enough. If you're looking for a list of all of the maintainers, then there are a few missing from https://github.com/libuv/libuv/blob/v1.x/MAINTAINERS.md.
mhdawson		@rvagg @jbergstroem can one of you take a look as I don't have access to the release machines.
jbergstroem		It's back online. Must've rebooted and jenkins never came back online.
mhdawson		Thanks ! nightly jobs complete closing.
mhdawson		Approvals were given here: https://github.com/nodejs/TSC/issues/381
mhdawson		Got key from John and created user account for him.
addaleax		Isn‚Äôt the AIX problem already solved by https://github.com/nodejs/node/pull/16219? I kind of considered @gireeshpunathil‚Äôs approval on the PR a general ‚Äúyea we are aware & this does fix it‚Äù statement, maybe I was mistaken about that?
gireeshpunathil		@addaleax - the awareness I had is that the test failure has a reasonable explanation from your insights. We still would like to see if there are platform specific issues due to:  - the exact error detail was not captured in the test failure (assert on the type masked off the real error).   - the issue was never reproducible in dev systems. Hope this clarifies.
addaleax		@gireeshpunathil Okay, sure ‚Äì I don‚Äôt want to stop anybody from investigating, just want to avoid duplicate work :)  But generally, I think I have at least some idea of what‚Äôs going on; the string size limit was raised in V8 6.2, therefore the file that the test reads is now 1 GB instead of 256 MB in size, therefore the test consumes a lot more memory trying to read it in one pass, which fails because the CI machine has limited memory available‚Ä¶ Does that sound plausible to you?
gibfahn		>which fails because the CI machine has limited memory available‚Ä¶   The AIX boxes have 32GB RAM each, of which 5G is used for the Ramdisk. So if the available memory is being limited for the test, then we should be able to fix that.
mhdawson		It does seem strange that it would be failing due to memory. These are the ulimits:  ``` # ulimit -a time(seconds)        unlimited file(blocks)         2097151 data(kbytes)         131072 stack(kbytes)        32768 memory(kbytes)       32768 coredump(blocks)     2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user)  unlimited ```  for memory I see this in the AIX docs:  -m | Specifies the size of physical memory (resident set size), in number of K bytes. This limit is not enforced by the system.   
jBarz		It seems like the file size limit might be the issue. This is a comment i posted on https://github.com/nodejs/node/pull/15362#issuecomment-337108705  The maximum file size on the aix machine (512 MB) is smaller than the `kStringMaxLength`. The Maximum string length was increased here https://github.com/v8/v8/commit/e8c9649e2570c7e278e70a6584738a3c3f828b2b from  ~256 MB to ~1 GB. So now, the maximum string size exceeds the maximum file size. I think that's why the [error](https://github.com/targos/node/blob/fbfa5a8a7f0e861a354220fee11b47b529bf4f15/test/sequential/test-fs-readfile-tostring-fail.js#L31) never gets triggered.  ```javascript fs.readFile(file, 'utf8', common.mustCall(function(err, buf) {   assert.ok(err instanceof Error);   assert.strictEqual('"toString()" failed', err.message);   assert.strictEqual(buf, undefined); })); ``` Can we try to increase the file size limit to at least 2GB? > ulimit -f > 1048575  To set it to 2GB > ulimit -f 4194304
jBarz		I also submitted a PR to skip the test in case the file size ulimit wasn't sufficient.  https://github.com/nodejs/node/pull/16273
maclover7		Since nodejs/node#16273 has landed, is access still needed?
jBarz		Not needed
mhdawson		Ok removed access, closing.
rvagg		apparently logrotate doesn't support globs ... so there you go
jbergstroem		Wow. I've been "using" globs for I don't know how long..
rvagg		perhaps it was the commas? or perhaps this is a linux thing and you're just used to some dodgy *bsd hack
jbergstroem		Ah, I think it was the commas. Was just baffled by the fact that it might have been broken and I've never noticed on my servers either.
rmg		LGTM 
retrohacker		looks good! :heart: 
fhemberger		@nodejs/build Can we please get this change online? Link to "stable" downloads is atm broken in production! 
jbergstroem		@fhemberger i will test and deploy shortly after our build meeting in an hour.  Edit: found a few minutes while brewing coffee; it should be working now. Will have a closer look when I have more time. 
fhemberger		@jbergstroem Great, thank you! Noticed a little typo: It's "download" instead of "downloads" 
jbergstroem		> @fhemberger said: > noticed a little typo: It's "download" instead of "downloads"  Fixed. Will have a closer look now. 
jbergstroem		LGTM. Will land. 
jbergstroem		Merged in 9486ed2e39348e26b49ecb4fe38c9c5f418b4028. 
refack		I'm assuming this was resolved.
rvagg		Sorry about this, see comment @ https://github.com/nodejs/nodejs.org/issues/515#issuecomment-182251052 for reason, eventual solution and intermediate workaround 
jbergstroem		ohh! can we get a `cat /proc/cpuinfo`? 
rvagg		Not too interesting actually  ``` Processor   : AArch64 Processor rev 1 (aarch64) processor   : 0 processor   : 1 processor   : 2 processor   : 3 processor   : 4 processor   : 5 processor   : 6 processor   : 7 Features    : fp asimd evtstrm CPU implementer : 0x50 CPU architecture: AArch64 CPU variant : 0x0 CPU part    : 0x000 CPU revision    : 1  Hardware    : APM X-Gene Mustang board ```  This is fun though  ``` $ cat /proc/meminfo MemTotal:       32970352 kB ... ```  ``` $ df -h Filesystem      Size  Used Avail Use% Mounted on /dev/sda2       439G  2.2G  415G   1% / ... ```  Whatever shall we do with all that capacity? I did imagine using it to put Docker containers for all of the distros that are now supporting armv8 but of course Docker doesn't support armv8 yet .. 
rvagg		revisiting this, I've rebased and removed the start.sh and monit stuff if we're just going to jump in on systemd, I have some questions inline 
Trott		This has been dormant for over a year. I'm going to close it. Feel absolutely free to re-open or comment if there is active work going on, or this is something that desperately needs to happen, or you otherwise feel it is a mistake to close this. 
jasnell		I'll follow up on the progress on this side. I'll get back to you. On Nov 5, 2015 3:34 PM, "Rod Vagg" notifications@github.com wrote:  > There's a lot of changes making their way into core where only IBM folks > are able to make a judgement on their appropriateness because the rest of > us don't have the ability to run them on AIX. I'm concerned about creating > a dark corner in both our code and in our community of collaborators > because of this division. >  > What are the barriers to being able to hook up an AIX machine to Jenkins > for testing? We have the PPC Linux boxes there from IBM, is it a big reach > to also include AIX in the mix? >  > /cc @mhdawson https://github.com/mhdawson @jasnell > https://github.com/jasnell >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/237. 
mhdawson		I've been trying hard to get AIX machines.  The problem is that it has not been common practice up to this point to provide AIX machines to OS projects so there is no existing mechanism/pattern to draw on.   I've been talking to different groups within IBM to see if I can find one that will be able to fund/provide the machines.  There is support for the concept but the best I've been able to do so far would likely result in machines in the first half of next year.  I'm still pushing to see if we can find a more immediate solution.   
jbergstroem		I think its great that we're getting improved coverage and test runs for AIX but the lack of third party review is unfortunate. 
mhdawson		Internal discussions (at IBM) have landed on a plan which would hopefully end up with machines being available end of Jan timeframe.  I'm still working to see if we can find any options to bridge the gap between now and then. 
mhdawson		Ok so still working on the long term solution to get a machine to osuosl that can host AIX VM's, however the AIX team has provided me with a temporary vm (1-2 month) through siteox that I can connect up and allow people to log into in cases where they want to run/test on AIX.  Will start looking at getting it hooked up. 
mhdawson		PR to add changes for ansible config https://github.com/nodejs/build/pull/323 Running builds on machine which has been added: https://ci.nodejs.org/job/node-test-commit-aix/nodes=aix61-ppc64/33/console  This is a temporary machine until we get longer term resources from osuosl and there are a few issues  1) smallish machine.    2) I had to limit build to 1 cpu because otherwise I saw failures with "directly already exists" in the v8 part of the build.  One suggestion was there may be some code doing a mkdir -p with out allowing for the already existing case.  I'll have to investigate this one further as our internal (IBM) builds don't suffer from this although we don't use the same run-ci target.  I don't seem to have ssh keys working yet.  So far I have done ssh into the box using the password provided.  It is some kind of shared resource at siteox.com but not sure if that is related (we do have to use a specific ssh port and each server seems to get its own port). - **Fixed**  3) I don't have ccache enabled.  Will have to see what you need to do for that on AIX.   4) the combination of a few of the above means that the build currently takes ~ 1hr 49 min so we can't add to the standard regression test.  I'll look at setting up a daily run for how. (Done)  5) There are some failures in the CI run, I have opened new issues for them.    At this point I think we are at least in the state where we can give people access to the machine to debug/review when necessary.  I don't necessarily want try and resolve all of these until we get the full h/w horsepower from osuosl (which is waiting on us getting them a new AIX box).  I'd suggest we close this issue and I can open a new one to track completion on the issues mentioned. 
mhdawson		Closing let me know if anybody else disagrees 
mhdawson		Ideally we'd migrate ourselves so that we can check if there any issues.  I'm coping with a family matter and had already booked off on holiday next week so I'm not sure I'll be able to do it between now Thursday but will do my best, (I'm off starting Friday).  
mhdawson		Additional affected hosts:  Customer Identification: SoftLayer Internal - Node.js Community Account (729957) Start Date: Wednesday 02-Aug-2017 01:00 UTC End Date: Monday 04-Sep-2017 01:10 UTC Duration: 1 month Event Type: Planned Event Subject: Event 45643101 - VSI Migrations to LTS Hosts  ================================================================= / Event Description / As detailed in our Planned Maintenance - VSI Migrations Event Notification (37820643), IBM Bluemix extended hot patching capabilities for Virtual Server Instances (VSIs) through customer migrations and that effort is now complete. This feature has already been utilized as observed in mid-June with the potential security vulnerability (Event Notification 42787279). The hot patching capability allowed IBM Bluemix to mitigate the potential vulnerability and apply the security patch, without customer impact.   Through months of observation and teamwork with our vendor, our System Engineers and vendor have made several updates to the original release of the hot patch capable operating system. The improved hot patch capable operating system is now considered a Long Term Support (LTS) version and IBM Bluemix will use for several years. Some of the earliest VSIs we migrated landed on the original hot patch capable operating system and in an effort to remain consistent throughout our fleet, we will be migrating these VSIs to host with the LTS operating system.   To expedite the migrations and provide greater flexibility, we are going to allow customers 3 weeks from receipt of this message to conduct self-migrations via the portal. Once the three weeks has concluded (August 22), we will begin mandatory migrations of any remaining VSIs.   As with the previous automatic migrations, we will make reservations for the VSIs and schedule the migrations 7 days in advance. Customers will receive a ticket notification listing the VSIs to be migrated and the date and time of the migration. This will provide customers with an additional 7 days to initiate a self-migration through the portal before the automated migration occurs.   We sincerely apologize in advance for any inconvenience this may cause.  ** To enable ticket subscriptions, open your user profile and scroll to the bottom. In the section titled Subscriptions, select Tickets. **  Please refer to https://knowledgelayer.softlayer.com/faqs/3#7463 for information regarding host migrations.   / Items Associated With This Event / Item ID | Hostname | Public IP | Private IP | Item Type 15327925 | test-softlayer-debian8-x86-1.softlayer.com | 169.44.16.126 | 10.121.36.9 | Virtual Server 15329195 | test-softlayer-centos5-x64-2.softlayer.com | 173.193.25.109 | 10.37.83.150 | Virtual Server 16320983 | infra-softlayer-ubuntu14-x64-1.softlayer.com | 169.44.16.104 | 10.121.36.28 | Virtual Server  / Event Updates /   ================================================================= The times above are shown in UTC/GMT. Please visit http://www.timeanddate.com/worldclock/converter.html?iso=20170802T0100&p1=0 to convert start time of this event to your local time.  IBM Bluemix (formerly SoftLayer) sent you this email because your contact address is linked to an account in our customer database. This email is used exclusively as our channel for notifying you about critical issues that may affect your service, and for important company news that could affect your business.  You are receiving this note because the message above directly concerns one of your accounts. If you do not wish to receive these important notifications at this address, please update your contact information or notification subscriptions in the customer portal.  If you have received this email in error, or you are concerned about suspicious activity concerning your account, please contact security@softlayer.com. IBM Bluemix | 14001 North Dallas Tollway Suite M100 Dallas, TX 75240 | +1-866-325-0045 
attritionorg		Is this in reference to CVE-2017-11499? If so, why is there no reference to the CVE ID, the Node.js announcement, or any other cross-reference?
bnoordhuis		@attritionorg No (edit: as in: completely unrelated.)
maclover7		ping @mhdawson -- did this ever happen?
mhdawson		Long past, closing.
mhdawson		Have a replacement machine now for this one, will close this once I bring this one offline or remove it. 
mhdawson		ok, renamed machines and delete from jenkins.   
rvagg		Also FYI I discovered that pi1p-4 and pi1p-5 didn't have their NFS shares mounted so they were working on local caches only, this could explain some recent slowness; fixed now 
rvagg		left out the symlinks for /usr/lib/ccache so my test build has been doing it raw, ooops 
orangemocha		<3  Going to look at the next few builds to see if things get better.  
rvagg		I'm suspecting that my config tweaks are basically invalidating the existing cache so it might take some time 
jbergstroem		Did the earlier version not work as intended? I haven't really tinkered too much with ccache options but as long as it "works" I'm happy with anything.  How about using the generic ccache installer I created a while ago? It lives in `ansible-tasks/ccache.yaml` -- feel free to suggest improvements such as adding a stub so we can specify options (guessing you're adding to `~/.ccache`)? 
joaocgreis		The ccache installer by @jbergstroem looks good, please ignore my comments above if you decide to use it. 
rvagg		aw crap, sorry @jbergstroem, your ccache.yaml looks really good, can you point me to a use of it so I can integrate here instead of my hax please? 
rvagg		baaaaah!   ```        sudo ln -sf $(which gcc) /usr/lib/ccache/gcc && \        sudo ln -sf $(which cc) /usr/lib/ccache/cc && \        sudo ln -sf $(which g++) /usr/lib/ccache/g++ && \        sudo ln -sf $(which c++) /usr/lib/ccache/c++ && \  ```  In other words, linke `gcc`, `cc`, `g++`, `c++` to themselvs, not `ccache`, hence no ccache since this was changed, which is why the Pi's have been exceptionally slow recently .. I've fixed and will revisit this and use @jbergstroem's ansible module instead for ccache. I'll put a few runs through to warm up the cache again and let's monitor to see if we're back to decent speeds. 
rvagg		might be a bit picking and this is just a suggestion, but maybe "powers" isn't the best name here, "access" or similar might be a bit more .. gentle?  also, who do those links to the team members work for? what level of access does someone need to be able to see who is in jenkins-admins for example and do we care for example if non-collaborators can't see that?
mcollina		üëç on access.  Also, is there a crash course on the infra or some documents I can read. According to this doc, I have access to the infrastructure secrets and keys. However, there is little guidance in the CTC onboarding process on those.  Moreover, is there a process to _revoke_ access? In other terms, how do we rotate keys?
piccoloaiutante		I like `access` or even `infra-access.md`.  @rvagg it seems to me that in order to see`jenkins-admins` you need to be at least a `member` of the org. I think that having jenkins-admins not seen by non-collaborators isn't a big issue, as if a new comer needs to contact someone in the team can always ask in issues or irc.  @mcollina I got onboarded by @jbergstroem and @joaocgreis. Maybe would be a good idea to arrange one with you.
gibfahn		>also, who do those links to the team members work for? what level of access does someone need to be able to see who is in jenkins-admins for example and do we care for example if non-collaborators can't see that?  @rvagg They only work for org members. It would be great if they were public, but that's a Github thing, so there's not much we can do. The only question is whether it's better to have them or not. I'd say it's better to have them. I can add a sentence explaining that if it'd be helpful.  _**EDIT:**_ I added an explanatory sentence and emailed Github to ask if it could be made public, I think that's about all that can be done.  <br/>  >Also, is there a crash course on the infra or some documents I can read.   @mcollina This is my first attempt at having build WG documentation. I figured we needed to work out what we actually have before we talk about how to do stuff.   _**EDIT:**_ I should clarify that there is a tonne of really good technical documentation in [ansible/](https://github.com/nodejs/build/tree/master/ansible), which @jbergstroem added. However to understand how to use that you already have to know quite a lot of stuff.  >According to this doc, I have access to the infrastructure secrets and keys. However, there is little guidance in the CTC onboarding process on those.  You have access to the secrets repo, but not to the gpg encrypted secrets within. Access is documented in that repo, which I think is the best place for it. If it's inadequate then we should add to that repo's docs. I'll add a sentence saying that.  FWIW I don't think it's particularly intentional that org owners have access, it's just how Github works. You won't actually be able to decrypt any of the files inside it. 
mcollina		EDIT: I understand now. It needs to be reworded and explained. Having access to the secrets repo means nothing, but given it is under "Machine Access" heading it seems to refer to machine access, not to the secrets repo.
gibfahn		>For a list of machines, see the [inventory.yml][]. Access is controlled through the [secrets repo][], which [@nodejs/build][] (and [org owners][]) have access to.  You have access to the _repo_, just not to the individually encrypted secrets within. I've tried to clarify, does it make more sense? If not please suggest some text (I don't want to get too verbose here).
mcollina		How about  > Access to the secrets repo does not imply access to the machines themselves.
gibfahn		I changed `powers.md` to `access.md`. I quite liked `powers`, reminded me of superpowers (which is apt, as with other superpowers, it usually means you have to fix stuff at inconvenient hours etc.) But `access` is fine too (just bland).
gibfahn		@mcollina okay I pretty much added that, PTAL.
mhdawson		In terms of employer, I don't think we want to add that as its not necessarily tied to why/who has access.  Having is discover-able through the github profile is good though.
gibfahn		>Generally LGTM. Thought: do we want to add employer or is that perhaps better expressed through a GitHub profile? A few of us have "keys to the kingdom" of sorts.  I agree with @mhdawson that adding the employer gives the wrong impression, people don't have access because of their employer. At the same time I think we should definitely be open about who we work for. Making sure everyone has it clearly documented in their Github profile would be good.   Github allows you to fill in the company field in https://github.com/settings/profile, maybe we should encourage everyone to fill it in.  ![image](https://user-images.githubusercontent.com/15943089/28968044-fc5a63d2-7915-11e7-9abd-5bd2302d89ad.png)  ![image](https://user-images.githubusercontent.com/15943089/28970725-79ccee24-7921-11e7-80ad-1fb4823d4184.png) 
gibfahn		@rvagg I'd like to land this, any objections?  Answers to the questions in my initial comment would be great too!
jbergstroem		No objections to landing as-is. Let me try and answer other outstanding questions.  > @gibfahn said: > @bnoordhuis has read access to nodejs/secrets, as he's an org owner I assume this is no longer necessary?  I would agree. He was historically very active in part of the creation of `io.js` and the general structure of all repos. I would guess this is the remnant of that.  > @gibfahn said: > what about website access? Is there any special access that people have that we should document?  Guessing you're referring to updating the website. This is handled through the website group in this repo: https://github.com/nodejs/nodejs.org as well as work that is done and landed through the build team for building those assets.
gibfahn		Landed in efb11cc08574026dda0d0eeb45007bccf6f36a17 ef1f86315886b115e9536fa8e3578318ed21546f
Fishrock123		Looks like this caused the CI to get a bit funny? Is this only on the CI's end, or also core's? 
Fishrock123		Ohh, looks like this was something else: https://github.com/iojs/io.js/issues/1527 
jbergstroem		Going on a limb here and closing: FIXED 
gibfahn		This node-gyp issue seems related: https://github.com/nodejs/node-gyp/issues/1100
Trott		Still something we'd like to do, I imagine? Should we put this on the agenda for a meeting or something to try to get some traction? Maybe this is an opportunity for someone to be a mentor to an eager individual via the mentorship program that's just getting started?
lrvick		My org is very interested in this. I have done reproducible build work for other projects, and even a list of known issues would go a long way to help me know what path to take.
refack		Hello @lrvick, would you be willing to help us and break down the items required for us to achieve this? That would be a great help, and very much appreciated.
rvagg		@nodejs/testing (update above on some build infra matters that may be interesting) FYI I've been messing with my NAS today and it's been having some nasty disk troubles which has impacted on the NFS exports that all the Pi's are using. There's going to be a lot of failures today on the Pi's that can be written off to this. They _should_ have settled down (assuming my NAS is in proper order again, no guarantee) now and be working like normal. If Pi failures persist though I may need to dig in and do some bulk restarting/remounting/re-somethinging, ping me if you're seeing anything that's not related to standard test failures from tomorrow onward.
jasnell		@rvagg ... Thank you for the update. I've been wondering about the possibility of convincing nearForm to work on setting up a redundant pi cluster to help both with reliability, performance, and just reducing general bus factor. I haven't looked around on the infra side lately so forgive me if I've missed it, but do you have any write-up on how the pi cluster is built, what components are necessary, and how things are configured/managed?
rvagg		@jasnell no, I haven't done a writeup on the specifics but could provide specifics as needed.  If there's staffing and required capacity there to actually manage a small cluster then it may make sense for me to ship over a chunk of this cluster, maybe half (including power and a network switch), so we have proper redundancy. Unfortunately it's not a trivial commitment and needs someone to become a minor expert in monitoring and maintaining these things when they go awry. I could provide full instructions for how I do it, but it's not essential that it's set up exactly the same way.  Some things it'd need:  * Space, obviously * A network with enough spare IPs, a private NAT would be ideal, I like to keep the cluster network isolated from my private network because a fairly wide group of people have access (build/test) but also the code that gets tested isn't strictly safe (we ask collaborators to tick "certify safe" as an acknowledgement that a _contributor_ could slip in malicious code that could mess with our cluster) * A way to provide external access to the machines‚ÄîI have a computer here that acts as a jump host that can be used by the build/test team, so everyone's SSH configs make a connection there and then jump in to the internal host by internal IP. An alternative might be to provide specific ports on an externally exposed router that each go to one of the machines. * A host that can expose some NFS disk. This is probably going to be the trickiest bit. I have a NAS host that exposes an SSD to all of the Pi's that they share for their own workspace and a shared ccache directory (the NAS and SSD are mine, not technically part of the Foundation's hardware). The Pi's don't have enough storage on them to be reliable and the NFS disk is (I think) slightly faster than a local SD Card. Having a shared space is also helpful for ccache (not actually used much since we cross-compile for tests) and for providing shared access to a git repo that's used to download binaries from the cross-compiler.  In terms of people-power, it needs someone who can understand how it all works, can monitor the cluster for failures, can wipe and reimage SD cards, replace ones that look dodgy, go through the manual steps to prepare hosts for running ansible against ([here](https://github.com/nodejs/build/tree/master/setup/raspberry-pi#raspbian)). We'd probably need to make sure this person was part of our build/test group so they have enough access to do this all themselves.
joaocgreis		@rvagg is https://github.com/nodejs/build/blob/master/ansible/inventory.yml up to date? It's not great that we have to keep two ansible inventories, but the one under `ansible/` should be kept up to date because it's used to generate the ssh config file (and already does that quite well - it's very straightforward to update with it). The one under `setup/` should perhaps only have the machines that are still deployed using the old scripts.
rvagg		Good question! So ansible/inventory.yml is up to date. I've been very strict with my use of it and have been pushing updates to it whenever I provision a new host (not using PRs, I hope nobody objects to pushes changing IP addresses). On the other hand setup/ansible-inventory.yml is woefully out of date _except_ for the raspberry pi's, I keep that updated properly (not that I needs regular changing). I don't believe it's used for anything else, although I could imagine that there are things not properly in ansible/ where it's relevant, like the website. At this stage I'm not quite sure what to do about it because if we clean it up then maybe we should be cleaning up all of the directories in setup/ that we're no longer using too? I'm deploying all other standard hosts from that new config now and it's working great and I know the old setup/ ones are getting out of date (e.g. see the recent EPEL PR from me that's to ansible/ not setup/).  I suppose that I'm currently in the best position to make a first-pass call on what's redundant now since I've been doing a lot of the standard provisions lately. I'll put it on my list. 
gibfahn		>At this stage I'm not quite sure what to do about it because if we clean it up then maybe we should be cleaning up all of the directories in setup/ that we're no longer using too?  Yes we definitely should, :100: to that.
mcollina		> In terms of people-power, it needs someone who can understand how it all works, can monitor the cluster for failures, can wipe and reimage SD cards, replace ones that look dodgy, go through the manual steps to prepare hosts for running ansible against (here). We'd probably need to make sure this person was part of our build/test group so they have enough access to do this all themselves.  The major problem is people the nearForm side, and providing a quick enough response time. It seems the Pi cluster needs more care than a bunch of normal servers in term of maintenance, so it might be a blocker. We'll see what we can do. (For everyone, I am remote and not located at the office either).
jasnell		Yep, this is the main reason I was asking about documenting the process more. In order to make a decision about whether it is possible for us to set up a redundant cluster we need to get an idea of the resource commitment required. We may need to put out a call to Node.js Foundation member organizations to sponsor the effort by offering resources.
joaocgreis		> I don't think I've ever seen msft-serialport-win1 online and I'm not sure who was working with it so perhaps we should remove it.  Ref: https://github.com/EmergingTechnologyAdvisors/node-serialport/issues/879  @munyirik do you still want to bring this server back online or can we delete the configuration? (I can make a backup of the job config somewhere)
munyirik		@joaocgreis you can delete it
refack		Strange configuration on `ubuntu1604-arm64_odroid_c2` https://ci.nodejs.org/job/node-test-commit-arm/11807/nodes=ubuntu1604-arm64_odroid_c2/console (tests run with `-j 0`) ```Console Regular expression run condition: Expression=[linux-gnu], Label=[linux-gnu] Run condition [Regular expression match] enabling perform for step [BuilderChain] [ubuntu1604-arm64_odroid_c2] $ /bin/sh -xe /tmp/hudson6207292464805124188.sh + test true = true + FLAKY_TESTS_MODE=dontcare + echo FLAKY_TESTS_MODE=dontcare FLAKY_TESTS_MODE=dontcare + TEST_CI_ARGS= + echo TEST_CI_ARGS: TEST_CI_ARGS: + NODE_TEST_DIR=/home/iojs/node-tmp PYTHON=python FLAKY_TESTS=dontcare TEST_CI_ARGS= make run-ci -j 0 ```
orangemocha		Sounds like a good idea. Are you looking for something like node-test-pull-request / node-test-commit to test changes in this repo before they are merged? 
bnoordhuis		That would be ideal, yes. 
orangemocha		I'll add it to my queue. If someone else wants to pick it up, be welcome :wink:   I am assuming it would make sense to run on all the core support platforms (see https://github.com/nodejs/build#test-configurations).   This new CI job would re-use the current Jenkins slaves. In order to avoid port conflicts / interfering node instances, it would have to run exclusively from other node-test-\* jobs, so it will essentially be competing for those slaves. I am assuming that it won't be an issue, as we don't get too many pull requests on this repo. 
bnoordhuis		I don't think so either but FWIW, the tests don't start any servers and I don't think they ever will, so it's probably fine to run them in parallel with other jobs. 
bnoordhuis		Friendly ping.  Has there been any progress on this?  There are pull requests I'd like to land that are held up by this. 
jbergstroem		How do you see this work? Build node similar to how we do in test-pr, pull node-gyp and test? 
bnoordhuis		I don't think there's a need to build node but it could be valuable to test node-gyp against a range of node release binaries on different platforms.  The current test suite mostly consists of self-tests, it doesn't try to build any add-ons (yet - that's probably the next step.) 
orangemocha		I haven't made any progress, sorry. Let's bump it up.  I see a few ways to go about this: 1. A job that can test any ref on the node-gyp repo. Instead of including the logic for building node, it downloads node from https://nodejs.org/dist/ with a version parameter. This can be run on a matrix of a handful of platforms. This is similar to what we did for https://ci.nodejs.org/job/serialport-test-commit-windows/build?delay=0sec, though that job is only limited to Windows. (Implementation note: perhaps we can reuse the rebase job and the temp repo approach). 2. Extend node-test-commit to take an optional ref from the node-gyp repo. In the beginning of the job, pull the ref and overwrite what's in the node tree. Then proceed to build node as usual. Add node-gyp tests to node-test-commit. This probably adds unnecessary complexity to the jobs. 3. Start directing node-gyp changes to the node repo directly. I think we have already accepted the fact that the only supported version of node-gyp is the one that ships with node, so it does make sense to test the node/node-gyp combo from the same branch. However this introduces additional PR noise in the node repo. 4. Similar to 3, but with submodules/subrepos. I wish we had a better way to manage our dependencies with a sub-module like approach. But I am just rambling now...  So I think option 1. is the most practical at the moment. @joaocgreis is there a better way? 
orangemocha		This was discussed at the last WG meeting. The plan is to do something similar to  https://ci.nodejs.org/job/serialport-test-commit-windows/build?delay=0sec but to have it run on multiple versions of node rather than a specific version. The node versions and platforms would be part of the job's matrix.  A similar issue was raised about NAN, even though I can't find a specific issue about that. We agreed at the WG meeting that the same job structure would apply to NAN.  And once we have figured out how to run node-gyp and NAN tests in CI, we might want to add (part of) those tests to node-test-commit, to prevent breakage due to changes in node.  This is all still on my and @joaocgreis' queue. Of course if anyone wants to make progress on it sooner we won't complain :wink:  
bnoordhuis		@joaocgreis @orangemocha Have you had time to look into this? 
rvagg		do we really have enough tests to make this worthwhile? 
bnoordhuis		Yes.  It's steadily growing. 
joaocgreis		Finally got the ball rolling on this one, sorry for the delay.  @bnoordhuis can you take a look and try https://ci.nodejs.org/view/All/job/nodegyp-test-commit/ ? Only two platforms now, but the job should do the right thing. What would be the next priority? - Add more platforms, all as configurations in the same job? (much easier to check results IMO) - Add more platforms with subjobs, as `node-test-commit`? (allows launching only one subjob) - Add tap parser? (Is there a way to make `npm test` produce a `test.tap` file?) - Something else? 
bnoordhuis		@joaocgreis Thanks, looks great.  > Add more platforms, all as configurations in the same job?  Sounds good.  I don't expect sub-jobs is something we'll need.  > Add tap parser? (Is there a way to make npm test produce a test.tap file?)  tape produces tap by default but npm seems to swallow the output.  If I run `node node_modules/.bin/tape test/test-*`, it works. 
joaocgreis		Added https://ci.nodejs.org/job/nodegyp-test-pull-request/ to spawn nodegyp-test-commit with multiple versions of node, but for now the versions still have to be manually updated in the job config and there is still no rebase support.
maclover7		Looks like the job has been created, and is working well so far. Please open up a new issue if additional work is needed (more platforms, etc.)
refack		Ref: https://github.com/nodejs/build/issues/1294
targos		`-std=gnu++14` would be even better and Clang supports it since version 3.4: https://clang.llvm.org/cxx_status.html
targos		Ref: https://github.com/nodejs/node/commit/e43c109891d  I'm going to try and change this option to see if CI can pass.
targos		CI: https://ci.nodejs.org/job/node-test-commit/13446/  I already see that LinuxONE does not support it.
targos		/cc @nodejs/build and @nodejs/v8 
gibfahn		cc/ @joransiu @jbarz 
targos		So, by looking at [this build](https://ci.nodejs.org/job/node-cross-compile/11850/label=cc-armv6/console), I see that we are still compiling with GCC 4.8. but we officially [stopped to support it for Node 8](https://github.com/nodejs/node/commit/d13a65ad68c429884434cdcd94c52b79a6d69717).  New CI with `-std=gnu++11`: https://ci.nodejs.org/job/node-test-commit/13447/  Edit: GCC 4.8 does not support C++14. I don't know if 4.9 does.
targos		/cc @bnoordhuis
refack		RE migration to a newer version: we're trying to balance the new requirements (GCC 4.9 for example is a "soft" requirement) and still keeping the old configuration for LTS node versions. Refs: https://github.com/nodejs/build/pull/809 Refs: https://github.com/nodejs/build/pull/797  1) Some old platforms (CentOS5 & 6) don't have well supported binaries of newer GCC.  2) changing the global GCC changes the global `stdlib++` which requires the users to upgrade, so we're trying figure out way to either have it side-by-side, or have multiple build-bots 3) getting prebuilt toolschains for 32bit environments is also a challenge. https://github.com/nodejs/build/issues/885
targos		From https://chromium-review.googlesource.com/c/v8/v8/+/728026#message-697b6bb5ae15281bdd7c9b457aa75b75ef8a14ea:  > C++14 is generally allowed in chromium (https://chromium-cpp.appspot.com/), hence also in V8. The only restriction is that it has to compile in GCC 4.8 (and all the other compilers we use on the bots). GCC 4.8 only supports C++11, so all code used by this compiler has to compile with c++11.  If CI is green, I'll start a new run with `-std=gnu++11` with the `canary` branch. If it is still green, I'll submit a PR to core.
targos		Canary build: https://ci.nodejs.org/view/MyJobs/job/node-test-commit-node-v8/193/  Edit: that obviously does not work :(
targos		It's looking good on osx with `-std=gnu++14`.  Here is a proposition that should work: - Build with `-std=gnu++11` in GCC - Build with `-std=gnu++14` in LLVM (including osx and freebsd)  @refack do you think it's possible to modify `common.gypi` do achieve that?
refack		> @refack do you think it's possible to modify common.gypi do achieve that?  Yep, I'll make a PR
refack		https://github.com/refack/node/compare/master...higher-c++-ver-by-default  https://ci.nodejs.org/job/node-test-commit/13465/  For now I see that `freebsd10` is not happy - `error: invalid value 'c++14' in '-std=c++14'`
bnoordhuis		> changing the global GCC changes the global stdlib++  Right, that might be a problem.  _Technically_ it already matters if you compile with 4.8.2 or 4.8.3 (libstdc++ 6.0.18 vs. 6.0.19; 4.9 is 6.0.20) but yeah.
fhinkel		I know this is closed already, but I would very much like it if we could go up to GCC 4.9. We can't use a bunch of C++14 feature in V8, because then Node wouldn't compile anymore with 4.8.
targos		@fhinkel do you mind opening a new issue? IMO It's important that we arrive to a conclusion before the release of Node 10.
mhdawson		I've set myself a calendar entry to remove LinuxOne from the main regression runs for the period of the outage.  Also asking if there  any more details to understand they will really all be out for the full period.
mhdawson		Disabled the linuxOne job in node-test-commit.  The rest of the jobs I'll just cancel as they don't run that frequently.
mhdawson		Machines seem to be back online so optimistically adding them back into the jobs even though the maintenance window is not over until tomorrow.
mhdawson		Ok outage time has passed, machines up and running, and all recent jobs have been ok. Closing.
jbergstroem		LGTM 
mhdawson		Landed as 74fa6c2e43d06177ca7e8cf5dcb725623c04ea5f 
Trott		The Testing WG charter needs to be ratified by @nodejs/build to take effect, so maybe that can be on the agenda? https://github.com/nodejs/node/pull/5461 
Trott		Also, just to confirm: The U.S. goes to daylight savings time this weekend. This meeting is fixed UTC time so the time of the meeting will change by one hour for folks in the U.S.? 
orangemocha		Yes, the meeting time is fixed with respect with UTC. Thanks for the heads up though. 
orangemocha		If there is time, I would like to discuss the shared account management for the Azure subscription. 
jbergstroem		I'd also like to discuss per-job acl at ci.nodejs.org.  
rvagg		ARM additions, do we want a Pi3 cluster? Also Pi1B+ donations needed.  OSX donations & hosting, not expecting a resolution but let's talk options that are being worked on so far. 
jbergstroem		Something me/michael discussed that we might be able to resolve: do we want a benchmark security group (share access to machines)? 
orangemocha		Withdrawing the Azure account management from the agenda, as the internal discussion has not been resolved yet. 
mhdawson		One more, ok with my proposed download page template ?  
rmg		I was really hoping to get back into things this week, but it seems like I'll have to miss today's call :-( 
orangemocha		Minutes doc: https://docs.google.com/document/d/1kwN7aNBrF257yxBaSYtQyerxXZnfmu7hOjdJiAFKqbo/edit?usp=sharing 
Starefossen		I won't be able to attend this meeting either. Hoped to have more updates on the Alpine build, but I don't. 
Starefossen		Meeting was held. Closing. 
rvagg		sorry @mhdawson, didn't know we were onto a local `ansible` branch, can you point me to discussion or PR or something about that? I was still tracking jbergstroem/feature/refactor-the-world which is a bit out of date so I haven't used it.
rvagg		oh, it's not a branch, it's a whole directory! ok .. will look into that
mhdawson		@rvagg its still a work in progress.  We agreed at the vm summit in Berlin (with @jbergstroem via remote) to land the refactor and then move platform by platform over to it as we had time to validate.  
rvagg		OK, that's great! I might ammend this PR to include the new work, that should give me a chance to have some proper hands-on experience with it. 
refack		Cross-Ref: https://github.com/nodejs/citgm/issues/423
refack		PR opened: nodejs/citgm#424 I'm keeping this open until confirmed fix.
mhdawson		Looks like nodejs/citgm#424 landed.  Closing.  Please re-open if thats not the right thing to do.
refack		üëç Thanks. https://github.com/nodejs/citgm/pull/424 solved this. General issue covered by https://github.com/nodejs/citgm/issues/426
gibfahn		My preference would be for a gradual roll-out:  1. Enable the flag on Jenkins, make sure nothing goes horribly wrong 2. Enable it for all jobs for nodejs/build, leave that for a while and see if people notice secret things 3. Expand to core collaborators 4. Expand to other teams (like nodejs/automation)  Not sure we ever _need_ to make it world visible.
maclover7		Interesting idea! Big +1 to gradual rollout... should probably start with build wg itself, then tsc, then collaborators, and then forward from there?
rvagg		re https://github.com/nodejs/build/issues/419:  > perhaps we could make a subset of nodejs/Collaborators for this kind of thing, nodejs/Collaborators is very broad and it might be best if we restrict it to just people that are interested enough in doing this kind of work; like a nodejs/jenkins-job-config or something that doesn't give full access? maybe we can talk about this at our next meeting.  I like enabling more read access but we should also look at the write access and how we deal with that. Can't bring GitHub-based pipeline builds soon enough to solve these problems.
gibfahn		I think we should go with a policy of "all new jobs are stored in source control" (whether the whole pipeline or just the shell script parts).  +1 on the idea of expanding write access. Perhaps we could start by giving nodejs/build write access, it's a bit odd currently that you have to get admin access to get write access to jobs.
gdams		SGTM
rvagg		Well .... It would be possible to store all the config.xml files in GitHub and allow them to be edited here and automatically pushed to Jenkins using the same mechanism that we use to update the website (and the bot I believe). But then we'd have to make it two-way so if you edit it in the interface then it gets pushed back here. Not impossible and it would certainly be nice to have our configs stored here. Editing them in XML wouldn't be pleasant though, that'd be the main drawback.   If you're going to open access to jobs, just do a quick look through them first to make sure we don't have any secrets stored in the scripts. I can't think of any off the top of my head but just in case! Also if you could confirm that you can't get access to any of the Jenkins secrets when you do this. Particularly the ssh keys that are available in dropdowns and the worker secret keys.  Also, we are probably going to have a separate SSH key to access node-private. This should not be selectable for any jobs that don't need it. If any Collab can select it from a dropdown in any job config then we're potentially exposing things we shouldn't.
gibfahn		Update: @gdams enabled this on the adoptopenjdk Jenkins instance (see https://github.com/AdoptOpenJDK/openjdk-infrastructure/issues/40#issuecomment-346212177) and it seems okay there.
rvagg		Looks good on OpenJDK, really it's just missing the buttons at the bottom (and I hope the backend support if you were to fake it!)
mhdawson		Removing from build agenda @gibfahn can you define next steps and then add back if/when needed.
refack		I found two plugins that fit this: 1. [Extended Read Permission](https://wiki.jenkins.io/display/JENKINS/Extended+Read+Permission+Plugin), allows us to enable read only view to job configs 2. [SCM Sync configuration](https://wiki.jenkins.io/display/JENKINS/SCM+Sync+configuration+plugin) - automagicly synchs the config directory with an SCM repo of our choosing  For (1) we just need to flip the switch in the auth matrix For (2) someone with `infra` access (@rvagg ?) needs to init a git repository in `/var/lib/jenkins/` and add/ignore the right files, and setup push access (AFACT it's all at the local `.git` level)
rvagg		How about we try the read-only permissions for logged in users (maybe not anonymous?) and add that extended read permissions to expose more. Do you want to experiment with that @refack? I'm +1 on trying it out at least.  I guess the SCM sync thing would be helpful too, it'd certainly help us by providing us with a backup in case of catastrophic failure! I'm just not sure what the potential for secrets leakage is here so it might not be appropriate for sharing in a public repo?
refack		> How about we try the read-only permissions for logged in users (maybe not anonymous?) and add that extended read permissions to expose more. Do you want to experiment with that refack? I'm +1 on trying it out at least.  Done.  On that note, I figured out that is we disabled "Anonymous Users" access (like in ci-release) it forces Jenkins to refresh its auth cookie, eliminating the need to click "Log in". Can we live with that &mdash; require an active GitHub login to access the public CI?
mhdawson		I think its probably ok if you need a github login.
gibfahn		>Can we live with that ‚Äî require an active GitHub login to access the public CI?  QoL improvement for everyone who uses ci.nodejs.org, üíØ on that.
refack		New baseline matrix: ![image](https://user-images.githubusercontent.com/96947/41599619-bab35d44-73a1-11e8-8208-5b16cf0a0526.png)  <details> <summary>and in XML form:</summary>  ```xml authorizationStrategy class="hudson.security.ProjectMatrixAuthorizationStrategy">     <permission>com.cloudbees.plugins.credentials.CredentialsProvider.Create:nodejs*jenkins-admins</permission>     <permission>com.cloudbees.plugins.credentials.CredentialsProvider.Delete:nodejs*jenkins-admins</permission>     <permission>com.cloudbees.plugins.credentials.CredentialsProvider.ManageDomains:nodejs*jenkins-admins</permission>     <permission>com.cloudbees.plugins.credentials.CredentialsProvider.Update:nodejs*jenkins-admins</permission>     <permission>com.cloudbees.plugins.credentials.CredentialsProvider.View:nodejs*jenkins-admins</permission>     <permission>hudson.model.Computer.Build:nodejs*build</permission>     <permission>hudson.model.Computer.Build:nodejs*jenkins-admins</permission>     <permission>hudson.model.Computer.Configure:nodejs*build</permission>     <permission>hudson.model.Computer.Configure:nodejs*jenkins-admins</permission>     <permission>hudson.model.Computer.Connect:nodejs*build</permission>     <permission>hudson.model.Computer.Connect:nodejs*jenkins-admins</permission>     <permission>hudson.model.Computer.Create:nodejs*build</permission>     <permission>hudson.model.Computer.Create:nodejs*jenkins-admins</permission>     <permission>hudson.model.Computer.Delete:nodejs*build</permission>     <permission>hudson.model.Computer.Delete:nodejs*jenkins-admins</permission>     <permission>hudson.model.Computer.Disconnect:nodejs*build</permission>     <permission>hudson.model.Computer.Disconnect:nodejs*jenkins-admins</permission>     <permission>hudson.model.Hudson.Administer:nodejs*jenkins-admins</permission>     <permission>hudson.model.Hudson.Read:authenticated</permission>     <permission>hudson.model.Hudson.Read:nodejs*build</permission>     <permission>hudson.model.Hudson.Read:nodejs*Collaborators</permission>     <permission>hudson.model.Hudson.Read:nodejs*jenkins-admins</permission>     <permission>hudson.model.Hudson.Read:nodejs*node-chakracore</permission>     <permission>hudson.model.Hudson.Read:nodejs*releasers</permission>     <permission>hudson.model.Item.Build:nodejs*build</permission>     <permission>hudson.model.Item.Build:nodejs*Collaborators</permission>     <permission>hudson.model.Item.Build:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Build:nodejs*node-chakracore</permission>     <permission>hudson.model.Item.Build:nodejs*releasers</permission>     <permission>hudson.model.Item.Cancel:nodejs*build</permission>     <permission>hudson.model.Item.Cancel:nodejs*Collaborators</permission>     <permission>hudson.model.Item.Cancel:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Cancel:nodejs*node-chakracore</permission>     <permission>hudson.model.Item.Cancel:nodejs*releasers</permission>     <permission>hudson.model.Item.Configure:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Create:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Delete:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Discover:nodejs*build</permission>     <permission>hudson.model.Item.Discover:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.ExtendedRead:authenticated</permission>     <permission>hudson.model.Item.ExtendedRead:nodejs*build</permission>     <permission>hudson.model.Item.ExtendedRead:nodejs*Collaborators</permission>     <permission>hudson.model.Item.ExtendedRead:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.ExtendedRead:nodejs*node-chakracore</permission>     <permission>hudson.model.Item.ExtendedRead:nodejs*releasers</permission>     <permission>hudson.model.Item.Move:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Read:authenticated</permission>     <permission>hudson.model.Item.Read:nodejs*build</permission>     <permission>hudson.model.Item.Read:nodejs*Collaborators</permission>     <permission>hudson.model.Item.Read:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Read:nodejs*node-chakracore</permission>     <permission>hudson.model.Item.Read:nodejs*releasers</permission>     <permission>hudson.model.Item.Workspace:nodejs*build</permission>     <permission>hudson.model.Item.Workspace:nodejs*Collaborators</permission>     <permission>hudson.model.Item.Workspace:nodejs*jenkins-admins</permission>     <permission>hudson.model.Item.Workspace:nodejs*node-chakracore</permission>     <permission>hudson.model.Item.Workspace:nodejs*releasers</permission>     <permission>hudson.model.Run.Delete:nodejs*jenkins-admins</permission>     <permission>hudson.model.Run.Replay:nodejs*jenkins-admins</permission>     <permission>hudson.model.Run.Update:nodejs*jenkins-admins</permission>     <permission>hudson.model.View.Configure:nodejs*build</permission>     <permission>hudson.model.View.Configure:nodejs*jenkins-admins</permission>     <permission>hudson.model.View.Create:nodejs*build</permission>     <permission>hudson.model.View.Create:nodejs*jenkins-admins</permission>     <permission>hudson.model.View.Delete:nodejs*build</permission>     <permission>hudson.model.View.Delete:nodejs*jenkins-admins</permission>     <permission>hudson.model.View.Read:authenticated</permission>     <permission>hudson.model.View.Read:nodejs*build</permission>     <permission>hudson.model.View.Read:nodejs*Collaborators</permission>     <permission>hudson.model.View.Read:nodejs*jenkins-admins</permission>     <permission>hudson.model.View.Read:nodejs*node-chakracore</permission>     <permission>hudson.model.View.Read:nodejs*releasers</permission>     <permission>hudson.scm.SCM.Tag:nodejs*jenkins-admins</permission>   </authorizationStrategy>   <securityRealm class="org.jenkinsci.plugins.GithubSecurityRealm">     <githubWebUri>https://github.com</githubWebUri>     <githubApiUri>https://api.github.com</githubApiUri>     <clientID>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</clientID>     <clientSecret>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</clientSecret>     <oauthScopes>read:org,user:email</oauthScopes>   </securityRealm> ```  </details> 
jbergstroem		LGTM. Perhaps keep this open and add the other floating ip's which is likely to cause issues shortly? 
mhdawson		@jbergstroem  believe we have not lost/fixed all that will happen.  Updated PR to include all that needed updates as well as renaming of machines for le to use the standard naming convention.    Should be good to land now. 
jbergstroem		Lets get it in! 
mhdawson		Landed as 96ba7214cb996a9396b06c1e6eae5484551f2d3f 
MylesBorins		I'm going to be a bit late to today's meeting, if I'm able to make it at all. Sorry for the last minute notice.  Re: tap plugin  TLDR; we need to update the node test runner to report tap using YAMLish for the stack traces. Once that is done the tap -> junit converter should work using an off the shelf module.  I'm also curious where we are on better OSX ci stuff. We've gotten burnt by quite a bit of stuff on more modern OSX than we are currently testing. 
rvagg		live @ https://www.youtube.com/watch?v=qFJh5Wagdhs (link is wrong in OP) 
mhdawson		https://github.com/nodejs/build/pull/502 
mhdawson		Once question is who to change the remote_user properly as what is in the commit is obviously not right for other platforms and I don't think the line trying to do it in the Marist section works.
mhdawson		These are now good enough so that we can run libuv jobs on zOS.  See https://ci.nodejs.org/job/libuv-test-commit-zos/92/  @jbergstroem any suggestions on a better way to change the remote_user ?
jbergstroem		Sorry for the silence. Will review shortly.
mhdawson		@jbergstroem thanks, I think we are about ready to image additional machines and would be great to land before then.
joaocgreis		Some notes: - The `remote_user` issue must be solved before landing, as landing it in its current form will break every other system. Strange that the override in the section does not work. - Is there a mechanism to restart Jenkins if it crashes (monit/...)? This can be done later, but please consider it as it's quite important to keep the server reliably online. - Is `home` needed in the `group_vars` when it is defined in `ansible.cfg`?  Other than that rubber stamp LGTM. I didn't test this but nothing looks wrong.
joaocgreis		@mhdawson does a `user` tag in the inventory also not work? Like this:  https://github.com/nodejs/build/blob/1512c6c1c583e188eb04b1fd880fcc639f8e5d1e/ansible/inventory.yml#L40
mhdawson		@joaocgreis thanks for the comments will rebase and I'll look to see how I can address them.
mhdawson		I've rebased  The suggestion from @joaocgreis allowed me to resolve the remote_user issue with one additional tweak.  Thanks!  I also did some other small clean and addressed an issue I believe would have affected other platforms.  
refack		@mhdawson if you need `GYP` I keep my mirror fairly updated https://github.com/refack/gyp
mhdawson		@refack, I've updated to pull from the ci master were we have setup a mechanism to store artifacts.
mhdawson		@joaocgreis pushed changes to address your latest comments. Please take a look.  I've used it to configure the 2 z/OS machines connected to the CI and the libuv job is running nightly (still red though as we have 2 test failures we are working on)
mhdawson		@joaocgreis @gibfahn any chance you can review so that I can get this landed.
joaocgreis		@mhdawson I left a comment inline above, is the only thing blocking this now for me: https://github.com/nodejs/build/pull/796#discussion_r141757097
mhdawson		@joaocgreis pushed commit to fix your last comment.
gibfahn		Given how much zOS specific stuff we now have in `main.yml`, would it be worth spinning that out into a partial? Not sure how easy that is with ansible, and not a blocking request, more a question.
mhdawson		Point taken about trying to pull the zOS stuff out, but would like that to be a future TODO.  The comment I'll add.  Just going to wait to see if @joaocgreis has any other comments so I can do the final pass together if possible.
joaocgreis		@gibfahn I started looking into moving Windows to `ansible/` and I suspect we should probably split `baselayout` into partials like `bootstrap` already is, with a difference: it can have a `default`, so that only `windows` and `aix` need to be exceptions. Still not 100% sure of this though, having groups of subtasks or splitting `baselayout` into smaller roles might make some sense. The best way forward is probably for me (or someone else) to come up with a proposal when time permits.
mhdawson		Landed as 96af6dc55f0baa39db6ef9e8af6e8ae36c56133d
rvagg		@joaocgreis looks like the scm change thing is awry again, perhaps an update has fixed the original problem and we need to reverse the option reversal? 
orangemocha		The first set of failures seems due to https://issues.jenkins-ci.org/browse/JENKINS-31527. The comments in the issue indicate that this affects version 1.19 of the MultiJob plug-in, but not 1.18.  The second set of failures seems the same as the issue we had last week, where Jenkins was polling for SCM changes even though it's not supposed to. @joaocgreis had fixed it but I don't know all the details.  @nodejs/jenkins-admins did you upgrade any plug-in updates recently?  
orangemocha		Disabling the node-test-\* jobs while we figure this out, to avoid any false positives. 
orangemocha		@nodejs/collaborators Jenkins is not working reliably because of this issue. I have disabled node-test-pull-request and node-test-commit to avoid any false positives while we sort this out. 
orangemocha		It seems to be back to normal after downgrading the Multijob plugin to version 1.19 (it has been updated to 1.20). I kicked off 3 builds of node-test-pull-request with different PRs as listed in the description of this issue, to gain some confidence that it's indeed working. 
orangemocha		The original issue seems solved after downgrading the multijob plugin to 1.19.  There are still persistent failures in the ARM cross-compile job, which seem unrelated and possibly legitimate as they reproduce on master: https://ci.nodejs.org/job/node-cross-compile/588/nodes=cross-compiler-pi1p/console 
bnoordhuis		> /home/iojs/build/workspace/node-cross-compile/nodes/cross-compiler-pi1p/out/Release/mksnapshot: 1: /home/iojs/build/workspace/node-cross-compile/nodes/cross-compiler-pi1p/out/Release/mksnapshot: Syntax error: word unexpected (expecting ")")  Looks to be the result of enabling snapshots in https://github.com/nodejs/node/pull/3962. 
indutny		Looking into it. 
indutny		From what I just read it seems that `--with-snapshot` is not compatible with cross-building. Should we consider passing extra configure flag on CI instead of reverting it? cc @bnoordhuis @orangemocha  
indutny		cc @nodejs/build  
orangemocha		Looking into it.  
orangemocha		I changed the cross-compile job to pass `--without-snapshot` to `configure`, and it seems to be working: https://ci.nodejs.org/job/node-test-commit-arm-fanned/619/. CI is usable again.  I will open a separate issue to discuss whether it's a good long-term approach. 
indutny		Thanks! 
orangemocha		Closing as node-test-pull-request is working again. Opened follow-up issues at https://github.com/nodejs/build/issues/265 and https://github.com/nodejs/build/issues/266. 
rvagg		see https://github.com/nodejs/build/pull/344 
Trott		I'm pretty sure `node-accept-pull-request` is not coming back any time soon, so I'm going to close this. Feel free to re-open if you think it should be open. 
rvagg		I've just added a systemd config to this as well as a "start" at the end for it so this is the first fully setup & start slaves we have, no setup.sh script. Critique welcome. 
rvagg		also puts ccache into the build path, first time we've done this on a non-arm machine 
retrohacker		Mind if I join?  Want to bring up the work we are doing in @nodejs/docker-iojs for automated testing.  For reference: - https://github.com/nodejs/docker-iojs/issues/77 - https://github.com/nodejs/docker-iojs/issues/46 
Starefossen		I am traveling abroad so I won't be able to join this time. 
jbergstroem		Sounds good to me.  
rvagg		That's OK but it means you can't be as liberal when adding members to that team, it can't serve as a backdoor to Jenkins access. The Collaborators gating process is a handy way to make it difficult for malicious actors to get Jenkins access so you can't just go adding anyone to this new team because they put up their hand to help. 
jbergstroem		@rvagg I assumed that members of chakra-core would follow the same collaboration/contribution guidelines as node.js. Is this not the case? 
orangemocha		Duly noted, @rvagg . The plan is for the collaboration guidelines to be as close as possible to the guidelines for Node.js. Currently the team is comprised of existing Node.js collaborators + a few Microsoft employees who worked on the project before. I am comfortable that the current team meets the trust standards, and will keep this in mind before admitting any new collaborators.  On a related note, it would be in the interest of the build & testing efforts in Node.js to be able to grant Jenkins access to more individuals, so we should think of ways that we can sandbox their activities and mitigate the security concerns. https://github.com/nodejs/build/issues/331 is on the wg-agenda for next Tuesday's meeting and it can serve as a starting point for this discussion. 
orangemocha		Moving forward with this (on Monday) unless there are any objections. The bar for joining the node-charkacore group will be kept in line with that for nodejs/collaborators. 
orangemocha		Done. The members of nodejs/node-chakracore now have the same access to Jenkins as do nodejs/collaborators. 
orangemocha		I think that sounds good. We need to be able to experiment / change things in Jenkins quickly anyway.  BTW, the run-ci rule was introduced because v0.x needed to invoke configure with an additional argument IIRC. Please make sure that you still support all branches when you unfold. 
jbergstroem		Hm, didn't know that. I'll have a deeper look before I do any changes. 
jbergstroem		so:  ``` bash $ $(PYTHON) ./configure $(CONFIG_FLAGS) ```  ..can't really see where `CONFIG_FLAGS` is defined. Guessing jenkins? 
joaocgreis		I don't know if it is the only use, but at least that's needed for https://ci.nodejs.org/job/iojs+release/ 
orangemocha		I am planning it to also use it to enable debug builds in CI (https://github.com/nodejs/node/pull/3293) as seen in https://ci.nodejs.org/job/orangemocha-test-commit-linux/2/  You could also use it to pass the appropriate flags for v0.x.  
jbergstroem		I can't see anything stopping us from doing something similar in the configure step; we'd have to move some logic away from the generic build step in jenkins but that would be it. 
rmg		The problem with changing this target is that either changes need to be backported to previous release branches so the jobs can stay generic or each release branch gets and maintains its own snowflake jenkins job but without the benefit of storing it in that branch.  It's entirely possible these aren't actually problems at the relatively small scale of node (repos \* jobs \* branches), but that was the perspective I was coming from. 
jbergstroem		> @rmg said: >  either changes need to be backported to previous release branches so the jobs can stay generic or each release branch gets and maintains its own snowflake jenkins job  Do they really though? What i would do is take this -- which today is one invoked step in jenkins (slightly different based on which job we have):  ``` bash $ $(PYTHON) ./configure $(CONFIG_FLAGS) $ $(MAKE) $ $(MAKE) test-ci ```  ..then turn that into three jobs. The idea is not to touch anything in the codebase, just split the steps up in jenkins.  For now, lets stick with `node-test-*` only to avoid any issues then. 
rmg		> $ $(PYTHON) ./configure $(CONFIG_FLAGS) > $ $(MAKE) > $ $(MAKE) test-ci  I haven't kept up with what the jobs are actually doing and what each branch supports. If that's already the lowest common denominator then I have no concerns. :-) 
orangemocha		It's probably not too often that we need to alter that structure, but when we do then it is a bit of a pain to make sure all branches are updated. If we wanted to move away from that approach, we could consider doing branch-specific logic in Jenkins through `if` statements in the scripts. This is already the case with this change as run-ci invokes configure with different parameters on v0.x.  I wouldn't go as far as having separate jobs for each branch. Maintaining multiple jobs is pain with the Jenkins UI and we already have different instances for each platform. Having one job for each of `platforms * branches` combinations would be a nightmare. 
Trott		@nodejs/build Should this remain open?
gibfahn		I don't think we actually still use `make-ci`, so I think this can be closed.  (if I'm wrong please reopen).
mikeal		Prediction: this thread will end in people wanting to build a new CI tool in Node and bike-shedding features :) 
andrewlow		Oh no! Not my intention at all. Building a new tool is a fine idea, but that's not a good choice as it'd have no prior proof of being useful. If you want to build a CI tool in Node - go build one, but not here please.  I have quite a bit of experience with Jenkins, and my personal opinion is once you grow past a couple of dozen jobs, it becomes unwieldy to manage.  There may be other CI tools which will deal with the scope of what we want to do better.  One that springs to mind is http://buildbot.net/ - so I'll suggest that as one alternative. 
mikeal		A good thing to remember as well is that, as important as picking a good tool, is picking a tool that enough people are comfortable using who are stepping up to do the work. In that case, along with suggestions people may want to put down their experience, as you have, with different build systems and also how much work they are putting in to this particular project. 
rmg		The more I use Jenkins the less comfortable I am with it. I haven't used buildbot before, but the fact that Chromium uses it is encouraging (http://build.chromium.org/p/chromium/waterfall).  Based on the other thread, @dshaw @ingsings @pquerna and @rvagg might already be coordinating resources, which may or may not mean a CI tool has already been chosen by those doing the work. 
andrewlow		Mozilla uses buildbot https://wiki.mozilla.org/Buildbot as well.  I'm not sure if the resource coordination thread https://github.com/node-forward/build/issues/1 has locked anyone into tooling just yet.  I do agree with @mikeal that familiarity with the tooling on the part of those doing the work will also play a role here. I'm willing to help out to get the CI tooling setup and roll together some builds. Ideally the core technical team needs to also feel comfortable with the tooling, not just those that care about getting builds out the door. 
rvagg		I'll shave some yaks and come up with a more firm opinion about Buildbot vs Jenkins. What I do know is that we have lashings of JVM experience laying around that would likely be useful, plus the existing Jenkins infra that exists and the expertise around that. 
rvagg		I'd love to support drone.io but their tooling is Docker specific, however they are looking to deal with this because one of their goals is to "replace Jenkins": https://github.com/drone/drone/issues/135  Sadly not ready yet. 
rvagg		[CruiseControl](http://cruisecontrol.sourceforge.net/) and maybe [Apache Continuum](http://continuum.apache.org/) are the only other serious candidates I'm aware of; I find this landscape depressing and ready for Disruption<sup>:tm:</sup>. 
bnoordhuis		Until 0.4 or 0.6, node.js used buildbot in an ad-hoc fashion where individuals donated machines.  It was okay, I liked it.  The benchmark page, for example, made it real easy to spot performance regressions.  We eventually stopped using it but that wasn't because of any fault in buildbot.  I don't remember why we settled on Jenkins but @tjfontaine probably does. 
bnoordhuis		Apple is in the process of setting up a new CI cluster for LLVM and they're switching from buildbot to Jenkins ([link](http://thread.gmane.org/gmane.comp.compilers.llvm.devel/75776)).  I infer from the email thread that the reason for the switch is that Jenkins has more out of the box functionality.  Sounds plausible but undoubtedly also depends on your use case. 
andrewlow		The current https://github.com/node-forward/build/blob/master/README.md calls out Buildbot as the CI tooling.  This is an appropriate issue to comment on concerns with this initial selection. 
rvagg		Indeed, and let me fill in a bit more detail here. The current thinking (mostly mine) is that there are a few sensible choices here: 1. Jenkins and all that it involves (bloat, yelling at the JVM, etc.) 2. Buildbot and its programatic configuration and messy frontend (and yelling at Python) 3. Some combination of Buildbot and a custom Node.js application because Buildbot is relatively extensible and it's practical to lean on it for what it's good at and replace what it's not good at 4. Something new entirely  As it stands, the README refers to something like # 3, however I'm now leaning more strongly towards #4 for the following reasons: 1. The CI world is in a terrible state at the moment, Jenkins is still "best of breed" but it's pretty terrible and is in serious need of disruption 2. Buildbot is an attempt to do something new but it's still very hacky and requires a lot of programatic configuration so there would be a significant amount of investment in Python code to make this all work and you still end up with a compromised interface that isn't _quite_ designed for what we want but can be sort of made to fit (the Chromium team have done a fairly decent job of customising it for their needs but it's no small effort). 3. There are other _new breed_ tools but they are far from ready. One is drone.io but they are stuck in Docker/container-land and even though they have a goal of "replacing Jenkins" they are mostly distracted with their CI aaS offering which is all about Linux. Their Open Source efforts are worth keeping an eye on but there is basically no significant movement towards a multi-platform solution. 4. Our needs are relatively straight forward and the complication comes in trying to adapt an existing tool to our needs 5. Instead of investing significant amount of time and effort in the JVM, Python or some other platform that doesn't quite suit our needs that same effort could be put towards building something customised to our needs and we could be investing directly in Node.js while doing so. Perhaps we could come out at the end of it with a more serious contender in the CI space.  So that's what I'm exploring right now in parallel with trying to pull together the platforms we need. The basic needs are simple enough that it ought to not be too much of a drain on time. And, of course, this will be open source, although obviously sponsored by NodeSource since there's at least 3 of us interested in working towards this at the moment. 
othiym23		I think it might be sensible to consider yet another option, which is to bootstrap on the back of either Jenkins (bleah) or Buildbot (less bleah) and do piecewise replacement of the pieces we don't like until we have something we do like. This seems like one of those things where having absolute trust in the toolchain is imperative, and the effort to bootstrap an entirely new solution that satisfies all the criteria currently outlined in this repo's README seems pretty tremendous.  If the plan is to start with something and move towards something a custom solution over time, that means that maybe everything doesn't have to be completely nailed down and perfect to start, and start replacing bits with custom solutions from the rough edges inwards. The goal should be high-quality, repeatable builds first and foremost, and everything else later. 
rvagg		@othiym23 that was plan # 3 actually, use Buildbot for the build coordination and front it with something better, however, there's the build coordination is not actually the tricky bit, in our case in particular. 
othiym23		I see it as different ‚Äì¬†with one, the goal is to have buildbot remain the core thing and give it a prettier / nodier front end; with the other, the goal is to eventually replace buildbot completely, and to not overinvest in making the non-buildbot pieces play too nicely with buildbot given the certainty that eventually it will go away completely. 
andrewlow		Option # 3 is appealing. It let's us collectively dislike python while giving us a CI system that doesn't require a huge initial investment to get going. We can start by improving the front end, but I can see that over time becoming an effort to replace parts.  Simply extending buildbot to allow for logic written today in python to be implemented in JS (node) would be sweet.  I guess my view is a blend of both @othiym23 and @rvagg - we start with something very basic, but leave the door open to changing everything. 
rmg		Disclaimer: Of all the yaks in the industry, build systems is my favourite to shave. Years ago, before node and at a previous company, I did option 4. It was written in Python, supported Linux, Windows, and Mac build machines, and had no coordinator (the build agents were self organizing from a shared queue because the cluster was small enough). I've wanted to re-create a similar system in Node for a while because it's basically all I/O shuffling without actually doing anything computationally intensive/interesting.  What I found to be the hardest was picking the right level of abstraction for your build instructions and the output. Running `git fetch && git clean && git checkout && make test-simple` is easy enough you could just install something like saltstack on all the build machines and just run the commands directly. But that would stop being adequate almost immediately, and the evolution path from that starting point is painful. Starting from buildbot will likely result in a system that feels a lot like buildbot, unless you abstract it to the point where you aren't really building on buildbot anyway. I attribute my dislike of Jenkins to the same effect when I say it was "built for Java in a time before git".  Personally I'm in favour of option 4 because I agree whole-heartedly with @rvagg's observation that cross-platform CI is an inadequately solved problem and I'd really like to see it solved. That said, I don't think I have time to build it myself, so I'd be willing to help with option 3 where/when I can. 
andrewlow		It did take us nearly a month to complete the cycle https://github.com/node-forward/build/issues/4#issuecomment-51647996 /cc @mikeal  
dshaw		Haha :8ball: 
retrohacker		Throwing in my .02, not that it is of value.  /\* begin context */  I'm somewhat familiar with Jenkins, and it is indeed garbage. The default toolchain is limiting, and falling back to shell scripts to get things done is another pandora's box of nightmares. It can be extended though.  I have zero experience with buildbot but, if the knowledge in this thread is accurate, it is extremely extensible. That is valuable for our use case.  /\* end context */  If we focus on getting a MVP out first, it shows that we _can_ do this and it gets the ball rolling. Even if that MVP is a PITA, it will bring us all together in our mutual hatred for the current solution. This mutual hatred will lead to iteration after iteration resulting in @othiym23's scenario of building buildbot of the solution entirely.  If we start with buildbot (in scenario 3) we end up with a custom solution (scenario 4 in which the oracle @mikeal foretold) and everyone wins. 
Qard		In any other situation I would be very much against reinventing the wheel. But I despise current CI tools with a burning passion. I'd love to help with option 4 in any way I can! Something composed of small, unixy tools would be super cool. 
z0mt3c		Didn't find the repo yet. Where did we get stuck? :whale:  
rvagg		To be honest, we're still at such an early stage that we don't have much to show for ourselves! All current energy is simply getting to all-green so we can start wiring things up to be more automatic and flexible: http://jenkins-node-forward.nodesource.com/  Jenkins is the tool to get us started, and the platforms we're testing are very limited, so the trajectory of progress will be about expanding the test platforms as well as building our own tooling in parallel and that's when things will get interesting! I'll be putting out a call to rally everyone who's interested in collaborating when we're ready for that, I'd love to build a team of passionate people that can contribute to something that's distinct from Node core yet vital to it. 
henrytseng		Hmmm, building one seems like an interesting project.  Would this be a good place to list some features?  Or a README.md?   
bnoordhuis		Is there still anything actionable here?  If not, I move to close.  I know @piscisaureus has opinions on how to make interaction with the CI more effective and I personally would like to see us start using something like https://github.com/graydon/bors but perhaps that's something for a new issue. 
objarni		Pardon my ignorance, but what about [Travis CI](https://travis-ci.org/)...? I thought node.js was an open source project?  
bnoordhuis		@objarni Its matrix of architecture/platform/distro/etc. is not good enough for our purposes.  It's also very flaky for anything that's timing sensitive or I/O-heavy.  Libuv used it for a while and it was terrible.  I'm going to close the issue, I think it ran its course.  Last real action was in November. 
objarni		OK, thank you. (out of curiosity - what way did you go...?)  
Fishrock123		Jenkins. (See https://jenkins-iojs.nodesource.com/)  That being said Jenkins is a consistent pain and I definitely wouldn't mind making a Node CI some day. 
cjihrig		@Fishrock123 it's still in the very early stages, but @lloydbenson has started working on this - https://github.com/fishin/ficion. 
rvagg		lgtm 
jbergstroem		Merged in a209398de048296b0ad6da1139f393765a9376ae. I'll adjust my trigger-happiness-level accordingly from here on. Thanks. 
rvagg		I've just done this .... but run into some hiccups. I forgot to confirm that we had Java 8 installed on all the workers before I did it and now I suspect that's why we have some machines failing to come back online.  @joaocgreis these two haven't come back online and they're not in secrets/build/release/windows-servers.md, could you have a look and also update the secrets file please?  * release-rackspace-win2012r2-x64-1 * release-rackspace-win2012r2-x64-2  @mhdawson @gibfahn release-marist-zos13-s390x-1 is down, it doesn't have the nodejs_build_release key in it and you can just use nodejs_build_test (that should be fixed). /u/iojs/start.sh appears to say that it's running Java 8 but I don't _think_ that it's running atm. It's not connected to ci-release anyway. Probably borked when the server went down and it doesn't have a auto-restart mechanism? Hard to tell, it's an alien environment for me and I can't work out how to restart the service as the IOJS user. Could someone @ IBM work out how to breathe life back into this thing please?
rvagg		false alarm maybe, none of those machines seem to be in iojs+release anyway so not critical? I'll leave it up to you guys cause you've obviously been responsible for adding them and know best what to do with them.
joaocgreis		@rvagg those are the 2 new machines I'm getting ready for VS2017 releases, created them only to grab the secret. Just ignore them for now.
jbergstroem		LGTM 
orangemocha		@silverwind : now [node-merge-commit](https://ci.nodejs.org/job/node-merge-commit/build?delay=0sec) has drop-downs too. 
silverwind		@orangemocha looking good. Only thing that's not immediately obvious is `APPLY_CHANGES`. Is that like a dry-run when left off?  Regarding this issue: Jenkins does host its own git repo, right? I think it'd be better to just use these repos instead of Github's, if there's a way to keep keys with push access in sync. 
orangemocha		@silverwind : yes, `APPLY_CHANGES` is a dry-run when left off. Everything is the same except for nothing is pushed to GitHub.  > Jenkins does host its own git repo, right?  Not sure what you mean by that. Every Jenkins slaves clones locally the repo that it's trying to test. 
orangemocha		Related to this issue, I should have mentioned that there is already a mechanism for triggering jobs from scripts.   > **Trigger builds remotely** > Enable this option if you would like to trigger new builds by accessing a special predefined URL (convenient for scripts). > One typical example for this feature would be to trigger new build from the source control system's hook script, when somebody has just committed a change into the repository, or from a script that parses your source control email notifications. > You'll need to provide an authorization token in the form of a string so that only those who know it would be able to remotely trigger this project's builds.  Those tokens enable triggering jobs, so they should be considered as sensitive as a Jenkins user passwords. With proper care though, they might already enable doing everything that you need. 
silverwind		> APPLY_CHANGES is a dry-run when left off  Maybe it should default to on? We still have `node-test-commit` for pure CI runs.  > mechanism for triggering jobs from scripts  How about hosting a repo that stays in sync with github master and start off the jenkins job in a `post-receive` hook? 
silverwind		Or, simpler: use branches on Github itself, with a special `merge-` prefix in the branch name. If such a branch is pushed to, it should first wait a few minutes in case more is pushed, and then start off the job after a certain period of inactivity. If that is possible. 
orangemocha		> Maybe it should default to on? We still have node-test-commit for pure CI runs.  Yes. I changed it to ON by default.  > How about hosting a repo that stays in sync with github master and start off the jenkins job in a post-receive hook?  It sounds like an interesting idea but I am not sure I understand the details. When you push to mirror-repo:master, it would trigger a job to merge changes into node:master? Or how does it know which repo/branch to merge into? And how would it synchronize if multiple collaborators were to push multiple updates to the same branch? 
silverwind		> When you push to mirror-repo:master, it would trigger a job to merge changes into node:master? Or how does it know which repo/branch to merge into? And how would it synchronize if multiple collaborators were to push multiple updates to the same branch?  I think one branch per job is necessary. Targeting a repo/branch is a hard question, one could get fancy with the branch name like `merge-node-master-pr1234` or `merge-node/v0.12-pr1234` (if slashes are valid branch names), but that's a extra bit of information everyone has to remember, hmm. 
orangemocha		Right, one branch per job is necessary. Even though I like the idea of the post-receive hook (it feels like pushing to the repo manually, and it leverages GitHub for authentication), encoding all the parameters in a branch name seems too complicated. A script could do all of it easily and then trigger the job.  I'll email you the authorization token to trigger the job from a script, so you can experiment at will. 
silverwind		I guess it's fine if this only covers merges to master. I'm thinking the simplest way to accomplish it are special branches on Github, so the guidline would be to push the changes to `merge-pr-1234` where 1234 is the ID of the PR, so the script can possibly report back the status of the merge on the PR itself.  Not sure yet if the Github hooks allow for something like this, but I'll see what I can learn. 
orangemocha		:+1:  
orangemocha		I created a webhook in the nodejs/node repo so that @silverwind can experiment with this. 
silverwind		The current plan is to push to special branches on Github and a node app at the end of that webhook watches for these branches and triggers the `node-merge-commit` job. The app itself doesn't have commit access to the repo, and merging/deleting branches would all be done by Jenkins. 
silverwind		@orangemocha I don't think I'll get motivated enough to complete this merge proxy thing in the near future. You can remove the webhook you've set up for me! 
orangemocha		I have marked the webhook as inactive. Closing this issue for now. We can revisit once we resume the experiment with automated merging of pull request.  
martinheidegger		LGTM (thank you for being patient with me) 
rvagg		See my comments at https://github.com/nodejs/new.nodejs.org/pull/342#issuecomment-155749451 
phillipj		Closing this it got down voted in https://github.com/nodejs/nodejs.org/pull/342. 
rvagg		added more instructions to this and scripts and launchd configs with instructions on how to manage that, should be a bit more resilient and easier to manage via ssh. 
MylesBorins		LGTM  perhaps there could be a bit of copy included at the beginning to describe what these instructions are for, dropping right into the ssh config without much context makes it a bit harder to grok 
jbergstroem		I'd suggest making start.sh a template and use variables to replace things like secrets or hostname. Also, perhaps replace the static `JOBS=` with either `$(getconf _NPROCESSORS_ONLN)` or `{{ ansible_processor_vcpus }}`. 
rvagg		@jbergstroem none of this is ansibleised, I don't even know if this is even possible on osx. `getconf _NPROCESSORS_ONLN` is not really appropriate for the machines we're using atm, the VMs are allocated 6 cores each but we're limiting builds to using `-j2` because we're only working with 2 bare machines and usually in parallel. I don't really want to reduce core allocation to 2 to make this work. Perhaps we can revisit this when we get new machines, it's all very hacky. 
jbergstroem		Thanks for elaborating; lets add it to our todos for next setup. LGTM then. 
Trott		I've been inactive forever, so if you want to remove my name while you're at it, feel free. Or not. Up to you and everyone on the Build WG I suppose.
gibfahn		+1  @refack it'd be great to have you in the Build WG, you've been pretty active elsewhere. As @jbergstroem said to me though, with great power comes great responsibility :grin: .  @Trott my feeling is that if it were easier for you to look after CI you would (because you use it a lot). If that's true I'd rather we make that easier than have you leave. Thoughts?
rvagg		+1 to having @refack join /cc @nodejs/build 
Trott		@gibfahn So, yeah, I definitely use the access I was granted as part of the Build WG to do minor/simple fixes (like terminating stalled processes that are messing up CI, although that's less of an issue lately because of changes to the Makefile to improve things, but you get the idea).  I'm happy to stay on for that reason alone, but I have been inactive as far as actual WG stuff (attending meetings, paying close attention to the build issue tracker, doing build stuff beyond reporting problems in CI that I'm unable to fix).  It's the WG's decision as far as I'm concerned.  Let's table this and put it in another issue if it's something anyone wants to discuss further. Otherwise, I'll keep chugging along until I'm booted. :-D  +1 on @refack being added to this WG. He's been absurdly active and we need to get him on a WG or two to absorb some of his excess energy. :-D
piccoloaiutante		yes, +1 for @refack to join the group and @Trott to stay since you're still doing some job here üòÉ. Actually just noticed that i still need to add my name to that list
phillipj		+1 for @refack  I've also been inactive the last few months, selling/buying a new house and other family related things, but I try to keep an eye on the bot at least.
mhdawson		+1 from me
MylesBorins		+1 from me  On Aug 15, 2017 10:15 AM, "Michael Dawson" <notifications@github.com> wrote:  > +1 from me > > ‚Äî > You are receiving this because you are on a team that was mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/pull/824#issuecomment-322478942>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecVw_kJIw9SrJcmz6D-SxElkj46CaNks5sYaftgaJpZM4Oyf6f> > . > 
jbergstroem		+1 from me!
Trott		There's a Build WG onboarding process, right? Who can onbaord @refack? Is that @jbergstroem?
gibfahn		>There's a Build WG onboarding process, right? Who can onbaord @refack? Is that @jbergstroem?  I think @jbergstroem and @joaocgreis normally handle it.
joaocgreis		+1 from me as well, and given the thread above I don't think this needs to wait for a meeting.  Count me in for the onboarding. @jbergstroem are you also available, same as last time? @refack are you in EDT? If so, any time during your daytime should be good for me.
refack		> Count me in for the onboarding. @jbergstroem are you also available, same as last time? @refack are you in EDT? If so, any time during your daytime should be good for me.  Yep I'm EDT. Will ping you on IRC.
joaocgreis		Anyone else wants to join the onboarding? We're planning for monday morning, let us know until then if you want to join.
gibfahn		I'd like to join, what time you planning? 
joaocgreis		@gibfahn 1pm UTC / 2pm BST. Does that work for you? I'll email you the hangouts link before (or IRC if you're there).
gibfahn		@joaocgreis sounds great.  Could you just put the hangouts link in this thread?
refack		I've invited you both to a google-calendar meeting.
joaocgreis		@gibfahn @refack I'm on the google calendar meeting @refack has sent.
Trott		> @refack is now onboard! @rvagg @jbergstroem @mhdawson can one of you please add him to https://github.com/orgs/nodejs/teams/build/members ?  Done.
gibfahn		@refack do you want to land this?  Squash and merge is fine in this repo.
rvagg		Welcome onboard @refack! Hit me up on IRC if you have trouble accessing the ARM cluster, it's a bit more awkward than most of the other hosts and I don't know how much of this was covered in the on-boarding.
mhdawson		@jbergstroem can you take a look at this as we don't have access to the release machines.
jbergstroem		Sure, but do we need to? `pip` (unless we use it for more things i'm unaware of) is to pave way for the `tap2junit` script I wrote which is only used in testing.
gibfahn		Makes sense, if we don't need it then we can close.
rvagg		nfs problems messed up the Pi3's, see #975, narrowed down to a timing problem, things coming up too quick and out of order. I've inserted a delay for mounting and another delay for starting Jenkins in 9731170d85.
rvagg		More tweaks after reprovisioning 3 of them and running this against them. Happy with this now. I'm going to assume that nobody else wants to review this or has the experience to do so so I'm just going to merge it.
refack		@StefanScherer do you have `ansible` fu? If not even a list of step for settings up a Windows Container Host and Spinning up a container? Something like: 1. PS: `Enable-WindowsOptionalFeature -Online -FeatureName Containers` 2. install something 3. download and tag image 4. bind some port to the world 5. Setup `Windows RM` (as per http://docs.ansible.com/ansible/latest/intro_windows.html) 6. ...
StefanScherer		@refack I don't have an Ansible playbook, but can provide some steps how to install Docker.  With the steps in the docs ( https://store.docker.com/editions/enterprise/docker-ee-server-windows ) there are two relevant steps  ```powershell Install-Module DockerProvider -Force Install-Package Docker -ProviderName DockerProvider -Force ```  This PowerShell module installs the Containers feature, downloads Docker binaries and installs the Windows service.   A reboot is required after installing the Containers feature. After the reboot the docker.exe CLI is available in PATH (C:\Program Files\docker). All Docker relevant data can be found in C:\ProgramData\docker.  There is also a PowerShell module by Microsoft (DockerMsftProvider and DockerMsftProviderInsider), but the most recent versions are found with the module by Docker, Inc. (DockerProvider). In https://github.com/OneGet/MicrosoftDockerProvider you find some more commands how to use these provider packages, eg. find all versions, install a specific version, update, etc.  In my Packer templates there are similar steps in [scripts/docker/2016/](https://github.com/StefanScherer/packer-windows/tree/my/scripts/docker/2016) if this two-liner doesn't work directly. The Hyper-V feature is not mandatory, but helps running older Docker images with older kernel versions.  For Windows Server 2016 LTS channel there are two Windows base images: * microsoft/windowsservercore:10.0.14393.xxx (xxx=1770 at the moment) or latest * microsoft/nanoserver:10.0.14393.xxx (xxx=1770 at the moment) or latest  For the Windows Server 1709 Semi-annual channel there are another two base images that fits the host kernel version: * microsoft/windowsservercore:1709 * microsoft/nanoserver:1709 But you probably want to start with Windows Server 2016. But enhancing CI landscape to the newer Windows Server version might also be interesting to test with the newer Docker images.  Building and running containers then is straight forward.  ``` docker run -d -p 3000:3000 stefanscherer/hello-stuttgart ```  Runs a small Node.js web server which listens on port 3000 and can be accessed either from another machine at port 3000 or on the same server with its container IP address. See https://blog.sixeyed.com/published-ports-on-windows-containers-dont-do-loopback/ for details.  Let me know if you need more information. 
StefanScherer		To build Node.js images I use these scripts in AppVeyor at the moment: 1. https://github.com/StefanScherer/dockerfiles-windows/blob/master/node/build.ps1 2. https://github.com/StefanScherer/dockerfiles-windows/blob/master/node/test.ps1  Step 2 now tests node.exe in nanoserver containers and checks if exit code is fine and expected stdout matches :-)  I will update my PR soon as some PGP keys have to be updated there.
StefanScherer		And yes WSL is coming to more and more developer machines on Windows 10. Microsoft also started to provide WSL on Windows Server 2016 so there might be even more adoption in the future. 
bnoordhuis		I've commented on it before but I don't see the point in testing WSL.  It's an emulation layer; any issues that come up on WSL but not on native Linux are bugs that MS has to fix, not us.
joaocgreis		Creating a job to test on Windows containers sounds good.  Testing on WSL wouldn't be very useful, issues should be reported in https://github.com/Microsoft/WSL/issues, not here. A WSL job could perhaps be useful for comparison with Linux, but we shouldn't fix issues here.
rvagg		lgtm, but again I'm going to defer to @jbergstroem, I have nginx-config fatigue 
jbergstroem		I need to look further into this before I can LGTM. 
jbergstroem		Did some googling; couldn't find any references to `new.nodejs.org` (besides the git repo) so lets just get rid of it. I'll create a PR that'll also take care of the https redirect. 
jbergstroem		See #389. 
jbergstroem		Ok, I'll kill it; rationale being less cruft. 
jasnell		+1 
rvagg		sooo .. just to raise one point of concern; allowing third-party code to run on our CI machines opens us up to new injection points for malicious actors. We (mostly) keep our release machines separate but we are dealing with the integrity of our testing system here. Before we go too hard on citgm I wouldn't mind us considering the security implications a bit more and finding ways to mitigate the risks. Some examples could include running separate dedicated machines for citgm runs, or even just running dedicated citgm Jenkins slaves under separate user accounts on our machines to limit the impact. 
Trott		@rvagg Good point, unfortunately. 
jbergstroem		I think the real impact is that jobs has access to environment. In fact, its probably even likely for some test or test runner out there to dump the environment so its easier to "debug" (think libuv diagnostic info pre-run but verbose). There's a lot of trust issues here. 
mhdawson		Time has passed and OSU machines are online. Closing.
nicholascapo		If you are using `reprepo` (which only supports a single version) I recommend aptly [1] which generates repos that provide multiple versions. I am using it to provide private internal repos, and it works great.  [1] https://www.aptly.info/ 
nicholascapo		Found https://github.com/nodesource/distributions/issues/33 
indutny		There are `-darwin.tgz` files in those folders. Is it what you are looking for?
stevenvachon		I'm looking for *.pkg files. All previous nightlies have such.
mhdawson		Last few nightly release job runs show red for OSX.   Looks like it might be an issue related to signing.  I think we need either @rvagg or @jbergstroem to take a look as we need somebody with access to the release machines.
rvagg		``` + codesign -s 'Developer ID Application: Node.js Foundation' out/dist-osx/usr/local/bin/node out/dist-osx/usr/local/bin/node: timestamps differ by 1.21717e+06 seconds - check your system clock ```  System clock hasn't been updating on this set of machines for some reason, I've given it a kick and they now appear to be syncing properly.  Ran the latest one again and it's working properly, see .pkg @ https://nodejs.org/download/nightly/v8.0.0-nightly20170218ca37ec084a/
stevenvachon		*.pkg is missing again: https://nodejs.org/download/nightly/v8.0.0-nightly201702238e8fbd80d0/
rvagg		this is certainly a job for @joaocgreis, he's our multi-job master, the rest of us just :see_no_evil: 
maclover7		ping @kunalspathak 
joaocgreis		Let's leave this open. The Windows jobs will have to be updated to pipelines some time (I plan to do it when I find the time), this is a nice feature to include in that effort.
phillipj		Took this for a test drive after merging https://github.com/nodejs/github-bot/pull/76 by running the deploy script manually -- worked flawlessly üéÜ    @nodejs/github-bot if there's no objections, I'd like to create the webhook needed in the bot repo to get the webhook setup tested as well. 
williamkapke		Nice!  AFA the webhook in the bot repo, no objections from me- but I can't stop thinking we're eventually going to cause an infinite loop (the bot responding to its own actions) ;) 
phillipj		> .. but I can't stop thinking we're eventually going to cause an infinite loop (the bot responding to its own actions) ;)  Combined with AI it'll end up fixing issues by itself? üò∏  
jbergstroem		No objections -- LGTM. This might change moving forward depending on how our security talks fan out though like we discussed in the first gh-bot chat (just adding it for record keeping). 
phillipj		Sorry about the loooong delay here, I had a very hard time getting help adding the repo webhook needed. That issue was resolved a few days ago, when everyone in the github-bot team got owner permissions to the github-bot repo (https://github.com/nodejs/TSC/issues/153).  Just verified that this works as expected. Merged the first commit which made the bot redeploy automatically üéâ  There's definitely room for improvements. Tho seeing this enables everyone in the @nodejs/github-bot team to deploy the bot whenever they see fit, and removes me as the bottleneck for pushing changes into productions, the added value of getting this in is far greater than fine tuning auto deploys for several weeks/months. 
rvagg		sgtm
gdams		Is there any harm in having `duplicate` and `wontfix` labels? I know that we can always recreate them later.
maclover7		¬Ø\_(„ÉÑ)_/¬Ø We can always readd them later on if we need them... my guess was to remove since they haven't been used yet.
maclover7		Removed for now, can always be added again later (names and colors are above).
samkrew		@rvagg Hello, Is there any progress ? 
rvagg		no ... I was going to use the Raspbian Wheezy repo to get gcc 4.8 but that appears to have disappeared from the list so now I either need to compile or find a new source for it, that's the blocker here  also, @julianduque handed me a BBB a few weeks ago so I'll hook that up to CI soon too. 
julianduque		\o/ 
brycejacobs		Are there any updates for getting this working?  
Trott		This issue has been dormant for over a year. Any chance it's either been resolved by now or else is unlikely to be resolved before the Wheezy distro is EOL'ed? Maybe a candidate for a `help wanted` tag? 
maclover7		ping... should this remain open?
jbergstroem		I'd optimally like to get this fixed but I don't have time to look into it. Does this look ok?  <img width="998" alt="screen shot 2016-11-11 at 6 30 20 pm" src="https://cloud.githubusercontent.com/assets/176984/20230876/f004521e-a83c-11e6-8245-b667c79e76de.png"> 
MylesBorins		lgtm  On Fri, Nov 11, 2016, 4:31 PM Johan Bergstr√∂m notifications@github.com wrote:  > I'd optimally like to get this fixed but I don't have time to look into > it. Does this look ok? >  > [image: screen shot 2016-11-11 at 6 30 20 pm] > https://cloud.githubusercontent.com/assets/176984/20230876/f004521e-a83c-11e6-8245-b667c79e76de.png >  > ‚Äî > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub > https://github.com/nodejs/build/issues/530#issuecomment-260060147, or mute > the thread > https://github.com/notifications/unsubscribe-auth/AAecV5a2hNU8_HDFLm9JZN5XWBAo4Vz9ks5q9N6cgaJpZM4KwJJK > . 
jbergstroem		Ok, in place now. Please ping me when it messes up (it will). 
mhdawson		Snapshot of perf data while CITGM was running:  ``` Topas Monitor for host:    power8-nodejs2       EVENTS/QUEUES    FILE/TTY Thu Nov 17 14:41:33 2016   Interval:  2         Cswitch     383  Readch    13887                                                 Syscall     277  Writech     231 CPU  User%  Kern%  Wait%  Idle%  Physc   Entc   Reads        47  Rawin         0 ALL    1.5    2.8    0.0   95.7   0.08    7.8   Writes       45  Ttyout       76                                                 Forks         0  Igets         0 Network  KBPS   I-Pack  O-Pack   KB-In  KB-Out  Execs         0  Namei        73 Total     1.9     18.5    16.5     0.9     1.0  Runqueue    0.6  Dirblk        0                                                 Waitqueue   0.0 Disk    Busy%     KBPS     TPS KB-Read KB-Writ                   MEMORY  Total    32.0    356.0    89.0     0.0   356.0  PAGING           Real,MB   32768                                                 Faults       22  % Comp     14 FileSystem        KBPS     TPS KB-Read KB-Writ  Steals        0  % Noncomp   9 Total             13.5     4.2   13.5    0.0    PgspIn        0  % Client    9                                                 PgspOut       0 Name            PID  CPU%  PgSp Owner           PageIn        0  PAGING SPACE node        7274518   4.2 453.3 iojs            PageOut       0  Size,MB     512 java        6619362   0.0  48.0 iojs            Sios          0  % Used      3 syncd       2752684   0.0   0.6 root                             % Free     97 getty       6422728   0.0   0.7 root            NFS (calls/sec) gil         2293830   0.0   0.9 root            SerV2         0  WPAR Activ     0 IBM.Mgmt    7405802   0.0   5.8 root            CliV2         0  WPAR Total     0 j2pg        2818138   0.0   8.1 root            SerV3         0  Press: "h"-help rmcd        5243066   0.0  12.2 root            CliV3         0         "q"-quit xmgc         917532   0.0   0.4 root            SerV4         0 clcomd      5701812   0.0   1.8 root            CliV4         0 topasrec    6357190   0.0   1.4 root rpc.lock    5046456   0.0   1.2 root ``` 
mhdawson		Does not look to be CPU constrained at least in the snapshot I grabbed and from having watched the output for a few minutes.  
mhdawson		Nothing looks constrained, in that the disk busy%, network number etc don't seem excessive.  @georgeadams95  I think next step is to figure out if we believe its slow consistently across the test or just while testing specific modules.  Once we know that we can shorten up what we use as a recreate to help with investigation.  
gdams		@mhdawson i'll take a look tomorrow and see if I can see anything obvious 
gibfahn		#### vmstat output:  ``` bash bash-4.3$ vmstat -v               8388608 memory pages               8133856 lruable pages               6306949 free pages                     4 memory pools                788729 pinned pages                  80.0 maxpin percentage                   3.0 minperm percentage                  90.0 maxperm percentage                  11.8 numperm percentage                961457 file pages                   0.0 compressed percentage                     0 compressed pages                  11.8 numclient percentage                  90.0 maxclient percentage                961457 client pages                     0 remote pageouts scheduled                 12889 pending disk I/Os blocked with no pbuf                     0 paging space I/Os blocked with no psbuf                  2228 filesystem I/Os blocked with no fsbuf                  3093 client filesystem I/Os blocked with no fsbuf                220085 external pager filesystem I/Os blocked with no fsbuf                  13.4 percentage of memory used for computational pages ```  #### df output (in MB)  ``` bash bash-4.3$ df -m Filesystem    MB blocks      Free %Used    Iused %Iused Mounted on /dev/hd4         512.00    223.35   57%    17716    26% / /dev/hd2        2880.00     65.11   98%    52092    70% /usr /dev/hd9var      576.00    220.30   62%     6739    12% /var /dev/hd3         128.00     76.95   40%     7592    21% /tmp /dev/hd1          64.00     63.63    1%        7     1% /home2 /dev/hd11admin    128.00    127.63    1%        5     1% /admin /proc                 -         -    -         -     -  /proc /dev/hd10opt    1792.00    889.16   51%    17190     8% /opt /dev/livedump    256.00    255.64    1%        4     1% /var/adm/ras/livedump /dev/fslv00    57216.00  47078.34   18%   317263     3% /home /aha                  -         -    -        40     1% /aha ``` 
gibfahn		`/tmp` on both machines is 128MB, which seems very small.  ccache is definitely installed and in the path, in [this build](https://ci.nodejs.org/job/gibfahn-citgm-smoker-more-platforms/node=aix61-ppc64/lastBuild/consoleFull) I'm using `export CC=$(which gcc)` and `export CXX=$(which g++)`, which means builds look like:  ``` bash   /opt/freeware/bin/ccache/g++  -I../deps/gtest -I../deps/gtest/include  -pthread -Wall -Wextra -Wno-unused-parameter -maix64 -Wno-missing-field-initializers -O3 -fno-omit-frame-pointer -fno-rtti -fno-exceptions -std=gnu++0x -MMD -MF /home/iojs/build/workspace/gibfahn-citgm-smoker-more-platforms/node/aix61-ppc64/out/Release/.deps//home/iojs/build/workspace/gibfahn-citgm-smoker-more-platforms/node/aix61-ppc64/out/Release/obj.target/gtest/deps/gtest/src/gtest-death-test.o.d.raw   -c -o /home/iojs/build/workspace/gibfahn-citgm-smoker-more-platforms/node/aix61-ppc64/out/Release/obj.target/gtest/deps/gtest/src/gtest-death-test.o ../deps/gtest/src/gtest-death-test.cc ```  On [the main CI build](https://ci.nodejs.org/job/node-test-commit-aix/nodes=aix61-ppc64/lastBuild/consoleFull) the build commands look like:  ``` bash   g++  -I../deps/gtest -I../deps/gtest/include  -pthread -Wall -Wextra -Wno-unused-parameter -maix64 -Wno-missing-field-initializers -O3 -fno-omit-frame-pointer -fno-rtti -fno-exceptions -std=gnu++0x -MMD -MF /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/.deps//home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/obj.target/gtest/deps/gtest/src/gtest-death-test.o.d.raw   -c -o /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/obj.target/gtest/deps/gtest/src/gtest-death-test.o ../deps/gtest/src/gtest-death-test.cc ```  So I'm not sure whether they're using ccache or not (I can't currently see the configuration).   @GeorgeAdams95 suggested that this vmstat configuration might not be optimal:  ```                   3.0 minperm percentage                  90.0 maxperm percentage ``` 
gibfahn		Running ccache manually on the machine the cache directory looks odd:  ``` bash bash-4.3$ /opt/freeware/bin/ccache/ccache -s cache directory                     //.ccache primary config                      //.ccache/ccache.conf secondary config      (readonly)    /usr/local/etc/ccache.conf cache hit (direct)                     0 cache hit (preprocessed)               0 cache miss                             0 files in cache                         0 cache size                           0.0 kB max cache size                       5.0 GB ```  Actually it looks fine from within a jenkins job:  ``` + /opt/freeware/bin/ccache/ccache -s cache directory                     /home/iojs/.ccache primary config                      /home/iojs/.ccache/ccache.conf secondary config      (readonly)    /usr/local/etc/ccache.conf cache hit (direct)               1882768 cache hit (preprocessed)           20257 cache miss                         37345 called for link                    22387 called for preprocessing            2712 compile failed                       167 preprocessor error                    26 couldn't find the compiler             5 bad compiler arguments                39 autoconf compile/link                624 no input file                       1047 files in cache                    129510 cache size                           3.3 GB max cache size                       5.0 GB ``` 
jbergstroem		Can confirm that ccache works as intended. Is `/tmp/` used for something? If disks are slow, one way to solve it would be using a `tmpfs` for the npm stuff. 
jbergstroem		Thinking this output is related üòÑ   ``` console verbose:                     | Error: ENOSPC: no space left on device, write        ``` 
gdams		@jbergstroem that is just because the /tmp drive is running low. Shouldn't be affecting the job speed should it? 
jbergstroem		@GeorgeAdams95 I just noticed that the test was stalling and that was the last error. Perhaps unrelated.   Anyway, npm tempdir is usually `$HOME/tmp`. Can we perhaps make sure we use that instead? That way we'll avoid the disk fulls. 
gibfahn		@jbergstroem Would you want to use `$HOME/tmp` or something in the workspace? I normally just use `$WORKSPACE/npm-tmp` and `$WORKSPACE/npm-cache`. 
jbergstroem		@gibfahn both sounds good; was just thinking we should make sure it doesn't use `/tmp` (or any other disk thats small) 
mhdawson		Agreed its best if we can make sure not to use /tmp and instead use one of the other larger disks.  We can, however, if necessary increase the size of /tmp if there is no other option.
gdams		this has now been rectified by skipping the time module which was taking nearly 45 minutes to run on AIX
gdams		This is still an issue, the machine is still getting a lot of `npm install` timeouts running citgm
maclover7		Seems to be largely fixed in latest CITGM builds, closing for now. Please reopen if still needed.
bnoordhuis		I would say it can go.  The baseline is centos 5 / linux 2.6.18 and that shakes out most of the bugs. 
jbergstroem		+1 
rvagg		attn @iojs/collaborators: Ubuntu 10.04 is going to be pulled from CI because it's now beyond EOL. I don't expect this to impact you in any way but just so you're aware. 
Starefossen		Ubuntu 10.04 CI configurations has been removed though #123 (f464251). 
rvagg		I'm working on it right now fyi
refack		BTW: I think I crashed it with https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/809/nodes=osx1010/
rvagg		whole machine was down, so something dramatic happened  all sorted now
refack		Bless your soul... üôè
rvagg		* Mentions in conferences - gather a list of talks
gibfahn		For the website, something like https://adoptopenjdk.net/sponsors.html would be ideal.
rvagg		* Quotes from us for use on their material, within reason, e.g. scaleway quotes from me
mhdawson		Talked to Tracy about  * Badge and/or authorised sponsorship wording they can list on their website, e.g. "Proud infrastructure sponsor of the Node.js project" * Regular tweets from @nodejs  Response was positive, and she's going to ask the team at the Foundation to look at how they can help make these happen.
mhdawson		Once we are ready on our side to generate tweets pointing to the donor list, we have agreement to move forward and setup with Zibby at the foundation to start sending them out.  We can either wait until we've move it to nodejs.org or simply start with the pointer to the existing doc in our README.md.  I'm tempted to just start by pointing to the existing README.md and then update once we have moved.  Thoughts ? 
gdams		yeah so as @gibfahn pointed out.... I've had a really strong reaction from companies when I have mentioned putting their logo on the website for AdoptOpenJDK(https://adoptopenjdk.net/sponsors.html). Over there we also started by using a GitHub sponsors file but most of our sponsors seemed much more keen to be on the website
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
jbergstroem		Fixed by installing `iptables-persistent` and saving rules in the appropriate directory. Tested on the new (upcoming) release ci. 
jbergstroem		Yes, we need to create a playbook for our jenkins master. 
Trott		Ping @nodejs/build: This seems like something someone with the right awareness of where all the pieces are can check on and probably address relatively quickly? Or maybe I'm missing a subtlety here?
gibfahn		@jbergstroem LGTY?
bnoordhuis		Seems fine to me.  If there is a way to make it time-based, i.e. number of days, that's arguably a little more convenient than number of builds. 
jbergstroem		Yeah, you can combine both days and no of builds (including type of build status). Check the plugin page for a few combinations. 
bnoordhuis		Yes, I saw that.  I mean making it time-based if that's not too onerous on storage requirements. 
jbergstroem		Right now we have unlimited. Since November 11 we've grown with 70G; so just using that timespan we'd go from ~150G storage to <50G. 
jbergstroem		How about we start with 30 days on all jobs and tweak as we go? 
jbergstroem		I'm not sure; but I suspect reducing job history will have a positive outcome on java memory consumption (based on what I've seen in memory dumps). Does anyone think this is a bad idea? Different limits? cc @nodejs/build   
joaocgreis		30 days might even be too much for successful/unstable builds, so sure, go ahead. For failed builds I'd be comfortable with 180 days. I understand that might not have a huge impact given the amount of failed builds we have, so go with 30 days on all jobs and perhaps open an issue in the main repo to warn collaborators. Is it possible to keep a single backup of everything, made when this plugin is enabled? 
mhdawson		One option we have used is to delete the generate artifacts for builds that pass at the end of the job.  That would let us have a longer history.  I find the captured screen output the most useful part of the history and it likely is not a big part of what gets retained. 
jbergstroem		@mhdawson job history is likely related to the recent jenkins slowness. I'd like to cut that down if we don't really use it. 
jbergstroem		@joaocgreis the plan was to back everything up before activating the job. 
joaocgreis		Sounds like a good plan then! 
jbergstroem		I'll proceed with this in the coming days. Hopefully we'll get a more performant jenkins as a result. 
jbergstroem		Proceeding with this now. I'll go with 30 days or keeping 10 builds and see how we go. Might be good to have some history for tooling surrounding CI. 
jbergstroem		There doesn't seem to be a "minimum amount of builds to keep" which is what I was gunning for. I'll leave it at 30 days for now and see if we can get upstream to add it. 
jbergstroem		I'm pretty sure that didn't work as intended. I added a post-build task to the `node-test-pull-request` job that were supposed to discard all type of builds older than 30 days. I ended up with 4 visible jobs through ci and 8 jobs on disk:  ``` drwxr-xr-x  2 jenkins jenkins  4096 Jan 15 19:34 1257 drwxr-x---  2 jenkins jenkins  4096 Aug 23 07:13 133 drwxr-xr-x  2 jenkins jenkins  4096 Jan 25 12:46 1370 drwxr-xr-x  2 jenkins jenkins  4096 Jan 27 14:21 1397 drwxr-xr-x  2 jenkins jenkins  4096 Jan 27 17:07 1405 drwxr-xr-x  2 jenkins jenkins  4096 Jan 27 17:38 1406 drwxr-xr-x  2 jenkins jenkins  4096 Nov 10 16:41 693 drwxr-xr-x  2 jenkins jenkins  4096 Nov 10 18:52 697 lrwxrwxrwx  1 jenkins jenkins     4 Jan 27 14:21 lastFailedBuild -> 1397 lrwxrwxrwx  1 jenkins jenkins     4 Jan 27 17:38 lastStableBuild -> 1406 lrwxrwxrwx  1 jenkins jenkins     4 Jan 27 17:38 lastSuccessfulBuild -> 1406 lrwxrwxrwx  1 jenkins jenkins     4 Jan 25 12:46 lastUnstableBuild -> 1370 lrwxrwxrwx  1 jenkins jenkins     4 Jan 27 14:21 lastUnsuccessfulBuild -> 1397 -rw-r-----  1 jenkins jenkins     0 Jul  7  2015 legacyIds ```  I can't identify a reliable pattern as to what that plugin decided to do; so I'm going to rsync the job history back and reload jenkins. My next step is manually deleting jobs on disk and see how that works. I'm starting to suspect that we'll be more reliable writing some bash scripts for this. 
jbergstroem		After further tinkering; deleting stuff in job folders (slightly different based multijob or no) and reloading the jenkins configuration seems to work fine. I'll put some scripting together and post it here for review; hopefully we can get a pretty quick LGTM so we can also establish that reducing job history will have a positive impact on jenkins performance. 
jbergstroem		Also, reloading jenkins configuration from disk doesn't seem to affect the running state (builds or jobs in queue) so adding a reload at the end of the cronjob/however we choose to deploy seems to be a winning combo. 
jbergstroem		So, how about something like:  ``` bash #!/bin/sh DAYS=30 ROOTDIR=/var/lib/jenkins/jobs JOBS=$ROOTDIR/*/builds/ MULTIJOBS=$ROOTDIR/*/configurations/axis-nodes/*/builds/ # append when sure: -exec rm -rf "{}" \; find $JOBS $MULTIJOBS -maxdepth 1 -type d -mtime +$DAYS curl https://ci.nodejs.org/reload # including magic session somehow ``` 
jbergstroem		Was hoping for jenkins accepting a unix signal for reloading disk but no. It'll have to be token based and since we run the github auth plugin we'll have to control tokens from there. The alternative would be restarting but that's not graceful. 
rvagg		You're going to need `-mindepth 1` in there as well to avoid it matching actual `build` directories as it does now for older jobs, and just to be safe. 
jbergstroem		@rvagg yep, I noticed that but jenkins doesn't really mind if the folder doesn't exist (just creates one) which is why I left it. I'll update. 
joaocgreis		LGTM. According to the man page, that will only delete files modified exactly 30 days ago, so this needs to run everyday. We can safely assume that happens for now, but it would be nice to find a better solution.  Suggestion: I would make a simple log of what's being deleted, something like:  ``` diff  MULTIJOBS=$ROOTDIR/*/configurations/axis-nodes/*/builds/ +LOGDIR=$ROOTDIR/log +LOGFILE=$LOGDIR/histclean_$(date -u +"%Y%m%d-%H%M%SZ").log.xz -# append when sure: -exec rm -rf "{}" \; +# append when sure: -print -exec rm -rf "{}" \; |& xz > $LOGFILE  find $JOBS $MULTIJOBS -maxdepth 1 -type d -mtime +$DAYS ``` 
rvagg		The `+` prefix for `$DAYS` turns it in to "more than `$DAYS`", without it, it would mean "exactly `$DAYS`". 
jbergstroem		@joaocgreis got this error: `./test.sh: 10: ./test.sh: Syntax error: "&" unexpected`. I've saved a list of all deleted folders for now. 
joaocgreis		@jbergstroem you're using dash, `|&` is not available there. Replace it by `2>&1 |`, that should work. 
jbergstroem		@joaocgreis yeah; I just saved it separately in the name of progress. I'll spend some more time on a cron job and post it in a PR later. 
jbergstroem		todo: needs axis jobs as well (edit: these kinds of axis: `axis-RUN_SUBSET/2/axis-VS_VERSION/vs2015/axis-label/win10/builds/699`) 
jbergstroem		I think we should run this as a post-script in the backup routine. That way we'll make sure whatever's deleted remains in backup as part of the rotation. 
jbergstroem		This was implemented as part of the backup routine. Closing. 
joaocgreis		@jbergstroem the axis jobs are accumulating, what about something like `find $ROOTDIR -name builds -type d -exec find '{}' -mindepth 1 -maxdepth 1 -type d -mtime +$DAYS \;` ? 
joaocgreis		Landed in https://github.com/nodejs/build/commit/d2d5dd2506951d0dfdd2ffa83aeba8b37b3e6cbc
mhdawson		@nodejs/build please comment.  Would like to start having tweets go our monthly starting next week.
gibfahn		Wording LGTM
piccoloaiutante		Tweet looks good to me. I was wondering if we could convey the message of "give back to the community" but I wasn't able to come up with something meaningful in 140 chars üòÑ 
kunalspathak		LGTM
joaocgreis		LGTM
mhdawson		Ok, I've landed the logos for Intel and MacStadium so going to ask that the tweet now be sent out monthly.
mhdawson		First tweet went out and should now go out monthly, closing.
refack		@mhdawson has this been going on? Can I help? Can @zibbykeaton help?
ZibbyKeaton		I post a tweet about it once a month - it will be coming out later this June, but here is May's tweet: https://twitter.com/nodejs/status/999772162335657984. If you want to retweet that would be great! I can also create a special hashtag for this effort, so it's easy to find on social when you want to give a retweet to it. Just let me know if that would be helpful to this group. We could even do #nodejsdonations (open to other hashtags as well).
refack		+1 on hashtag (just what I was thinking while I was trying to find the history)
ZibbyKeaton		Upcoming social posts have been updated with above hashtag for easier tracking :) 
mhdawson		@ZibbyKeaton thanks :)
bnoordhuis		> Is it absolutely impossible to produce a snapshot on the build platform, if it's different from the target platform?  Only if you have an emulator.  What the snapshot does is compile and execute JS to native machine code, then save the generated machine code to an image. 
indutny		qemu? 
indutny		Also a question how does V8 handle this? I'm sure that Google cross-compiles it for arm. cc @ofrobots 
targos		There is some documentation about this here: https://github.com/v8/v8/wiki/Cross-compiling%20for%20ARM 
indutny		@targos I'm afraid that this page is very likely to be outdated. V8 does not use `scons` for a very long time now. 
indutny		@orangemocha is there any way to check that `want_separate_host_toolset` was `1` during our builds? 
indutny		It looks like it was `0`. This is very likely a cause of the problem. 
indutny		Wait, are we cross-compiling there? Or is it an actual raspberry pi? 
ofrobots		Looking output:  Building remotely on node-msft-cross-compiler-1 (cross-compiler-pi1p) in workspace /home/iojs/build/workspace/node-cross-compile/nodes/cross-compiler-pi1p  The job ran on https://ci.nodejs.org/computer/node-msft-cross-compiler-1/ which is described as "Ubuntu 14.04 LTS with Raspberry Pi cross compiler installed", so I would say this is a cross-compilation. 
indutny		This is a config.gypi:  ``` { 'target_defaults': { 'cflags': [],                        'default_configuration': 'Release',                        'defines': [],                        'include_dirs': [],                        'libraries': []},   'variables': { 'arm_float_abi': 'hard',                  'arm_fpu': 'vfpv2',                  'arm_thumb': 0,                  'arm_version': '6',                  'asan': 0,                  'gas_version': '2.24',                  'host_arch': 'arm',                  'icu_small': 'false',                  'node_byteorder': 'little',                  'node_install_npm': 'true',                  'node_prefix': '/usr/local',                  'node_release_urlbase': '',                  'node_shared_http_parser': 'false',                  'node_shared_libuv': 'false',                  'node_shared_openssl': 'false',                  'node_shared_zlib': 'false',                  'node_tag': '',                  'node_use_dtrace': 'false',                  'node_use_etw': 'false',                  'node_use_lttng': 'false',                  'node_use_openssl': 'true',                  'node_use_perfctr': 'false',                  'openssl_fips': '',                  'openssl_no_asm': 0,                  'python': '/usr/bin/python',                  'target_arch': 'arm',                  'uv_parent_path': '/deps/uv/',                  'uv_use_dtrace': 'false',                  'v8_enable_gdbjit': 0,                  'v8_enable_i18n_support': 0,                  'v8_no_strict_aliasing': 1,                  'v8_optimized_debug': 0,                  'v8_random_seed': 0,                  'v8_use_snapshot': 'true',                  'want_separate_host_toolset': 0}} ```  For some reason `host_arch` is `arm` there... 
ofrobots		Indeed. Odd. 
orangemocha		Definitely cross-compiling. Better get the nitty gritty details from @joaocgreis, who set this up and also did some experiments with qemu. 
ofrobots		I think @indutny might be right. The job seem to be doing a `configure` instead of `configure --dest-cpu=arm`. On my x64 box, this is the config.gypi I get:  ``` lemongrab :: ~/src/node ‚Äπmaster‚Ä∫ % ./configure --dest-cpu=arm creating  ./icu_config.gypi { 'target_defaults': { 'cflags': [],                        'default_configuration': 'Release',                        'defines': [],                        'include_dirs': [],                        'libraries': []},   'variables': { 'arm_float_abi': 'default',                  'arm_fpu': 'vfpv2',                  'arm_thumb': 0,                  'arm_version': 'default',                  'asan': 0,                  'gas_version': '2.24',                  'host_arch': 'x64',                  'icu_small': 'false',                  'node_byteorder': 'little',                  'node_install_npm': 'true',                  'node_prefix': '/usr/local',                  'node_release_urlbase': '',                  'node_shared_http_parser': 'false',                  'node_shared_libuv': 'false',                  'node_shared_openssl': 'false',                  'node_shared_zlib': 'false',                  'node_tag': '',                  'node_use_dtrace': 'false',                  'node_use_etw': 'false',                  'node_use_lttng': 'false',                  'node_use_openssl': 'true',                  'node_use_perfctr': 'false',                  'openssl_fips': '',                  'openssl_no_asm': 0,                  'python': '/usr/bin/python',                  'target_arch': 'arm',                  'uv_parent_path': '/deps/uv/',                  'uv_use_dtrace': 'false',                  'v8_enable_gdbjit': 0,                  'v8_enable_i18n_support': 0,                  'v8_no_strict_aliasing': 1,                  'v8_optimized_debug': 0,                  'v8_random_seed': 0,                  'v8_use_snapshot': 'true',                  'want_separate_host_toolset': 1}} ``` 
joaocgreis		I believe I have this working now and will be able to re-enable snapshots when https://github.com/nodejs/node/pull/4117 lands. V8 can handle cross compiling of snapshots if the `{CC,CXX}_host` variables are defined, by compiling the `mksnapshot` executable with the host compiler. 
joaocgreis		https://github.com/nodejs/node/pull/4117 has landed and CI is building snapshots for arm now: https://ci.nodejs.org/job/node-test-commit-arm-fanned/768/ . I will close this for now, please reopen if there are issues. 
kikijhu		It's still failed when building v5.4.1 on ubuntu 14.04 for android  _make[1]: *_\* _[/home/kiki/node.js/node-v5.4.1/out/Release/obj.target/v8_snapshot/geni/snapshot.cc] Error 2 make[1]: Leaving directory `/home/kiki/node.js/node-v5.4.1/out' make: *_\* _[node] Error 2_  ``` javascript { 'target_defaults': { 'cflags': [],                        'default_configuration': 'Release',                        'defines': [],                        'include_dirs': [],                        'libraries': []},   'variables': { 'OS': 'android',                  'arm_float_abi': 'default',                  'arm_fpu': 'vfpv3',                  'arm_thumb': 0,                  'arm_version': '7',                  'asan': 0,                  'gas_version': '2.24',                  'host_arch': 'arm',                  'icu_small': 'false',                  'node_byteorder': 'little',                  'node_enable_v8_vtunejit': 'false',                  'node_install_npm': 'true',                  'node_prefix': '/usr/local',                  'node_release_urlbase': '',                  'node_shared_http_parser': 'false',                  'node_shared_libuv': 'false',                  'node_shared_openssl': 'false',                  'node_shared_zlib': 'false',                  'node_tag': '',                  'node_use_dtrace': 'false',                  'node_use_etw': 'false',                  'node_use_lttng': 'false',                  'node_use_openssl': 'true',                  'node_use_perfctr': 'false',                  'openssl_fips': '',                  'openssl_no_asm': 0,                  'python': '/home/kiki/node.js/node-v5.4.1/android-toolchain/bin/python',                  'target_arch': 'arm',                  'v8_enable_gdbjit': 0,                  'v8_enable_i18n_support': 0,                  'v8_no_strict_aliasing': 1,                  'v8_optimized_debug': 0,                  'v8_random_seed': 0,                  'v8_use_snapshot': 'true',                  'want_separate_host_toolset': 0}} ``` 
kikijhu		I have to modified android-configure to pass --without-snapshot to configure.  ``` export TOOLCHAIN=$PWD/android-toolchain mkdir -p $TOOLCHAIN $1/build/tools/make-standalone-toolchain.sh \     --toolchain=arm-linux-androideabi-4.9 \     --arch=arm \     --install-dir=$TOOLCHAIN \     --platform=android-19 export PATH=$TOOLCHAIN/bin:$PATH export AR=$TOOLCHAIN/bin/arm-linux-androideabi-ar export CC=$TOOLCHAIN/bin/arm-linux-androideabi-gcc export CXX=$TOOLCHAIN/bin/arm-linux-androideabi-g++ export LINK=$TOOLCHAIN/bin/arm-linux-androideabi-g++  ./configure \     --dest-cpu=arm \     --dest-os=android \     --without-snapshot ``` 
joaocgreis		@kikijhu Instead of adding `--without-snapshot`, can you try to add  ``` export CC_host=cc export CXX_host=c++ ```  I don't have the Android NDK here for a quick test, so let us know if this works. Thanks! 
fornwall		@joaocgreis When setting `CC_host` the host build picks up the `CFLAGS` environment variable, which I've used to configure the target, so the build fails for me.  How should `CFLAGS` be set for the target when wanting to cross compile with snapshot? 
joaocgreis		GYP uses the variables `{CC,CXX}_host` and `{CC,CXX}_target` to override `{CC,CXX}` when cross compiling. I'm not sure if it also handles `CFLAGS`, but it would need a similar duplication to be complete. On our side, it would be great to have `configure` supporting every combination, but most people committing to `configure` are not cross compiling so it would break easily. Work like that would require someone to keep a more constant attention over it, that's why I chose to do the least disruptive modification to `configure` and support only `{CC,CXX}_host`. Contributions are welcome!  @fornwall Perhaps the best way is not using the `CFLAGS` variable at all, and adding its contents to the proper `{CC,CXX}` variable in the script. Something like:  ``` export CC="$TOOLCHAIN/bin/arm-linux-androideabi-gcc -O? -f..." export CXX="$TOOLCHAIN/bin/arm-linux-androideabi-g++ -O? -f..." export CC_host="cc -O? -f..." export CXX_host="c++ -O? -f..." ``` 
kikijhu		I add these lines to **./android-configure**  ``` export CC_host=cc export CXX_host=c++ ```  Build failed again.  ```   ln -f "/home/kiki/node.js/node-v5.4.1/out/Release/obj.target/deps/v8/tools/gyp/libv8_libbase.a" "/home/kiki/node.js/node-v5.4.1/out/Release/libv8_libbase.a" 2>/dev/null || (rm -rf "/home/kiki/node.js/node-v5.4.1/out/Release/libv8_libbase.a" && cp -af "/home/kiki/node.js/node-v5.4.1/out/Release/obj.target/deps/v8/tools/gyp/libv8_libbase.a" "/home/kiki/node.js/node-v5.4.1/out/Release/libv8_libbase.a")   c++ '-D_GLIBCXX_USE_C99_MATH' '-DV8_TARGET_ARCH_ARM' '-DCAN_USE_ARMV7_INSTRUCTIONS' '-DCAN_USE_VFP3_INSTRUCTIONS' '-DCAN_USE_VFP32DREGS' '-DENABLE_DISASSEMBLER' '-DUSE_EABI_HARDFLOAT=0' -I../deps/v8  -Wall -Wextra -Wno-unused-parameter -m32 -ffunction-sections -fdata-sections -fno-omit-frame-pointer -fPIE -fdata-sections -ffunction-sections -O2 -fno-rtti -fno-exceptions -std=gnu++0x -MMD -MF /home/kiki/node.js/node-v5.4.1/out/Release/.deps//home/kiki/node.js/node-v5.4.1/out/Release/obj.host/mksnapshot/deps/v8/src/snapshot/mksnapshot.o.d.raw   -c -o /home/kiki/node.js/node-v5.4.1/out/Release/obj.host/mksnapshot/deps/v8/src/snapshot/mksnapshot.o ../deps/v8/src/snapshot/mksnapshot.cc   c++ -rdynamic -m32 -fPIE -pie  -o /home/kiki/node.js/node-v5.4.1/out/Release/mksnapshot /home/kiki/node.js/node-v5.4.1/out/Release/obj.host/mksnapshot/deps/v8/src/snapshot/mksnapshot.o /home/kiki/node.js/node-v5.4.1/out/Release/obj.host/deps/v8/tools/gyp/libv8_base.a /home/kiki/node.js/node-v5.4.1/out/Release/obj.host/deps/v8/tools/gyp/libv8_nosnapshot.a /home/kiki/node.js/node-v5.4.1/out/Release/obj.host/deps/v8/tools/gyp/libv8_libplatform.a /home/kiki/node.js/node-v5.4.1/out/Release/obj.host/deps/v8/tools/gyp/libv8_libbase.a -llog -ldl -lrt /usr/bin/ld: cannot find -llog collect2: error: ld returned 1 exit status make[1]: *** [/home/kiki/node.js/node-v5.4.1/out/Release/mksnapshot] Error 1 make[1]: Leaving directory `/home/kiki/node.js/node-v5.4.1/out' make: *** [node] Error 2 ``` 
peter279k		Hi all, I have same problems during crossing compile node.js. When I edit android-configure file and add this text : --without-snapshot , It's successful to cross compile. Here is my android-configure file :  ``` #!/bin/bash  export TOOLCHAIN=/home/lee/android-toolchain export PATH=$TOOLCHAIN/bin:$PATH export AR=$TOOLCHAIN/bin/arm-linux-androideabi-ar export CC=$TOOLCHAIN/bin/arm-linux-androideabi-gcc export CXX=$TOOLCHAIN/bin/arm-linux-androideabi-g++ export LINK=$TOOLCHAIN/bin/arm-linux-androideabi-g++  ./configure \     --dest-cpu=arm \     --dest-os=android \     --without-snapshot ```  Following texts are a part of crossing compile nodejs's message.  ``` openssl/libopenssl.a /home/lee/node/out/Release/obj.target/deps/zlib/libzlib.a /home/lee/node/out/Release/obj.target/deps/http_parser/libhttp_parser.a /home/lee/node/out/Release/obj.target/deps/uv/libuv.a /home/lee/node/out/Release/obj.target/deps/v8/tools/gyp/libv8_base.a /home/lee/node/out/Release/obj.target/deps/v8/tools/gyp/libv8_libbase.a /home/lee/node/out/Release/obj.target/deps/v8/tools/gyp/libv8_nosnapshot.a -Wl,--end-group -llog -lm -ldl   touch /home/lee/node/out/Release/obj.target/node_dtrace_header.stamp   touch /home/lee/node/out/Release/obj.target/node_dtrace_provider.stamp   touch /home/lee/node/out/Release/obj.target/node_dtrace_ustack.stamp   touch /home/lee/node/out/Release/obj.target/node_etw.stamp   touch /home/lee/node/out/Release/obj.target/node_perfctr.stamp   touch /home/lee/node/out/Release/obj.target/specialize_node_d.stamp make[1]: Leaving directory '/home/lee/node/out' ln -fs out/Release/node node  ``` 
fornwall		Should a new issue be created for `Android cross-compile does not work with v8 snapshots`, since this issue is closed (and the Android issue doesn't seem ARM-specific)? 
joaocgreis		Discussion moved to https://github.com/nodejs/node/issues/4860 
michael-ts		I am seeing this issue cross-compiling for Linux on ARM (but NOT Android). I'm using the following commands:  ``` export CC=/home/michael/tools/arm-unknown-linux-gnueabi/bin/arm-unknown-linux-gnueabi-gcc export CXX=/home/michael/tools/arm-unknown-linux-gnueabi/bin/arm-unknown-linux-gnueabi-g++ export AR=/home/michael/tools/arm-unknown-linux-gnueabi/bin/arm-unknown-linux-gnueabi-ar export RANLIB=/home/michael/tools/arm-unknown-linux-gnueabi/bin/arm-unknown-linux-gnueabi-ranlib ./configure --without-snapshot --dest-cpu=arm --dest-os=linux --with-arm-float-abi=hard make ``` Per the above suggestion, I also tried adding:  ``` export CC_host=cc export CXX_host=c++ ``` I'm not sure I understand what the fix for this is supposed to be.  Is there something I am doing wrong, or is this a bug?
fornwall		@michael-ts I think this is the issue to track: https://github.com/nodejs/node/issues/9707
michael-ts		@fornwall Ah, i see they are using `--without-intl`.  Using that I seem to get a successful build.
fornwall		@michael-ts Are you building node 6.x or 7.x?
michael-ts		@fornwall 6.9.1
apapirovski		Looks like this isn't the first time in recent history: https://github.com/nodejs/build/issues/899
refack		@apapirovski the acute symptom should be alleviated.  As for the chronic problem, AFAICT the main culprit is the `.npm` cache. I think we should move it into the CitGM workspace, and clear it after a CitGM run. Any objections?
gibfahn		@refack where is that currently stored, `/home/iojs/.npm`?  We should definitely be setting all the npm storage directories to be inside the job workspace for the CitGM jobs (with the exception of the tmpdir on AIX, which goes in the `/ramdisk0` dir). I thought we already were.
refack		Yes `/home/iojs/.npm` altought now it seems like it's only 4G (just millions of files). Still jobs are worse: ```   0B    ./check-java-version 3.7G    ./citgm-smoker 3.4G    ./citgm-smoker-private 3.4G    ./gibfahn-test-npm  42M    ./libuv-test-commit-osx 109M    ./node-inspect-continuous-integration   0B    ./node-test-commit-osx 3.3G    ./node-test-npm ```
gibfahn		`3.4G    ./gibfahn-test-npm`  You can clear that one up üòÅ 
maclover7		ping -- are there are any further steps that need to be taken here?
gibfahn		Should be good for now.
jbergstroem		Just got an email about maintenance being completed. Closing. 
rvagg		sounds reasonable to me, what strings would we use in binary names for these? `ppcbe` and `ppcle`? what is standard practice for these? 
bnoordhuis		`ppc`, `ppc64`, `ppcel` and `ppc64el`?  Does little-endian 32 bits PowerPC exist?  I don't think I've ever seen it. 
mhdawson		There is no 32bit LE version only 64 bit.  For both BE and LE initially we'd only be looking to include 64 bit.  32 bit does not exist for LE and on BE it is lower priority due to the level of use.   I'd suggest we use these names  ppc64 and ppcle64 
bnoordhuis		Interesting how I made the same typo twice, I intended to write 'ppc64le'....  Anyone have data on what is more common, ppcle64 or ppc64le?  I've seen both used. 
jbergstroem		[Wikipedia](https://en.wikipedia.org/wiki/Ppc64) and some linux dists seems to point to `ppc64le`. 
mhdawson		Talked to our lead from the power side and it should be ppc64le  
mhdawson		Added 2 additional machines to the CI, one for BE and one for LE so we have the machines needed on the regression test side.  They are already up and running in the CI, last step is to just add to the ansible inventory - PR here - https://github.com/mhdawson/build/commit/e6e3747d631b3dda1c514ba7dc99795da8ecc7d6  Will need to get 2 more for releases  
mhdawson		I now have 2 additional machines for releases. What is the process of getting them configured/added to wherever they need to go ? @rvagg Johan suggested you'd probably be the right person to answer that 
rvagg		first step is getting them into nightly builds, I'm going to work on that this week and will ping you when I'm onto it 
mhdawson		Is getting them into the nighties something I could help out with  ?  
rvagg		@mhdawson: first thing I'm noticing is that they're taking a _very_ long time to clone the repo, could you put a bare mirror on them to speed it up please?  ``` mkdir /home/iojs/git/ git clone https://github.com/nodejs/node.git --mirror /home/iojs/git/io.js.reference ```  the job is set up to use refs found in that location when cloning so it ends up only getting any newer commits for each build.  Next, they are missing `$ARCH` and `$DESTCPU` environment variables:  https://ci.nodejs.org/job/iojs+release/275/nodes=ppcle-ubuntu1404-release-64/console  `ARCH` is mainly used in determining the filename (i.e. we have x86, x64, armv6l, etc.) while `DESTCPU` is what `configure` cares about, often they are the same.  This can be done in the startup configuration for the service somewhere (@jbergstroem has stronger opinions on what the _correct_ way to do this is than me), or even in the slave config in Jenkins were you set custom environment variables.  I see we allow `ppc64` and `ppc` in both `configure` and `Makefile` but we have both `ppcbe` and `ppcle` machines and both are 64-bit, what is the intention here? What binaries are you wanting to produce for download and what do you think their names should be? 
mhdawson		We only have 64 bit machines and based on the conversation that was started when I investigated getting a second set for 32 bit to be consistent with other architectures the feedback from the Power team was that 64 bit is what matters (it does not exist at all for LE).  Based on that I was looking to have only the 64 bit BE and 64 LE platforms as part of the standard releases.  The names should be ppc64le and ppc64 
mhdawson		Looking at the download links as an example I was thinking along these lines:  https://nodejs.org/dist/v4.2.2/node-v4.2.2-linux-ppc64.tar.gz https://nodejs.org/dist/v4.2.2/node-v4.2.2-linux-ppc64le.tar.gz  Is that what you were looking for in terms of the naming.  Next I'll have to see how that works out in terms of what we want for $DESTCPU and $ARCH 
mhdawson		I think what we want is:  BE: DESTCPU = ppc64 ARCH = ppc64  LE: DESTCPU=ppc64 ARCH = ppc64le  I see at least ARCH is set as environment variables for Windows in jenkins so I'll so ahead and do that and we can update if Johan suggests a better way  I'll add those  
mhdawson		Ok environment variables added on jenkins and added the bare mirror to each machine using the commands listed by Rodd run as the iojs user.    @rvagg I think that's everything you asked for 
jbergstroem		Reckon these guys should go in the init script. 
mhdawson		@jbergstroem is there an example of a machine where they are in the init script that I can take a look at ? 
jbergstroem		these machines failed to build as part of 5.1: https://ci.nodejs.org/job/iojs+release/287/nodes=ppcbe-fedora20-release-64/ https://ci.nodejs.org/job/iojs+release/287/nodes=ppcle-ubuntu1404-release-64/  Going to pull them off the release job to not delay our release any further.  @mhdawson regarding init scripts: - fedora20: https://github.com/nodejs/build/blob/master/setup/fedora20/resources/jenkins.service - ubuntu1404 still lacks one, so gun for `start.sh`. 
rvagg		sorry @Fishrock123, they weren't meant to be included in release builds, only nightlies at this stage, we still have work to do on getting them ready! 
rvagg		@mhdawson I'm told that you're not happy with the switch we made to Ubuntu 14.04 for ppcbe and that there's a technical reason this is a problem? Can you explain in here for me please?  I have a big concern with using a non-LTS-style distro for release builds because it means we don't get a stable platform over time. Consider how we've been building Node.js for Linux on EL5 for _years_ and we get the same libc as a benefit. With Fedora, they are strict on deprecation so we have to upgrade every year and have a changing libc and have to scramble each time they have a new release in order to make sure that we can release any binaries (ppc holdups will be holdups for everyone once this becomes part of the official flow).  So if there's a way we can use EL (RHEL is fine by me fwiw, doesn't have to be CentOS), Debian or Ubuntu LTS then that would be ideal. 
rvagg		@jbergstroem https://ci.nodejs.org/job/iojs+release/350/nodes=ppcbe-fedora20-release-64/console  ``` Bad owner or permissions on /home/iojs/.ssh/config ```  Is this from a script now or was that manual?  FYI everyone else, these machines are active for nightly builds https://nodejs.org/download/nightly/ but we're still ironing out wrinkles so they are not reliably showing up in each new nightly yet. 
mhdawson		The reason I chose Fedora 20 was because it was the earliest OS for BE that was supported by osuosl (they don't have RHEL available).  If we use ubuntu 14.04 instead then it means that the binaries will only support the later OS levels.  The issues we have seen are usually related to the default libc/libstdc++.  Having said that,  some of these issues were seen before the move of v8 up to the 4.8.X level of the compiler forced our hand in terms of default support for some platforms so the reasonable levels to support have moved up a bit.    At this point my main concern is if the whether binaries built on ubuntu 14.04 will run on RHEL 7.  Given your concern I suggest we create an ubuntu 14.04 release machine. I can then test whether the binaries run on our internal RHEL 7 machines.   
jbergstroem		@rvagg inconsistencies when I manually set it up. Fixed it a while ago. 
rvagg		@mhdawson Fedora 20 had an EOL on 2015-06-23, so we'd be building on an unsupported platform which is probably not a great idea. RHEL7 is libc 2.17 while Ubuntu 14.04 is 2.19 which is not a great mix I suppose. Is there any chance of getting an EL variant onto osuosl? 
jbergstroem		available os'es: <img width="388" alt="osuosl-toomanychoice" src="https://cloud.githubusercontent.com/assets/176984/12345889/b41c58a0-bba2-11e5-8f4b-045185aa3199.png">  ..perhaps Michael can contact their helpdesk and see if we can get more options on there. 
rvagg		Oh, and the currently oldest supported Fedora, 23, uses libc 2.21, which is obviously far from ideal and 23 is EOL in 6 months I think anyway. This is the problem I have with using Fedora in general, it moves too fast and doesn't have a long-term strategy. 
rvagg		Debian 7 is a bit of a sweet-spot in terms of having an old enough libc. Compiler problems are the cause for concern there however it's something we've had to deal with before: https://github.com/nodejs/build/tree/master/setup/debian-wheezy-gcc and I'm currently dealing with it for ARM: https://github.com/nodejs/node/issues/4531 (I have new gcc 4.9 packages built on a new machine, just need transfer them to the test & release machines and store them somewhere for future use). 
rvagg		ping @mhdawson  
jbergstroem		@rvagg he's currently on vacation. I can query osuosl about getting debian7 on there? 
rvagg		it can wait till he gets back, debian7 gets complicated with gcc and I'd rather avoid that complication if we can manage it .. somehow .. centos6 / rhel6? 
jbergstroem		I've been meaning to do this for a while as well. Thanks for sorting it -- LGTM. 
rvagg		Imma leave it to you @jbergstroem, it seems that whenever I change anything at the top of the `server` sections I break everything, no matter how trivial. 
jbergstroem		I have a vm somewhere with a ton of changes, this being one of them. I'll spin another one up tomorrow and verify it. 
phillipj		Thanks @jbergstroem. Guess I should try getting ansible up-n-running myself 
jbergstroem		Small note: you cannot redeclare `ipv6only=on`, but it's not really needed. Will merge/land without it. 
jbergstroem		Merged in e0880ae7cb5da7ec4270d805e9c131c043409506 without the extra `ipv6only=on`. 
jbergstroem		ping @rvagg
Trott		Looks like this has been resolved. Would be great to get a comment describing the fix if someone knows what fixed it. In any event, closing...
gibfahn		@jbergstroem remotely!   It'd be good to try to work out a rough agenda for what we should try to work on.   One good goal might be to get everyone running Johan's refactor branch and flushing out bugs, with the aim of getting it landed.
MylesBorins		I'll be there
santigimeno		I'll be there too and would love to help even though I'm not a member. Would that be ok?
gibfahn		>I'll be there too and would love to help even though I'm not a member. Would that be ok?  üëç üíØ Yes!
jbergstroem		> @gibfahn said: > One good goal might be to get everyone running Johan's refactor branch and flushing out bugs, with the aim of getting it landed.  That'd be awesome. I can only assume that there's a lot of things fixed/new bugs introduced with ansible upgrades.  The big gaps are: 1. aix -- most of the setup scripts still seem fully manual 2. windows 3. its falling out of date; there's been a few changes to things like www we need to re-integrate
refack		I'm jealous of you all going. I can't leave the US :( I'd be happy to join your session remotely to help with windows issues.
refack		P.S. VS2017
gibfahn		>aix -- most of the setup scripts still seem fully manual  FYI, we're looking at this, and I *think* we've got something working
mhdawson		Signed us up here, not sure what time was going to work for @jbergstroem  so just chose one after what I think his work hours were, Johan feel free to move. https://docs.google.com/spreadsheets/d/1gJoyiq0z6rjS7jEoHkRe6jte8Dd7ZBkwD3WjOh6djvQ/edit#gid=1606836561
piccoloaiutante		One thing we could also do is to identify one or two issues as `good-as-first-contribution` so that new comers can start helping the build group.
refack		Yes please!
gibfahn		We did something similar internally for monitoring machines.  The easiest way is probably just to monitor the node-test-commit job (you could also check other jobs), and write a quick script to work out whether the last X builds that ran on that machine failed.   cc/ @mhdawson @bgriggs @akpotohwo
Trott		Related, I suppose:  https://nodejs-ci-health.mmarchini.me/#/job-summary https://nodejs-ci-health.mmarchini.me/#/detailed-jobs  Not a node-by-node summary, but still pretty useful.  Anyway, given that this is inactive for almost 2 years, I'm going to close it. Feel free to re-open if you think that's the wrong thing to do. I'm mostly just panic-closing after noticing we have 228 issues open in this repo.
jbergstroem		LGTM 
jbergstroem		Another minor thing: Since path doesn't seem to be provided through `/etc/profile`we might want to add it as a envvar in the manifest (otherwise jenkins won't find path to git). For now, I've changed tooling path through jenkins admin.  **Edit**: tooling path nor env injection seemed to work. We'll going to have to edit the manifest. Suggesting adding this:  ``` xml <envvar name='PATH' value='/usr/local/sbin:/usr/local/bin:/opt/local/sbin:/opt/local/bin:/usr/sbin:/usr/bin:/sbin' /> ``` 
jbergstroem		(btw, confirmed that adding path fixed Jenkins) 
geek		Closing in favor of #64  
jbergstroem		Closing, we weren't affected.
jbergstroem		Oops, was referring to the wrong issue. Reopening.
MylesBorins		Has someone been able to lock down CI? We are ready to test  On Oct 23, 2017 10:58 AM, "Johan Bergstr√∂m" <notifications@github.com> wrote:  > Reopened #935 <https://github.com/nodejs/build/issues/935>. > > ‚Äî > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/935#event-1305823470>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecV5hSbtSTh2pnMKLuE0c-RrjEyi6Dks5svKmMgaJpZM4QCGwz> > . > 
benjamingr		@MylesBorins I have access to the CI at the moment - so I guess no.
gibfahn		@MylesBorins so who should still have access? I can change, but not sure who to leave.  Maybe nodejs/release and nodejs/jenkins-admins ? Not sure about build, collaborators and chakracore shouldn't obviously.
gibfahn		![image](https://user-images.githubusercontent.com/15943089/31899444-7f4a801a-b813-11e7-8c97-8b3d781c25da.png)  This is the current access list
MylesBorins		Jenkins admin and release makes sense
gibfahn		![image](https://user-images.githubusercontent.com/15943089/31900722-5830ef06-b817-11e7-9769-ec84d20dc43a.png)   Should be done.  _**EDIT:**_ Not sure why we tick every box for jenkins-admins, the first box == all the rest.  Also I didn't remove anonymous read permissions, now done.
benjamingr		Can confirm <img width="520" alt="screen shot 2017-10-23 at 19 27 15" src="https://user-images.githubusercontent.com/1315533/31900744-3084a0b8-b828-11e7-8d76-492ba702d026.png">  Good luck :) 
MylesBorins		A++  We should keep this open until we remove the embargo tomorrow
jbergstroem		@MylesBorins possibly file a bug over in the nodejs/node repo for visibility as well (link here to keep discussion in the appropriate place)?
MylesBorins		@jbergstroem done
gibfahn		>@MylesBorins possibly file a bug over in the nodejs/node repo for visibility as well (link here to keep discussion in the appropriate place)?  TBH I think an issue here cc/ing nodejs/collaborators (i.e. this issue) should be fine in general.
refack		@gibfahn can we have the CI back 
MylesBorins		The release is done... you can have CI back
gibfahn		Access should be restored, could someone confirm?
gibfahn		Actually I'm going to leave this open till I document how to disable access
refack		I'm closing and opening a doc issue, so that we can close this and remove of incident board
gibfahn		@refack did you open the doc issue?
refack		> refack did you open the doc issue?  did now https://github.com/nodejs/build/issues/949
rvagg		hardwired into Jenkins (node-test-commit-aix): `NODE_TEST_DIR=${HOME}/node-tmp PYTHON=python FLAKY_TESTS=$FLAKY_TESTS_MODE CONFIG_FLAGS="--dest-cpu=ppc64" CC=gcc gmake run-ci -j 5`  I've changed that to: ``` export JOBS=5  NODE_TEST_DIR=${HOME}/node-tmp PYTHON=python FLAKY_TESTS=$FLAKY_TESTS_MODE CONFIG_FLAGS="--dest-cpu=ppc64" CC=gcc gmake run-ci -j $JOBS ```  Depending on the cache situation, this may be making up to a 15 min difference. See https://ci.nodejs.org/job/node-test-commit-aix/10145/ compared to same run, same machine with `JOBS` in place https://ci.nodejs.org/job/node-test-commit-aix/10151/
mhdawson		All looks ok, closing. 
mhdawson		@rvagg 
rvagg		Our rsync is: `rsync -aqz --delete benchmark:charts/ /home/www/benchmarking/charts/`  Is `index.html` in the root of /home/benchmark/? if we change it to `rsync -aqz --delete benchmark: /home/www/benchmarking/` aren't we going to pick up a lot of guff in /home/benchmark? Perhaps we should shift this all to a subdirectory so we can do a full rsync?
mhdawson		index.html is in https://github.com/nodejs/benchmarking/tree/master/www.    What I though was set up was that there were 2 transfers:  1) The first from https://github.com/nodejs/benchmarking/tree/master/www 2) The mirror from the benchmarking machine to get the latest charts which I think is the one you show above.  I thought 1) was possibley setup as a trigger that would push changes when they were committed to the repo.  And I think it worked in the past as there have been a few updates to the index.html file.  
mhdawson		@rvagg, would it make sense to manually transfer over the updated index.html while we figure out whats up ? 
jbergstroem		Not sure if it helps but it looks like images are updated (last update: dec 6) while index.html still remains unchanged as of oct 12.
mhdawson		@jbergstroem correct.  The graphs are regularly updated. index.html comes from a different place (the benchmarking repo) and it should be updated when the version in the repo changes.  Is that something you can help with or something I can help with (I'm thinking I don't have enough access).  Right now this chart is being generated/transferred but does not display because index.html has not yet been updated: https://benchmarking.nodejs.org/charts/octane.png
jbergstroem		I can have a look at this tomorrow.
mhdawson		@jbergstroem still does not not seem to be showing up.  I'm wondering if it might make sense to expand the set of people who can chase down these kinds of issues.
jbergstroem		@mhdawson you're absolutely right. I still haven't looked at it, and to be frank haven't really prior  this either. What @rvagg seems to mention above is relevant though; index.html is from the parent directory and isn't being picked up in the rsync. Perhaps it can be moved into the "charts" directory and link to images in the same folder? that way it would be easier to rsync? 
mhdawson		Looks like @jbergstroem resolved yesterday and updated occurred last night. Closing.
mhdawson		Received the hardware, followed instructions to install OS and currently running ansible  
mhdawson		Ansible config complete  A few notes:  * There may be an issue with ansible 2 and the existing ansible scripts to install packages.  I had to make the following change in `ansible-playbook.yaml` to get the script to run with ansible 2: ``` @@ -58,7 +59,7 @@          name: "{{ item }}"          update_cache: yes          state: latest -      with_items: packages +      with_items: "{{ packages }}"        tags: general ``` * there are a bunch of warnings like this in other tasks(not the one I changed):  ``` WARNING]: when statements should not include jinja2 templating delimiters such as {{ }} or {% %}. Found: git_version.stdout.find('{{ version }}') == -1 ```   * I updated the script to skip setting up an APT proxy, which is probably fine since I'll only have the 1 machine.  * I updated the script to skip mounting the nfs mounts as I don't have them.  @rvagg do you think I'll need an nfs server in order to avoid running too slowly or wearing out the SD card ?  Judging by how long it took to compile git I'm guessing the answer might be yes.  * I used the name `release-devrus_mhdawson-debian7-arm_pi1p-1`.  devrus is as close to a "provider" that I have :)  * at this point I used the test ssh key, but assume we can simply replace with the release ssh key at the same time that we add any of the other additions that are required on the release machines.    
mhdawson		Ok added an nfs server to the local setup and re-ran ansible to include mounting instead of skipping.  @jbergstroem I think next step is to figure out the ssh tunnel.
mhdawson		@rvagg looks like the existing ARM6 release  raspberry PI's are off line.  Can you give them a kick ?
mhdawson		Ok, my raspberry pi is connected as release-devrus_mhdawson-debian7-arm_pi1p-1  I've already run a test job that builds on this new arm machine and its passed a few times.  For example:   https://ci-release.nodejs.org/job/iojs+release-dawson-test/9/  I also did a quick check of downloading the resulting archive from the test directory and made sure it started on another ARM6 raspberry PI that I have.  PR for the Ansible tweaks I needed to support the ssh tunnel is here:  https://github.com/nodejs/build/pull/924  @nodejs/build is there anything else I should do before we enable this machine for use in regular release builds ? 
mhdawson		@rvagg @jbergstroem I'm going to go ahead and enable the machine and we can validate that the nighlies are ok when they build on the machine.
rvagg		@mhdawson one thing I don't know if it's documented anywhere or not, you need the staging ssh set up. See one of the two existing release pi's for config and key:  ``` pi@release-requireio--rvagg-debian7-arm--pi1p-1 ~ $ sudo cat /home/iojs/.ssh/config Host iojs-www node-www   HostName direct.nodejs.org   User staging   IdentityFile ~/.ssh/node-www-staging ```  You'll need /home/iojs/.ssh/node-www-staging and you'll need to `chmod 700 .ssh` and `chmod 600 .ssh/*` and then `ssh node-www` and make sure it works and that it goes into host_vars. If this isn't done it won't be able to upload binaries to the server.
mhdawson		I did some config based on info from @joaocgreis and had validated that test binaries were uploaded to the download server. 
mhdawson		@rvagg checking now, there is a run going that has been running for 17 hours.  I had done a few tests runs and the first one was long, but the later ones varied between a couple of hours and 4-5.    Is this normal for some runs to take a really long time ?   I can tell it is still compiling....
mhdawson		Seems like it completed ok...
rvagg		that is a long time .. I've been running these release machines overclocked so they have been artificially high and will probably take longer now I've reset them back down. ccache should help a bit but release builds tend to be unique enough in their defines that it doesn't get to reuse old object files. 4-6 hours would be what I expect for a normal release.
mhdawson		This is an example of a 16hr run on a different machine so maybe its normal https://ci-release.nodejs.org/job/iojs+release/nodes=pi1-raspbian-wheezy/2249/console
mhdawson		Not perfect but run times and reliability seems as good as other arm 6 release machines.
rvagg		hmm, this was intentional in core (I haven't done a `git blame` to see who did it) and those are the exported values, it's correct either way but perhaps more friendly just showing an integer  /cc @bnoordhuis @maxogden thoughts on this? 
maxogden		the web UI (https://nodejs.org/en/download/releases/) just displays the data from https://nodejs.org/dist/index.json and does not modify it at all 
fhemberger		@maxogden Yes, the question is, do we need to fix the generator (are those values on purpose?) or do we fix this on the website, when importing index.json? 
rvagg		Well my question is whether a "fix" is even needed. I'm at +0 on that question.  
fhemberger		It looks weird having the majority of releases using decimal values and a handful using hex notation instead. Can we please adjust this at least on the website? 
fhemberger		Adding an explanation to nodejs.org, let's keep it this way. 
gibfahn		So we're starting with `power8-nodejs2.osuosl.org` (a.k.a. `test-osuosl-aix61-ppc64_be-2`). If everything works okay we can try with the other test machine, `power8-nodejs1.osuosl.org` (a.k.a. `test-osuosl-aix61-ppc64_be-3`)
gibfahn		If we go ahead with this, we need to make sure we update the Ansible script (and temporarily the manual instructions).
gdams		okay so running citgm on the ramdisk was much quicker (less than 20 minutes)
gdams		Commands to add the ramdisk:  ```bash mkramdisk 4G mkfs -V jfs2 /dev/ramdisk0  mkdir -p /ramdisk0 mount -V jfs2 -o log=NULL /dev/ramdisk0 /ramdisk0 mkdir /ramdisk0/citgm chown /ramdisk0/citgm iojs ```
gdams		From googling, [this](http://www.unix.com/aix/244552-how-can-we-re-mount-ram-disk-automatically-after-reboot-aix.html ) seems to be the easiest way to mount the ramdisk at startup: @edelsohn @mhdawson   ```bash echo 'ramdisk:2:once:/usr/local/bin/ramdisk/ramdisk.ksh >/dev/null 2>&1' \   >/usr/local/bin/ramdisk/ramdisk.ksh ```   ##### ramdisk.ksh  ```zsh RAMDISK=`mkramdisk 5G` mkdir /ramdisk0 >/dev/null 2>&1 DEVICE="/dev/ramdisk`echo $RAMDISK | awk -F'ramdisk' '{print $2}'`" echo "yes" | mkfs -V jfs2 $DEVICE mount -V jfs2 -o log=NULL $DEVICE /ramdisk0 mkdir /ramdisk0/citgm chown iojs /ramdisk0/citgm ```
mhdawson		@gdams, ramdisk.ksh seems to say '5M" is that correct or should it be 5G ?  A pull request against the build repo, adding ramdisk.ksh and updates to the ansible scripts to make it run on startup would be the best way to review 
gdams		Oh sorry, it looks like @gibfahn edited my comment, take the configuration from the above comment that mentions 4G
gibfahn		>ramdisk.ksh seems to say '5M" is that correct or should it be 5G ?  I copied that verbatim from the linked website. I've changed that to `5G`, but I have no idea if the rest of it is applicable.
gdams		I also added the same config to  power8-aix-nodejs3.osuosl.org  (a.k.a. `test-osuosl-aix61-ppc64_be-1`)
gibfahn		@gdams any chance you could PR this to the setup instructions?
CJKwork		On the adoptopenjdk infrastructure, I've created ksh scrip AIX_ramdisk.ksh and updated aix.yml to call it. It adds a 4Gb ramdisk, provided the ram is available.
rvagg		I've changed perms on .bashrc and .profile to `444` to prevent this happening again (crossing fingers that nvm isn't _too_ clever)
refack		> who's is this?  Ack, that's me. That job - https://ci.nodejs.org/job/node-inspect/ - is still in "beta" and hasn't landed yet (ATM only admins should have "run" capa) - https://github.com/nodejs/build/pull/956.  /CC @jkrems
refack		Ping @ljharb is there a way to use `nvm` in a local scope, without it changing global state?
jkrems		Ugh, changed `NVM_HOME` but forgot that its install script is messing with `.bashrc` w/o asking.  @refack It's already installing it in local scope (that's why it's trying to load nvm from the workspace's tmp dir). The problem is that I'm running the install script instead of just git-cloning nvm into the directory myself.
jkrems		Switching to direct git clone which should remove the side effect on bashrc: https://github.com/nodejs/build/pull/956/commits/1fb7b6046ed7304fef291133a6599decb06243c7
ljharb		The primary purposes of the install script is to install nvm and update your profile files; a direct git clone works fine too.  I'm confused tho why simply having those lines in bashrc would be a problem; unless you have a default alias specified, sourcing nvm.sh should be a noop.
refack		The directory defined by `export NVM_DIR="$HOME/build/workspace/node-inspect@tmp/.nvm"` is ephemeral and when it's doesn't exist those statements return exit code 1.
ljharb		They shouldn't; it's in a `test` - both lines are guarded by `-s` checks. The $NVM_DIR never has to exist; it's designed to gracefully do nothing in that case.  Are you setting some sort of bash options that might make it abnormally exit nonzero?
rvagg		darn, I wish I had have saved the error output because I can't replicate it now by putting it back in. Basically I noticed this when I logged in as root and did `su iojs` to switch to that account and was confronted with 1/2 a screen of error messages, stack traces and whatnot.  so my guess now is that `$HOME/build/workspace/node-inspect@tmp/.nvm` was partly intact so the bash was executing to load it, but something about the node installation wasn't, causing the problem. I'm pretty sure the error I was seeing was Node stack traces, so it was loading Node but not _fully_ somehow?
jkrems		Sorry for the confusion! Should've thought of that. :)
refack		@jkrems I'm trying to learn from this. For example, we should do beta testing only on sandboxed bots.
ljharb		It seems like the issue was that `[ -s "$NVM_DIR/nvm.sh" ]` will not, in fact, fail with `set -e` - but if it's the last line of `.bashrc`, it will cause the process to exit 1.  A simple fix is to make the last line of `.bashrc` exit zero, perhaps by adding `test ||:` or something.
rvagg		lgtm 
mhdawson		lgtm 
mhdawson		From here:  http://www.ibmsystemsmag.com/aix/tipstechniques/systemsmanagement/ahafs_event_monitoring/?page=2  trying this to see if that is all we need:  mkdir /aha mount -v ahafs /aha /aha 
mhdawson		It seems it was straight forward. Simply mounting seems to work.  This PR adds it to /etc/filesystems so it will be mounted at boot.  https://github.com/nodejs/build/pull/369 
mhdawson		PR landed closing this 
gdams		@gibfahn want to land?
rvagg		SGTM, we have plenty of downtime between Americans going to bed and Europeans waking up‚ÄîAsia primetime, my afternoons when nobody is around and CI is very quiet!
rvagg		https://ci.nodejs.org/view/Node.js Daily/  Hooked up v6.x-staging and v8.x-staging to new jobs and made a new view to hold these daily jobs. They are now distributed as following in Eastern time (don't ask me why it's doing this in Eastern):  * 2am coverity https://ci.nodejs.org/view/Node.js%20Daily/job/node-daily-coverity/ (all green) * 3am master https://ci.nodejs.org/view/Node.js%20Daily/job/node-daily-master/ (recent green) * 4am v4.x-staging https://ci.nodejs.org/view/Node.js%20Daily/job/node-daily-v4.x-staging/ (some platform-specific consistent failures) * 5am v6.x-staging https://ci.nodejs.org/view/Node.js%20Daily/job/node-daily-v6.x-staging/ (running first one now) * 6am v8.x-staging https://ci.nodejs.org/view/Node.js%20Daily/job/node-daily-v8.x-staging/ (running first one now) 
mhdawson		Great to see these. I had been thinking daily jobs for all shipping streams would be useful. 
refack		+1 Also if they need extra help, I'd be happy to pitch in too.
aruneshchandra		Lets start with the following as admins for now: @digitalinfinity @kfarnung  @jackhorton  @mike-kaufman  @MSLaguana 
joaocgreis		Can someone with access please create `node-chakracore-admins` with the members listed above? (perhaps @mhdawson @Trott @rvagg ?) Thanks!
mhdawson		I created the team and added joao, Hitesh and Kyle, I don't know the other ids by sight.  Can you give us a bit of context of who they are, involvement in Node.js etc.
mhdawson		Also to note that I've made @joaocgreis a maintainer for the team.  @joaocgreis if that is ok with you I'll remove myself as a maintainer/group member.
joaocgreis		I've given @nodejs/node-chakracore-admins access to ci-release, but I've made sure this only applies to the `node-chakracore` job. Everything else remains hidden.  Jack Horton (@jackhorton), Mike Kaufman (@mike-kaufman) and Jimmy Thomson (@MSLaguana) are all part of @nodejs/chakracore and are active contributors to https://github.com/nodejs/node-chakracore . They are all Microsoft employees, and I believe working directly with @digitalinfinity and @kfarnung .  Also, going forward, the @nodejs/node-chakracore-admins team can be managed by opening issues in the node-chakracore repo. Only members of @nodejs/node-chakracore who are active collaborators can be accepted. @nodejs/build let me know if you think this needs an extra layer of approval, given that this team has access to ci-release. Since access is limited to one job, I believe this is enough.  @mhdawson let me know if you have any questions or if I can add them. I can be a maintainer of the team, feel free to remove yourself. Thanks for creating the team! 
gibfahn		>@nodejs/build let me know if you think this needs an extra layer of approval, given that this team has access to ci-release.  So do the team have read access to https://ci-release.nodejs.org/job/iojs+release/ ? Ideally we'd make it so they couldn't even see that a build was happening, then there'd be 0 need for us to care about it. IDK if that's possible though.
joaocgreis		The team cannot see any job other than `node-chakracore`. I didn't check running jobs, but (unless Jenkins decides to surprise me) they should only be able to see the workers blocked by jobs they don't have access to. That's what happens in ci-test when private jobs are running.
gibfahn		Then no issue from me with node-chakracore-admins managing their group themselves.
joaocgreis		@mhdawson can you make clear if you have any objections or I can add the remaining members to @nodejs/node-chakracore-admins ? Thanks!
mhdawson		@joaocgreis its ok with me.
joaocgreis		All done. Thanks!
kfarnung		Thanks for taking care of this @joaocgreis!
digitalinfinity		Thanks @joaocgreis! 
jbergstroem		It looks like this on the aix release host: `install_assist:2:wait:/usr/sbin/install_assist </dev/console >/dev/console 2>&1` 
mhdawson		Ok, even though the line is slightly different I think we still need to comment it out or remove that line.  
jbergstroem		Is there anything else actionable here? (me and Michael did some irc talk) 
jbergstroem		Closing this; I believe the issues have been fixed. 
mhdawson		Agreed 
misterdjules		I think that tests that are currently testing the `--abort-on-uncaught-exception` command line option don't use the core file that is produced as the result of the process aborting.  Thus, disabling core file generation on the system on which the tests run is enough to not fill up space.  This is already done in most tests of `--abort-on-uncaught-exception` by [using `ulimit -c 0` on anything except Windows](https://github.com/nodejs/node/blob/master/test/parallel/test-domain-abort-on-uncaught.js#L236-L240).  Moreover, I think core dump are _not_ generated when a process aborts on Windows unless explicitly enabled, so we should be already OK on that platform.  Thus, I think `test/abort/test-abort-uncaught-exception.js` could implement the same mechanism and be moved to a folder that contains tests that are run by the CI platform. 
gibfahn		@misterdjules I think you are correct regarding windows not dumping with `--abort-on-uncaught-exception`, when I last checked it was impossible to enable coredumps without using an external tool (see https://github.com/nodejs/node/issues/7733).  
Trott		Is this still an issue? Or can it be closed?
maclover7		Seems to be a stale issue, closing for now. Please reopen if this is still needed.
gibfahn		I also noticed that [test-softlayer-ubuntu14-x86-1](https://ci.nodejs.org/computer/test-softlayer-ubuntu14-x86-1/) is misnamed (should be `1404` not `14`). I assume I should change the name in Jenkins rather than the inventory, thoughts?
refack		> I assume I should change the name in Jenkins rather than the inventory, thoughts?  According to the naming convention, I think rename in Jenkins https://github.com/nodejs/build/blob/89e15bed2131366c1ea4d38370a69ec879b7f298/ansible/README.md#L94
gibfahn		Yeah, I'll rename tomorrow if no-one corrects me.
joaocgreis		About renaming: renaming in Jenkins would be the right thing to do, `ubuntu1404` is better than just `ubuntu14`. However, the workers use that name to connect, so you'll have to rename in the workers as well (looks like that worker was deployed before the ansible refactor, so you'll have to figure out where to rename - in this case is `/etc/init/jenkins.conf`). There are many others `ubuntu12/14`, feel free to rename however many, or just leave them.
jbergstroem		Regarding "basic hints": I wrote something similar, but linking to the collaborators guide or having more info as part of the onboarding process would be better. Since we're doing some minor changes to this workflow we should perhaps wait until that lands and then add documentation somewhere. 
rvagg		I started docs here: https://github.com/nodejs/build/pull/48  I'm unlikely to find time to complete that in the near future though. 
orangemocha		Great. One more prescription: the user name and email address in Jenkins need to be what you want to appear in the git logs, because node-accept-pull-request uses it to populate the committer name & email. 
orangemocha		> @jbergstroem, @orangemocha and @misterdjules @joaocgreis  Shall we create a new team "jenkins-admins" or is @nodejs/build good for it? 
rvagg		@orangemocha yes, that's a good idea 
orangemocha		Done: https://github.com/orgs/nodejs/teams/jenkins-admins 
rvagg		Added @mhdawson to the list 
jbergstroem		All users mentioned above has been added. closing. Bonus points if anyone opens a PR that documents we expand our collaborator list. 
joaocgreis		@jbergstroem that's good news, I approve. 
Trott		:+1: 
MylesBorins		lgtm 
mhdawson		LGTM 
jbergstroem		I've now added his key to `test-joyent-freebsd10-x64-2`. 
jbergstroem		@brendanashworth: still need access?
brendanashworth		@jbergstroem sorry! I do not. Thanks!
jbergstroem		Removed. Closing.
jbergstroem		Progress update: A new master is now up and running and will likely be deployed early next week post the security release cycle. Access to it will be limited to @nodejs/jenkins-admins and @nodejs/release groups. I'll look at creating a PR at the main repo once it's in place so release guidelines are up to date.  Also, #243. 
jbergstroem		Fixed; closing. 
maclover7		Hmm, maybe better to pull the `log-collect` stuff out into a different PR, and land everything else first? The rest of the changes don't look too bad.
rvagg		OK, removed log-collect from here, will push that in a moment. I've added an upgrade to Node 8 in this too, see https://github.com/nodejs/nodejs.org/pull/1459 for website team's agreement on this.
rvagg		lgtm
mhdawson		I've asked for a BE 14.04 image in the new environment so that I can add some temporary machines there as well. 
mhdawson		Manged to get one of the be machines back online 
mhdawson		soft rebooted another machine which came online, but then the be one seems to be off line so back to no BE machines, trying to see if I can get the be one working 
mhdawson		Trying to get at least one BE machine by pausing the le one that came back and then soft-rebooting one of the be ones. 
jbergstroem		@mhdawson rebooting doesn't seem to work -- I've tried that a few times. Thinking the host is out of disk again like last time? 
mhdawson		Agreed it must the same issue as before. I had managed to get one back on-line by turning off an le machine that was running and then rebooting the be one.  But they are all off line again so I've pulled be from the plinux tests for the node regression test. 
mhdawson		Message from osu is that they are working on it but not sure if it will be resolved today.  There are also some BE 14.04 images on th new infra now but they don't quite look right as the keys are not getting injected as expected so I'm hesitant to use them until that gets resolved 
mhdawson		Space cleaned up and all machines back online, adding PPC BE back to the jobs 
mhdawson		Have not seen issues for ~ 1 week so closing. 
jbergstroem		Thanks for preparing this. I will attend; adding to calendar.
gibfahn		Is the plan to start recurring every 3 weeks again?
jbergstroem		@gibfahn correct.
joaocgreis		I will attend today.
mhdawson		Yes we should be back on the recurring 3 week schedule, believe it is as on the community google calendar.
mhdawson		meeting link : https://hangouts.google.com/hangouts/_/ytl/nIHGuM4xqsCuNs-64_MB-jd5fqKt2g_YhamrTxifLPY=?eid=100598160817214911030
gibfahn		Supported platforms PR: https://github.com/nodejs/node/pull/8922
mhdawson		Minutes for last meeting: https://github.com/nodejs/build/pull/651
mhdawson		Minutes landed, closing
santigimeno		Updated. PTAL. Thanks!
mscdex		It is especially painful stress testing on pi1 :-) 
maclover7		Closing for now as something that would be nice to have, but seems to not currently be within our means. If someone wants to tackle this, please feel free to reopen.
joaocgreis		This actually happened a long time ago: https://ci.nodejs.org/view/All/job/node-stress-single-test-pi1-fanned/ , but I probably was not aware of this issue. FWIW, this could probably be de-duplicated by using a pipeline job.
mhdawson		@jbergstroem don't think this would be a PPC issue but something like a firewall issue 
MylesBorins		It would appear that both the rc build and nightly on the 12th worked without issue. I have a new rc build going right now, if we see it fail I'll make a note of it in this comment 
jbergstroem		We don't firewall port 22. Network issues? 
mhdawson		Looks like it was ok in the run mentioned by @TheAlphaNerd.  The 2 failures were within 1 and 1/2 hours of each other so may it was something to do with the network.  In any case will close and we can re-open if it re-occurs. 
mhdawson		? It says December ?   Regards  Michael Dawson  770 Palladium Drive  J9VM Software Developer  Ottawa, Ontario K2B 8K1 Master Inventor  Canada QWM  IBM Software Group, Application and Integration Middleware Software  Phone: +1-613-270-4793  e-mail: michael_dawson@ca.ibm.com  From:   Johan Bergstr√∂m notifications@github.com To:     nodejs/build build@noreply.github.com Date:   01/14/2016 05:42 PM Subject:        [build] Softlayer reboots (#305)  Copy pasted message below. We'll be rebooting Saturday. Our Systems Engineers have been notified of a potential vulnerability  affecting Virtual Server Instances (VSIs). We have been working nonstop  with our technology partners to prepare a remediation. The remediation  will require maintenance to hypervisor nodes and a reboot of the virtual  server instances on those nodes. Listed below you will find the master  schedule for when maintenance will be applied on a data center basis. We  will post real-time progress updates in the subsequent data  center-specific maintenance notifications. Customers can watch those  either via the portal, or via email (if subscribed). New Virtual Machines  provisioned before the maintenance will be included in reboots. Saturday, December 12 15:00 UTC - SYD01 17:00 UTC - HKG02 20:00 UTC - CHE01 23:00 UTC - AMS01 Sunday, December 13 00:00 UTC - LON02 04:00 UTC - SAO01 05:00 UTC - TOR01 06:00 UTC - DAL05 08:00 UTC - SJC01 14:00 UTC - MEL01 17:00 UTC - SNG01 23:00 UTC - FRA02 Monday, December 14 05:00 UTC - WDC01 06:00 UTC - DAL09 16:00 UTC - TOK02 23:00 UTC - PAR01 Tuesday, December 15 04:00 UTC - DAL06 05:00 UTC - MON01 05:00 UTC - WDC04 06:00 UTC - HOU02 08:00 UTC - MEX01 08:00 UTC - SEA01 23:00 UTC - AMS03 Wednesday, December 16 07:00 UTC - DAL01 08:00 UTC - SJC03 23:00 UTC - MIL01 ‚Äî Reply to this email directly or view it on GitHub. 
jbergstroem		Haha; didn't even consider that -- just assumed it was related to the openssh issues that's been flying around. I will update this once I get a proper email. 
jbergstroem		We passed the period. Closing.  
gibfahn		It's listed here: https://github.com/nodejs/node/blob/master/tools/test.py#L1559, any special casing will be done in `tools/test.py`.   All our tests are currently in `test/*` (i.e. only one directory deep). Not sure what you mean by `tree-factor`, are you talking about having `test/sequential/buffer/` and `test/sequential/fs`? If so why? There are only 40 tests in sequential, there doesn't seem to be much point to moving tests around and adding complexity. Moving things has been done in the past, but it generates a load of churn, so is normally done sparingly.  In any case, I don't think there's anything build specific that requires that layout, but I suspect `test.py` probably does.
refack		@gibfahn I ment `test/parallel`. Thank. I'll try a little bit and adjust `test.py` to make sure it works.
gibfahn		`parallel` will be special-cased, as `tools/test.py -j4` will only run 4 at a time for the parallel test suite.  I think breaking it up was proposed before and didn't happen, it might be having a look for previous discussions before going down that road again.
refack		I can tell `test.py` that it's a single suite, just a two dirs deep. Anyway I'll try too look up the previous discussions.
joaocgreis		Ref: https://github.com/nodejs/node/pull/6230
maclover7		I think this is an issue for nodejs/node, going to close in favor of discussion there.
rvagg		I'm OK with pip _if we must_ have it, but often the popular packages are available via apt anyway, in this instance `python-numpy`, `python-requests` and `python-eventlet` all seem to be available in both 2.7 and 3 form.
gibfahn		>I'm OK with pip if we must have it, but often the popular packages are available via apt anyway, in this instance python-numpy, python-requests and python-eventlet all seem to be available in both 2.7 and 3 form.  We're hopefully moving to tap2junit at some point, and I don't think that'll be available on apt (and yum etc.).
mhdawson		@rvagg   so they can be installed with apt-get ie  apt-get install python-numpy apt-get install python-request apt-get install python-eventlet   ?  I had used pip as I believe it was already going to be available due to the need for tap2junit but when we work on the ansible scripts we can install the apt packages instead if they are available. 
rvagg		Ah, well if we're using pip already then I guess it comes down to updating. apt-get is often run manually by myself and maybe others on our test machines but that's all for updating. If there's a pip update process it's unlikely to ever be run. Is that a problem? I don't know! I feel like this is one for the python folks among us to answer. Sorry for the non-answer, my preference is still for apt-get install (`apt-get install python-numpy python-request python-eventlet -y`) but it's no big deal if we do it with pip _I guess_.
gibfahn		I'd prefer using `apt` where available too FWIW.  It would make sense to periodically rerun our ansible scripts (once we have them set up) to update everything.
mhdawson		@uttampower have you had a chance to start working on this yet ? 
maclover7		Related issues: #821, #955 
mhdawson		Related issue https://github.com/nodejs/build/issues/1147
orangemocha		This is happening on 15.04 as well. https://jenkins-iojs.nodesource.com/job/node-test-commit-linux/62/nodes=ubuntu1504-64/console  Would it be safe to apply the same to all Ubuntu versions? 
joaocgreis		@orangemocha It looks different though. I'll investigate as soon as I get access to that machine. 
orangemocha		Ah, you are right. Perhaps 15.04 is just our of disk space? 
orangemocha		LGTM 
joaocgreis		I updated the server manually.  Here are the errors that we had, for future reference: - While fetching a tag:  ```  > git -c core.askpass=true fetch --tags --progress https://github.com/joyent/node.git +refs/heads/*:refs/remotes/origin/* +refs/tags/jenkins-accept-commit-temp2:refs/remotes/origin/_jenkins_local_branch # timeout=20 ERROR: Error fetching remote repo 'origin' hudson.plugins.git.GitException: Failed to fetch from https://github.com/joyent/node.git     at hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:763)     at hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1012)     at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1043)     at hudson.scm.SCM.checkout(SCM.java:485)     at hudson.model.AbstractProject.checkout(AbstractProject.java:1284)     at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:610)     at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:532)     at hudson.model.Run.execute(Run.java:1741)     at hudson.matrix.MatrixRun.run(MatrixRun.java:146)     at hudson.model.ResourceController.execute(ResourceController.java:98)     at hudson.model.Executor.run(Executor.java:381) Caused by: hudson.plugins.git.GitException: Command "git -c core.askpass=true fetch --tags --progress https://github.com/joyent/node.git +refs/heads/*:refs/remotes/origin/* +refs/tags/jenkins-accept-commit-temp2:refs/remotes/origin/_jenkins_local_branch" returned status code 128: stdout:  stderr: remote: Counting objects: 5, done. remote: Compressing objects: 100% (1/1)    remote: Compressing objects: 100% (1/1), done. remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 error: no such remote ref refs/tags/jenkins-accept-commit-temp2 ``` - While fetching a branch:  ```  > git -c core.askpass=true fetch --tags --progress https://github.com/orangemocha/node-1.git +refs/heads/*:refs/remotes/origin/* +refs/heads/orangemocha-ConvergeCI:refs/remotes/origin/_jenkins_local_branch # timeout=20 ERROR: Error fetching remote repo 'origin' hudson.plugins.git.GitException: Failed to fetch from https://github.com/orangemocha/node-1.git     at hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:763)     at hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1012)     at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1043)     at hudson.scm.SCM.checkout(SCM.java:485)     at hudson.model.AbstractProject.checkout(AbstractProject.java:1284)     at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:610)     at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:532)     at hudson.model.Run.execute(Run.java:1741)     at hudson.matrix.MatrixRun.run(MatrixRun.java:146)     at hudson.model.ResourceController.execute(ResourceController.java:98)     at hudson.model.Executor.run(Executor.java:381) Caused by: hudson.plugins.git.GitException: Command "git -c core.askpass=true fetch --tags --progress https://github.com/orangemocha/node-1.git +refs/heads/*:refs/remotes/origin/* +refs/heads/orangemocha-ConvergeCI:refs/remotes/origin/_jenkins_local_branch" returned status code 128: stdout:  stderr: remote: Counting objects: 75, done. remote: Compressing objects:  33% (1/3)    remote: Compressing objects:  66% (2/3)    remote: Compressing objects: 100% (3/3)    remote: Compressing objects: 100% (3/3), done. remote: Total 75 (delta 46), reused 46 (delta 46), pack-reused 26 error: no such remote ref refs/heads/orangemocha-ConvergeCI ``` 
rvagg		https://github.com/nodejs/build/issues/148#issuecomment-127220354 see also comment about 15.04 errors 
Trott		/cc @joaocgreis 
joaocgreis		Added. Let me know if there's any issue. 
refack		> Added. Let me know if there's any issue.  Thanks!
gibfahn		If https://github.com/nodejs/node/pull/12672 lands then we're good to update mid-stream.  _**EDIT:**_ It landed
jbergstroem		Relevant discussion in here too: https://github.com/nodejs/build/issues/688#issuecomment-297478568
mhdawson		I think technically it should be semver-major to switch the libc base as I could see that there would be a reasonable expectation that unless you upgrade to a new major Node.js version you don't have to upgrade the OS that you are running your application on.  Going forward we should probably look at the EOL of the platforms we build on and make sure the minimum level specified for an LTS release will live until the end of its lifespan.  In this specific case, it would be useful to know how many customers might still be using older versions to know what the impact of switching would be.  Note sure how we'll get the data though.   
jasnell		> Going forward we should probably look at the EOL of the platforms we build on and make sure the minimum level specified for an LTS release will live until the end of its lifespan.  This is definitely a good policy in general. It would require being a bit more proactive about finding and documenting the expected EOL dates for supported platforms. At the moment we're fairly loose about that. 
jasnell		Just to clarify, what would be the specific impact for an average user if we switch to building on CentOS 6?
jbergstroem		What lifespan though? There's Rhel, Centos or even Oracle and Unbreakable. Some extends EOL, some doesn't. I think an interesting add to this mix is the fact that although we provide packages it is not always our responsibility to cater for every permutation. Thats why some distributions gets paid and some doesn't.   Personally, I'd be happy with following the "majority oss upstream" distributions and just drop at their EOL.
bnoordhuis		> I could see that there would be a reasonable expectation that unless you upgrade to a new major Node.js version you don't have to upgrade the OS that you are running your application on  False equivalence, IMO.  If you willfully stay at an OS version that its vendor no longer cares to support, you have no reasonable expectation that third-party software keeps working.
gibfahn		>Just to clarify, what would be the specific impact for an average user if we switch to building on CentOS 6?  You might be unable to run the binary on a machine with an earlier `libc` (or kernel version) than the machine we build on (AIUI).
gibfahn		>Going forward we should probably look at the EOL of the platforms we build on and make sure the minimum level specified for an LTS release will live until the end of its lifespan.  @jasnell @mhdawson as I said in https://github.com/nodejs/node/pull/12672, that means dropping support for Ubuntu 14.04 in Node 8. I don't think that's a reasonable thing to do. Basically we have to drop support for platforms 2¬Ω years before their maintainers do.
mhdawson		@gibfahn that specific example makes a good point. Maybe I should have said that we should take it into consideration.   It is also an argument for the "its supported as long as the OS is".  Personally I'm ok with that, although if an OS is going to go EOL the month after the release then maybe just not supporting it at all makes sense.
joyeecheung		IMO if there is no significant problem building Argon and Boron on CentOS5, then keep building them on CentOS5 will at least give people enough time to upgrade to a newer OS without having to build a binary themselves / deal with libc upgrades. It doesn't even need to be supported on these old platforms, but it should at least be able to run there when no additional compatibility work is needed.
gibfahn		>You can't even do a yum update now on CentOS 5  >if there is no significant problem building Argon and Boron on CentOS5, then keep building them   The fact that there are no more updates means that we won't get security patches, which isn't really acceptable for our main build machine. RHEL 7 has been out for almost 3 years, and RHEL 6 for 6 years.  People can continue to use Node 6.10.3 for as long as they like, but if they don't get updates for their OS then they probably don't update their Node instance. The [supported platforms](https://github.com/nodejs/node/blob/v4.x/BUILDING.md#supported-platforms-1) lists the lowest kernel level as `glibc >= 2.19`, so people should really have been upgrading after Node 0.12 went EoL.  >deal with libc upgrades  Updating the libc on a machine isn't really possible.  >It doesn't even need to be supported on these old platforms  The problem is that maintaining machines is a constant cost. I understand the argument of "why stop using a working platform", but the problem is that when the next thing breaks we won't be able to stop all releases while we transition over to CentOS 6, that's something we need to be looking at in advance.
joyeecheung		> People can continue to use Node 6.10.3 for as long as they like, but if they don't get updates for their OS then they probably don't update their Node instance  From my experience this is not usually true in an enterprise if the OS is managed by one team and the application runtime is managed by another team, as you don't have to use the global node in `PATH` to run an application so they could be updated separately, and it is common to bundle the application with the Node instance together as a standalone unit. (Same goes for desktop OS where people are using Node to build GUI applications, although that's usually Windows).  > Updating the libc on a machine isn't really possible.  AFAIK not safely for sure, but it is possible, with hacks..which is why that's not what people usually want to do.  > The fact that there are no more updates means that we won't get security patches, which isn't really acceptable for our main build machine. RHEL 7 has been out for almost 3 years, and RHEL 6 for 6 years.  I understand that, but is it possible to provide an distro built on CentOS5 alongside the ones built on a newer OS? If that's still too much hassle then nevermind, people can still get a binary somewhere other than the official website, although the official website is where people trust the most.
bnoordhuis		The issue I have with saying "yes, we'll keep on supporting centos 5" is that keeping the code base working with obsolete technology often requires a disproportionate amount of effort.  Case in point: the fix we had to develop and the buildbot we had to bring up to work around bugs in clang 3.4.1 that had been fixed in 3.4.2.  The case of centos 5 is doubly aggravating because anyone still using it today had **6 years** to upgrade to centos 6 but chose not to for whatever reason.  That's irresponsible bordering on negligent and not something that should be condoned or encouraged by committing to its continued support.
gibfahn		>From my experience this is not usually true in an enterprise if the OS is managed by one team and the application runtime is managed by another team  sure, but from my experience if the devops team managing the OS haven't seen the need to upgrade the system in the last 6 years, they probably won't do it until they have to, which will be when the next Node binary stops working and the applications team start pestering them.
rvagg		See https://github.com/nodejs/build/pull/740, added 2 new hosts and some Jenkins config to make this >=8.0.0, still testing but if all goes well and there are no objections, 8.0.0 should be libc 2.12 instead of 2.5. Built using devtoolset-2 so same gcc as we are using on CentOS 5 still.
rvagg		Existing builds:  ``` $ $ objdump -T /usr/local/bin/node | awk '{ print $5}' | grep GLIBC | sort | uniq GLIBC_2.2.5 GLIBC_2.3 GLIBC_2.3.2 GLIBC_2.3.3 GLIBC_2.4 GLIBCXX_3.4 ```  New builds for Node 8:  ``` $ objdump -T /usr/local/bin/node | awk '{ print $5}' | grep GLIBC | sort | uniq GLIBCXX_3.4 GLIBCXX_3.4.10 GLIBCXX_3.4.11 GLIBCXX_3.4.9 GLIBC_2.2.5 GLIBC_2.3 GLIBC_2.3.2 GLIBC_2.3.3 GLIBC_2.7 ```  So technically I think that means we maintain Debian 5 (Lenny) & Ubuntu 8.04 support (yay?) since we're not even reaching up to 2.12.  There's a test build @ https://nodejs.org/download/test/v8.0.0-test201705296aa5896d1b14eb59d4d2c5b1a725b8eed1abfa64/ if anyone else wants to poke at it.
rvagg		Removed `ctc-review`, we've pretty much locked this in now. Node.js 4 & 6 LTS will continue to be compiled on CentOS 5 (somehow we'll have to make this work!), 8+ are on CentOS 6.  Since this is also reflected in Node's BUILDING.md I'm closing this. If anyone has further discussion points on this feel free to reopen.
rvagg		Thanks @no9! I've fixed this in 631fda6 but you were right that this is a `server_name` issue, I've just opted for being explicit in the redirect instead. 
rvagg		My primary experience with selinux has been in disabling it so I don't think I can help beyond that! I'm fine with disabling it here but I guess it's something we'll want in our test environment at some point if it's coming out enabled like this on systems.  Since the playbooks are looking the same, perhaps they could be collapsed into a single "fedora" playbook with some rules to differentiate between them where needed? I did this in the Raspberry Pi one to support both wheezy and jessie. 
rvagg		this lgtm for me though, merge as is if you want, the above are just suggestions 
jbergstroem		Just so I don't forget: fedora23 will bail since it has python but not python-libselinux. I reckon we should start splitting up in proper ansible roles here and then parse `inventory_hostname` to figure out all the cases (if os is fedora23..) 
rvagg		I think `ansible_distribution_release` will probably do what we need 
jbergstroem		@rvagg: problem being we don't have facts at that stage. I'm writing a filter plugin that parses our naming so we can extract os and use that. Will probably be applicable at a lot of places! 
jbergstroem		complete support can be found in my branch: https://github.com/jbergstroem/build/tree/feature/refactor-the-world/ansible 
jbergstroem		Closing in favor of the refactor. 
mhdawson		Nits address and landed as 431d9a2eab74e6f055582e5d3b42d8097c9b8ded 
jbergstroem		Typo: Novembeddr -> November  We should also mention that we will offer an alternative for clients that doesn't support this at `unencrypted.{nodejs,iojs}.org` that allows http and rsync protocols. These services have been given an EOL at 2022-01-01 but might be extended if no other viable alternative has been made available.  @rvagg lgty? 
mhdawson		@jbergstroem @rvagg updated based on @jbergstroem comments.  Left out the part about an extension as 2022 is pretty far out anyway so I don't think softening by mentioning a possible extension helps. 
gibfahn		ref: https://github.com/nodejs/build/issues/55 
mhdawson		@jbergstroem  @rvagg any comments or should we plan to send this out ? 
mhdawson		Added to WG agenda in case we don't close on it before then. 
jbergstroem		(i have no more comments) 
MylesBorins		quick ping to @ljharb and @qw3rtman to confirm this would not negatively affect n or nvm 
joaocgreis		Text LGTM.  Perhaps when we enable this completely we can add a visible notice to the main page warning users about this (for one week perhaps?). Users that are not able to connect will probably try another computer/browser and if they can access there they will find an explanation. 
Trott		Text LGTM 
gibfahn		So presumably `unencrypted.nodejs.org` would be identical to the current `nodejs.org`? 
jbergstroem		@gibfahn: correct. it rsyncs every N minutes (I forget) and makes it available over rsync and http. We might look at adding ipfs at some point. 
ljharb		This will absolutely break anyone using an older version of nvm (luckily, a much much older version) that has the mirror pointed at the HTTP version - however, anyone who wants io.js support, or working node4+ support, has a version of nvm that points at the HTTPS URL, so I think this should be fine.  Note that https://github.com/nodejs/build/issues/233 is still open and prevents older clients from accessing iojs.org even with SSL - fixing that first would be very helpful to mitigate the impact of this change. 
jbergstroem		@ljharb seeing how not much has changed in #233 I don't think we should rely on it being supported in the near future. Is it possible for you to fallback to `unencrypted` or similar? Old clients is one of the reasons we offer this service in the first place. 
ljharb		Possibly - I don't really have any way of detecting that fallback reliably, so I'd have to code it to unconditionally retry the HTTP version, and I'm loath to do that. 
rvagg		1. there's mention of rsync in here but we haven't announced it anywhere else yet, perhaps we should roll it up into a single post? @jbergstroem did we come up with initial text for that? 2. the plan with unencrypted.nodejs.org was to just support the downloads directory, we can extend that to do all of www content, what does @nodejs/build think of that? I think I'm in favour of keeping it simple at the moment just have /downloads/ stuff like is there now (see http://unencrypted.nodejs.org, it's up rn). 3. We should probably describe the state of play right now wrt http access, it's fairly limited: `location ~ ^/(?!(dist/|dist$|\.json$))` - everything else is effectively HSTS, just via redirect. Also a mention of why it's like this, i.e. old clients like nvm since nodejs.org was http-only for a lot of its life. When doing breaking changes it's always good to give people a heads-up on specifics so they know what to look out for. 
ljharb		@rvagg the `dist` directory would be necessary to keep, since that's the only one `nvm` currently uses. 
rvagg		@ljharb on unencrypted.nodejs.org you mean? there's no plan to remove it from nodejs.org, we're stuck with it forever methinks 
ljharb		I suppose it doesn't matter on `unencrypted`, it just means my messaging to people that this change breaks would be slightly more complex (ie, rather than just "add the unencrypted cname", it'd be "use this URL") 
rvagg		@ljharb we can do /dist/ on unencrypted if it makes it easier for nvm, that's no problem, it's just an alias for /download/release/ 
jbergstroem		1. Initial rsync text: Can't find it. I'll throw something together. 2. I'm `-1` to expanding what will be available through `unencrypted.nodejs.org`. Downloads should have to do. It's 2016 for pete's sake. 3. Good idea. Perhaps merge suggested text into a bigger one that covers general direction. 
ljharb		You say "it's 2016"  as if this thread isn't the first moment I've (the largest consumer of it) ever heard about deprecating "dist" 
jbergstroem		@ljharb wasn't referring to dist, rather `www` stuff (website, api docs, et al). I don't think we'll ever be able to move away from `dist`  
Trott		Seems like this must be close-able at this point, but by all means, comment or re-open if I'm wrong.
mhdawson		Its an interesting idea.  linuxone does finish much faster than the others. @jbergstroem what's your thought ?
maclover7		ping -- any updates here?
gibfahn		I think the use of this in [node-test-pull-request-lite](https://ci.nodejs.org/job/node-test-pull-request-lite/) covers part of this, and the github-bot auto-run CI discussions cover the rest of it.  @refack correct me if I'm wrong :grin:.
joaocgreis		cc @nodejs/build  
rvagg		rubber-stamp lgtm but I'd like to see input from @jbergstroem if he has time 
geek		LGTM 
jbergstroem		LGTM 
joaocgreis		LGTM, welcome! 
jbergstroem		LGTM üéâ  
mhdawson		LGTM 
Starefossen		LGTM üòÑ  
mhdawson		Landed as f4fff5eda3513c287b47e91c5e769dfc58fef45e 
Starefossen		‚ú®üéâ‚ú® 
jbergstroem		@mhdawson fyi - I just force pushed; changed the `oe` in my last name to `√∂`. I usually don't care but since I copy paste from `git log` I reckon others likely does too :) 
refack		seems like the linuxone bots are choking on IPv6 addresses: ``` $ time curl -v -4 --trace-time http://google.com > /dev/null  real    0m0.095s user    0m0.004s sys     0m0.006s ```  ``` curl -v --trace-time http://google.com > /dev/null   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     013:24:19.184415 * About to connect() to google.com port 80 (#0) 13:24:19.184584 *   Trying 2607:f8b0:4006:811::200e...   0     0    0     0    0     0      0      0 --:--:--  0:01:19 --:--:--     0 ``` `0:01:19` and counting
refack		As a workaround I've set `/etc/gai.conf`. Should be removed when we know the problem is fixed. @nodejs/platform-s390 
gibfahn		Hmm, maybe an issue with the infra it's hosted on, not sure.  cc/ @mhdawson , we should probably ask Marist
refack		Just tested, and problem has been resolved. I moved `/etc/gai.conf` to `/etc/gai.conf.disabled`.
jbergstroem		I'm ok with this and can update. Anyone else? 
rvagg		done 
rvagg		probably do a daily `mysqldump`, compress the output and we'll rsync it off the machine with the same ssh key we'll use to get the data onto the web. we don't have a clear strategy for this yet and are currently copying things between machines but we need to get more organised with backups from www, jenkins and now this. 
jbergstroem		I'll have a think about backup over the weekend and put together my thoughts. 
mhdawson		Sounds good.   As an FYI I'm away all next week so I won't respond to any comments until the week of the 25th. 
mhdawson		Ok so backups are now setup to generate new back up in /home/benchmark/backups on the benchmark data machine  There will be 2 files in that directory:  backup-current - latest backup backup-previous - backup from the previous day 
mhdawson		@jbergstroem  can you let me know what I need to do to get these backup files mirrored into the overall backup solution you are working on in https://github.com/nodejs/build/issues/308 
jbergstroem		@mhdawson my thoughts were to run mysqldump over the wire (see [this comment](https://github.com/nodejs/build/issues/308#issuecomment-173438546)). If you create a user that has read only access to the database and copy the backup pubkey from the secrets repo we should be ready to go. 
rvagg		@sdayan FYI it wasn't strictly necessary to move the issue, we can pull in the appropriate people to where you opened it. Ideally this would go in nodejs/node but it's fine here now and we could have managed in nodejs/help.  Going to need to pull in @nodejs/platform-windows folks to help on this one, specifically I think @joaocgreis and @orangemocha might have a clue. My guess is that this has to do with the new infrastructure we're building the .msi's on. 
orangemocha		@sdayan : can you please double-check that the msi is indeed present at E:\DIR\node-v0.12.9-x64.msi ? I get the same error when the path is wrong. It installs fine for me otherwise. 
sdayan		Closing the issuer,thanks in fact i forgot to include the package in my Azure/workerRole (Copy if newer) so the VM was booting without the msi. Thank you very much @orangemocha  ! 
ljharb		Thanks - I'd need `index.tab` to list every version down to v0.1.14 for me to be able to throw out my legacy scraping code :-/ any chance of that happening?  (this unblocks https://github.com/creationix/nvm/issues/870 as well, so thanks!) 
mhdawson		Machines are back.  Ireland (where they are hosted) was hit by a category 3 hurricane around the time of the outage...
mhdawson		We'll need to have both the existing version of gcc (to build older node versions) as well as the newer version and figure out how to make them co-exist.  I'm +1 for going with 6.3.0
seishun		@gibfahn Any progress on this? Any way I can help?  >We'll need to have both the existing version of gcc (to build older node versions) as well as the newer version and figure out how to make them co-exist.  Is it not possible to run node built with gcc 4.9 on AIX with gcc 4.8 installed?
gibfahn		>Is it not possible to run node built with gcc 4.9 on AIX with gcc 4.8 installed?  Unfortunately not, you need the right version of `libstdc++`.
seawatcher2011		I am receiving the same error on AIX 7.1 while while verifying node ibm-8.9.4.0-node-v8.9.4-aix-ppc64.bin installation. Is there now a fix for the problem? Thanks.  oracle@stpisa02sa:/opt/freeware/lib/pthread/ppc64/ [] node --version exec(): 0509-036 Cannot load program node because of the following errors:         0509-130 Symbol resolution failed for node because:         0509-136   Symbol _ZNKSt9type_infoeqERKS_ (number 359) is not exported from                    dependent module /opt/freeware/lib/pthread/ppc64/libstdc++.a[libstdc++.so.6].         0509-192 Examine .loader section symbols with the                  'dump -Tv' command. 
gibfahn		@seawatcher2011 do you have gcc 4.8 installed? See https://github.com/nodejs/node/issues/14785 for more info.
seawatcher2011		@gibfahn - no, not gcc 4.8. I have gcc 6.3.0. What is the suggestion workaround? Deinstall gcc6.3.0 and install gcc 4.8?  ``` oracle@stpisa02sa:/opt/freeware/lib/pthread/ppc64/ []  gcc -v Using built-in specs. COLLECT_GCC=gcc COLLECT_LTO_WRAPPER=/opt/freeware/libexec/gcc/powerpc-ibm-aix7.1.0.0/6.3.0/lto-wrapper Target: powerpc-ibm-aix7.1.0.0 Configured with: ../gcc-6.3.0/configure --prefix=/opt/freeware --mandir=/opt/freeware/man --infodir=/opt/freeware/info --with-local-prefix=/opt/freeware --with-as=/usr/bin/as --with-ld=/usr/bin/ld --enable-languages=c,c++,fortran,objc,obj-c++ --enable-version-specific-runtime-libs --disable-nls --enable-decimal-float=dpd --with-cloog=no --with-ppl=no --disable-libstdcxx-pch --enable-__cxa_atexit --host=powerpc-ibm-aix7.1.0.0 Thread model: aix gcc version 6.3.0 (GCC) oracle@stpisa02sa:/opt/freeware/lib/pthread/ppc64/ [] lslpp -Lq | grep c++   libstdc++                  6.3.0-1    C     R    GNU Standard C++ Library oracle@stpisa02sa:/opt/freeware/lib/pthread/ppc64/ []  lslpp -Lq | grep gcc   gcc                        6.3.0-1    C     R    GNU Compiler Collection   gcc-cpp                    6.3.0-1    C     R    The C Preprocessor (/bin/rpm)   libgcc                     6.3.0-1    C     R    GCC version 6.3.0 shared ``` 
gibfahn		>@gibfahn - no, not gcc 4.8. I have gcc 6.3.0. What is the suggestion workaround? Deinstall gcc6.3.0 and install gcc 4.8?  Unfortunately you will need 4.8. However you should be able to have them both installed side-by-side. I've heard this is possible but haven't confirmed it myself, that's in fact the point of this issue.
gibfahn		I have now managed to get this working, but there was one pain point.  On one machine I tested on I got this error persistently, implying that the `LIBPATH` wasn't being passed down to the `g++` call.  ```   LD_LIBRARY_PATH=/home/gib/node/out/Release/lib.host:/home/gib/node/out/Release/lib.target:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; cd ../.; mkdir -p /home/gib/node/out/Release/obj/gen; python tools/js2c.py "/home/gib/node/out/Release/obj/gen/node_javascript.cc" lib/internal/bootstrap/loaders.js lib/internal/bootstrap/node.js lib/async_hooks.js lib/assert.js lib/buffer.js lib/child_process.js lib/console.js lib/constants.js lib/crypto.js lib/cluster.js lib/dgram.js lib/dns.js lib/domain.js lib/events.js lib/fs.js lib/fs/promises.js lib/http.js lib/http2.js lib/_http_agent.js lib/_http_client.js lib/_http_common.js lib/_http_incoming.js lib/_http_outgoing.js lib/_http_server.js lib/https.js lib/inspector.js lib/module.js lib/net.js lib/os.js lib/path.js lib/perf_hooks.js lib/process.js lib/punycode.js lib/querystring.js lib/readline.js lib/repl.js lib/stream.js lib/_stream_readable.js lib/_stream_writable.js lib/_stream_duplex.js lib/_stream_transform.js lib/_stream_passthrough.js lib/_stream_wrap.js lib/string_decoder.js lib/sys.js lib/timers.js lib/tls.js lib/_tls_common.js lib/_tls_wrap.js lib/tty.js lib/url.js lib/util.js lib/v8.js lib/vm.js lib/zlib.js lib/internal/async_hooks.js lib/internal/buffer.js lib/internal/cli_table.js lib/internal/child_process.js lib/internal/cluster/child.js lib/internal/cluster/master.js lib/internal/cluster/round_robin_handle.js lib/internal/cluster/shared_handle.js lib/internal/cluster/utils.js lib/internal/cluster/worker.js lib/internal/crypto/certificate.js lib/internal/crypto/cipher.js lib/internal/crypto/diffiehellman.js lib/internal/crypto/hash.js lib/internal/crypto/pbkdf2.js lib/internal/crypto/random.js lib/internal/crypto/sig.js lib/internal/crypto/util.js lib/internal/constants.js lib/internal/encoding.js lib/internal/errors.js lib/internal/freelist.js lib/internal/fs.js lib/internal/http.js lib/internal/inspector_async_hook.js lib/internal/linkedlist.js lib/internal/modules/cjs/helpers.js lib/internal/modules/cjs/loader.js lib/internal/modules/esm/loader.js lib/internal/modules/esm/create_dynamic_module.js lib/internal/modules/esm/default_resolve.js lib/internal/modules/esm/module_job.js lib/internal/modules/esm/module_map.js lib/internal/modules/esm/translators.js lib/internal/safe_globals.js lib/internal/net.js lib/internal/os.js lib/internal/process/esm_loader.js lib/internal/process/next_tick.js lib/internal/process/promises.js lib/internal/process/stdio.js lib/internal/process/warning.js lib/internal/process.js lib/internal/querystring.js lib/internal/process/write-coverage.js lib/internal/readline.js lib/internal/repl.js lib/internal/repl/await.js lib/internal/socket_list.js lib/internal/test/binding.js lib/internal/test/unicode.js lib/internal/timers.js lib/internal/tls.js lib/internal/trace_events_async_hooks.js lib/internal/tty.js lib/internal/url.js lib/internal/util.js lib/internal/util/comparisons.js lib/internal/util/inspector.js lib/internal/util/types.js lib/internal/http2/core.js lib/internal/http2/compat.js lib/internal/http2/util.js lib/internal/v8.js lib/internal/v8_prof_polyfill.js lib/internal/v8_prof_processor.js lib/internal/stream_base_commons.js lib/internal/vm/module.js lib/internal/streams/lazy_transform.js lib/internal/streams/async_iterator.js lib/internal/streams/buffer_list.js lib/internal/streams/duplexpair.js lib/internal/streams/legacy.js lib/internal/streams/destroy.js lib/internal/streams/state.js lib/internal/wrap_js_stream.js deps/v8/tools/splaytree.js deps/v8/tools/codemap.js deps/v8/tools/consarray.js deps/v8/tools/csvparser.js deps/v8/tools/profile.js deps/v8/tools/profile_view.js deps/v8/tools/logreader.js deps/v8/tools/arguments.js deps/v8/tools/tickprocessor.js deps/v8/tools/SourceMap.js deps/v8/tools/tickprocessor-driver.js deps/node-inspect/lib/_inspect.js deps/node-inspect/lib/internal/inspect_client.js deps/node-inspect/lib/internal/inspect_repl.js deps/acorn/dist/acorn.js deps/acorn/dist/walk.js ./config.gypi tools/check_macros.py src/notrace_macros.py src/noperfctr_macros.py tools/nodcheck_macros.py   touch e54431f42526df1774c55ade864265c11db54855.intermediate   LD_LIBRARY_PATH=/home/gib/node/out/Release/lib.host:/home/gib/node/out/Release/lib.target:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; cd ../deps/v8/gypfiles; mkdir -p /home/gib/node/out/Release/obj/gen/src/inspector/protocol /home/gib/node/out/Release/obj/gen/include/inspector; python ../third_party/inspector_protocol/CodeGenerator.py --jinja_dir ../third_party --output_base "/home/gib/node/out/Release/obj/gen/src/inspector" --config ../src/inspector/inspector_protocol_config.json   /home/gib/tmp/gcc-6.3.0-1/opt/freeware/bin/g++ '-DU_COMMON_IMPLEMENTATION=1' '-DU_ATTRIBUTE_DEPRECATED=' '-D_CRT_SECURE_NO_DEPRECATE=' '-DU_STATIC_IMPLEMENTATION=1' '-DUCONFIG_NO_SERVICE=1' '-DUCONFIG_NO_REGULAR_EXPRESSIONS=1' '-DU_ENABLE_DYLOAD=0' '-DU_HAVE_STD_STRING=1' '-DUCONFIG_NO_BREAK_ITERATION=0' -I../deps/icu-small/source/common  -pthread -Wall -Wextra -Wno-unused-parameter -maix64 -Wno-deprecated-declarations -O3 -fno-omit-frame-pointer -fno-exceptions -std=gnu++1y -frtti -MMD -MF /home/gib/node/out/Release/.deps//home/gib/node/out/Release/obj.target/icuucx/deps/icu-small/source/common/appendable.o.d.raw   -c -o /home/gib/node/out/Release/obj.target/icuucx/deps/icu-small/source/common/appendable.o ../deps/icu-small/source/common/appendable.cpp exec(): 0509-036 Cannot load program /home/gib/tmp/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1plus because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  gmake[1]: *** [/home/gib/node/out/Release/obj.target/icuucx/deps/icu-small/source/common/appendable.o] Error 1 gmake[1]: Leaving directory `/home/gib/node/out' gmake: *** [node] Error 2 ```  With @refack's help I eventually tracked this down to this mailing list discussion: https://lists.gnu.org/archive/html/bug-make/2001-03/msg00067.html  ```console $ ls -l `which gmake` lrwxrwxrwx    1 root     system           27 Jul 25 2013  /usr/bin/gmake -> ../../opt/freeware/bin/make                                                                                   $ ls -l /opt/freeware/bin/make -rwxr-sr-x    1 root     system       280288 Sep 22 2011  /opt/freeware/bin/make ```  Running `sudo chmod 755 /opt/freeware/bin/make` did indeed resolve the issue.  The `setuid` bit is not set in the `test` or `release` machines (although it was on the machine I was using to test).
gibfahn		@seawatcher2011 your mileage may vary, but what worked for me is to extract the contents of the libstdc++ rpm into a directory, then set the LIBPATH to point to that directory.  I used rpm2cpio to extract the rpm, which isn't available on AIX (I got it from a RHEL box, but I think it's available on Ubuntu).  So first you need to download the rpm from wherever you got gcc from, either [AIX Toolbox](https://www-03.ibm.com/systems/power/software/aix/linux/toolbox/alpha.html) or [Bull Freeware](http://www.bullfreeware.com/toolbox.php). Either should work, but you can't mix and match.  AIX Toolbox: ftp://public.dhe.ibm.com/aix/freeSoftware/aixtoolbox/RPMS/ppc-6.1/gcc/libstdcplusplus-4.8.5-1.aix6.1.ppc.rpm Bull Freeware: I can't get a direct link, but for AIX 6.1 the file is `libstdc++-4.8.4-6.aix6.1.ppc.rpm`  You extract the file (on a different machine) with:  ```bash rpm2cpio libstdcplusplus-4.8.5-1.aix6.1.ppc.rpm | cpio -idmv ```  Then zip it up and scp it to the machine. Unzip it, and then add the `opt/freeware/lib/pthread/ppc64` subdirectory to your `LIBPATH`  ```bash export LIBPATH="$PWD/opt/freeware/lib/pthread/ppc64:$LIBPATH" ```  You should then be able to do `ldd ./node` and `./node` and it should all work.  Note that Node 10 is expected to be built with gcc 6.3, so you won't need this workaround once you upgrade to that version.  For info, the reason this is necessary is because there was a breaking change in libstdc++ on AIX between gcc 4.8 and 4.9.  Let me know if it works!
gibfahn		I have generated a zip file containing gcc and dependencies, by downloading the zip from Bull Freeware. For some reason the dependency resolver on Bull Freeware downloaded the wrong version of libstdc++ (7.1 instead of 6.1) so I had to manually fix up the with-deps zip.  ```bash #!/bin/bash -ex  # Builds a zip file containing gcc and dependencies, installed from Bull Freeware rpms.   rm -rf rpms gcc-6.3.0-1 rm -f gcc-6.3.0-1.zip mkdir rpms gcc-6.3.0-1  cp gcc-c++-6.3.0-1.aix6.1.ppc.rpm-with-deps.zip rpms (cd rpms && unzip gcc-c++-6.3.0-1.aix6.1.ppc.rpm-with-deps.zip)  for rpm in rpms/*.rpm; do   (cd gcc-6.3.0-1 && rpm2cpio ../$rpm | cpio -idmv) done  zip -r gcc-6.3.0-1.zip gcc-6.3.0-1/ ```  Once you have the zip on the system, it's just a matter of unzipping it somewhere, and then setting up the paths correctly, so:  ```bash # Setting the LIBPATH doesn't seem to be necessary. # export LIBPATH="$PWD/gcc-6.3.0-1/opt/freeware/lib/pthread/ppc64:$PWD/gcc-6.3.0-1/opt/freeware/lib" export PATH="$PWD/gcc-6.3.0-1/opt/freeware/bin:$PATH" cd ~/node export CC=`which gcc` CXX=`which g++` CXX_host=`which g++` ./configure --dest-cpu=ppc64 && gmake -j4 ```  The version I generated is stored at https://ci.nodejs.org/downloads/aix/ (see https://github.com/nodejs/build/issues/871#issuecomment-329952135), there's a zip and an md5sum for checking that everything downloads correctly.
gibfahn		I also ran with C++ 14 enabled to make sure that worked, by applying this diff:  ```diff diff --git a/common.gypi b/common.gypi index 6d83d8a..dcbb2a0 100644 --- a/common.gypi +++ b/common.gypi @@ -321,7 +321,7 @@        }],        [ 'OS in "linux freebsd openbsd solaris android aix cloudabi"', {          'cflags': [ '-Wall', '-Wextra', '-Wno-unused-parameter', ], -        'cflags_cc': [ '-fno-rtti', '-fno-exceptions', '-std=gnu++1y' ], +        'cflags_cc': [ '-fno-rtti', '-fno-exceptions', '-std=gnu++14' ],          'ldflags': [ '-rdynamic' ],          'target_conditions': [            # The 1990s toolchain on SmartOS can't handle thin archives.  ```
seishun		`'-std=gnu++1y'` and `'-std=gnu++14'` are synonyms.
gibfahn		>'-std=gnu++1y' and '-std=gnu++14' are synonyms.  Ahh, so how would I enable gcc 14 features (to check that it works)? I want to get something that fails on 4.8.4 and passes on 6.3.0.
seishun		You could try to build this PR: https://github.com/nodejs/node/pull/13676
mhdawson		Other platforms seem to need to export LDFLAGS, sounds like that is not required on AIX for your testing?
mhdawson		Seems to fail on community machine with: ``` compiler set to 6.3 + gmake run-ci -j 5 + NODE_TEST_DIR=/home/iojs/node-tmp PYTHON=python FLAKY_TESTS=dontcare CONFIG_FLAGS= --dest-cpu=ppc64 python ./configure  --dest-cpu=ppc64 exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1plus because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1 because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  WARNING: C++ compiler too old, need g++ 4.9.4 or clang++ 3.4.2 (CXX=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/g++) WARNING: C compiler too old, need gcc 4.2 or clang 3.2 (CC=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/gcc) ```  Will try to set LIBPATH 
mhdawson		LIBPATH does not seem to help and just causes other problems.
mhdawson		Despite it not looking like the bit was set on the test machine, this was still required:  `sudo chmod 755 /opt/freeware/bin/make` 
mhdawson		~~That seems to start building~~(That was incorrect, was running with old compiler, fails locally as well) when run locally through the console,but still fails when run through the CI with: ```+ NODE_TEST_DIR=/home/iojs/node-tmp PYTHON=python FLAKY_TESTS=dontcare CONFIG_FLAGS= --dest-cpu=ppc64 python ./configure  --dest-cpu=ppc64 exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1plus because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1 because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  ```
refack		@mhdawson watch out for the following in the Jenkins config... ![image](https://user-images.githubusercontent.com/96947/38746217-de017bd0-3f14-11e8-9424-6c078efc877c.png)  BTW: I've found that the following enables building (and apparently the order in important): ``` export LIBPATH=/opt/freeware/lib:/home/iojs/gcc-6.3.0-1/opt/freeware/lib ```
mhdawson		@refack Thanks ! trying that now.
mhdawson		Still failing in CI  ``` Setting compiler for Node version 10 on AIX + [ 10 -gt 9 ] + export LIBPATH=/opt/freeware/lib:/home/iojs/gcc-6.3.0-1/opt/freeware/lib + export PATH=/home/iojs/gcc-6.3.0-1/opt/freeware/bin:/opt/freeware/bin/ccache:/home/iojs/jdk8/jre/bin:/usr/bin:/etc:/usr/sbin:/usr/ucb:/usr/bin/X11:/sbin:/usr/java6/jre/bin:/usr/java6/bin + which gcc + which g++ + which g++ + export CC=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/gcc CXX=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/g++ CXX_host=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/g++ + echo Compiler set to 6.3 Compiler set to 6.3 + gmake run-ci -j 5 + NODE_TEST_DIR=/home/iojs/node-tmp PYTHON=python FLAKY_TESTS=dontcare CONFIG_FLAGS= --dest-cpu=ppc64 python ./configure  --dest-cpu=ppc64 exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1plus because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1 because of the following errors: 	0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded. 	0509-152   Member libmpc.so.3 is not found in archive  WARNING: C++ compiler too old, need g++ 4.9.4 or clang++ 3.4.2 (CXX=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/g++) WARNING: C compiler too old, need gcc 4.2 or clang 3.2 (CC=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/gcc) ```  probably unrelated but the compiler detection is also failing:  ``` WARNING: C++ compiler too old, need g++ 4.9.4 or clang++ 3.4.2 (CXX=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/g++) WARNING: C compiler too old, need gcc 4.2 or clang 3.2 (CC=ccache /home/iojs/gcc-6.3.0-1/opt/freeware/bin/gcc) ```
refack		So it's more delicate: GCC need LIBPATH to be `export LIBPATH=/home/iojs/gcc-6.3.0-1/opt/freeware/lib:/opt/freeware/lib` ```console -bash-4.3$ export LIBPATH=/home/iojs/gcc-6.3.0-1/opt/freeware/lib:/opt/freeware/lib -bash-4.3$ ./configure --dest-cpu=ppc64 WARNING: openssl_no_asm is enabled due to missed or old assembler.             Please refer BUILDING.md creating icu_config.gypi * Using ICU in deps/icu-small creating icu_config.gypi ```   gmake need it the other way: ```console -bash-4.3$ gmake exec(): 0509-036 Cannot load program gmake because of the following errors:         0509-022 Cannot load module /opt/freeware/lib/libintl.a(libintl.so.1).         0509-150   Dependent module /home/iojs/gcc-6.3.0-1/opt/freeware/lib/libiconv.a(shr4.o) could not be loaded.         0509-152   Member shr4.o is not found in archive         0509-022 Cannot load module make.         0509-150   Dependent module /opt/freeware/lib/libintl.a(libintl.so.1) could not be loaded.         0509-022 Cannot load module . -bash-4.3$ export LIBPATH=/opt/freeware/lib:/home/iojs/gcc-6.3.0-1/opt/freeware/lib -bash-4.3$ gmake gmake -C out BUILDTYPE=Release V=1 gmake[1]: Entering directory `/home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out' node.target.mk:13: warning: overriding recipe for target `/home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node.exp' cctest.target.mk:13: warning: ignoring old recipe for target `/home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node.exp' ‚ô•gmake: *** [node] Interrupt ```
mhdawson		Looks to me that there is a circular issue, gmake requires LIBPATH in particular order and CC in a different order: ``` bash-4.3$ sh doit.sh exec(): 0509-036 Cannot load program gmake because of the following errors:         0509-022 Cannot load module /opt/freeware/lib/libintl.a(libintl.so.1).         0509-150   Dependent module /home/iojs/gcc-6.3.0-1/opt/freeware/lib/libiconv.a(shr4.o) could not be loaded.         0509-152   Member shr4.o is not found in archive          0509-022 Cannot load module make.         0509-150   Dependent module /opt/freeware/lib/libintl.a(libintl.so.1) could not be loaded.         0509-022 Cannot load module . bash-4.3$ export LIBPATH=/opt/freeware/lib/home/iojs/gcc-6.3.0-1/opt/freeware/lib                         bash-4.3$ export LIBPATH=/opt/freeware/lib:/opt/freeware/lib/home/iojs/gcc-6.3.0-1/opt/freeware/lib bash-4.3$ sh doit.sh python ./configure --dest-cpu=ppc64 exec(): 0509-036 Cannot load program /home/iojs/gcc-6.3.0-1/opt/freeware/bin/../libexec/gcc/powerpc-ibm-aix6.1.0.0/6.3.0/cc1plus because of the following errors:         0509-150   Dependent module /opt/freeware/lib/libmpc.a(libmpc.so.3) could not be loaded.         0509-152   Member libmpc.so.3 is not found in archive  Traceback (most recent call last):   File "./configure", line 1472, in <module>     check_compiler(output)   File "./configure", line 716, in check_compiler     ok, is_clang, clang_version, gcc_version = try_check_compiler(CXX, 'c++')   File "./configure", line 615, in try_check_compiler     proc.stdin.write('__clang__ __GNUC__ __GNUC_MINOR__ __GNUC_PATCHLEVEL__ ' IOError: [Errno 32] Broken pipe gmake: *** [build-ci] Error 1 ```
mhdawson		Ok, ignore the previous comment.  I see that I have at least one of the paths wrong trying again.
mhdawson		Or maybe I had things right after all as @refack just said the same thing.
mhdawson		Tried taking the gmake from one of our internal machines that might have worked with @gibfahn tests.  And tried it on the CI machine, still fails with the same problem :(  The message was only slightly different:  ``` bash-4.3$ export LIBPATH=/home/iojs/gcc-6.3.0-1/opt/freeware/lib -bash-4.3$ gmake exec(): 0509-036 Cannot load program gmake because of the following errors:         0509-022 Cannot load module /opt/freeware/lib/libintl.a(libintl.so.8).         0509-150   Dependent module /home/iojs/gcc-6.3.0-1/opt/freeware/lib/libiconv.a(shr4_64.o) could not be loaded.         0509-152   Member shr4_64.o is not found in archive          0509-022 Cannot load module gmake.         0509-150   Dependent module /opt/freeware/lib/libintl.a(libintl.so.8) could not be loaded.         0509-022 Cannot load module . -bash-4.3$  ```1 with the difference being libintl.so.8 instead of libintl.so.1
gibfahn		>Tried taking the gmake from one of our internal machines that might have worked with @gibfahn tests. And tried it on the CI machine, still fails with the same problem :(  Assuming you're talking about `dal-pax`, then it worked fine (feel free to test on that machine). Given that they're both AIX 6 boxes with Bull Freeware rpms and gcc 4.8 (I didn't update any of the packages on that machine) it should be pretty simple to work out what the difference between them is.
mhdawson		I finally found something that works.  Still want to get some feedback from the internal AIX team to see if it makes sense.  The environment is set as follows: ```  export LIBPATH=/home/iojs/gmake/opt/freeware/lib:/home/iojs/gcc-6.3.0-1/opt/freeware/lib  export PATH="/home/iojs/gcc-6.3.0-1/opt/freeware/bin:$PATH"  export CC="ccache `which gcc`" CXX="ccache `which g++`" CXX_host="ccache `which g++`" ```  with the difference from before being the addition of `/home/iojs/gmake/opt/freeware/lib` where I've copied the libiconv.a from the local system.  The net is that we use the libiconv.a that gmake needs as opposed to the one that comes from the bundle build for gcc.  Have tested from command line, next is to see if it works from the CI.
mhdawson		Running here: https://ci.nodejs.org/job/node-test-commit-aix-mdawson/nodes=aix61-ppc64-x/10/
orangemocha		Could potentially be addressed by restarting Jenkins, like https://github.com/nodejs/build/issues/169 
orangemocha		@nodejs/jenkins-admins  
rvagg		happening a lot recently, perhaps we need to update slave.jar 
jbergstroem		Haven't seen these in a good while. Fwiw, we've updated slave.jar's throughout our fleet but is likely unrleated. 
jbergstroem		This just occured again (spotted by @trott):  https://ci.nodejs.org/job/node-test-binary-arm/159/RUN_SUBSET=1,nodes=pi2-raspbian-wheezy/console 
Trott		And again: - https://ci.nodejs.org/job/node-test-binary-arm/160/RUN_SUBSET=addons,nodes=pi2-raspbian-wheezy/console - https://ci.nodejs.org/job/node-test-binary-arm/160/RUN_SUBSET=6,nodes=pi2-raspbian-wheezy/console  Should this be re-opened? 
Trott		Is it useful to keep reporting these here as they occur or is that just noise? - https://ci.nodejs.org/job/node-test-binary-arm/168/RUN_SUBSET=0,nodes=pi2-raspbian-wheezy/console 
rvagg		interesting they are just pi2-9 and pi2-2 so far  @Trott feel free to keep on reporting if they keep showing up, a pattern may be interesting 
jbergstroem		@Trott - seen anything of this recently?  
Trott		Haven't noticed it lately, but I haven't been looking for it either. 
jbergstroem		Speaking of the devil: https://ci.nodejs.org/job/node-test-binary-arm/RUN_SUBSET=addons,nodes=pi2-raspbian-wheezy/902/console  Edit: again just on ARM it seems. Are these perhaps trying to all pull at the same time? 
jpsim		Sorry to bump into this thread, this is a last resort... was this ever resolved? If so how?  I'm seeing the same exception stack trace in my Jenkins jobs, though running in OS X, so not ARM:  ``` java.io.IOException: remote file operation failed: /Users/realm/workspace/objc_pr at hudson.remoting.Channel@4ec66dd4:sf_host1_3: java.io.IOException: Remote call on sf_host1_3 failed     at hudson.FilePath.act(FilePath.java:1014)     at hudson.FilePath.act(FilePath.java:996)     at org.jenkinsci.plugins.gitclient.Git.getClient(Git.java:131)     at hudson.plugins.git.GitSCM.createClient(GitSCM.java:741)     at hudson.plugins.git.GitSCM.createClient(GitSCM.java:733)     at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1080)     at hudson.scm.SCM.checkout(SCM.java:485)     at hudson.model.AbstractProject.checkout(AbstractProject.java:1269)     at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:604)     at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:529)     at hudson.model.Run.execute(Run.java:1741)     at hudson.matrix.MatrixBuild.run(MatrixBuild.java:301)     at hudson.model.ResourceController.execute(ResourceController.java:98)     at hudson.model.Executor.run(Executor.java:410) Caused by: java.io.IOException: Remote call on sf_host1_3 failed     at hudson.remoting.Channel.call(Channel.java:789)     at hudson.FilePath.act(FilePath.java:1007)     ... 14 more Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.sun.proxy.$Proxy11     at sun.reflect.GeneratedConstructorAccessor17.newInstance(Unknown Source)     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)     at java.lang.reflect.Constructor.newInstance(Constructor.java:423)     at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:739)     at hudson.remoting.RemoteInvocationHandler.wrap(RemoteInvocationHandler.java:143)     at hudson.remoting.Channel.export(Channel.java:621)     at hudson.remoting.Channel.export(Channel.java:590)     at org.jenkinsci.plugins.gitclient.LegacyCompatibleGitAPIImpl.writeReplace(LegacyCompatibleGitAPIImpl.java:201)     at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:498)     at java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1118)     at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)     at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)     at hudson.remoting.UserRequest._serialize(UserRequest.java:157)     at hudson.remoting.UserRequest.serialize(UserRequest.java:166)     at hudson.remoting.UserRequest.perform(UserRequest.java:128)     at hudson.remoting.UserRequest.perform(UserRequest.java:48)     at hudson.remoting.Request$2.run(Request.java:326)     at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:68)     at java.util.concurrent.FutureTask.run(FutureTask.java:266)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)     at hudson.remoting.Engine$1$1.run(Engine.java:62)     at java.lang.Thread.run(Thread.java:745)     at ......remote call to sf_host1_3(Native Method)     at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1416)     at hudson.remoting.UserResponse.retrieve(UserRequest.java:253)     at hudson.remoting.Channel.call(Channel.java:781)     ... 15 more ``` 
jbergstroem		Haven't seen these in a long while, but others; "remote host closed connection" 
huzhijiang		Again:  ``` Wiping out workspace first. java.io.IOException: remote file operation failed: /home/jenkins-ci/opnfv/slave_root/workspace/daisy4nfv-verify-build-master at hudson.remoting.Channel@61f2a4c1:lf-build1: java.nio.file.AccessDeniedException: /home/jenkins-ci/opnfv/slave_root/workspace/daisy4nfv-verify-build-master/daisycloud-core/.git/refs/heads/master     at hudson.FilePath.act(FilePath.java:986)     at hudson.FilePath.act(FilePath.java:968)     at hudson.FilePath.deleteContents(FilePath.java:1183)     at hudson.plugins.git.extensions.impl.WipeWorkspace.beforeCheckout(WipeWorkspace.java:28)     at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1013)     at hudson.scm.SCM.checkout(SCM.java:485)     at hudson.model.AbstractProject.checkout(AbstractProject.java:1276)     at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:607)     at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:529)     at hudson.model.Run.execute(Run.java:1738)     at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)     at hudson.model.ResourceController.execute(ResourceController.java:98)     at hudson.model.Executor.run(Executor.java:410) Caused by: java.nio.file.AccessDeniedException: /home/jenkins-ci/opnfv/slave_root/workspace/daisy4nfv-verify-build-master/daisycloud-core/.git/refs/heads/master     at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)     at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)     at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)     at java.nio.file.Files.delete(Files.java:1126)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:498)     at hudson.Util.deleteFile(Util.java:255)     at hudson.FilePath.deleteRecursive(FilePath.java:1203)     at hudson.FilePath.deleteContentsRecursive(FilePath.java:1212)     at hudson.FilePath.deleteRecursive(FilePath.java:1194)     at hudson.FilePath.deleteContentsRecursive(FilePath.java:1212)     at hudson.FilePath.deleteRecursive(FilePath.java:1194)     at hudson.FilePath.deleteContentsRecursive(FilePath.java:1212)     at hudson.FilePath.deleteRecursive(FilePath.java:1194)     at hudson.FilePath.deleteContentsRecursive(FilePath.java:1212)     at hudson.FilePath.deleteRecursive(FilePath.java:1194)     at hudson.FilePath.deleteContentsRecursive(FilePath.java:1212)     at hudson.FilePath.access$1100(FilePath.java:190)     at hudson.FilePath$15.invoke(FilePath.java:1186)     at hudson.FilePath$15.invoke(FilePath.java:1183)     at hudson.FilePath$FileCallableWrapper.call(FilePath.java:2719)     at hudson.remoting.UserRequest.perform(UserRequest.java:120)     at hudson.remoting.UserRequest.perform(UserRequest.java:48)     at hudson.remoting.Request$2.run(Request.java:326)     at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:68)     at java.util.concurrent.FutureTask.run(FutureTask.java:266)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)     at hudson.remoting.Engine$1$1.run(Engine.java:62)     at java.lang.Thread.run(Thread.java:745)     at ......remote call to lf-build1(Native Method)     at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1416)     at hudson.remoting.UserResponse.retrieve(UserRequest.java:220)     at hudson.remoting.Channel.call(Channel.java:781)     at hudson.FilePath.act(FilePath.java:979)     ... 13 more Finished: FAILURE ```
chr0n1x		*bump*  Did you guys ever figure out what was going on here? Any Jenkins JIRA tickets?
gibfahn		@chr0n1x afraid not, there's also not much we can do about this, you'd need to raise (or find) an issue in Jenkins. 
chr0n1x		@gibfahn one thing that I tried was [this](http://www.linuxbrigade.com/reduce-time_wait-socket-connections/). I seems to have helped with what I was seeing on my instance. In my case it was many many many slaves spinning up and the connections to them not terminating fast enough, getting stuck in the `TIME_WAIT` state and causing further connections that the master attempted to stall. Full resource that I read up on for `TIME_WAIT` [here](http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html).
maclover7		Has this happened recently? Otherwise I think this can be closed
maclover7		Closing due to inactivity. Please reopen if this is still needed :)
refack		I've found a possible workaround &mdash; restarting the java worker (`slave.jar`). It just solved this issue for me on two workers, so it's worth giving it a shot.
refack		A bit more info (for posterity). Looking at these seemingly key lines from the most recent trace: ``` java.lang.NoClassDefFoundError: Could not initialize class com.sun.proxy.$Proxy11 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) 	at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:739) ```  1. The class failing is `com.sun.proxy.$Proxy11` while in the OP it's `$Proxy9`, and in other traces, here and on the internet, the class numarator continues to vary. 2. Seeing as this looks like code that auto-generates a remoting proxy by reflection, it seems likely that code version mismatch is the culprit, hence updating the client (or even restarting it) could solve this issue.
rvagg		I had to take one of the smartos16 machines offline yesterday because of this, every build failed the same. No matter what I did on the node fixed the problem, including updating slave.jar. I got nuttin'. 
rvagg		lgtm 
jbergstroem		Merged in 482e3ed0a99bf462a66056a72d92128e6909a7fa. Thanks for the review, Rod. 
jbergstroem		Lets try and put forward something to discuss at that meeting; for instance a suggestion on how we want it to be? This needs to be streamlined with our jenkins config as well as `Makefile`. Likely also ported back to at least some branches. 
MylesBorins		so I think we need - `beta` (ongoing build with moving changes)   - labelled `-beta.#` - `rc` (pre releases that are not subject to changes except when neccessary)   - labelled `-rc.#` - `test` (test releases being build for a variety of reasons)   - labelled `-test-$DATE` - `nightly` (automated releases)   -  - labelled `-nightly-$DATE`  and of course `release`  thoughts? 
Trott		@TheAlphaNerd Especially if `nvm` is cool with all that, we could just start using something like that now and accept the fact that there are mis-labeled binaries in the past. I personally imagine that is acceptable, as it is probably pretty unusual for someone to want an rc or a beta from a year ago. So as long as the current ones are labeled correctly, everything is great. 
MylesBorins		I think it would be valuable to agree to the release process on this and document it... then stick to it  /cc @nodejs/release  
jbergstroem		From ci-release point of view we also have custom builds (which is sort of abused atm) to generate from other branches. 
ljharb		My ideal scenario, whatever terminology we decide, would be one where the directory name and filename suffix were in sync, and _were updated historically for all previous releases that had RCs_ (using symlinks on the server for this is an implementation detail, and fine)  I'm also fine with "RC", "beta", "test", and "nightly" - four branches theoretically is no harder than two, especially if they're consistent across both io.js and node versions.  While it might be unusual for someone to want old RCs etc, I think the continuity is important both for my testing, and to allow for 100% phaseout of any older terminology. 
Trott		> While it might be unusual for someone to want old RCs etc, I think the continuity is important both for my testing, and to allow for 100% phaseout of any older terminology.  I guess the problem is what to do about (for example) old "RCs". OK, so we can symlink or whatever and have them called betas instead. But we can't break the existing URLs for the files that contain an "rc" directory and an "rc" suffix. So they will forever be non-RC RC releases that are available. Maybe I'm overthinking it and that's just not a problem. Like, hey, it's available as an RC _and_ a beta _and_ it's the same thing and that's just a historical quirk, so deal with it if you're one of the three people who will be affected by that? I'm not sure. 
ljharb		I don't think it will be a problem, especially if most people end up using `nvm` to install these things anyways.  You could always include a README in the old directories that explains the change, and that these are deprecated? 
Trott		This stalled. I'm going to close it. If someone else wants to pick it up and advocate for it, leave a comment, or re-open (if GitHub lets you), or open a new issue. Sorry/thanks.
ljharb		:-/ it stalled, but I have no idea why, nor what/who was missing to make progress on it. Any guidance?
Trott		@ljharb If you're willing to advocate, maybe the thing to do is put it back on the Build WG agenda and invite you as a guest. The last time it was on the agenda (in March), the minutes just say that the conversation should continue in GitHub. I don't think it's on anyone's radar. Having someone at the meeting who cares about it and can explain the value (and pitfalls) would probably help push it along, or at least document an explanation as to why it won't happen or isn't happening for now.
ljharb		That sounds great; please loop me in and I'll do my utmost to attend.
rvagg		FYI a lot of this naming stuff is hard-coded in Node's Makefile. Look for the bug `DISTTYPE` section which even includes "next-nightly" which we no longer use now. When I first got that in there I tried to make it more generic but during negotiation we ended up with a mid-way state where it's partly generic and partly hard-coded and it's been frustrating ever since.  The state we have now is that we have this awkward push and pull that goes on between Makefile, Jenkins and the [promote scripts](https://github.com/nodejs/build/tree/master/setup/www/tools/promote). In Jenkins we have this duplicated for each of the release build scripts (including a Windows variant):  ```sh RELEASE_URLBASE=https://nodejs.org/download/${disttype}/ if [ "X${disttype}" == "Xtest" ]; then   disttype=custom   CUSTOMTAG="test${datestring}${commit}"   RELEASE_URLBASE=https://nodejs.org/download/test/ elif [ "X${disttype}" == "Xv8-canary" ]; then   disttype=custom   CUSTOMTAG="v8-canary${datestring}${commit}"   RELEASE_URLBASE=https://nodejs.org/download/v8-canary/ elif [ "X${disttype}" == "Xrc" ]; then   disttype=custom   CUSTOMTAG="rc.$rc"   RELEASE_URLBASE=https://nodejs.org/download/rc/ fi ```  i.e. we are overloading "custom" in Makefile to introduce these new types (test, v8-canary and rc), meanwhile Makefile handles "release", "nightly" and "next-nightly" (which we don't even use).  What I'd like to see is a complete overhaul of all of these 3 things, particularly yanking out that stuff in Makefile and replacing it with something much more simple and generic that we can bend to our will whenever we want to do something new, probably in Jenkins with some simple env vars. Having to get changes into nodejs/node to make this happen is pretty annoying and there's not a whole lot people confident enough to review Makefile changes anyway.  While we're at it we can fix the long-standing `release-only` problem we have in Makefile which checks for `REPLACEME`. We have a `perl -pi -e "s/: release-only/:/g" Makefile` hard-wired in Jenkins for everything but release builds because it gets in the way every time someone wants to make a non-release build (rc builds in particular)!
mhdawson		@ljharb can you make the next meeting on 6th of February at 4EST ?
ljharb		@mhdawson absolutely; please add my gmail to the meeting and I'll calendar it.
ljharb		Per the WG meeting, @nodejs/version-management will come up with a concrete proposal, and will bring it back to interested parties.
Trott		I'm inclined to close this until a concrete proposal comes back. If anyone thinks that's A Bad Idea, feel free to re-open or leave a comment here requesting that this be re-opened.
jbergstroem		The main README.md should probably be updated as well, since it's referring to Buildbot. 
rvagg		probably going to have to lean on you @jbergstroem and others to get the README buttoned up, it's been stale for a while but I'm spread too thin 
orangemocha		Useful stuff. I was thinking of doing something similar in the wiki, where edits would be easier. Thoughts on that?  We also might want to cross links from/to https://github.com/nodejs/io.js/blob/master/COLLABORATOR_GUIDE.md#landing-pull-requests 
jbergstroem		I feel this may have lived its course. I suggest we add more info to `COLLABORATOR_GUIDE.md` instead. Close? 
mhdawson		We generate nightlies when there is a code change  For 4 and 6 the nightlies make sense if they come off the staging branches  4,6,7 and master  Jo√£o will point me at the scripts and I'll put together a PR to doc
joaocgreis		Nightlies are started by the www server, as configured here: https://github.com/nodejs/build/blob/2ac05fe/setup/www/tasks/tools.yaml#L9-L30 .  This uses `nodejs-nightly-builder` from https://github.com/nodejs/nodejs-nightly-builder .  The `crontab` in the server matches the Ansible script, so we are releasing nightlies from the `v6.x`, `v7.x` and `master` branches. This was introduced by @rvagg in https://github.com/nodejs/build/commit/9678bd21937e71020b828b362edecfbec0a920b2 , before there was a `nightly-builder` but I don't know how it was started.  I think that building nightlies from all active `*-staging` branches (and `master`) would make more sense, perhaps @nodejs/lts wants to weight in on this. 
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
gibfahn		Interesting idea, and I know other projects do this (e.g. Rust, which has rollup commits).   What would the advantages be? I find the process of landing commits pretty straightforward today. A CI job to verify commit metadata [already exists](https://ci.nodejs.org/job/node-test-commitmsg/), it just needs reviving. I think other projects have CI runs that take a lot longer, so it probably makes more sense for them, but it also adds complexity to the landing process.
refack		Mainly taking out human error out of the straight forward but manual steps. * reject on fail of validation of commit message structure & metadata * reject on CI fail * Automatic close of PR  If the PR has a good message and could be just squashed: * automate adding metadata * automate squash  i.e. turn these cases to a one-click merge
gibfahn		What do you mean by commit queue then?  Those are reasons to have a CI flow that runs the commit validation, adds metadata, runs CI, squashes, and lands (although it'd only work for cases where you want to squash), but why do you need a queue?
refack		A CQ is in essence a different git remote that colabs can push to, and will either automate the land to `master` or reject. one-click-merge is a bonus feature: collab tell CQ to pick up a PR, and CQ does everything. The design doc by Chromium is quite good http://dev.chromium.org/developers/testing/commit-queue/design
refack		P.S. queue is a borrowed term from ["mercurial patch queues"](http://hgbook.red-bean.com/read/managing-change-with-mercurial-queues.html), the main metaphor is a side storage that can accept or reject without changing `master` beforehand, and allows for temporary storage of manually rebased commits.
mhdawson		This was in place for a short period (basically a job which did the test and if tests passed, landed the PR)  but it was backed out because at the time the tests were not stable enough and it would take multiple attempts due to flakes even when nothing was wrong with the PR.    I think we still have a number of flakes on the arm fanned builds and to a less extend the fanned windows builds.  I know that I've often add the "arm failures seem infra related so CI is ok".   I think we'd need those to be more solid before proposing this again.  
refack		Sounds legit. Thanks.
thomasuster		@refack Has anyone built a generic solution for this? Like a github integration?
refack		> @refack Has anyone built a generic solution for this? Like a github integration?  I have no experience with GitHub integrations, but there are `gerrit` (maybe as http://gerrithub.io/) and  `rietveld`
refack		With the inauguration of the Automation SIG IMHO it's a good time to revive this concept.
rvagg		For reference, prior art is still here I believe: https://ci.nodejs.org/view/All/job/node-merge-commit/  we can drop in the Jenkins scripts here if that's interesting at all
maclover7		Been playing around with Jenkins locally on my machine, and I think I have a job working now where if you provide a PR identifier, like `nodejs/node#14998` for example, it will land the commit with PR metadata into the master branch of the repo.
gibfahn		>Been playing around with Jenkins locally on my machine, and I think I have a job working now where if you provide a PR identifier, like nodejs/node#14998 for example, it will land the commit with PR metadata into the master branch of the repo.  Does it squash commits?
maclover7		> Does it squash commits?  @gibfahn Kinda sorta -- before figuring out if this is something we want to actually use, we'd probably have to take a look at our contributor policy around commits. Since Jenkins is doing the landing, you'd wouldn't be able to manually amend/squash commits, so puts more of an emphasis on making sure the contributor has the PR commits ready to land -- but I think that's a little premature.
mhdawson		No current action item, removing from build agenda based on discussion in last meeting.
maclover7		Moving this to nodejs/commit-queue#1, since there is a whole separate team for this now
gibfahn		Isn't this just because https://github.com/nodejs/build/pull/785 still hasn't landed?
gibfahn		i.e. dup of https://github.com/nodejs/build/issues/833 ?
gibfahn		So that's landed now, I think the ansible scripts would need to be rerun on the machines.
MylesBorins		ping on this for ubuntu. Have the ansible scripts not been run yet? I'm still seeing failures in CITGM
MylesBorins		:(
MylesBorins		Hey all.. this is still not fixed, and has not been fixed for months  I'm getting really frustrated as this is breaking our ability to have green CI runs with CITGM. Can someone please prioritize this  /cc @nodejs/tsc 
rvagg		What's the purpose of the `tsc-agenda` label here?
rvagg		You have nodejs_build_test access don't you @MylesBorins, your ability to execute on this is pretty much the same as ours. Someone's either going to have to manually log in to each of these machines to make this directory or run parallel-ssh or Ansible across them all. The blocker here is having a person with the time to work through all of the hosts but everyone with nodejs_build_test is equally able to do this.
refack		I'll pick this up.  There might be a "take home" here, in that maybe whomever lands an ansible change should run the scripts. Alternatively we could have point man for parts of the inventory. Since this seems like a ["bystander effect"](https://en.wikipedia.org/wiki/Bystander_effect) issue otherwise
gibfahn		Alternatively the CitGM job could just do a `mkdir -p "$NODE_TEST_DIR"` in the config.
rvagg		> Alternatively the CitGM job could just do a mkdir -p "$NODE_TEST_DIR" in the config  yes! let's just do that eh?
refack		> >Alternatively the CitGM job could just do a mkdir -p "$NODE_TEST_DIR" in the config  > yes! let's just do that eh?  üëç  running an ansible playbook just for syncing each of the needed machine still needs some yak shaving... 
MylesBorins		The reason to TSC-Agenda item was added was that this is something that has been brought up for months. It was last brought up in https://github.com/nodejs/build/issues/833 which was claimed that a fix was to be landed in https://github.com/nodejs/build/pull/785. An early request https://github.com/nodejs/build/pull/658 was also closed, an issue opened in March.  To be honest I have never been on boarded to using our ansible scripts and I don't feel comfortable doing a mass update of our infrastructure because of that. This did not seem like a laborious request for help, and is blocking our ability to test `Readable-Stream`, which is a foundation managed module.  I think that we should be having a conversation both at the WG + TSC level about why a request like this has remained unresolved for over 6 months.
rvagg		Anyone know if `$NODE_TEST_DIR` is actually what's used here? I'm not seeing any reference to it in the citgm-smoker job so it might not be what we need. Perhaps it'd be useful to figure out how `~/node-tmp` is referenced here by the tools that use it‚Äîand what tools actually use it! Then we can work backward from there.
refack		It comes from `readable-stream` test code - https://github.com/nodejs/readable-stream/blob/56d9356637b5d20aba8e8e26faa602222f8500ab/test/common.js#L69  /cc @mcollina 
trevnorris		Can that line be replaced with `os.tmpdir()`?
rvagg		Interesting, but I still don't see where `NODE_TEST_DIR` is set! I don't believe this is something we initialise in Jenkins.
gibfahn		https://github.com/nodejs/build/blob/master/ansible/roles/jenkins-worker/templates/openrc.initd.j2#L22
mcollina		>  It comes from readable-stream test code - https://github.com/nodejs/readable-stream/blob/56d9356637b5d20aba8e8e26faa602222f8500ab/test/common.js#L69  Those things are pulled from node core.  At some point we discussed changing that line or fixing CI, and the general sentiment was that fixing CI was the best path forward. I can dig up the issue if you'd like. However I prefer changing as little as possible in our tests.
rvagg		OK, so there's a bunch of this kind of things in our node-test-commit-XXX configs:  ``` NODE_TEST_DIR=${HOME}/node-tmp FLAKY_TESTS=$FLAKY_TESTS_MODE make run-ci -j $(getconf _NPROCESSORS_ONLN) ```  I don't know the history of those (wasn't me so I have no clue _why_ they were set). But there's nothing in the citgm jobs for it and there's no `mkdir` for it in any of the jobs either. So I guess at some point we (a) had a requirement for `NODE_TEST_DIR` imposed on the infra and (b) decided that it's best done at infra setup time rather than at test time like we do with `~/node-icu/` and a smattering of others. The thing I still don't understand is why citgm jobs would be looking for `~/node-tmp/` if we're not setting `NODE_TEST_DIR` for them.  Personally I'd like to keep our specific hardware config to a minimum as it keep on being a pain, and if that means pushing more into Jenkins jobs then so be it (for the record, that sucks too cause the config isn't public and only a few of us can even see it! But it's one thing that GitHub-hosted pipeline jobs could solve for us).
rvagg		Just had a brain-wave before I drop off to bed. We have this _check-java-version_ job that we've been using to get ourselves up to Java 8 on all our Jenkins machines and it has all our non-Windows machines wired up to it, so I just stuck `mkdir -p ${HOME}/node-tmp` in the script for it and ran it! So @MylesBorins & @mcollina I think you should be able to try out your jobs now and ü§û.
refack		Might be one caveat: https://github.com/nodejs/build/blob/10668d4e519305e2d78d75774d3584efb4b38204/ansible/roles/jenkins-worker/templates/openrc.initd.j2#L22 sets the deafult NODE_TEST_DIR So any code in the tested modules that looks up NODE_TEST_DIR will take the default. So we should either set it in the job, or remove it from `openrc.initd`
gibfahn		> We have this check-java-version job that we've been using to get ourselves up to Java 8 on all our Jenkins machines and it has all our non-Windows machines wired up to it,   It runs on our Windows machines too.
gibfahn		>To be honest I have never been on boarded to using our ansible scripts and I don't feel comfortable doing a mass update of our infrastructure because of that. This did not seem like a laborious request for help, and is blocking our ability to test Readable-Stream, which is a foundation managed module.  @MylesBorins this seems like the issue then. You're one of the senior members of the Build WG, and you're not comfortable updating our machines. That's not a judgement, I'm not comfortable doing it either, and I'm not sure anyone else is.
rvagg		From my understanding from the discussion over the last couple of meetings, this is resolved in the specific but the general concern about our confidence in our Ansible scripts remains so I've opened that up as a separate issue @ #959
rvagg		Are you talking about detached signature files here or key trust? Or somehow both?  We've been making detached signatures since https://github.com/nodejs/node/pull/9071, although we haven't updated the docs to demonstrate how to use this as we've been waiting for all release lines to be updated with the new sig style, tracking in https://github.com/nodejs/node/issues/6821. Since it's been in place for a while now we should probably go ahead and update the docs to tell how to verify with the detached signatures instead of the inline signatures.
rvagg		... catching up on IRC ... are you perhaps talking about detached signatures for _each_ file in the releases? So ~ double the number of files in the release directories than we have now, an .asc for each one?
schittler		It's somehow both I guess. I just noticed that the releases are done by different people and signed with the personal keys - if there's a release signing key that trusts those personal keys, that's great.  A small set of keys to expect would work too, decentralizes the trust a little better [Arch does it](https://www.archlinux.org/master-keys/).  it's mostly that it can be a little hard to automate the checks if files are not signed individually. If I had a `node-v.tar.xz.sig` I could automate the packaging entirely.  So yes, double the number of files (-2 because don't resign the shasums, obviously), seems to be quite common actually - if that's too much clutter, well, shame.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
gibfahn		LGTM, but do we normally have some sort of "observe for a couple of meetings first"? I don't think @gdams has been to any of the build meetings, but I'm also not sure what the precedent is.
joaocgreis		@gdams has already helped us before with https://github.com/nodejs/build/issues/616 and https://github.com/nodejs/build/issues/661.  Meeting attendance is not a requirement for WG membership. Meetings are a very useful tool, to the point of considering as official the decisions that are taken there, but GitHub is still our primary means of communication. We can just leave this open for some time (one week sounds good to me) to make sure there are no objections, then move forward.  I can do the onboarding. Other members of @nodejs/build are welcome, let me know if you want to join. 
gibfahn		>let me know if you want to join.  Would like to join
gdams		@joaocgreis thanks for offering to do the onboarding!
rvagg		+1  I'm pretty sure that's enough of us, let's get the onboarding done eh?
joaocgreis		Onboarding is scheduled for Wednesday.
gibfahn		Taking off the wg-agenda as this seems to be already decided
joaocgreis		@gdams is now onboard! @rvagg @jbergstroem @mhdawson @Trott can one of you please add him to https://github.com/orgs/nodejs/teams/build/members ? Thanks!
gibfahn		Would be good to also make @joaocgreis a nodejs/build maintainer as he's done the last few onboardings.
Trott		> can one of you please add him to https://github.com/orgs/nodejs/teams/build/members ?   Done!   
Trott		> Would be good to also make @joaocgreis a nodejs/build maintainer as he's done the last few onboardings.  Done! @joaocgreis if you'd prefer *not* to have the ability to add/remove people from the team, let me know, and I'll change your role back.
jbergstroem		Unfortunately, I won't be able to attend this one. 
joaocgreis		I won't be able to attend today, sorry. 
Starefossen		I haven't had much time for Build related stuff as of lately, sorry. But I can help facilitate the meeting if we decide not to cancel this one. 
geek		I am also unavailable today.   Thanks, Wyatt  > On Nov 2, 2016, at 3:18 AM, Hans Kristian Flaatten notifications@github.com wrote: >  > I haven't had much time for Build related stuff as of lately, sorry. But I can help facilitate the meeting if we decide not to cancel this one. >  > ‚Äî > You are receiving this because you are on a team that was mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. 
rvagg		With so many bails I think we should punt on this meeting and I'll stay on bed. @mhdawson you ok with that? 
rvagg		Cancelling. Let's try and resolve some of these things async.  
refack		üéâ üëè 
gibfahn		+1 on this landing, but @joyeecheung if you'd like to work on anything in the mean time, please let us know.
joyeecheung		@gibfahn  I am thinking about the continuation of https://github.com/nodejs/build/issues/887 , a CI job for `make lint` and doc tests? (see https://github.com/nodejs/node/pull/16208#issuecomment-337581773). I will open a issue for that.  EDIT: issue opened at https://github.com/nodejs/build/issues/929
gibfahn		@nodejs/build If there are no objections I think we should consider this settled. Does anyone have time to do an onboarding for @joyeecheung ? We might be able to combine it with one for @maclover7 (https://github.com/nodejs/build/pull/928).  I'd like to help out with the onboarding, but not comfortable leading it.
refack		I'd also love to help with onboarding (also don't mind leading if no one else is available)
joyeecheung		Rebased.  @refack @gibfahn I'd be happy to get onboard with @maclover7 if we can find a time that works for all of us.
joaocgreis		I can also join if you'd like me to, but my availability isn't so great at the moment so feel free to go ahead @gibfahn @refack . Ping me to add to the build team in the day before or so.  Welcome @joyeecheung and @maclover7 !
Starefossen		[This one](http://newrelic.com/server-monitoring) @rvagg ? 
rvagg		looks like it, apparently it's available in the free offering and can do some basic alerting 
Starefossen		Looks like someone has made an Ansible configuration for this as well https://github.com/Recras/ansible-newrelic 
bnoordhuis		I don't want to be the overzealous OSS hippie of the group but I'd be happier with something less proprietary. 
rvagg		the nice thing about using a SaaS for this is that it's yet another server that we have to monitor and we don't have to worry about monitoring the monitor 
bnoordhuis		I understand the appeal from a management perspective but I don't like the stamp of approval for a specific vendor it implies.  We're an OSS project and we should stay as neutral - and open - as possible.  (Aside, I have similar misgivings about Hangouts and Uberconference but I can see how a fully OSS approach there would have a too high barrier to entry for most.) 
wraithan		Totally understand @bnoordhuis, mostly suggested it since it is free and just kinda there. It can help find things like our low disk issues and have some alerting that gets the system a bit more stable until a more OSS solution can be built/evaluated/installed. The convenient thing about the newrelic solution is that they are in the business of being as turnkey as possible, which means less configuration up front.  In other monitoring concerns... which ties node closer to newrelic so requires much more of a decision... newrelic has non-profit stuff, if node would like to be a part of that, I'm happy to put in. Then we could use the newrelic java monitoring on jenkins which can monitor the health of the process, as well as the newrelic node monitoring for any node stuff. That said, I 100% understand trying to stay as neutral as possible. 
jbergstroem		How about making this issue slightly more neutral; have people suggest a provider and the rationale to why we should use them? I'm slightly `-1` to installing agents on all our clients -- would rather prefer a simple system that allows us to poll the json output from jenkins. The bottleneck is our master which should get slightly better treatment. 
jbergstroem		To sum the current monitoring issues up: 1. our jenkins master has a disk full issue we need to both mitigate and have a look at. Also, if this guy dives we're in trouble. 2. some slaves has had issues as a result of git versions -- this may be outside of monitoring. 3. our windows slaves has had issues with disk which is suggested to be fixed by `vcbuild.bat clean` between runs. 4. jenkins clients has died in rare occasions as a result of trying to reconnect to master. Not sure why, but I've seen this ~10 times since I started working on clients. This causes the jenkins java process to exit. 5. jenkins clients hardware goes offline either temporarily or for a longer period of time due to various reasons.  (please add if I'm missing something) 
bnoordhuis		I'd suggest something like cacti or ganglia.  One of the great things is that you can have a public dashboard, like http://ganglia.wikimedia.org/latest/.  If those are too enterprise-y to people's liking, it's easy to put something together with collectd or statsd + graphite.  They all require local agents but I don't think you can get away from that if you want detailed system information like CPU and memory usage, number of processes, etc.  Jenkins' JSON API doesn't return any of that, AFAIK. 
jbergstroem		@bnoordhuis I'm a big fan of collectd but it doesn't really solve the notification issue (the threshold stuff could be used but it would be hacky at best). I don't really care about cpu, disk or any other graphs, just knowing that jenkins and the host is working as intended. I think jenkins already gives us rudimentary information about the host should we want. Anyway ‚Äì since I'm starting to understand the different wants here, I'm probably not the guy to drive this. I'm happy as long as stuff is up :smile:  
orangemocha		I am neutral to using an enterprise-y solution vs OSS one. I mean, it would be nice to have an OSS tool so we could modify it ourselves but it's not like we have ever done that with Jenkins. So I am +1 for whatever gets the job done. New Relic certainly would and it's nice that it would be free.  I think basic server monitoring and a health dashboard would be useful. 
jbergstroem		I'm pretty happy with the monitor that @Starefossen has been maintaining. I personally wouldn't log into a dashboard  just to look at graphs; there's enough pie charts on the Internet. Does anyone else want to proceed here? Suggesting we close until we see a specific need. 
maclover7		ping -- what's the status here?
rvagg		meh, I think it's been dismissed. Worth noting that the monitoring that's been discussed here that we're apparently using _isn't_ actually working currently. We used to get updates to #node-build when workers were down, no more. Probably because of the whole crumb thing introduced into the Jenkins API that makes everything annoying.
jbergstroem		Perhaps add `ccache`? Otherwise LGTM (note: haven't tested the ansible script but reads fine to me) 
jbergstroem		I can possibly provide an init script if you haven't got one already.  
rvagg		systemd, I have one for something else, fedora perhaps .. or did I dump it in an issue here somewhere? 
jbergstroem		Probably this no? https://github.com/nodejs/build/blob/master/setup/fedora21/resources/jenkins.service 
rvagg		ah yes, but actually, this is probably closer to Ubuntu: https://github.com/nodejs/build/tree/master/setup/debian8 
MylesBorins		One other option would be to write a tap -> junit conversion utility and add that to CI. That way we can keep the tap output and still look at it but avoid the tap plugin 
jbergstroem		@TheAlphaNerd if there already is something like that we shoudl probably go for it, but changing `tools/test.py` wouldn't be too hard; just a bit squeezed in time the coming week. 
gibfahn		@TheAlphaNerd Makes sense, maybe something like this: https://www.npmjs.com/package/tap-xunit 
MylesBorins		@gibfahn I've experimented with it. Would likely work better with the node testsuite than with the citgm output. Shouldn't be too hard to put together a poc on Monday 
gibfahn		@TheAlphaNerd Okay. For testing purposes it's useful to run the npm test suite, as that has more complex TAP (subtests etc.) 
MylesBorins		@gibfahn we don't actually have the npm test suite running in CI.  If you want to collaborate on that we can chat about it in https://github.com/nodejs/build/issues/317 
jbergstroem		The more I've worked with this the more I'm convinced that we should fix the TAP plugin. Optimally, I'd like the tap plugin to just pass ok/fail/skip status as well as failed tests back to Jenkins (for storage). That way, we can make better use of the json api when we provide information to github. 
gibfahn		@jbergstroem So your suggested fix would be for the TAP plugin to still store the TAP results in the `build.xml` file, but to only store some of the info? 
jbergstroem		@gibfahn you tell jenkins to pick up the junit file(s), after which it stores it (more efficiently) in build.xml. 
gibfahn		@jbergstroem So the TAP plugin converts TAP -> junit and stores the junit in build.xml?  **EDIT**: So the TAP plugin _should_ be converting the TAP to junit and directly storing the junit in the build.xml file. 
jbergstroem		@gibfahn it provides primitives for parsing tap which seems to be done every time build.xml files are read (hint: a lot) instead of once. there's a few issues over at jenkins issue trackers about this. 
gibfahn		@TheAlphaNerd How's the TAP -> Junit converter coming along? Is there something I could get my hands on to test?   I'd like to test this on https://github.com/nodejs/node/pull/7867 (test-npm), as the npm test output is quite complex already. It'd also be useful to compare the converter output to the output of `make test-npm` with the reporter set to xunit. 
jbergstroem		Script that we should be running on all workers: https://github.com/jbergstroem/tap2junit
joyeecheung		What's the progress on this? https://issues.jenkins-ci.org/browse/JENKINS-20212 is still open. I think for option 3 it's possible for the github bot to parse the TAP output and do something about it, or it can trigger another bot to do that on test completion.  EDIT: or we can try out https://github.com/jbergstroem/tap2junit
gibfahn		>EDIT: or we can try out jbergstroem/tap2junit  We should do this. @jbergstroem had a job that was working using tap2junit, and it should be easy enough to get installed on all our machines, so it's just a question of adding it ansible, installing it, and then adding the tap2junit step to the jobs.  Obviously if someone would like to fix TAP in Jenkins that would be ideal, but we seem to have a scarcity of Java experts.
gdams		@gibfahn can this be closed now? AFAIK we are using tap2junit on CI now?
gibfahn		Worth checking to make sure we're using it in every node-test-commit subjob, otherwise yep, can be closed.
refack		There is a little friction. Some machines have python2.6 (no good `pip`, so installing `tap2junit` is a PITA). Also windows machines are harder to script. I would use the Jenkins script console, but it takes ages to initiate the first command). There's also a bug in `tap2junit` that it doesn't understand this: ``` not ok 2137 addons/stringbytes-external-exceed-max/test-stringbytes-external-exceed-max-by-1-binary   ---   duration_ms: 41.541   severity: crashed   exitcode: -9   stack: |-   ... ``` as a fail
gibfahn		> There's also a bug in tap2junit that it doesn't understand this:  That seems pretty serious, so we're getting green CI even though tests have failed?
refack		> > There's also a bug in tap2junit that it doesn't understand this: >  > That seems pretty serious, so we're getting green CI even though tests have failed?  Yellow.
rvagg		One was overclocked to 950 and the other to 900, I've turned them both down to 700. They're building a lot slower but stability is going to be more important. If this makes a difference to stability I might try bringing it back up a tiny bit and see how it goes.
gibfahn		Given that we're promoting the other builds and then coming back for the Pis, I'd say that makes stability even more important vs speed. Not overclocking SGTM.
mhdawson		I've also just enabled https://ci-release.nodejs.org/computer/release-devrus_mhdawson-debian7-arm_pi1p-1/  Will keep an eye on it as I think it was overclocked based on the earlier guidance
rvagg		happy with the machines for now, resetting the overclocking seems to have done the trick
mhdawson		@jbergstroem what you do think of adding configuration of the iptables in this way.  Other than having to apply to all of the machines it seems reasonable to me.
mcollina		@mhdawson @rvagg can we move this forward? it's blocking https://github.com/nodejs/node/pull/7855.
maclover7		ping -- is this still needed?
mhdawson		@jbergstroem any objections ?
rvagg		I'm not seeing _any_ firewall rules on the fedora hosts that I'm testing. Does it make a difference now that we only have Fedora 23+ and not 22?
lostnet		@rvagg I think most newer distributions were setup to accept all locally generated traffic by default, so 23 might have been the cut off for fedora.. Still needing rules for 127.0.0.2 for a host like [rhel72-s390x](https://github.com/nodejs/build/commit/d2942fe66f6b4bd2dba2b7ee37557ea7c3c8b608#diff-4bbb7b4f3a118086659d01bec9c83d28R62 ) implies that distribution doesn't have a setup to let all locally generated traffic pass which means a local test with either multicast and/or the nodes own external addresses selected would fail limiting what tests could be done.
gibfahn		How did you get this file?  Is there an easy way to restore this file?  cc/ @nodejs/citgm 
refack		https://ci.nodejs.org/job/citgm-smoker/jobConfigHistory/ ~Didn't see an import.~ `java -jar jenkins-cli.jar -s http://server create-job newmyjob < myjob.xml`
rvagg		Are we on?
gibfahn		@rvagg don't think anyone who can do the video is here.
rvagg		OK, let's skip this one eh? Awkward time of the morning for me anyway and I feel like we've been pretty good at staying in sync and making decisions on GitHub these past few weeks anyway. 
rvagg		not doing this into /ansible/ because we don't have the web server set up in there yet (or do we?), it's pretty special case so I'm not sure it's worth doing anyway.
jbergstroem		@mscdex following up; is this still a thing? 
mscdex		At the moment it's back on master. 
jbergstroem		good. reopen/ping if it goes the other way again. 
bnoordhuis		Not a build member but since Imran is already doing the AIX work for libuv, I think it makes sense. 
jbergstroem		So what we would do here is provide him access to the nodejs_build_test key (add his gpg to the correct subfolder of secrets), right? 
mhdawson		That's what I had in mind. 
rvagg		Is @IwuzHere an IBM person? 
mhdawson		Imran Iqbal, he works in my team with me here in Ottawa. 
rvagg		great, that's easier to establish trust then, +1 from me for adding to nodejs_build_test. 
jbergstroem		How about we make these things a bit more formal and accept through WG meetup? Suggesting for agenda. 
mhdawson		@jbergstroem sounds good to me. 
jbergstroem		I guess this also raises the question about onboarding. Lets have a think about a few things we should make sure every member of the build crew understands before venturing into the unknown. 
shigeki		I built and made test evaluations of Node on my rooted android about a years ago in https://github.com/nodejs/node/issues/1430. The test coverage were bad. To run tests, it needed to disable PIE to run python as https://github.com/nodejs/node/issues/1396#issuecomment-93277631 and it   is very hard to have such devices only for Node developments.  Unless we have more better environments to develop Node on android, I think that it is hard to continue supporting. 
cablegunmaster		- Is this something you've personally had any experience with?   Yes, well just a small project for school to use a cart system application on a mobilephone, initial setup:    Website -> API->Database.    Mobile phone (Android) -> API ->Database.     Mobile phone (Android) ->Socket.io ->API->Database.    It was fun to do. Wanted a live synchronisation with JSON to push updated list. so a cart system is updated when someone else added a item to it. (say your family ). also for a practicall use try making a chat application with socket.io (works really nice allong). - Do you have any insight into how Node is being used on Android at the moment?   Socket.io is used as a independent server so you need a Server port and a port open on your mobile phone. Basically its using the socket way of communicating pushing communication along.  http://socket.io/get-started/chat/ -> port open or a environment on the server side where you can control a node server.  Look at the example and experiment! - Do you have any thoughts on Node + Android to add to what I've listed above?   Depends on what you are making. socket.io is only used for making the application respond in Realtime. - Is it worth investing some investigative time on this task at least? (I'm not sure who would do this, while I'd love to do it, I'm probably one of the most time-poor people that could do it).   Basically it makes your application you are trying to develop more realtime, its only worth building in if it needs to alter in realtime data, lets say weather information, locations of other vehicles on maps (JSON data pushed , and updated). At least this is my analyses from the mobile point of view. - Do you have any strong opinions regarding a possible shift toward first-class support of Android (pending investigation and further discussion of course, but initially, what is your reaction to this)?   I like socket.io and love to make a game based upon the socket.io framework. but same as you starved for time. Its fun, but you have to wonder if this is the way your application should go upon. Does it need realtime updating? Or is it just because you love the feature? 
mhdawson		In terms of building apps that use javascript for android there is https://cordova.apache.org/ and you can use things like socket.io in those applications.  So my initial thinking is that use cases where Node would be used to support a server running on android as opposed to client applications would make a stronger case. 
bsegault		I'll try to answer your questions:  > Is this something you've personally had any experience with?  I'm currently working on a multi-platform project that partly relies on Node. Since we want to use smartphones, android is unavoidable; so we built our application for Android (ARM & x86), and recompiled Node.js for Android. So I'm no expert, but I'm working on it =)  > Do you have any insight into how Node is being used on Android at the moment?  Not really. I'm hearing about stuff like Cordova & jxCore, but I don't think that many developers are using Node as-is in Android (because of the structure of an Android package, it's not intuitive to use node in "standard" Android packages, IMHO).  > Do you have any thoughts on Node + Android to add to what I've listed above?  Not really.  > Is it worth investing some investigative time on this task at least?  I think it's worth investigating how developers can benefit of Node.js on mobile platforms, and what can the community & Node developers do about it. It's no random act that people talk more & more about Cordova & jxCore, though I have no idea of how those work. 
lamuertepeluda		:+1: for Android support. I can answer to the questions as well: 1. yes, since the time I was involved in [webinos](https://github.com/webinos) project. At that time we were relying on [anode](https://github.com/paddybyers/anode), a partial port of node.js which I think now is stopped. That said, both node.js and the hardware capable of running it have made giant leaps forward, so it should be even easier to achieve node running inside an android app. A great tool for running node on Android right now is [Termux](https://termux.com/). Another interesting project is [nodekit](http://nodekit.io/) which claims Android support too, but does not seem to implement it yet, perhaps because of its early stage of development. 2. I don't have really insight of how node is being used on Android right now, but I've tried jxcore (and it's companion jxcore-cordova-plugin) and they seemed to be fit for the scenario I had in mind. Sadly [Nubisa has stopped developing jxcore](http://www.nubisa.com/nubisa-halting-active-development-on-jxcore-platform/), which was also stuck to 0.10.x API. [Other projects seem to have similar targets](https://github.com/InstantWebP2P/node-android), including the aforementioned nodekit.io. I expect the use-case would be like on Windows10-based or Linux-based arm boards, where ad-hoc builds of node.js are used to behave like small server or whatever... anyway on a platform like android (let's not just focus on mobile phones, but also on TV, raspberries, tablets and other devices capable of running it) I expect even more possibilities of interaction. 3. Sockets.io/websockets is just one of the possibilities, even though one of the most interesting: it could be possible, for instance, of embedding small in-app web servers (static or dynamic), with superior performances and features compared to others (e.g. NanoHTTPD, quite simple but also really slow).     @mhdawson with cordova you can just add a socket.io **client** to your app, but not a server, afaik 4. Yes, I think it would be worth. Perhaps starting from what has been already made by other projects. 5. It would be great to have the possibility of embedding it as a library - basically having a sort of wrapper for Android apps. I think the jxcore model is the right direction. An official cordova plugin would be the icing on the cake, but that's perhaps asking too much. :smile:  
dianahinz		:100:   @lamuertepeluda  Agree on Jxcore direction. Jxcore did a very good integration with Android. node fs was able to read from APK.   ping @obastemur ! I guess you have the most experience here. 
gardner		It was fun while it lasted but jxcore is no longer maintained. It would be rad if there were jxcore style android-ndk build facilities in nodejs mainline. The cordova plugin makes it easy to create a hybrid app that can connect to the in-process node/jxcore instance.  For an example of a client-server application that runs locally you can checkout the decentralized app [patchwork](https://ssbc.github.io/patchwork/). I have been trying to compile that project for android with little success.  üôá 
mihailik		I've been using node on Android **on Debian chroot** as part of using a tablet for development instead of a laptop.  That is, using node-based tools for compiling, inlining, uglifying and self-hosting from a terminal.  Chroot is a royal pain to maintain, otherwise the usage and patterns repeat the pure naked Android nodejs patterns: terminal, localhost:XXXX, browser etc.  The short-term practical production usage limitations are: - no idea how to build/download node for Android (that's why I've gone chroot/Debian) - is rooting needed? (ideally no) - occasionally OS kills non-interactive processes to free resources - how to spot and terminate runaway node processes? - ability to prevent sleeping would be nice  On the other hand, easy way of hosting servers on a commodity Android device will be **a heaven's blessing** for poor countries. Cheap smartphones become a prevalence even in poorest areas, and being able to run school's website from a $40 phone makes a big difference. 
gardner		I was able to build the node binary for android using the [android-configure](https://github.com/nodejs/node/blob/master/android-configure) script in the main repo. You need to download the Android NDK and build from Linux. I used an Ubuntu 15 vm. It builds the node binary which can then be pushed to `/data/local/tmp` on the device.  My work is available in this repo: https://github.com/gardner/ssband 
eljefedelrodeodeljefe		I am trying to make some efforts about this. https://github.com/nodejs/node/pull/3098 landed just now and if you guys have other stuff that would be useful upstream, please feel free to give it a shout. I am collecting some info from your links in any case. Thanks! 
Trott		Should this remain open? If so, is there a label that might be useful to add such as `help wanted`?
rvagg		FYI I only closed this because there's been no movement. If someone wants to pick this up and figure out a path to better Android support then by all means reopen it or open a new issue.
gardner		I'm not part of the core group here. In my experience the first ticket to bring up an issue is left open so duplicates can be closed. Since this is the first issue that was logged to track android support, new github issues are closed by referencing #359 as the issue that it duplicates. I understand keeping things tidy but it may be hard to close a new issue as a dup if the original issue that it is duplicating has been closed.  That said, nodejs does build for Android. The `build` repo does not enable that in any way. I'm not sure if it's worth the effort to move it into this repo unless someone wants it.  > If someone wants to pick this up and figure out a path to better Android support then by all means reopen it or open a new issue.  I guess that's kind of the point @Trott was making. If we leave it open then new folks can see it as a potential way to help.  This was a pretty long comment for my very agnostic view on the matter.    
rvagg		Case made 
d3x0r		I picked up this today; I would like to use node as a .so I can load into some android app. I can't get it to build at all. ``` In file included from ../deps/icu-small/source/common/unicode/unistr.h:31:0,                  from ../deps/icu-small/source/common/unicode/brkiter.h:47,                  from ../deps/icu-small/source/common/locdispnames.cpp:23: /home/d3x0r/work/node/android-toolchain/include/c++/4.9.x/cstddef:44:28: fatal error: bits/c++config.h: No such file or directory  #include <bits/c++config.h> ````  I would think stddef should be able to find bits/c++config.h but I don't know where that would be defined; it's apparently internal? something --sysroot?  ``` find . -name c++config.h ./android-toolchain/include/c++/4.9.x/arm-linux-androideabi/armv7-a/thumb/bits/c++config.h ./android-toolchain/include/c++/4.9.x/arm-linux-androideabi/armv7-a/bits/c++config.h ``` https://github.com/nodejs/node/issues/9707 referenced the same missing .h and there was a modification to add --force installing the standalone toolchain?  (that didn't help) I might have missed some other thing that would fix this.
Trott		I'll likely be missing this one, I'm afraid.
piccoloaiutante		I'll be missing it but I just wanted to say that the PR for optimising windows provisioning (https://github.com/nodejs/build/issues/495) in Ansible is almost ready. I haven't open it yet because I'm still missing a full testing on Windows 2008 server R2. I tried many times but I wasn't able to run Ansible script over it. @joaocgreis are you able to make a test over this platform? Should I open the PR?  I already tested it on windows server 2012 and windows 10 and it worked.
joaocgreis		@piccoloaiutante please open the PR, I can test it on 2008R2. Thanks!
piccoloaiutante		@joaocgreis done https://github.com/nodejs/build/pull/576 . Let me know if you need anything else. Thanks!
gibfahn		PR for minutes: #577  Sorry for missing this, got my times confused!
gibfahn		Minutes merged, closing this...
orangemocha		Thanks for reporting it, @bnoordhuis .  It looks like the [build was aborted](https://jenkins-iojs.nodesource.com/job/node-test-pull-request/83/) and TAP processing started normally after that. What's not-so-normal is that make seems to be running after the build was aborted. Or maybe it could be that the outputs of different processes are showing out-of-order?  In any case, considered that this happened after an abort, I wouldn't worry about it. Previous runs on the same node look ok. 
bnoordhuis		It looks like @Fishrock123 aborted it but it was still running when I reported this issue, about 6 hours after starting it. 
Fishrock123		Oh. All I know is it was stuck and had been for a while so I aborted it. 
orangemocha		I'll probably just keep en eye on this should it become an issue again. 
orangemocha		Not sure why it was stuck in the first place. Make output followed the Jenkins abort message in the log but in the end the build was aborted and reported as failure correctly. It doesn't seem to be something that we need to worry about. 
joaocgreis		Contacts are on the secrets repo in `build/infra/admin_logins.md`.
joaocgreis		There are no contacts there for Cloudflare, but there are for other providers so the Cloudflare section should probably be updated.
jbergstroem		I just added contact info to Cloudflare. Closing.
mhdawson		@jbergstroem thanks.
misterdjules		@jbergstroem Actually, after further discussions with @jperkin, here's my current position on using g++4.8 to build Solaris binaries on our SmartOS machines: https://github.com/nodejs/node/issues/3349#issuecomment-148545997. **TL;DR**: using gcc/g++ 4.8 would actually break more users, and maybe https://github.com/nodejs/node/pull/3391 is a better path forward.  However, upgrading to pkgsrc 2014Q4 might fix another build issue with debug builds. See https://github.com/nodejs/node/issues/1967 and specifically https://github.com/nodejs/node/issues/1967#issuecomment-148511890. 
jbergstroem		Since #3391 seems to be stalling somewhat I'll proceed with getting us to 4.8 and see if we can improve the current situation. If we end up flagging 4.7 as safe we'll just revisit. 
misterdjules		@jbergstroem I don't t think building node releases with the g++4.8 package present in SmartOS' pkgsrc repository is a good idea, for the following reason that I mentioned in [a previous comment](https://github.com/nodejs/node/issues/3349#issuecomment-148545997):  > It would be possible to upgrade build machines to a pkgsrc version that provides gcc/g++ 4.8 and build sunos binaries with this toolchain. Unfortunately, that means node would be dynamically linked to a C++ runtime with a path that is not the default path (/opt/local/gcc48/x86_64-sun-solaris2.11/lib/libstdc++.so.6.0.19). Thus, any user who would want to run node on sunos would need to install that dynamic library at the same path by install the optional gcc-4.8 package on SmartOS.   However, as I mentioned before, using the binutils from the 2014Q4 pkgsrc repository would probably fix the issue with `objdump` crashing for debug builds. 
jbergstroem		@misterdjules I just updated the 32-bit machine to 2014Q4 but binutils still seems broken. doing a clean build now to verify. currently building with 4.8, but will try 4.9 next. 
jbergstroem		@misterdjules for me gcc48 installed itself as default instead of gcc49. Is there some magic regarding how smartos handles gcc installs? 
misterdjules		@jbergstroem What do you mean by "installed itself as default"? Also, what image did you use to create the virtual machine, and what pkgsrc version are you using? 
jbergstroem		1. this is `iojs-smartos-32-1` 2. changed package source from `2014Q3` to `2014Q4`, having gcc47 and gcc49 installed (`/usr/local/bin/gcc` was gcc49) 3. updated all packages 4. no longer any default gcc (living in `/usr/local/bin`) 5. installing gcc48 takes above place (`/usr/local/bin`) whereas gcc49 now lives in `/opt/local/gcc49/`. 
chorrell		Hi,  What does this show?  ``` cat /etc/product ```  That should show what image `iojs-smartos-32-1` is based on.  Upgrading pkgsrc to a new release can be tricky and just changing the source from 2014Q3 to 2014Q4 might not be enough.   For pkgsrc, all packages are installed under /opt/local so gcc will be installed in /opt/local/bin/. Don't rely on gcc being in /usr/local/. This is probably one of the most annoying quirks of SmartOS that people have to deal with...  Both gcc 4.8 and gcc 4.9 should be available in pkgsrc 2014Q4, so to install gcc 4.9 this should work:  ``` pkgin install gcc49 ```  To be honest, I think it would be a lot easier if we just got a replacement for `iojs-smartos-32-1` based on a base-32-lts 14.4.2 image. I can help with that :) 
jperkin		All gcc versions are prefixed into `/opt/local/gcc<version>`, e.g. `/opt/local/gcc49`, and then there are `pkg_alternatives(8)` links from `/opt/local/bin`, however we haven't added the necessary alternatives file for gcc49 yet which is why there is no `/opt/local/bin/gcc` symlink.  This will be fixed for 2015Q4. 
chorrell		Ah, ok. So if gcc47 was previously installed there should still be a /opt/local/bin/gcc wrapper script there pointing to /opt/local/bin/gcc47.  So you should be able to remove the /opt/local/bin/gcc wrapper script and then add a symlink for /opt/local/bin/gcc -> /opt/local/bin/gcc49 
jbergstroem		Looks like all I have to do is modify `/opt/local/libdata/pkg_alternatives/opt/local/bin/gcc`. Changed to be consistent with our 64-bit CI machine, but I'd still like to explore using `gcc48`.  For the record, we previously had gcc47 and gcc49 installed and after switching to 2014Q4 I installed gcc48 as well.  I've done a few base/package upgrades on SmartOS before, this was less painful than "usual".   Finally, `/etc/product`: `base 14.3.0` 
misterdjules		@jbergstroem What are we trying to achieve with building binaries on SmartOS with gcc-4.8? 
jbergstroem		@misterdjules I'm under the impression that binaries would work on solaris 11.2. I'm currently not pursuing this (esp since we're using these vm's do cut releases). 
misterdjules		See https://gist.github.com/misterdjules/eae9ec70dc1d91fb8dd1 for more details about what's going on with SmartOS binaries. 
jbergstroem		going to close this. We won't update any repos, rather reprovision - but the report from @misterdjules above is highly relevant. What we needs to be done here is open a new issue to outline the way forward -- most likely creating new release binaries for `smartos` and finding resources for baking proper releases on solaris that would replace the current builders (`sunos`). 
mhdawson		@joaocgreis 
joaocgreis		This was tracked and fixed for x64 in https://github.com/nodejs/v8/issues/4 .  We don't have 32 bit builds in the test CI because we don't have real 32 bit VMs, but this issue makes a very good case for adding WoW64 somewhere in the matrix.
kunalspathak		> We don't have 32 bit builds in the test CI  Is it possible to pass `x86` to `vcbuild.bat` and that should trigger 32 bit builds or is it that it won't simulate the behavior we would get while running on Wow64?
joaocgreis		Yes, that's even how we do the x86 releases. But we don't have 32 bit machines to test a 32 bit node.exe natively, WoW64 will have to do.
joaocgreis		Added a x86 build to the test matrix to `node-compile-windows` on VS2015. The resulting 32 bit binary is tested on the 64 bit Windows 2008 machines. Test runs:  - [`master`](https://ci.nodejs.org/view/All/job/node-test-commit-windows-fanned/8217/) ‚úîÔ∏è - [`v7.x-staging`](https://ci.nodejs.org/view/All/job/node-test-commit-windows-fanned/8218/) ‚úîÔ∏è (one failed test but not on the x86 build) - [`v6.x-staging`](https://ci.nodejs.org/view/All/job/node-test-commit-windows-fanned/8219/) ‚úîÔ∏è - [`v4.x-staging`](https://ci.nodejs.org/view/All/job/node-test-commit-windows-fanned/8220/) ‚úîÔ∏è 
gibfahn		Not going to make this one (2 a.m. Beijing time).
piccoloaiutante		I'm going to miss it. I'll be travelling for work at that time.
joaocgreis		I don't know if I'll be able to make it in time. Please start without me, I'll join if/when possible.
mhdawson		meeting link for participants https://hangouts.google.com/hangouts/_/ytl/JNay5hb-3oXGPt37GbiVrnZMvqfO8slRyE-qQ1XblAg=?eid=100598160817214911030 
mhdawson		Meeting is live just waiting until we have at least one other participant before starting the live stream.
mhdawson		Will give it another 5 mins and then assume we are cancelling for this time, and we'll meet at the next regular time as we met just last week.
MylesBorins		I won't be able to make it today.
mhdawson		Ok, don't think we are going to get any attendees this week.  Cancelling.
AndreasMadsen		I suppose `ggplot2` could be made optional since we are not making any figures. But right now I think it is a requirement.
gareth-ellis		Anyone in @nodejs/build who could take care of this? (I am assuming I'm opening this in the right place....please let me know if I've opened this in the wrong place... :) )
rvagg		OK, I've done this manually on both servers, but this _really_ needs to be automated in our ansible scripts (see ansible directory) so if someone could try and take care of that it'd be greatly appreciated. This work seems pretty good and would cover all of these tasks fairly simply I think: https://github.com/Oefenweb/ansible-r, otherwise it could be used as a base, or the following could just be hardcoded as shell executions:  ``` echo "deb https://ftp.heanet.ie/mirrors/cran.r-project.org/bin/linux/ubuntu xenial/" > /etc/apt/sources.list.d/r.list apt-get update apt-get install r-base r-base-dev -y echo 'install.packages("ggplot2", repo="http://cran.us.r-project.org")' | R --no-save echo 'install.packages("plyr", repo="http://cran.us.r-project.org")' | R --no-save ```  Crossing fingers that this is all that's required. There's a hefty amount of packages in there!
refack		I've asked a friend who is an R maven to port `compare.R` to node, but it'll take some time + the PR cycle.
AndreasMadsen		@refack have some concerns about that. There should be an open pull request about it, where I've commented on some of the issues.  _I'm on mobile so I can't find it for you, sorry._
refack		> I'm on mobile so I can't find it for you, sorry.  I'll look it up.
refack		~https://github.com/nodejs/node/pull/7094?~ Found it https://github.com/nodejs/node/pull/12585
mhdawson		I was out on holiday.  @rvagg was the only option to install on the machine itself.  It at all possible I would have preferred that the job do the install to a temporary directory but I understand that is not always possible.  
refack		Has this been resolved?
Starefossen		Here is the [SSL Server Test report for nodejs.org](https://www.ssllabs.com/ssltest/analyze.html?d=nodejs.org&s=104.20.23.46&latest), at least on the browser side of things almost all of them support SNI.  Also, I am pretty sure that, if configured correctly, non-SNI capable clients will be able to connect to the default vhost. 
jbergstroem		connecting to the default vhost won't help them since that would return the wrong certificate for iojs.org (`*.nodejs.org`) 
jbergstroem		It looks like the vm's at travis-ci.org seems to have a too old version of `wget`. 
ljharb		This issue affects all travis-ci VMs that aren't in their new "container" format (ie, `sudo: false` is the new one) - and it may indeed affect the new ones also, but I haven't checked. 
Starefossen		So the issue is somewhat limited to old wget-users downloading io.js binaries? Mostly through travis ci which there apparently exists a workaround for? 
jbergstroem		That'd be my conclusion based on wikipedia. We for instance have this issue on our centos5 slaves; but again limited to wget. 
rvagg		I've just tried out the new "Floating IP" DO feature on the web server, it now has 45.55.98.129 pointing to it, _but_ the server doesn't appear to be aware of this, it's obviously just a datacenter routing thing rather than going all the way down to the server like it can with AWS. So I'm not sure we're going to be able to do the separate IP thing unless we have a separate server which is a pain since we've gone through the process of integrating everything. 
jbergstroem		@rvagg doesn't that just mean we can assign another ip with network setup? I can try. 
rvagg		you can try I guess, I have left it on there for this kind of tinkering 
jbergstroem		So, the anchor ip you set up locally is where we want to point iojs (or nodejs, but i prefer messing up the prior). Not sure how to automate all of this ansible though. I'll look at that first. 
jbergstroem		OK, this likely needs to happen: 1. cloudflare: upgrade iojs.org to a business account and upload the certificate 2. cloudflare: change iojs.org to listen at the anchor public ip address  3. nginx: make iojs listen at the internal anchor address 4. (probably optional) nginx: make nodejs listen at the default public address 
rvagg		I recently downgraded iojs.org to a personal account because I discovered that my personal credit card was being charged $200/month for the pleasure of it being a business account... 
ljharb		@rvagg how recently? Perhaps that's what triggered these SSL issues. 
rvagg		pretty sure it's unrelated, we didn't lose any features that we were using as far as I know 
jbergstroem		As far as i know, going from business to free or using business won't affect SNI. What we need is an enterprise account to get our own ip. 
ljharb		ah, gotcha 
Trott		@nodejs/build @jbergstroem Should this remain open?
yorkie		How about after someone had ran `make test`, the test runtime will send back the result and testing machine information to our build system, it will help us collect more unexpected failure cases as possible, I'm unsure this is the right place to post this, just figure this out after reading @rvagg's article, thanks. 
rvagg		@yorkie, check out https://jenkins-node-forward.nodesource.com and drill down in to "Console Output", it's all there. We still have a failure on ARM and intermittent failures on others, those need to be cleaned up so feel free to dig around in https://jenkins-node-forward.nodesource.com/job/iojs+v0.12+multi/ if you want to contribute to making the test suite more solid. 
yorkie		Okay, thanks for the detailed guidance for me, i'm busy recently but i'm always interested in the build project for node(or iojs, haha), i will do check given links by you and wanna contribute to this upcoming project once i'm free, :cherries:  
bnoordhuis		> Do we mark Linux packages as "conflicting" with "nodejs" or set up an "alternatives" style system? (I know it's doable on Debian-based systems but am vague on RHEL/Fedora-based systems here)  You can put a `Conflicts: nodejs` header in the .spec file.  There's an additional safeguard in that rpm/yum won't let you install a package that overwrites files from an installed package (unless you force it to, of course.) 
bnoordhuis		> How are tarballs named and hosted, straight from GitHub?  Good question.  I would suggest mirroring what nodejs.org currently does and host them at a stable and predictable URL, like http://iojs.org/dist/v1.2.3/iojs-v1.2.3.tar.gz, but that would require checking in the tarballs because iojs.org is currently a GH pages website. 
davidbanham		dist.iojs.org/etc then?  It would make sense to keep big binary assets like that separate from the website content. On 5 Dec 2014 08:22, "Ben Noordhuis" notifications@github.com wrote:  > How are tarballs named and hosted, straight from GitHub? >  > Good question. I would suggest mirroring what nodejs.org currently does > and host them at a stable and predictable URL, like > http://iojs.org/dist/v1.2.3/iojs-v1.2.3.tar.gz, but that would require > checking in the tarballs because iojs.org is currently a GH pages website. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/iojs/build/issues/18#issuecomment-65705958. 
indexzero		:+1: to dist.iojs.org as a separate repo backed by Github pages 
davidbanham		Agree. Back it with github early on and we've always got the flexibility to switch it out to S3+CloudFlare or similar later on. On 5 Dec 2014 08:45, "Charlie Robbins" notifications@github.com wrote:  > [image: :+1:] to dist.iojs.org as a separate repo backed by Github pages >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/iojs/build/issues/18#issuecomment-65709192. 
ghostbar		@rvagg @chrislea I can help on the Debian releases, making up a reprepro on deb and helping on the infrastructure for it. Plus, I could be pushing the debian packages to the debian archive (I'm a debian developer) 
chrislea		@ghostbar Thanks for the offer! We already have a build box with reprepro set up that we're using. And I don't think we're allowed to push the packages as they are into the real Debian archive because we statically link in basically everything (v8, cares, etc), which as I understand it is against the Debian packaging guidelines. But, if I'm wrong then I'm sure @rvagg and others would be interested in talking about official Debian support. So if that's the case please let us know. 
ghostbar		@chrislea Ok yes, that's against the rules for getting into the archive and I understand the reasoning behind this as in: just download one .deb, install it and you're done.  Anyway, if it's going into a reprepro anyway, I don't see why we couldn't even make the separate v8 packages without adding too much issues into it. We could use the base as of how it's handled currently the v8 and nodejs packages on debian.  Let me know and I could work on it next weekend. 
stryju		Hey guys!  Any progress re: Debian packages? 
chrislea		We are working on them. Support for the current Debian stable (wheezy) is a godawful PITA because the compiler it ships with is too old to build iojs. And backporting a newer compiler brings its own headaches. So once we have that planned / sorted out we'll get packages built. 
stryju		sweet! thanks for taking the time to reply :-) 
ianare		Hi all, any plans for a CentOS 7 packages?  I would  think this would be easier than 5/6 due to the newer compiler and systemd... There are node packages also which could be used for inspiration.  In any case, excited about all the hard work done on this project, keep it up! 
Fishrock123		@rvagg looks like some of this is done? :) 
stryju		can we get a small update on what was achieved from the list? 
rvagg		Sorry! I've ticked some things off, but here's things that weren't achieved: - OS X CI machines are only 10.10. I have a 10.9 _image_ I might ship over to the CI machines but I have no way of getting hold of a 10.8. If anyone has ideas on how to get installation media for a virtual machine for 10.9 or 10.8 that would be greatly appreciated. Apple make this way too hard (and I don't understand why nerds embrace them so much!) - Linux releases are not being made available as .deb or .rpm _yet_. @chrislea is working on this but it's a non-trivial task. Expect news on this very soon though. - Solaris-variant is not being tested in any way and this doesn't appear to be missed. From memory there has only been one Solaris-related issue on the io.js issue tracker and that was simply about DTrace on an Illumos variant, by @no9. 
stryju		thanks for taking the time to reply, @rvagg! :beers:   ---  would be great to have it as a summary @ top - WDYT? 
No9		@rvagg Initial thread was DTrace compiling on FreeBSD   Problem I have is that my running illumOS instances are all 4.6.4 GCC and so iojs wouldn't build.   I can take the razor to that hirsuit mammal if there is we want to provide a SmartOS build but I don't know how much bandwidth I would need to give to integrate with CI  
rvagg		@no9 it's not really a technical problem but a resourcing one, we don't have the ability to provision solaris-variant machines at the moment, none of the providers we're working with have that capability and nobody has expressed an interest in providing it. I'm not too fussed at this point, I don't think it's in our target audience at this stage. 
No9		@rvagg ok ill leave this for now then.  If someone shows up with the need and we need to dig in you know where I am.  
jgrund		Hi, Is there still an io.js rpm in the works?  
rvagg		@jgrund yes, over @ https://github.com/nodesource/distributions  we're having some complications in getting everything up and working properly across all our supported platforms given the new complexity in the versions we're supporting but we're now in the process of throwing more people at this. 
stryju		thanks for the update @rvagg  
StefanoBalocco		Any news about a (maybe official) debian (jessie) package? 
ripper2hl		iojs.rpm ?  D:  
jbergstroem		LGTM 
jbergstroem		Merged in 6cacb25131e12a19261fd85d426b819bf4bb60b3. 
maclover7		ping -- what's the status here?
rvagg		not in use
ljharb		just to underscore the urgency, if the SSL certs for either of these sites stop working, every single node.js travis-ci test will instantly begin to fail. yay!
rvagg		How pessimistic of you @ljharb! We won't let it fail. This is merely an early call for help, if nothing moves then I'll just find some spare hours to do it myself and that might just mean paying for replacement certs cause it's the quickest solution.
ljharb		Oh I trust yall will figure it out in plenty of time :-)
MylesBorins		I'd say we should top up the infra we have that already works and start the process of moving over to lets encrypt
rvagg		I think I agree with that sentiment, it's not expensive to renew these multi-year ones (I'll get the actual price for you soon) and then we can tinker with letsencrypt. Unfortunately the need to regularly renew letsencrypt certs makes it a non-trivial exercise and with all of the things on our plate the moment we should probably give ourselves some room to move!
MylesBorins		@rvagg I have setup a cron job on one of my own servers to keep lets-encrypt up to date... although I will be honest that I think a hosted service is a better solution... would rather have an SLA and not be responsible for taking Travis offline. At the same time I really want to support let's encrypt... so I'm unsure of the best path forward
rvagg		letsencrypt is a Linux Foundation project, a path forward might be to reach into LF connections and find some help to making a successful collaboration, even if that means having them hold our hands on best practices to make sure we can reach the kind of reliability we need without having to have us constantly worrying about it!  <img width="742" alt="screen shot 2017-08-02 at 4 29 52 pm" src="https://user-images.githubusercontent.com/495647/28860418-e2644ed8-779f-11e7-96bd-6c2734cce771.png">  Unfortunately it's not as cheap as I remember but this is for wildcard certs for 24 months. It's $70.95 if we go for 12 months.  @MylesBorins @mhdawson @jasnell @nodejs/build we should probably get TSC agreement on this kind of expenditure right? How about we go for 12 month certs x 2 = $141.90 total, with the plan to replace it with a solid letsencrypt setup during that timeframe. What think you?
MylesBorins		LGTM
piccoloaiutante		Sounds good to me
mhdawson		LGTM on renewing now and then figuring out a migration path.
mhdawson		Opened https://github.com/nodejs/TSC/issues/302 for the formal TSC approval side of things.    @rvagg what is the standard way that we have done this in the past and then made sure that we reimbursed whoever does the payment ? 
jasnell		I have asked the foundation folks if funds are available or if an allocation is required. I agree with the short term approach here for the next year, but would definitely like to see us move to the letsencrypt option in the future.
rvagg		Hey, so digging in further to our GoGetSSL account I've found that we have a paid 24-month wildcard cert that was never issued! This was intended for `*.libuv.org` which is still cert-less and we should fix along with this current round of updates.  So, how's this for a plan:  * Use the 24-month wildcard we currently own for a renewal on `*.nodejs.org`  * Use new NF funds for 2 x 12-month wildcards for `*.iojs.org` and `*.libuv.org`  * Over the next 12 months migrate iojs and libuv over to a letsencrypt setup  * Give ourselves a window of up to 12 months from then to assess stability and operational costs of the letsencrypt setup before migrating nodejs.  /cc @saghul @cjihrig 
jbergstroem		I will have more time come mid-september and can do the letsencrypt implementation then. Will opt for DNS validation to simplify spreading the certificates.
rvagg		I've gone ahead and ordered the two additional certificates and requested certificates for the 3 wildcard domains.
rvagg		Certificates have been installed. nodejs.org (server and cloudflare so it's active for all subdomains like benchmarking.nodejs.org), iojs.org and dist.libuv.org which now has strict https which is a deviation because it was only http until now. Will talk to the libuv folks about bringing in libuv.org from gh-pages onto the server so we can https it too but that's not as urgent I think.  We have a year for *.iojs.org and *.libuv.org and 2 years for *.nodejs.org. So we need to start planning for a migration to new certificates.  I paid for the certs and have submitted an expense claim to the LF for NF funds for 2 x 70.95 USD.  Would appreciate having other folks testing my work and giving a :+1: that all is good.
refack		`nodejs.org` / `benchmarking.nodejs.org` (arbitrary `benchmarks.nodejs.org` redirects to main site üëç ) ![image](https://user-images.githubusercontent.com/96947/29316526-557668e8-8196-11e7-8cfc-61feffe1c88a.png) `iojs.org` ![image](https://user-images.githubusercontent.com/96947/29316659-d51b3902-8196-11e7-829d-c1814a1ad40b.png) `https://dist.libuv.org` ![image](https://user-images.githubusercontent.com/96947/29316725-069e2e44-8197-11e7-8ee1-77be1b03da4d.png)   
Spetnik		Just FYI: AWS offers free TLS certs (SNI-only) when using CloudFront or ELB to serve your application. Obviously if you're not already using AWS it opens up a whole bunch of other issues, but I figured I throw it out there.
Starefossen		Should we put nodejs/github-bot#66 on the agenda for this meeting? 
Trott		Maybe this too? https://github.com/nodejs/node/issues/8265 
jbergstroem		Added both. 
jbergstroem		@TheAlphaNerd suggested we added #453, so I did. 
gibfahn		@jbergstroem So is there a hangouts link? 
jbergstroem		Links to be published! Apologies for the slight delay 
rvagg		https://hangouts.google.com/hangouts/_/hoaevent/AP36tYfOUN-VgervByMOkRj9SpukzKndyyCF3SJK1KHSnXFxbYinSw for participation  http://www.youtube.com/watch?v=p6viQ2HyGqk for lurking 
mhdawson		PR for minutes here: https://github.com/nodejs/build/pull/485 
mhdawson		minutes landed, closing 
jbergstroem		Ping :) 
rvagg		I'd like us to do LTS + latest, which means inserting 16.10, aside from the fact that many users will be on 16.10 already it also gives us a stepping stone toward 17.04 and a headsup on anything that might be a surprise. 
jbergstroem		Consider it done. 
maclover7		ping -- should this stay open?
maclover7		Closing due to inactivity. Please reopen if this is still needed :)
targos		ping. I haven't tested either but I think it's important to test this (and update the machines) soon. It is blocking https://github.com/nodejs/node/pull/11752.
jbergstroem		Ah. Well,  seeing how at least one other build member agrees I'll log in to one of our testers and replace it. Will update this issue.
jbergstroem		Edit: all three builders now runs gcc 4.9. Looks like there were symlinks missing for ccache so i fixed that as well. Please let me know how testing goes and if we need to do something else.
targos		I started a build with V8 5.7 and it's fully green on Linux: https://ci.nodejs.org/job/node-test-commit-linux/8603/
targos		This seems to cause a problem with clang 3.4.1. https://ci.nodejs.org/job/node-test-commit-linux/8680/nodes=ubuntu1204-clang341-64/
bnoordhuis		@targos The ubuntu1204-clang341-64 buildbot was taken out of commission.  Does that mean this pull request can go through now?
targos		Yes, I think so.
targos		ping @nodejs/build 
gibfahn		I'm okay with this landing, but we're trying to move to the scripts in [`ansible/`](https://github.com/nodejs/build/tree/master/ansible), so this change probably needs to be made there as well.
targos		I can't find the place where Ubuntu 1204 is configured in `ansible/`
gibfahn		>I can't find the place where Ubuntu 1204 is configured in ansible/  It might not have been added yet, I haven't had a chance to check out the new scripts, that's more a question for @jbergstroem @joaocgreis @piccoloaiutante 
piccoloaiutante		@targos I think you could add a new section at the end of https://github.com/nodejs/build/blob/master/ansible/roles/baselayout/vars/main.yml like this:  ``` ubuntu12: [    'ccache',    'gcc-4.9',    'g++-4.9',    'git', ] ``` is it correct @jbergstroem ?
targos		@piccoloaiutante OK thank you. I added it. Let me know if something else is missing.
retrohacker		Proposal for docker-iojs to become its own working group: https://github.com/iojs/docker-iojs/issues/39 
mikeal		Just add it as a top level working group. There's no need to make it subordinate to another group, all the working groups are expected to collaborate with each other. The only difference, and this is actually covered already in the WORKING_GROUPS, is that if a new WG is being formed and is taking some responsibility from _another_ WG the charter has to get ratified by both the TC an the WG that previous had that responsibility (in this case build). 
retrohacker		:+1: 
rvagg		Any idea where this is coming from? If it's from slave machines then it's probably a no-go, I don't think we're going to want to set up SMTP on each of them, if this is from the Jenkins master then we could consider connecting it in with a mail delivery service to handle it for us. 
bnoordhuis		I'd like getting cc'd on V8 test results, by the way. 
jbergstroem		Perhaps set up aliases at `@iojs.org` until we get control of `@nodejs.org` for posting test results? It'd make it easier to add/remove people. As for outgoing; we could always look at something like mailgun or mailjet which both should work with ssmtp or similar. 
rvagg		I'm more concerned about delivery mechanism, it's not as easy as just setting up an smtp server these days with all of the hoops you have to jump through to make sure you're not automatically labelled as spam. 
jbergstroem		sstmp with spf/dkim through mailgun/mailjet should work? 
rvagg		yeah, if mailgun does smtp then that'd be fine, but again, doing this on the slaves is not so ideal; does anyone know where this email error is coming from? slave or master? 
jbergstroem		@rvagg I don't really know how propagation works either. 
mhdawson		@rvagg I think it should be configured at the jenkins server level.  Looking at the main configuration page https://ci.nodejs.org/configure  there is a section for E-mail Notification which is blank. 
mhdawson		@jbergstroem is there an existing email server I could try pointing jenkins to ?  
mhdawson		Had discussion with @jbergstroem this week we don't necessarily want to configure jenkins directly.  He is going to look at if we can get jenkins to call sendmail instead and is hoping to look at it next week 
jbergstroem		(By sendmail I mean `/usr/bin/sendmail`) 
mhdawson		@jbergstroem any progress on this or something I can help with ?  
mhdawson		Would like to make some progress on this, would really help me not have to go looking for failures every day.   
rvagg		taken briefly to email (ironically) to talk about servers and credentials 
mhdawson		I think I have what I need now to experiment just down to finding the time to do it. 
Trott		Ping! Is this something that might still get configured? If not, can we reconfigure the AIX hosts so that this doesn't happen on every test run?:  ``` Sending e-mails to: michael_dawson@ca.ibm.com ERROR: Could not connect to SMTP host: localhost, port: 25 javax.mail.MessagingException: Could not connect to SMTP host: localhost, port: 25;   nested exception is:     java.net.ConnectException: Connection refused     at com.sun.mail.smtp.SMTPTransport.openServer(SMTPTransport.java:1934)     at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:638)     at javax.mail.Service.connect(Service.java:295)     at javax.mail.Service.connect(Service.java:176)     at javax.mail.Service.connect(Service.java:125)     at javax.mail.Transport.send0(Transport.java:194)     at javax.mail.Transport.send(Transport.java:124)     at hudson.tasks.MailSender.run(MailSender.java:129)     at hudson.tasks.Mailer.perform(Mailer.java:170)     at hudson.tasks.BuildStepCompatibilityLayer.perform(BuildStepCompatibilityLayer.java:78)     at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)     at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:782)     at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:723)     at hudson.model.Build$BuildExecution.post2(Build.java:185)     at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:668)     at hudson.model.Run.execute(Run.java:1763)     at hudson.matrix.MatrixRun.run(MatrixRun.java:146)     at hudson.model.ResourceController.execute(ResourceController.java:98)     at hudson.model.Executor.run(Executor.java:410) Caused by: java.net.ConnectException: Connection refused     at java.net.PlainSocketImpl.socketConnect(Native Method)     at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)     at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)     at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)     at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)     at java.net.Socket.connect(Socket.java:589)     at com.sun.mail.util.SocketFetcher.createSocket(SocketFetcher.java:286)     at com.sun.mail.util.SocketFetcher.getSocket(SocketFetcher.java:231)     at com.sun.mail.smtp.SMTPTransport.openServer(SMTPTransport.java:1900)     ... 18 more Notifying upstream projects of job completion ``` 
mhdawson		As an FYI, finally going to try this out today.   
mhdawson		Ok, configured with server/userid/password provided by Rod, use SSL is enabled, otherwise left as default.  Done through Jenkins->configuration, E-mail Notificaiton section.  I validated by adding a job that just fails and adding a post-build action to the job:  ![email-post-build](https://cloud.githubusercontent.com/assets/9373002/20353630/7dd5f12e-abe9-11e6-9f39-42295864fd03.jpg) 
mhdawson		I'll open a separate issue to gather input on who would like to be added to email notifications 
annaZeesat		i want to enable notifications in jenkins but got error message again and again 
gibfahn		Just noticed Rod committed this to master already in https://github.com/nodejs/build/commit/589b5aada0b59cfea67a127b37d61dfbabb7093c and https://github.com/nodejs/build/commit/e9741a812ab4a4dd17099532b8e56b97ef001b38
rvagg		yeah, the root account isn't active on that machine, we could fix it up but for now let's just leave it as is cause we're migrating to MacStadium anyway. There's a user account "rvagg" that has admin privs, if need be I could probably stick the test key in there too. For full admin though you really need remote desktop, which you could do via SSH tunnel but you then need a password to log in and do anything useful. We're going to have to figure out an answer for this problem, it'll probably mean assigning the same password to an admin account for all of the test OSX machines. /cc @jbergstroem @mhdawson   @gibfahn while you're in here, could you add another entry in there, under `release`, same name, IP 192.168.2.211 please
gibfahn		>could you add another entry in there, under release, same name, IP 192.168.2.211 please  Done
rvagg		lgtm
jbergstroem		It might be worth mentioning that the create playbooks with fail now seeing how we don't have any support for macos builders. Hopefully this will not be the case any longer in the coming week or two, seeing how we now have a few machines up and running at macstadium we will fully ansible-fy (if possible).
jbergstroem		No objections. I can pull the plug if others agree. 
gibfahn		SGTM
mhdawson		+1 from me
MylesBorins		+1  This does raise a question for @nodejs/LTS. Should we continue to build / test for platforms that go eol during lts?  On Apr 21, 2017 12:21 PM, "Michael Dawson" <notifications@github.com> wrote:  > +1 from me > > ‚Äî > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/688#issuecomment-296236632>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecV_-9VJqrq68j3Iy9Cf_lpgzUrk2Pks5ryNdugaJpZM4NAzwr> > . > 
bnoordhuis		Good question.  CentOS / RHEL 5 is another example, it's officially EOL as of last month.  Windows XP we stopped supporting when MS did so it probably makes sense to do the same for other platforms.
jasnell		I agree. When a platform goes EOL, we should simply drop support and do any more releases for it past that point, even in LTS.  Considering that we shorten the lifetime of LTS for OpenSSL EOL, this would be consistent, I think. 
gibfahn		If you specify the supported platforms for a version, then dropping support within a release line is arguably `semver-major`. I think we could be explicit about this by simply adding a sentence to [`BUILDING.md`](https://github.com/nodejs/node/blob/master/BUILDING.md#supported-platforms-1) to say something like:  >The below platforms are only under support while the upstream Platform or OS is supported by its maintainers.  or  >Node.js is not supported on Platforms or OSs that are EoL.  _**EDIT:**_ See https://github.com/nodejs/node/pull/12672
gibfahn		From https://github.com/nodejs/build/issues/699  @shigeki  > ``` > The Ubuntu 12.04 LTS (Long Term Support) period will end on Friday, April 28, 2017. > ``` > https://insights.ubuntu.com/2017/03/14/introducing-ubuntu-12-04-esm-extended-security-maintenance/ >  > Although it has an extended commercial support by Canonical, I would be happy if it is retied because it has an old gas version of 2.22 that needs `deps/openssl/asm_obsolete/`.
ljharb		Supported platforms are absolutely part of the API; dropping one that works is absolutely semver-major.
gibfahn		>Supported platforms are absolutely part of the API; dropping one that works is absolutely semver-major.  Unless you specify otherwise right?
ljharb		@gibfahn that would be the debate; personally I believe that documentation doesn't invalidate what code does at runtime, and that what code does at runtime is the API.  If the documentation takes precedence over the code, then *adding* such specification is also semver-major.
bnoordhuis		> Supported platforms are absolutely part of the API; dropping one that works is absolutely semver-major.  Says who? First time I've heard anyone make this claim w.r.t. semver.
refack		> @gibfahn that would be the debate; personally I believe that documentation doesn't invalidate what code does at runtime, and that what code does at runtime is the API.  IMHO these cases should be judgment calls (for fear of breaking) Ref: https://github.com/nodejs/node/pull/12293#issuecomment-292803271
ljharb		@bnoordhuis to be more precise; it's the act of shipping a change in a non-major that doesn't work, on a platform where the code previously worked, that's semver-major - simply declaring that you drop support for a platform isn't the important part, but effectively it ends up being the same thing over time.  Semver fails to precisely specify a number of things; one of these is "backwards compatible" - i believe the common definition is "did i upgrade from a version that works to this one, and something broke? it's semver-major" - which doesn't special-case "and something broke on a supported platform?".
targos		There is an issue on Ubuntu 12.04 with V8 5.9 and 6.0: https://github.com/nodejs/node-v8/issues/2 I am +1 for retiring or disabling this buildbot before the 8.0.0 release.
refack		Cross-ref: https://github.com/nodejs/build/issues/730 There is only one `fedora22` worker, and it gets clogged (>6hr wait) a few times a week.
gibfahn		>There is only one fedora22 worker, and it gets clogged (>6hr wait) a few times a week.  There [are actually two machines](https://ci.nodejs.org/label/fedora22/), but I agree that the lack of redundancy is an issue.
bnoordhuis		So, anyone going to put it out to pasture?
mhdawson		How about we update the main regression job so that in master it skips fedora22, at least until we come to agreement here ?
gibfahn		>How about we update the main regression job so that in master it skips fedora22, at least until we come to agreement here ?  Done (I just removed it from the labels dropdown in [node-test-commit-linux](https://ci.nodejs.org/job/node-test-commit-linux/), I don't **think** that will break anything, if it does we can just add it back).
refack		‚ò∫Ô∏è  Could you take it out of the CITGM jobs as well.
mhdawson		@gibfahn I think that will remove it for all versions (4.x, 6.x etc) as well ? I was thinking we'd need to tweak the job to not run if the node version was 8 or higher and instead simply "pass".     That would not remove coverage for older streams while taking the pressure off since most runs are on master.
gibfahn		>I was thinking we'd need to tweak the job to not run if the node version was 8 or higher and instead simply "pass".  Seems reasonable, added this to the job instead:  ```bash # fedora22 is not supported in Node 8 and above if [[ "$nodes" = fedora22 && ${MAJOR_VERSION} -gt 7 ]]; then   RUN_TESTS=DONT_RUN fi ```  confirmed that `fedora22` is skipped on master but not on 6.x.  We can revisit what we're doing for older releases later.
mhdawson		Thanks that is what I originally had in mind.  
MylesBorins		I have a DMV appointment at 11am that morning. I'll do my best to be ready for noon. 
Starefossen		I would like to add #75 to the agenda. Linux Alpine is the preferred distribution to run really small Docker Containers and we should really make sure that Node.js versions works on this distribution. 
jbergstroem		I'd like to add https://github.com/nodejs/build/pull/294 for no other sake than landing it sooner than later. 
orangemocha		Unfortunately I don't think I will be able to attend tonight. Can someone else facilitate? https://support.google.com/plus/answer/4386744?hl=en 
rvagg		I did say in IRC that I could facilitate but at this stage I don't think I'll be able to attend. It's very early for me and I have a bit of a sleep debt that I'm apparently not paying off tonight. @jbergstroem I think that means its your call re the meeting going ahead. Very sorry.  
mhdawson		I'll be there and can facilitate unless @jbergstroem would perfer to do it. 
jbergstroem		I'm happy to lead but haven't done the google hangout setup bits. To avoid me messing that up it might be better if @mhdawson runs with it? 
MylesBorins		Do we have a hangouts link yet? 
Starefossen		I took the liberty to create a Hangouts On Air for the meeting https://plus.google.com/hangouts/_/hoaevent/AP36tYcPRcck4vdjQ-xYMbl5n7Hig1UkTZZ4UGt2NypYngRjtwzrFA?hl=en-GB&authuser=0 
jbergstroem		google doc for notes: https://docs.google.com/document/d/1MuN5J4JD7RvnxT3GvzzrY_U377rWWT5xyZXF0DtjSDs/edit?usp=sharing 
mhdawson		Landed as https://github.com/nodejs/build/commit/7d9e41b31b94a3190e836c40e7c268daed7f1cde 
refack		See https://github.com/nodejs/build/issues/951 So if you have a chance to kill the zombie, else could take it off line
gibfahn		Adding `wg-agenda` to check that this is okay in the meeting.
gibfahn		Couple of points brought up in the meeting.   - Is it an easy process?    - As I noted above, it should be a two minute change.  - Will there be any loss of data from moving it?    - There shouldn't be, and in any case if we lose the cache that just means everyone has to click the button again, which isn't the end of the world.  - Does the app know any sensitive information that we wouldn't want to be accessible to `nodejs` owners?    - I think it just gets read access to email addresses, and org/team membership, so that shouldn't be too worrying.  Overall no-one had any particular concerns, so @rvagg does this make sense?
rvagg		Fine by me, it was done under my name because nodejs is an org, not a user, but if this is possible then :+1:
rvagg		![screenshot 2017-04-19 12 37 09](https://cloud.githubusercontent.com/assets/495647/25161018/eef3770a-24fc-11e7-9f05-cfccc18b9c36.png)  OK, so it's possible to transfer to an org, the next question is permission. I don't know who gets access to the secret on this in the org, and we have a _lot_ of org members. Instead of trying to find a doc (I can't so far), how about we test. I've made a new OAuth app for the nodejs org, can you all report whether you can see this or not? https://github.com/organizations/nodejs/settings/applications/517086
rvagg		There's a second app btw, the Node.js Release Jenkins one
gdams		@rvagg I believe that it's only the owner of the org who has full permissions
gibfahn		>can you all report whether you can see this or not?   I can't see it
rvagg		OK, well that's no good, since @gibfahn isn't even an org owner, we don't really want that key to be leaked
gibfahn		>OK, well that's no good, since @gibfahn isn't even an org owner, we don't really want that key to be leaked  To clarify, I am **not** able to see it, which suggests to me that it's only org owners who can.  ![image](https://cloud.githubusercontent.com/assets/15943089/25267545/1af7ff4e-266d-11e7-932c-ddf2a252fd7c.png) 
mhdawson		I **can** see it.
gibfahn		ping @rvagg , given that I think we've established that only org owners can see it, maybe time to make the move?  cc/ @Trott 
Trott		Came up again today onboarding @BridgeAR. Not a big deal, but don't want this to be forgotten about. kthxbai
rvagg		Well that was much easier than I thought, nothing needed to change on the Jenkins end, just needed to do the transfer on the GitHub side and didn't need to touch Jenkins. So all done!
phillipj		I'll take a look asap. Any idea why author needs be set when the bot doesn't try to commit anything?  On Thu, 8 Dec 2016 at 20:44, Jeremiah Senkpiel <notifications@github.com> wrote:  > See nodejs/github-bot#100 > <https://github.com/nodejs/github-bot/issues/100> > > > I wasn't able to get it to work by setting it globally, but setting it in > the cloned node repo worked. > > > git config user.name "nodejs-github-bot" > > git config user.email "github-bot@nodejs.org" > > > > > > ‚Äî > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/561>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/ABLLE1-O7nDFsKNgzRizvs-MNWQYWIDYks5rGF4RgaJpZM4LINRj> > . > > > > > > > > > > > > > > > > > > > 
Fishrock123		@phillipj Technically it does "commit" stuff -- using `git am` applies with the user as the commiter.
phillipj		Ah right! I'll try to get it done tonight. Setting that config locally to that repo rather than global seems fine to you @Fishrock123?  On Fri, 9 Dec 2016 at 15:52, Jeremiah Senkpiel <notifications@github.com> wrote:  > @phillipj <https://github.com/phillipj> Technically it does "commit" > stuff -- using git am applies with the user as the commiter. > > > > > ‚Äî > You are receiving this because you were mentioned. > > > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/561#issuecomment-266031883>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ABLLE0xWPM4GcUO-crOpfCk_e466mDR9ks5rGWs4gaJpZM4LINRj> > . > 
Fishrock123		@phillipj Either or should be fine. On-repo might be simpler since I could not get globally to work on that machine.
phillipj		Opened a PR fixing this permanently. It seems to have been set correctly on the server tho, I assume someone has logged in and fixed it by hand.
Fishrock123		@phillipj (That was me... @rvagg gave me temporary access to sort these things out)
phillipj		Fixed in https://github.com/nodejs/build/commit/857f5900f3cde69080f4f64b2c725bca2d24728f
gibfahn		I've been thinking that this would be a good idea for a long time (and it just came up in https://github.com/nodejs/build/pull/804#issuecomment-323406841). It's a fairly massive change though, as pipelines are pretty different from the existing build trigger plugins and methods.  >where should the pipeline files live? should they be in the project itself (travis-style), or should they live in nodejs/build in a jenkins directory?  I think I agree with @joaocgreis 's point in https://github.com/nodejs/build/pull/804#issuecomment-323234788, we want to have one master copy of the build scripts, not one for each version, so it makes more sense to have them in a separate repository. I think a `jenkins/` directory in this repo might make sense.  >since job configuration is being checked into source control, is there a better way to manage admin groups/users, with the additional metadata?  I think the current system of using Github teams to manage access works quite well, but if Jenkins has some alternative method of managing this we should definitely consider it.  >migrating the jobs with no downtime  I think the first thing we need to look at is whether this is worth doing, and how much work it's going to entail. [Node CI](https://ci.nodejs.org/view/All/) includes non-core CI tests (like CitGM, node-report, and libuv). Those would be a good place to start, if we convert some of those jobs, then that would give us a good indication of how difficult/worthwhile it's going to be.  Another issue is that no-one in the build WG is a Pipeline expert. Many of us are familiar with the existing (Jenkins 1) configuration methods, but pipelines are a new way of doing things.
maclover7		> I think a jenkins/ directory in this repo might make sense.  I agree -- I think having them in nodejs/build alongside other relevant configuration (ansible, etc) would be helpful  > Node CI includes non-core CI tests (like CitGM, node-report, and libuv). Those would be a good place to start, if we convert some of those jobs, then that would give us a good indication of how difficult/worthwhile it's going to be.  Would definitely be worthwhile to do some beta testing with some of the smaller jobs before trying to take on larger ones. I created [a small CLI utility](https://github.com/maclover7/jenkins-analyzer) to pull some metadata about the cluster, and it looks like there are a bunch of freestyle jobs that might be good candidates to experiment with. Without knowing too much or having access to the CI admin portal, it looks like the benchmark tasks might be on the easier side to port.  > Another issue is that no-one in the build WG is a Pipeline expert. Many of us are familiar with the existing (Jenkins 1) configuration methods, but pipelines are a new way of doing things.  I've done some work with pipelines/Jenkinsfiles in the past, but definitely not going to say I'm an expert üòÖ 
joaocgreis		This would be great, if pipelines can replace all other types of jobs.  Having no downtime would be nice, but we could do with some downtime during the weekend or even during the week at nighttime.  Also note that much of what we do is reacting to issues, and we need to be able to change the jobs quickly without going through a review process. It would be good to have a review process after the fact: it's good for notifying other WG members of what changed and gathering suggestions for improvement. But we need a way of reacting fast to issues.
gibfahn		>and we need to be able to change the jobs quickly without going through a review process  @joaocgreis agreed, I assume we'd have a rule that fixes for broken jobs can be landed without review (although if someone is around to take a quick look that's always good). As you say, having the PR is helpful even if it lands straightaway.  I'll put this on the agenda as well, we're meeting in a couple of days, and it'd be good to get everyone's thoughts on this.
rvagg		I'm torn on this. On one hand I'm strongly in favour of getting as much config out of Jenkins as possible and the new github-hosted pipelines are really nice for this (and Groovy can be quite expressive if done well .. although it can be equally terrible). On the other hand, I've not had fantastic experience with the new Jenkins pipeline build processes, it feels like poorly implemented hack on a framework that was never designed to work that way. It could very well be that I've just not seen a well implemented Jenkins pipeline build project that's elegant and demonstrates the full potential of the feature. Have any of you? Perhaps this the way forward here is to simply implement some minor part of our CI with it and see how it goes side-by-side with our existing setup.
mhdawson		I'm in favor of trying out, possibly on the jobs for one of the Work Groups.
refack		I'll try to do https://github.com/nodejs/build/issues/844 as a POC
gibfahn		@maclover7 FYI, we discussed this in the meeting (minutes in https://github.com/nodejs/build/pull/845, recording [here](https://www.youtube.com/watch?v=WF7oa1heAko)). Agreement was basically what was already said, people are positive in theory, but unsure how well things will work in practice. So we should try to do it gradually, one job at a time would be good.  It's a bit difficult to work out how you could contribute (that's one of the things that will be much easier if we do this).  @refack maybe you could raise a PR adding a pipeline to `/jenkins/` in this repo, and run the CI job from that PR, that way people could review and suggest (and see it working). Ideally as @mhdawson said, we'd have a generic pipeline that did "pull down node, run a custom extra setup script if necessary, git clone project, npm install, npm test", that could work for other CI jobs as well.
maclover7		@gibfahn watched the meeting, good discussion about pros/cons. Just wanted to clarify that pipelines are implemented as a Jenkins plugin, so should be unrelated to Java version on build machines. Also, agree about strategy as discussed earlier about converting over jobs slowly, to make sure the end result is helpful.  > we'd have a generic pipeline that did "pull down node, run a custom extra setup script if necessary, git clone project, npm install, npm test", that could work for other CI jobs as well.  While I think this would be great, most CI jobs don't run in this cookie-cutter fashion, and I'm not sure if this is practical. I think it makes sense to convert jobs over by "batch" (good example of this are the benchmark jobs)  I'm not sure how Jenkins permissions are assigned to contributors, but I'd be willing to take a look at pipeline-izing the various benchmark jobs, since those seem fairly straightforward.
rvagg		@maclover7 good to hear re plugin, that makes this safer to tackle I think. Currently we only assign admin permissions to a smallish group under the `@nodejs/jenkins-admins` team and we probably shouldn't go much wider because there are broad security implications (simply in access and ability to mess around but also in providing too much access to details of security patches that are under test). However, we should be able to do it per-job so my guess is that we could make a job (or two, or three) and assign you and @refack as admins of it so you can tinker with it all you need to get it to a right state. I don't know if our per-job permissions matrix will show up for pipeline jobs but we could give it a try. @refack: want to propose a job name for #844 and we can try and set that up in Jenkins as a Pipeline, pointing to a new test repo for now perhaps?
joaocgreis		I learned a bit about pipelines and started experimenting, but found one big issue. There is no good way to visualize jobs that use a matrix (like current multi-configuration jobs, such as [`node-test-binary-windows`](https://ci.nodejs.org/job/node-test-binary-windows/)). Currently, we have a table of configurations displaying green/red icons, and the console outputs can be displayed individually. However, with a pipeline job, everything would be mixed in the console and there would be no table in the job run page. This is an issue tracked by Jenkins: https://issues.jenkins-ci.org/browse/JENKINS-33185  Current pipeline jobs (like [`citgm-smoker-pipeline`](https://ci.nodejs.org/view/All/job/citgm-smoker-pipeline)) already have this issue. Clicking on any run will not show the jobs started by it, we need to see the console output to find the links. Since this only starts other jobs, there is no output mixed in the console, but this would be a problem otherwise.  If not for this issue, I believe pipelines would be a major improvement for us. If that issue is solved in Jenkins or if there's some solution that I did not find, pipelines would certainly improve our workflow.
gibfahn		@joaocgreis that page says that Blue Ocean (apparently a UI rewrite and a Jenkins plugin) has some nicer UI for looking at multi-config jobs. It might be useful to look at that.
mklebrasseur		Hey everyone, I'd love to assist here if at all possible.   We recently converted all legacy builds and DSL scripts to pipelines using shared libraries (also using blueocean). We use source control and branching to control our releases from shared libraries. We ran these in parallel with (no downtime, no risk config).  We used this model to control granular access to certain modules to where access was needed. All secrets are encrypted and refereced as environment variables. Allowing us to use a single pipeline for multiple projects. As well as allowing for a more standard infrastructure as code feeling.  I have only needed the UI to check failed logs, we use another plugin GitHub source branch that will scan an entire organization for Jenkinsfiles so that you never need to add another job to Jenkins.   Can go in to greater detail but it cut a lot of our repetitive code more than 50%.  We now only use pipelines for environments. It takes far less effort to maintain so many pipelines and resources etc. 
gibfahn		Okay, so how should we do this?  I guess options are:  1. Run doc linting as part of the regular linter job, and then just run the linter only on doc PRs. 2. Have a new job to run the doc linting, and then have an option in `node-test-pull-request` to only run linter + doc linter (off by default).  How do you run the doc linting btw? I should really know, but I'm not sure. Is it a `make` target?
joyeecheung		@gibfahn A make target indeded.See https://github.com/nodejs/node/pull/16377 (need to fix the shell quirks first)
joyeecheung		It's already up and running in https://ci.nodejs.org/job/node-test-pull-request-lite/. This can be closed now.
joaocgreis		cc @refack @kunalspathak FYI
refack		Refs: https://github.com/nodejs/node/pull/13911 (I forgot about that)
joaocgreis		Done.  Also moved x86 testing to Windows 2012, because this leaves us with few Windows 2008 with each compiler.
mikeal		I think this is just a Coudflare config thing. /cc @jbergstroem @rvagg   We really need to stop getting orders to the old site.
jbergstroem		Just checked cloudflare: <img width="684" alt="screen shot 2017-05-18 at 5 28 12 pm" src="https://cloud.githubusercontent.com/assets/176984/26224226/6546e370-3bef-11e7-8bad-ac6c0c0b4691.png">  ..am I missing something? Are we all good? 
maclover7		``` Jons-MacBook-Pro:~ jon$ dig store.nodejs.org  ; <<>> DiG 9.8.3-P1 <<>> store.nodejs.org ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11492 ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0  ;; QUESTION SECTION: ;store.nodejs.org.		IN	A  ;; ANSWER SECTION: store.nodejs.org.	299	IN	CNAME	node-js-community-store.myshopify.com. node-js-community-store.myshopify.com. 3599 IN CNAME shops.myshopify.com. shops.myshopify.com.	1799	IN	A	23.227.38.64  ;; Query time: 76 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Fri Nov  3 21:30:03 2017 ;; MSG SIZE  rcvd: 121 ```  Looks good to me -- going to close this out as completed.
Trott		@gibfahn https://github.com/nodejs/build/issues/719#issuecomment-301282466
jbergstroem		Wipe it!
maclover7		Machine looks to be back up and has successfully completed builds today -- can this be closed?
rmg		+1 on using a bot if the goal is to replace Jenkins rather than get attached to it.  What's the plan for the GH status updates? Jenkins doesn't seem to expose enough parameters to get multiple statuses to work in a useful way, and all the github modules on npmjs.org are missing useful things like using numeric repo ids (who renames or moves repos these days anyway, right?), so I've found Ruby to generally be the most productive way to use GitHub's API. 
rvagg		@rmg I have a collection of packages in npm already for doing stuff with github, I'm not so concerned with that part, it's Jenkins that's the headache because it's API is kind of sloppy. Already I have a webhook to a test server collecting data with [github-webhook](https://github.com/rvagg/github-webhook) and I've been playing with some of the Jenkins libraries in npm but they all inherit Jenkin's crappy API problems but since it's fairly simple JSON it might be more productive to roll our own focused on just what we need.  The flow looks will look like this, and this is purely for iojs/io.js at the moment: 1. Node.js webhook server receiving pull request events and deciding how to handle them ("containers" for everyone, "multi" for trusted group, or perhaps no action because ...?) 2. Webhook server tells GitHub that the status of the PR is `"pending"` 3. Webhook server tells Jenkins to start a build for the appropriate job. The tricky bit here is capturing the build ID and attaching that to the PR ID, some awkward polling of Jenkins will have to play in to this. 4. Webhook server polls Jenkins for the job/build until there is a success or failure 5. Webhook server posts a status update to GitHub of either `"success"` or `"failure"` and links to the Jenkins build.  Additional step in there is to post a comment to the PR with a link that the TC can use (authenticated against GitHub) to trigger a "multi" build when only a "containers" build has been done. For joyent/node there is a Chrome plugin that gives them the ability to do this I believe but I'd rather not have to go that route.  Currently I have these libs for working with GitHub: - https://github.com/rvagg/ghauth - https://github.com/rvagg/ghteams - https://github.com/rvagg/ghissues - https://github.com/rvagg/ghusers  And I'm always looking for an excuse to expand the functionality and add more libs! 
rvagg		btw, the webhook server is very early at the moment but I'll open a repo here with the code as soon as I can so others can contribute! 
rmg		The github/janky docs mention a notifications plugin for Jenkins. I imagine it could post to the webhook server. Or worst case, add a curl one liner to the Jenkins jobs themselves to post their $BUILD_NUMBER to a PR specific URL given as a build parameter by the webhook server when it triggers the job.  
rvagg		Check this out: https://jenkins-node-forward.nodesource.com/job/iojs+any-pr+containers/3/api/json Given a job id `"iojs+any-pr+containers"` and a build number `3` I can get that info, which thankfully contains the parameters I submitted when I triggered the build:  ``` json   "parameters": [     {       "name": "user",       "value": "rvagg"     },     {       "name": "project",       "value": "io.js"     },     {       "name": "branch",       "value": "v0.12"     }   ] ```  Additionally I could add a "pr" parameter that's just informational for this purpose so it's even easier to cross reference. The frustrating bit is that when you ask to create a build you don't get an ID straight back (as far as I can tell so far), without that piece of the puzzle we need to do some polling and cataloging of what Jenkins is doing so we can link everything together. 
rmg		What I meant is create some id (UUID? git sha1?) before the build is submitted and include it as a build parameter. Then have Jenkins call you back, using that provided id (possibly something as lame as `curl -X POST -d bid=$REQID http://iojs-webhook-server/` as the first build step), to tell you what build number Jenkins has assigned to that request. 
rvagg		@rmg yes, except the list of hosts is quite diverse, the UUID would be public via the Jenkins API and web site and therefore filtering post-backs from build slaves would be a major security hassle, I think I'd rather poll than have to deal with this! 
rmg		@rvagg understood. I hadn't considered which parts would be public and which parts would not. 
JohnMorales		@rvagg I'm interested in contributing to this, when do you think you'll be ready for contributors/help? I tried the gitter room, but it seems to be dead/in-accessible. 
ghostbar		For PR and builds on Jenkins [this plugin](https://wiki.jenkins-ci.org/display/JENKINS/GitHub+pull+request+builder+plugin) should do the trick AFAIK. 
rvagg		I'm aiming on not investing too much in Jenkins and opt for building our own tooling. Within the next week or so I should be able to make a new repo here with the beginnings of a tool that does what we need and we can all contribute to awesomeifying it. 
ghostbar		Awesome, because I would hate to install java on my machine only for testing jenkins stuff out. :D 
retrohacker		So I'm thinking we might want to do docker webhooks as well, so we keep the docker containers on the Jenkins server in sync with the docker registry (and in turn with the [build-containers](https://github.com/iojs/build-containers) repo).  Thoughts? If worth while, I can take a stab at this. 
rvagg		@wblankenship intriguing, how would this work? Can we just ping the local webhook to make sure it has the latest image from hub?  
retrohacker		Simply point [https://registry.hub.docker.com/u/iojs/build/settings/webhooks/](https://registry.hub.docker.com/u/iojs/build/settings/webhooks/) to the server and trigger a `docker pull iojs/build` when the endpoint is hit. (note: url only works if you have admin on iojs/build) 
rvagg		neat, we'll need to do this from >1 server 
retrohacker		Certainly doable. Want me to open a ticket for this and self assign? 
rvagg		yes pls 
bnoordhuis		@rvagg Are there still actionable items left? 
rvagg		yeah, plenty of actionable items that are not properly documented elsewhere, should probably leave this open for now 
jbergstroem		Almost a year! I think we've covered most things on this list, with a gap in containers/auto-testing. At the same time I think our strategy in that department has changed. Inclined to close. 
rvagg		moving to #987
bnoordhuis		Yes and no.  The source tree in deps/v8 contains the test runner but there are build dependencies that must be checked out with `gclient sync` from depot_tools (making depot_tools a dependency as well.) 
rvagg		a mention of `depot_tools` makes my eyes glaze over .. sounds like it's going to be complicated and very slow 
bnoordhuis		Complicated maybe, but it doesn't have to be slow.  `gclient sync` is basically `git fetch` plus some post-checkout actions.  To keep checkout times down, keep a pristine source tree around, sync it, then `cp -a` it somewhere and build and test that. 
jbergstroem		I don't think we can use v8 from `deps/v8`; `gclient` expects a proper git tree. Running this on all slaves sounds like a bit of a stretch, but even getting one vm testing (debian 8, freebsd or smartos vm's has most cpu to spare) would be pretty great.  Edit: I guess we'd have to check out the tag and then reapply our floating patches to properly test it. 
mhdawson		+1 on the idea for me.  I think @jasnell had done some initial work looking at doing this. 
jasnell		There's a bit of work involved but I had this working in joyent/node master. I can update and get it running again. It'll just take a couple weeks given current workload 
pmq20		If we were to land fixes about v8 this definitely is a must. 
rvagg		right, particularly as we move into LTS-land where we're supporting old V8 
joransiu		Pinged @jasnell yesterday, and he pointed us to his old PR https://github.com/nodejs/node-v0.x-archive/pull/14185 on this topic.  We're looking into reviving and flushing out some of his changes there.   /cc @exinfinitum  
exinfinitum		@jasnell I found a way to run v8 tests in node. This was originally developed for zLinux version of node, to run tests on v8z (a port of v8 to IBM s390(x) CPUs), but should also work for v8. Checkout https://github.com/exinfinitum/node/tree/al-merge (branch "al-merge") and run the following commands:  make v8 DESTCPU=(ARCH) make test-v8 DESTCPU=(ARCH)  where (ARCH) is your CPU architecture, e.g. x64, ia32. DESTCPU MUST be specified for this to work properly.  Can also do tests on debug build by using "make test-v8 DESTCPU=(ARCH) BUILDTYPE=Debug". 
mhdawson		If its not too much work could you create a PR that applies to master so we could try it out there.  I think it would also trim down what needs to be reviewed to the minimal set.  James is on holiday this week so probably worth doing this instead of waiting to hear from him. 
mhdawson		PR here https://github.com/nodejs/node/pull/4704 
jbergstroem		Do we have other constraints? For instance -- do we need more than 2G memory? Is the idea to run this on all slaves or a subset? 
mhdawson		At least for now the patch does not support windows but other than that I think we do want to run across all platforms as there is platform specific code and so we might have floating patches that are platform specific  I'm not sure if it needs more than 2G, I know that all the machines we currently run on have more than that.  I'm away next week but can figure that out when I'm back. 
jbergstroem		Actually, most machines we deploy today run on 2G. We have alternate deployment for specific needs though which could also cater v8. I just need a better understanding on what's required. Also, how long would a run take? 
mhdawson		The actual run of the tests on an x64 box took 4 1/2 minutes on my x64 machine, the build was more in the 10-15 minute timeframe but that's without cc cache enabled so I'd like to see how long it takes on one of the CI machines.  If I could install svn on one of the x64 test machines I can get more specific numbers.   
jbergstroem		Sounds good to me. Test away. 
mhdawson		Have it working on x86 on a machine with 2 cpus/4G.  Took 33mins.  I think we could make this shorter if we ran with more cpus.  https://ci.nodejs.org/job/mdawson-RunV8TestsInNode/ 
mhdawson		Run on machine with 2G here https://ci.nodejs.org/job/node-test-commit-v8-linux-mdawson/3/nodes=test-softlayer-ubuntu14-x64-1/console 
jbergstroem		Thinking the crash is a memory issue:  ``` === mjsunit/regress/regress-crbug-514081 === Command: /home/iojs/build/workspace/node-test-commit-v8-linux-mdawson/nodes/test-softlayer-ubuntu14-x64-1/deps/v8/out/x64.release/d8 --test --random-seed=910294543 --stress-opt --always-opt --nohard-abort --nodead-code-elimination --nofold-constants /home/iojs/build/workspace/node-test-commit-v8-linux-mdawson/nodes/test-softlayer-ubuntu14-x64-1/deps/v8/test/mjsunit/mjsunit.js /home/iojs/build/workspace/node-test-commit-v8-linux-mdawson/nodes/test-softlayer-ubuntu14-x64-1/deps/v8/test/mjsunit/regress/regress-crbug-514081.js exit code: -9 --- CRASHED --- ``` 
mhdawson		It crashed with 4G as well but I did have the same thought.   Will do a run on a machine with 8G to see.  Separately machines with svn now test-softlayer-ubuntu14-x64-1 test-digitalocean-centos7-x64-1 iojs-ibm-ppcle-ubuntu1404-64-2 
mhdawson		PR to start the additional of svn https://github.com/nodejs/build/pull/330 
mhdawson		add svn to  iojs-softlayer-benchmark as it has 8G memory and is ubuntu 14 
mhdawson		Build here to compare across 2, 4 and 8 G https://ci.nodejs.org/job/node-test-commit-v8-linux-mdawson/ 
mhdawson		Build above shows that the crash failure does not occur on machines with 8G but that there is a new failure on the 2 ubuntu machines with 8G   https://github.com/nodejs/node/issues/5263 
mhdawson		Added svn to test-digitalocean-freebsd10-x64-1 
mhdawson		Added svn to test-rackspace-debian8-x64-1 
mhdawson		Added svn to test-joyent-smartos14-x64-1 
mhdawson		Ok, back to looking a this.  The last issue I was looking at was a failure on arm.  Now though,  after a few runs the runs under the CI to check current state the runs across the board,  on arm are not even getting through the compilation step.    The runs take a long time (2.5 hours+) the machines go offline before the compile actually finishes.  This was also occurring when I was trying to debug locally.    At this point I think the right answer is not to block setting up/running for arm and create a new issue to cover future work to add support for windows, mac, arm, smartos that interested people can pick up.  In some cases like mac and arm adding them may just require enough hw while for windows work will be required on the build files.  Current coverage is ubuntu 14 for ppcbe, ppcle, and x64 and I'll see if our smartos machines are big enough to run/pass the tests in a reasonable time.  I'll then enable the nightly job and open a new issue as a placeholder to remind us we may want to add the other platforms at some point when h/w time permits 
mhdawson		Tried smartos but the machines seems to choke with a bunch of :"virtual memory exhausted: Resource temporarily unavailable" messages.  I'm also thinking that since smartos is x86 we already have it covered by the unbuntu x86 case so will leave it out for now.  
bnoordhuis		Good idea, +1, but I think you mean @santigimeno?
Trott		+1 üíØ üëç 
jbergstroem		Sweet. I'll set it up.
santigimeno		I'm done for the moment. Thanks!
jbergstroem		> @santigimeno said: > I'm done for the moment. Thanks!  Revoked access.   The issue here is that the LX layer provided by joyent is behaving differently from other linux kernels. See https://github.com/libuv/libuv/issues/1206 for more info.
gibfahn		I think this should be handled by https://github.com/nodejs/build/issues/688, reopen if I'm wrong!
phillipj		Refs https://github.com/nodejs/github-bot/issues/88#issuecomment-262628736
jbergstroem		Wouldn't stdout be collected by systemd (when deployed on debian8, ubuntu, etc) which is already managed by journalctl?
phillipj		I'm all ears to better / simpler ways. I'd say it's important to be able to expose this log over HTTP like we currently have. Using bunyan to filter and trace logs as been really valuable so far.  On Wed, 23 Nov 2016 at 23:23, Johan Bergstr√∂m notifications@github.com wrote:  > Wouldn't stdout be collected by systemd (when deployed on debian8, ubuntu, > etc) which is already managed by logctl? >  > ‚Äî > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub > https://github.com/nodejs/build/pull/553#issuecomment-262644565, or mute > the thread > https://github.com/notifications/unsubscribe-auth/ABLLEyPU_hwJvKd1MBaCNCAX5D3iPy2qks5rBLzGgaJpZM4K7Es5 > . 
jbergstroem		I'm personally not a "log over http" guy (grep, awk, tail, ..) but if has been a big part of the teams debugging lets opt for that then.  If you're going that route, look at systemd's `StandardOutput=` directive and pass that to file. Another alternative could be using console+syslog and use the system logger (with an identifier) to control output. That way, the system logger could probably handle rotating as well.
jbergstroem		By the way, does stdout follow the same format bunyan expects?
phillipj		> By the way, does stdout follow the same format bunyan expects?  Yepp. Everything we log is through bunyan, which writes both to stdout and a log file ATM. When running locally via `npm start` the stdout output gets prettified for easier reading tho. That's not the case on the server, as the systemd service executes the server script directly.
phillipj		On second thought, doing this seemed more complex than simply writing logs outside the directory where the github-bot repo is cloned into.  Refs https://github.com/nodejs/build/pull/602 
phillipj		Closing this in favor of #602
rvagg		nice! lgtm 
geek		LGTM, I would merge if I had access :/ 
jbergstroem		Merged in edac962b32d28dd65ee74119bce2cf88818a1b5f. Thanks for reviewing, Wyatt and Rod. 
gibfahn		Easiest way to add this would probably be a choice parameter for `VS_VERSION`, with the same choices as the `test-binary` job (`vs2013 vs2015 vcbt2015 vs2015-x86`). It'd only be used if a windows build was chosen from the dropdown.  Happy to add this if there are no objections (cc/ @joaocgreis @Trott )
Trott		> Happy to add this if there are no objections (cc/ @joaocgreis @Trott )  I'm a frequent user of the stress tests but not knowledgable about the configuration etc. @joaocgreis is the authority AFAIK.
gibfahn		>I'm a frequent user of the stress tests but not knowledgable about the configuration etc.   Yeah, I was looking for user feedback from you and Jenkins feedback from Jo√£o.
joaocgreis		All our Windows 2008 machines have VS2013 installed (and should be only VS2013 - having only one version is good armor against errors in config scripts). We need to have VS2013 in the Windows 2008 machines to cover building addons tests with VS2013 as part of `node-test-binary-windows`.  But since we dropped VS2013 to build node, the stress-tests job is useless for Windows 2008. I believe the best way forward is to have a fanned stress-tests job (like we already have for pi1) that mirrors the configuration of the test-commit job. I actually have most of it already in place, I'll clean it up and let you know.   
maclover7		ping, does this need to stay open?
joaocgreis		Yes, to be able to use the stress-tests job with recent node versions (that compile only with vs2017) and any windows version, we'd need cross compiling on that job. Unfortunately this was not as straightforward as I initially hoped. Actually, this job is a good candidate for pipeline conversion.
jbergstroem		Any feedback? The reason java path is different between the linter machines and our builders is because packages available on the linter (through do) seems slightly different. 
jbergstroem		FWIW, this is already in production to improve auto-restarts. Unless anyone have objections I'll merge this to keep in sync with production in a few days. 
jbergstroem		Generally LGTM; feel free to fix nitpick if you care. 
mhdawson		LGTM, thanks for doing this 
Starefossen		@jbergstroem thanks for going over and finding improvements üëç I have pushed the updated meeting minutes now. 
mhdawson		@Starefossen you going to land ?   
Starefossen		@mhdawson AFK for a few hours, feel free to land this in the mean time :smile:  
jbergstroem		LGTM. I can land. 
jbergstroem		Merged in c59f87e522401886d817b61ad47fec4ff0370d83! 
mhdawson		I think you did the initial ccache build/install and that's were they went, I'll have to see what is most common for AIX. 
mhdawson		Talked to your AIX expert, sounds like the right place would be /opt/freeware/bin will move to there 
mhdawson		Fixed up ccache issues both in doc and updated ansible resource to use new ccache location. 
mhdawson		Agree with both general points that we should work to automate as much as possible with ansible, it is on the list just have not had time yet.  Also agree with updating to md, but I think doing that along with the ansible work makes sense. 
mhdawson		ok pushed fix for 755 -> 0755 I'll plan to land tomorrow unless I hear any objections. 
mhdawson		Landed as https://github.com/nodejs/build/commit/650fd384e17a1de970856b4bd9e69e060c4e1a74 
gibfahn		ping @mhdawson , we should talk to @edelsohn about how to get something auto-running on boot.
edelsohn		https://www.ibm.com/developerworks/aix/library/au-aix-autostartup-shutdown-scripts/index.html http://www-01.ibm.com/support/docview.wss?uid=isg3T1000661
refack		Ohh the master node is offline: ![image](https://user-images.githubusercontent.com/96947/27198627-fdc4edca-51e0-11e7-9b9a-0774954ea9d8.png) 
gibfahn		I can bring it back online, but we fundamentally need to clean up that disk (or allocate more space).
gibfahn		Oh wait, I can't bring it back online:  ![image](https://user-images.githubusercontent.com/15943089/27198696-34e774c4-520b-11e7-899c-0ce55f915e5c.png) 
refack		sad emoji
gibfahn		@refack Have you tried the node-build IRC channel? You need someone with the `nodejs_build_infra` ssh key (who can actually get into the machines).
refack		> @refack Have you tried the node-build IRC channel? You need someone with the nodejs_build_infra ssh key (who can actually get into the machines).  Yep, and #node-dev (posted here last since I thought maybe someone will get an e-mail)
rvagg		on it
rvagg		working again ... but we're using up _so much disk_, I've even gone back from 7 days to 5 days retention and we're still at 95% disk. We're going to have to switch to block storage for this
gibfahn		> I've even gone back from 7 days to 5 days retention and we're still at 95% disk.  Do we know what's using this up? Is it just loads of copies of Node repos? Jenkins artifacts? Something else?
refack		Maybe it's time to revisit https://github.com/nodejs/build/issues/739, gate the big jobs (or maybe just PR) on first passing linux one, it's also gives a very quick feedback.
joaocgreis		Found a huge job log file, cctest produced 40Gb of output (stack traces in the beginning). We are back at 50+ Gb free, which is about normal.
rvagg		we should move to block storage on DO for this now that we _can_
refack		Was it https://ci.nodejs.org/job/node-test-binary-windows/9117/RUN_SUBSET=0,VS_VERSION=vs2015,label=win2012r2/ I tried to ping ya'll https://github.com/nodejs/node/pull/13490#issuecomment-307460105
joaocgreis		@refack it was node-test-binary-windows 9196 (started to test https://github.com/nodejs/node/pull/13482), probably run until it crashed Jenkins because it is no longer accessible. Output was very similar to your comment there. If this is a pattern it'll be a problem, please let us know if you see it again. Right now, there is no other log file that big on the server. 
jbergstroem		@rvagg agere that we should move to block storage. It's just so incredibly annoying knowing that jenkins doesn't compress the build logs and these basically stand for the the majority of the space consumption. A gzip post job would do. Perhaps we even do it manually? There is a jenkins plugin that is supposed to to this for us but it obviously doesn't work.
joaocgreis		Another failure of `cctest`: https://ci.nodejs.org/job/node-test-binary-windows/9716/RUN_SUBSET=3,VS_VERSION=vs2015,label=win2012r2/ while running for https://github.com/nodejs/node/pull/14140
refack		The golden snippet: ``` [ RUN      ] EnvironmentTest.AtExitWithArgument  ==== C stack trace ===============================  	lh_node_usage_stats_bio [0x00007FF6CE172151+200673] 	lh_node_usage_stats_bio [0x00007FF6CE16B8DD+173933] 	lh_node_usage_stats_bio [0x00007FF6CE16B6BE+173390] 	lh_node_usage_stats_bio [0x00007FF6CE16B8B3+173891] 	lh_node_usage_stats_bio [0x00007FF6CE16B7B6+173638] 	RtlProcessFlsData [0x00007FFF3790D9F7+295] 	LdrShutdownThread [0x00007FFF3790BA49+73] 	RtlExitUserThread [0x00007FFF379085DE+62] 	FreeLibraryAndExitThread [0x00007FFF34DA2C7C+76] 	FreeLibraryAndExitThread [0x00007FFF35369CDA+10] 	lh_node_usage_stats_bio [0x00007FF6CE160B5D+129517] 	lh_node_usage_stats_bio [0x00007FF6CE160CA5+129845] 	lh_node_usage_stats_bio [0x00007FF6CE160AE0+129392] 	BaseThreadInitThunk [0x00007FFF353613D2+34] 	RtlUserThreadStart [0x00007FFF379054E4+52] ``` Stack repeats forever. P.S. @joaocgreis maybe delete the log, it's ~1GB
refack		Ping ping ping. Happening again.
refack		Well it's not the same: The `master` node is ok - https://ci.nodejs.org/computer/(master)/ Only one job suck ![image](https://user-images.githubusercontent.com/96947/28248387-e3b83218-6a11-11e7-9de9-061f0059ff04.png) ... ![image](https://user-images.githubusercontent.com/96947/28248391-fb954420-6a11-11e7-819f-095fa08067a5.png) 
refack		It probably boils down to https://ci.nodejs.org/job/node-test-binary-arm/ being slow, and the backlog limited to 15 jobs
Trott		@refack Yeah, it's an alarming thing to see that in the interface, but the issue fixes itself if you wait long enough.  I won't start any more C+L jobs unless the backlog gets into single digits. That should hopefully prevent that from happening too much more today at least.
refack		Can we learn something for this? Increase the backlog limit? Make Jenkins shut-up about reserving stuff and just say "Pending"?
jbergstroem		We can increase teh backlog but my experience is that we start failing/bleeding elsewhere :/
refack		And now something new, subjob are finishing but not propagating their status: ![image](https://user-images.githubusercontent.com/96947/28298097-5e61320c-6b3f-11e7-9c13-9b9c989ecc5f.png) 
refack		And this is new... Doing this "Loading" thing, but on "submit" does not create new jobs. ![2017-07-17-jenkins](https://user-images.githubusercontent.com/96947/28298171-c7fa840c-6b3f-11e7-9b61-3e28068af82e.gif) 
rmg		LGTM 
jbergstroem		Rsync complete (yes, I forgot to pass `-H` the first run) . Doing another manual rotation to verify that everything works, but it looks good so far.
Fishrock123		@jbergstroem Is this complete now?
jbergstroem		Yes; transfer and first periodic finished on new host. Just want to verify manually before retiring.
jbergstroem		Just did one more semi-random check that backups works (untar, check contents) and hardlinks usage works as previously. Below output is from a small script that finds duplicates for inodes:  ```console  $ ./checkfile.sh /backup/periodic/daily.4/ci.nodejs.org/jobs/node-test-commit/builds/6657/                Item: /[23186367] = /backup/periodic/daily.4/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml      -> /backup/periodic/daily.2/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml      -> /backup/periodic/daily.5/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml      -> /backup/periodic/daily.1/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml      -> /backup/periodic/daily.6/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml      -> /backup/periodic/daily.3/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml      -> /backup/periodic/weekly.0/ci.nodejs.org/jobs/node-test-commit/builds/6657/build.xml ```  Next step is shutting down the old machine and updating the inventory, but since that's not related to daily backups running (and verified restorable), I'll close this.
jbergstroem		Same place as well. I wonder if it dumps or something. I can look into it later. 
rvagg		that's weird, I've rebooted the machine (xgene-2), hopefully that does the trick 
rvagg		love it :+1:  
jbergstroem		Added `c++`. Merged in 0a761c7a61c51d85d0fe89ddcf6f551f27bda894. 
mhdawson		PR for ansible changes: https://github.com/nodejs/build/pull/796
mhdawson		Ok, 2 test machines have been added to CI and are running a nightly test on libuv. Also added 1 release machine but will leave final configuration until we need it for release jobs.  Still need to:  - [x] close out ansible changes. - [x] get libuv to green (only 2 current failures) - [x] add libuv as a platform in the regular libuv regression runs.
maclover7		ping @mhdawson -- can this be closed?
mhdawson		Yup, closing.
MylesBorins		@evanlucas can you share with me a tap result that has past results so I can compare?  There is a file created everytime afaik, and that file is new every time 
evanlucas		https://ci.nodejs.org/job/thealphanerd-tap-smoker/nodes=fedora21/4/tapResults/ is a good one. The tap results shows 5 failures, but the tap file only shows 1 
MylesBorins		OHHHH I get it now... the tap file it is writing is permanent on the machine and getting over written each time... that is odd 
maclover7		ping @evanlucas @MylesBorins -- is this still an issue? It looks like the `thealphanerd-tap-smoker` job doesn't exist anymore on ci.nodejs.org.
refack		AFAIK the job has been migrated from pure tap to [tap2junit](https://github.com/jbergstroem/tap2junit) and now Jenkins keeps the full results for us.  Closing, but if anyone knows otherwise, please reopen.
piccoloaiutante		On 3rd point how about public tweet from foundation thanking for donors support every X amount of time? Is it something feasible or meaningful?  Overall the rest of you proposition make sense to me. 
jbergstroem		- Create three tiers: Lets. I agree with your assessment on group criteria as well as picks. - New page: Should it live under the foundation though? Being a build group provider doesn't necessarily mean being part of the Node.js foundation. - Guest blogging/content: I'd be happy to write some stuff as well, we have a wide range of topics to cover, from infra to ci to deploy and orchestration. Not sure how we'd define frequency or "obligations" . - Regular posts: I think this is a great idea and would like to adopt it for all working groups [that have anything to add]. [The FreeBSD Quarterly reports](https://www.freebsd.org/news/status/status.html) is a good example of how they increase visibility in work done. Since we do a lof of deployments/changes to infra, the quarterly report would also serve to increase visibility of our sponsors. - Node names: Absolutely. I have an idea about a page where we summarize build status since job history isn't permanent in Jenkins. We can make sponsors more visible there too. 
mhdawson		Internally it would be nice to understand the tiers, even if we don't publish them.  I would like to be in the position to be able to explain why IBM falls into one tier versus another.  Otherwise I'm a +1 on the proposal. 
Trott		@rvagg Is this something that should still happen? Or should this be closed (because we've chosen a different path or whatever)?
mhdawson		@Trott we have some of the pieces in place (regular tweets - one should be coming out this week), but we still have work to do on many of the others.
maclover7		Just a note, this can be closed once #1075 and #1101 land.
mhdawson		This should stay open a bit longer as once those land we still have BE to update the ansible scripts for.  As an update, new ansible script has landed and there is one new work in the CI that is setting the compiler level.  Will leave over the weekend and if it looks ok then proceed to updating the other 2 workers and removing the logic in the job to only set the compiler level on the currently updated machine.
maclover7		ping @mhdawson -- can you confirm we're good here?
mhdawson		I tried to say in the last post that we will still need time to go through the other steps, these will take a while.
phillipj		For the record:  @jbergstroem you approving the changes as is, I assume we don't want to require ansible v2.1.
mhdawson		LGTM 
rvagg		working on it, separate issues but related to the missing builds in index.json are missing SHASUM files, this must have been screwed up when I did the last round of work to make releases easier for people who are not @rvagg 
mikeal		awesome :) 
rvagg		fixed, but I'm going to leave this open because ... the dirty little secret here is that nightlies aren't triggered automatically yet, I've been doing them myself. It's not a big leap to make them automatic but it's not trivial either.  I'm wondering if perhaps @Fishrock123 would like to join me in making sure we get at least one nightly out each day? At the moment I've been doing them towards the end of each day according to UTC time but that's not an absolute essential. We also are not restricted to exactly one per day if we need to throw more out there to test new commits.  @Fishrock123 if you'd like to help, have a look at https://jenkins-iojs.nodesource.com/job/iojs+release+nightly/build to see how it's done, you just need to fill in the commit to fetch (it should be HEAD on the v1.x branch), there are instructions there for how to easily get that in 10-character format, as well as the day in UTC time, there's a command to run to get that listed on the page as well.  I've done it for 20150207 already. 
Fishrock123		> ... the dirty little secret here is that nightlies aren't triggered automatically yet  Yeah, I had suspected that. XD  > Jeremiah Senkpiel if you'd like to help  Sure. Sounds good. I could set a reminder for like 9pm UTC (4pm my time) and do one if there isn't already there. 
ryanstevens		> ... the dirty little secret here is that nightlies aren't triggered automatically yet, I've been doing them myself.  I have long suspected @rvagg is literally a machine. The fact these builds still happen further supports my theory.  
bnoordhuis		I noticed that the nightlies at https://iojs.org/download/nightly/ have stalled since March 13.  Is there anything we can do to automate it?  Going by @rvagg's description, it sounds like it could be a simple cron job that POSTs to the Jenkins form once a day.  TBD how to handle authentication.  I suppose that pasting my session cookies into the script is not the smartest move; for one, they'll probably expire after a while. 
Fishrock123		Yeah, sometimes both me and @rvagg are not around on the weekend. Also missed monday tho. :s 
jbergstroem		Most stuff is automated now - closing. 
jbergstroem		Flavour irrelevant? Linux? 
bnoordhuis		More is better but Linux + Windows seems like a good start. 
jbergstroem		Slightly off-topic but would you see shared builds in the same light? 
bnoordhuis		I'm not dead set against testing it but it's mainly intended for distro packagers, and they know how to help themselves.  Disabling intl (crypto too, for that matter) is more of an end user thing for people on low-end systems. 
jbergstroem		I'll set this up on our most redundant setup. 
bnoordhuis		`--without-inspector` would be good to have too, see https://github.com/nodejs/node/pull/7258. 
bnoordhuis		Another one: https://github.com/nodejs/node/pull/9041 
bnoordhuis		Would also be good for testing pull requests like https://github.com/nodejs/node/pull/9040. 
TimothyGu		I'm tired of this issue being open and inactive for so long. For my purposes I've created https://github.com/TimothyGu/node-no-icu, which uses Travis CI and a local cronjob to run tests without ICU every 6 hours on master. Doubtless to say, the [first build](https://travis-ci.org/TimothyGu/node-no-icu/builds/288900245) will certainly fail as soon as it finishes, though the [second build](https://travis-ci.org/TimothyGu/node-no-icu/builds/288902809) with nodejs/node#16251 will probably be much more successful. PR CIs are run manually.
gibfahn		>I'm tired of this issue being open and inactive for so long.  Would you like to set up the Jenkins job? This is just waiting on someone having the time, the only bit that needs special access is the initial job creation. I'm happy to clone the Linux test job and give you access.
TimothyGu		@gibfahn I saw https://ci.nodejs.org/job/node-test-commit-linux-nointl/ has been created. I think this can now be closed.
gibfahn		>I think this can now be closed.  Is it actually run in the main flows? We should keep this open till that job is good to go. I also don't think running on that many platforms is a good idea, we have a lot of variations to test, so one or two platforms each is probably best.
TimothyGu		@gibfahn In that case I'd be happy to be given the permissions to modify the job.
gibfahn		@TimothyGu I've given nodejs/Collaborators configure access. I guess we can always revoke it if this becomes part of the main flows.  What I think we need is a job that runs a variety of configurations, but only on one platform each (if it turns out we need more coverage then we can expand).  Things I've seen asked for (cc/ @danbev, who I'm sure has some requests): - `--without-intl` (this one) - `--without-ssl` (https://github.com/nodejs/build/issues/643) - Some combination of the shared/static lib options (`--fully-static`, `--partly-static`, `--enable-static`,  `--shared-openssl`,  - `--without-inspector` - with OpenSSL 1.1.0 (https://github.com/nodejs/node/pull/16130#issuecomment-338109706) - Run other test suites, e.g. `internet` (https://github.com/nodejs/node/pull/16390#issuecomment-338499752)
TimothyGu		@gibfahn I've done the following modifications to `node-test-commit-linux-nointl`:  - Rename it to `node-test-commit-nointl` - Only run it on `ubuntu1610-x64`, which seems to be one of the most stable machine labels.  Things still needed to be done:  - Make `node-test-commit-nointl` part of `node-test-commit` - Add the other configurations  Something I'm worried about however, is if we eventually stuff all `nointl` builds into one machine label (i.e. `ubuntu1610-x64`), that label will be overused while other stable machine labels are left vacant. This will become even more of a problem once builds using other configuration flags (`nossl`, etc.) are added. Is there a way to provide a list of acceptable nodes for Jenkins to choose from randomly (like a load balancer)?
gibfahn		>Is there a way to provide a list of acceptable nodes for Jenkins to choose from randomly (like a load balancer)?  Yes and no, but that's a bigger question, so opened a separate issue: https://github.com/nodejs/build/issues/934
rvagg		Added some notes in https://github.com/nodejs/node/pull/16130#issuecomment-338599969 about how we might go about testing a shared openssl without requiring dedicated CI machines for it, this would also work for the other shared libs we support (http-parser, libuv, zlib, cares ‚Äî basically what Linux distro packagers are dynamically linking against.)
gibfahn		How about this as a proposal:  Add a pipeline to this repo that takes a set of sets of config flags as a build parameter. For each set it runs a `make run-ci` on Linux, macOS, and Windows. We can then have a single job that can run an arbitrary set of build types (as long as they just differ in config flags).  Example:  ```bash CONFIG_FLAGS='"--without-intl" "--enable-static" "--without-ssl"' ```  would run three different builds, one for `--without-intl`, one for `--enable-static`, and one for `--without-ssl`, each on the three main platforms.  Will attempt to raise PR if I get time.
rvagg		perhaps we could make a subset of nodejs/Collaborators for this kind of thing, nodejs/Collaborators is very broad and it might be best if we restrict it to just people that are interested enough in doing this kind of work; like a nodejs/jenkins-job-config or something that doesn't give full access? maybe we can talk about this at our next meeting.
rvagg		moving my comments to https://github.com/nodejs/build/issues/972
maclover7		@TimothyGu Can you confirm that https://ci.nodejs.org/job/node-test-commit-nointl/ works ok? If so, probably makes sense to close this issue, and then maybe move to a different one to discuss `--without-` flags in general
TimothyGu		@maclover7 Last time I checked, the job works fine. However, I really want it to get into the daily build routine (doesn't have to be in node-test-commit, but executed frequent enough for us to notice issues about it).
rvagg		Oh, hey, I think I missed this issue the first time around. We have infrastructure in place to set variations like this in https://ci.nodejs.org/job/node-test-commit-linux-containered/, each of the builds in there just have variations of `CONFIG_FLAGS` being passed in, this would fit really well. If we go ahead and ditch older Alpine then we have more than enough processing power on our Docker hosts to make these go just as quick as the others.  Is it as simple as `CONFIG_FLAGS=--without-intl`? Is there anything else special that I'm not seeing in node-test-commit-nointl? I can add a builder for that pretty easily if that's all there is to it and it'll get run on every commit.
rvagg		> I think I missed this issue the first time around  heh .. just noticed that I've been very active in this thread already last year .. I have no memory of this üò¨ 
TimothyGu		@rvagg  > Is it as simple as `CONFIG_FLAGS=--without-intl`?  Yes. 
rvagg		Done, this lives in node-test-commit-linux-containered along with the other `./configure` permutation builds. It's run inside Ubuntu 16.04 containers: https://ci.nodejs.org/job/node-test-commit-linux-containered/nodes=ubuntu1604_sharedlibs_withoutintl_x64/  I've restricted it to >= Node 6.  I've also added the following sanity check after everything runs to make sure the binary is as we expect it to be (we have something like this for all the variations):  ```sh INTL_OBJECT="$(out/Release/node -pe 'typeof Intl')" echo "Intl object type: $INTL_OBJECT" if [ X"$INTL_OBJECT" != X"undefined" ]; then   FAIL_MSG="Has an Intl object, exiting"   echo $FAIL_MSG   echo "1..1" > ${WORKSPACE}/test.tap   echo "not ok 1 $FAIL_MSG" >> ${WORKSPACE}/test.tap   exit -1 fi PROCESS_VERSIONS_INTL="$(out/Release/node -pe process.versions.icu)" echo "process.versions.icu: $PROCESS_VERSIONS_INTL" if [ X"$PROCESS_VERSIONS_INTL" != X"undefined" ]; then   FAIL_MSG="process.versions.icu not undefined, exiting"   echo $FAIL_MSG   echo "1..1" > ${WORKSPACE}/test.tap   echo "not ok 1 $FAIL_MSG" >> ${WORKSPACE}/test.tap   exit -1 fi ```  We don't quite have enough containers to do more than 2 node-test-commit-linux-containered's simultaneously now because this adds an extra build for each one. However I've just pulled out a heap of Alpine containers from active use so we have capacity to spin up more containers on our existing Docker hosts. I'll get to that in the next few days.
MylesBorins		getting the same error on ubuntu 1204 and 1404 as well and fedora 23
refack		This is CitGM, right?
MylesBorins		indeed CitGM
gibfahn		@MylesBorins could you give a link to a failing job?  What makes you think this is a machine problem? Which other modules are failing (just phantomjs?)  @nodejs/build has anyone changed these machines recently?
MylesBorins		Across multiple builds + multiple jobs + mutiple modules.  We don't test PhatomJS but rather it is a build tool in multiple stacks. All of them are failing with the same error regarding a missing lib. It is possible that this is a new dynamic dependency of the phantomjs binary
refack		One example (machine & package) as a thread I can start pulling?
MylesBorins		The phantomJS failure is due to not having [libfontconfig](https://packages.debian.org/cgi-bin/search_contents.pl?word=libfontconfig.so.1&searchmode=searchfiles&case=insensitive&version=unstable&arch=i386)  platforms exhibiting behavior  * [mocha on v6.x ubuntu1204](https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/1030/nodes=ubuntu1204-64/testReport/(root)/citgm/mocha_v4_0_1/)  I also discovered that some machine are missing libunwind on some machines  [zeromq on v6.x ubuntu1604](https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/1030/nodes=ubuntu1604-64/testReport/(root)/citgm/zeromq_v4_6_0/)  Here is a link to the citgm job in case you want to dig into other failures  https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/1030/
rvagg		This _should_ be fixed now, see #964. I couldn't find any of the libunwind failures when I went looking for logs (1030 was deleted by the time I looked). Let me know if you see that one again.
rvagg		re-ran citgm on v8.9.0 @ https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/1052/  all green on the two fedora boxes included (now 25 and 26 and in theory it should stay up to date with the latest two fedora releases), the failure on debian8 is unrelated (port in use)
jbergstroem		I brought this up with @mhdawson the other day as well. I'm not sure if this is a problem though? I don't know AIX standard layout well enough to say if its good or bad -- but the builder lives in `/home` which has lots of space.
gibfahn		@jbergstroem I guess the question is whether they're getting fuller.
mhdawson		Looking at another issue I see it says  ``` ksh: There is not enough space in the file system. ``` So I think we should add some space, because that seems like the system is unhappy, even if it does not affect our tests.  Will look at adding some space tomorrow.
mhdawson		Added space to both be-1 and be-2, complaining message no longer appears.  PR to add step to manual configuration: https://github.com/nodejs/build/pull/675
mhdawson		Landed PR to added manual step, closing for now.
orangemocha		When this happens, does Jenkins exit or just hangs? 
jbergstroem		As far as I've seen the process just nopes out of there. There might be some stale state, but at some point it just exits. 
orangemocha		In that case, we could just have a wrapper script (or the current start.sh) restart it after 30 seconds. We might want to do this only for certain exit codes. 
jbergstroem		was thinking in similar lines -- basically crontab something that looks for the correct java process (some vm's have two) and then calls whatever way of init we have (slowly migrating to proper init scripts). 
rvagg		So, monit basically .. 
jbergstroem		@rvagg sure. Crontab has less deps though :) 
orangemocha		@jbergstroem monitoring through crontab can also work, though if you do it in the wrapper script you would know immediately when the process exited. Are there any scenarios that that approach wouldn't catch? 
jbergstroem		@orangemocha I reckon we'd do it outside of the init script. Each init would have its own way -- for instance upstart and systemd would respawn by itself. On other systems my personal preference would be doing a simple crontab job that would zap/restart the init script when needed (thinking that we'd call the cron job every 5 minutes) 
jbergstroem		I think this was resolved by lowering the ping latency to 2 seconds (from the default 5). Closing but feel free to reopen if necessary. 
wgracelee		From jenkins admin point of view, how I can fix it? How do I change the ping latency to 2 seconds? Thanks. 
jbergstroem		@wgracelee we use `-Dhudson.slaves.ChannelPinger.pingInterval=2` to launch the java process. 
Fishrock123		@jbergstroem Still an issue? 
jbergstroem		@Fishrock123 yes, will likely persist until we've done something about it through ansible. 
jbergstroem		Would lineinfile /etc/hosts `::1 localhost.localdomain localhost`  ish be enough? 
Trott		@jbergstroem I think so-ish. 
jbergstroem		I've added `::1 localhost.localdomain localhost` to all /etc/hosts test machines that lives at softlayer. 
Trott		I believe this can be closed. If I'm wrong, by all means, re-open.
mhdawson		@rvagg 
mhdawson		@rvagg  Pushed changes to address comments
refack		I can update `provider-logos.ai` if everyone's Ok with the current layout.
mhdawson		Pushed commit to address comments.
mhdawson		@rvagg you happy with the latest version ? 
rvagg		yep, lgtm
refack		@mhdawson I added a suggested tweak-commit, feel free to push it out. 1. replaced logo png with svg 2. added to `.ai` 3. exported new layer from .ai
mhdawson		@refack I'll leave your commit in but we should talk about what tools are needed for the different formats.  If there is no available free version of the tool (like illustrator) I don't think our workflow should depend on it.
refack		I just checked. The .ai is totally workable in [inkscape](https://inkscape.org/en/) (which is GPL).
mhdawson		Had landed this a few days ago, should have closed.  landed as 376a889203bded39f0072147bbfb9ee03023d09c
mhdawson		@jbergstroem can you take a look  
jbergstroem		The only question would be whether we need that gyp slave, but I reckon we can refactor that later.  LGTM  
mhdawson		Landed as 0fea5dcccef0273d81fb70b623e75d7c80147ccd 
jbergstroem		I would prefer going with 1 and creating a script for updating v8 in `deps/`. I recall someone writing something similar at one point but we didn't land it. My rationale being the option of least moving parts and the burden of making sure everything checks out will be at merge/PR time. 
rvagg		I'd like to hear @bnoordhuis and @indutny on this as they have the most history wrangling deps/v8. I don't have strong opinions either way, just a touch of dependency-fatigue. 
ofrobots		I am not quite sure that creating a script for updating V8 is going to be trivial. We float quite a few patches over upstream and it is quite subjective which backports are going to be in what stream.  > the burden of making sure everything checks out will be at merge/PR time.  It would be better if this burden was not present on every update to deps/v8. Choices 2 and 3 avoid this completely. 
mhdawson		1 does have the advantage of not introducing additional dependencies.  Pulling some of the ones needed for the V8 testing requires svn.  Do you know what additional dependencies like svn would be required for 3 ? 
jeisinger		hum, all deps from v8 should be hosted on git. which one are you referring to? 
mhdawson		when using the gclient tools it seems to use svn  https://ci.nodejs.org/job/mdawson-RunV8TestsInNode/8/console  Related to https://github.com/nodejs/node/pull/4704 
jeisinger		that's odd - at least the deps/v8/DEPS file does not reference this 
ofrobots		This morning I have been spending some time bisecting to figure out why buffer tests are broken on the latest V8. This is on a `vee-eight-4.9` that I built using option 1. It was pretty painful. Any time V8 is updated (e.g. each step of the bisect) the `gitignore` edit and trace-event commits have to be cherry-picked back on.  This is going to be ongoing pain for anyone who updates or bisects V8 in node. I personally would like to avoid option 1. Apart from myself, @targos, @bnoordhuis and @indutny do the bulk of work updating and backporting fixes, so I would really like to hear their opinions on this.  I think we should be able write a simple script to implement option 3 that doesn't involve any additional dependencies (e.g. svn). 
jeisinger		@ofrobots are you hunting for issue #4701? 
targos		I think that whatever we do, it is crucial that it stays as simple as it is now for someone to build node from the git repository: `./configure && make` If we go for option 3, a step in the `configure` script should check wether the `trace_event` dep is there and clone or update it if needed. Ideally, the source tarball that we put on the website should have it included so that `git` is not necessary for people that want to build from it. 
bnoordhuis		Option 3 implies a dependency on depot_tools.  I'm +1 on option 1, it has the fewest moving parts. 
ofrobots		It sounds like option 1 is best for now. We can revisit if it causes too much overhead going forward.  For posterity, here's how I updated trace-event DEP for a V8 extract without using depot_tools:  ``` bash git_url="https://chromium.googlesource.com" dir="base/trace_event/common" git_repo=$(grep trace_event/common.git DEPS | awk -F+ '{ print $2 }' | tr -d " \",") commit=$(grep trace_event/common.git DEPS | awk -F+ '{ print $4 }' | tr -d " \",")  mkdir -p ${dir} pushd ${dir} git init git remote add origin ${git_url}${git_repo} git fetch origin ${commit} git reset --hard FETCH_HEAD rm -rf .git popd ``` 
jbergstroem		How about we create a repo with update procedures, floating patches and above script and whatnot to make bumping v8 more public? I still believe there must be some kind of scripting that should apply for most bumps -- especially minors. Also, last time I bumped a minor I didn't reapply floating patches, rather diff v8 trees and land commits on top. Perhaps thats not standard procedure but it was trivial to do. 
ofrobots		@jbergstroem Sure thing. Should these scripts in the `tools/` directory in node, or in a separate repo? A few things should be easily scriptable: 1) cherry-picking upstream fixes for backporting, 2) pulling down V8 deps such as trace-event as above. However, I am not sure that a major V8 version update is easily scriptable because it usually involves dealing with possible API changes, deprecations and test updates. 
ofrobots		also scriptable: 3) cherry-picking patch level updates from V8 stable branch.  I can help with writing scripts for these. 
jbergstroem		@ofrobots I don't have a strong opinion but keeping them separate perhaps makes it easier to get going? 
mikeal		Does this require new hardware resources or is it just a cross-compile? 
rvagg		tbh I have little interest in assisting official Debian or Ubuntu packages for node/io.js because the packaging rules don't make any sense for how Node should be distributed--i.e. breaking up every dependency into a separate package, including npm's many dependencies. This is why we're pushing our own releases via https://deb.nodesource.com/ so we can do it in a Node-friendly way.  However, the fact that there are compile problems on MIPS and hurd is news to me and I think the libuv team would be interested in hearing this and working how how we can get some hardware to test this. cross-compile isn't very helpful because you still need to execute tests so we need some kind of hardware to run these.  /cc @saghul 
smikes		Here are my notes from when I looked into this (last week) --  https://buildd.debian.org/status/package.php?p=nodejs 1. on mips, byte order is swapped, causing some TLS tests to fail (? this was my guess, did not verify as I don't have MIPS hardware) 2. on gnu-hurd-i386, IOV_MAX is not defined, causing the version of libuv bundled with node 0.10.29 to fail to compile.  This is still a problem with node through 0.10.33, unfortunately.  I made contact with J√©r√©my Lal (@kapouer) who is involved in javascript on debian ; his comment was  > You can try to make patches fixing those issues. > These fixes will certainly get accepted in testing (i suppose you know > it's freeze time,.. )  Since debian is in freeze, there is a window of time to make this happen for the following major release (not the next one, but one after). 
rvagg		> for the following major release  So that's in 3 to 5 years for Debian right?  :trollface:  
mikeal		How do we even go about getting this hardware donated to the build farm? 
smikes		> So that's in 3 to 5 years for Debian right?  Probably. I am not thrilled with the un-`iojs`-like way that a million `npm` modules get split up in debian/ubuntu, but it sure would be nice to be able to   ``` apt-get install iojs ... things just work ```  The current nodesource tools are a big help, and I point people to them (see https://github.com/npm/npm/wiki/Troubleshooting#updating-node-on-linux)  `curl -sL https://deb.nodesource.com/setup | sudo bash -` is much easier than telling people to install `software-properties-common` so they can add a ppa to do a thing... when all they want is to run some javascript in a docker image.  But having at least the base (`iojs` binary and `npm`) in the official deb tree would be a big, big help. 
rvagg		> un-iojs-like  I think we're allowed to say "un-ionian" now! 
smikes		HURD can run on x86 hardware.  Apparently MIPS can be run in QEMU ( http://www.aurel32.net/info/debian_mips_qemu.php ) on x86, but I'm not sure if that is considered valid enough.  Getting attention on   > the fact that there are compile problems on MIPS and hurd   and that this fact is what's blocking 0.10.33 on deb/ubuntu is enough for me, honestly. 
mikeal		maybe someone should fork debian not on the grounds that systemd is an affront to God but that developers should be able to run releases of software in the year they are actually released. 
kapouer		@rvagg https://wiki.debian.org/DebianReleases#Release_statistics you're just being accurate. @smikes indeed, all patches fixing builds, security issues, or important bugs will be accepted sooner or later in jessie (the coming stable). @mikeal there's a misconception here: debian/stable is for production servers that are supposed to only get security updates - and nodejs won't be supported by security team anyway - lack of human resources. debian/testing is for desktop/production/development for using latest and greatest software - it is perfectly usable as a nodejs dev platform. 
mikeal		> and nodejs won't be supported by security team anyway   Except for Paypal, Walmart, Netflix, Yahoo, Bloomberg, and the Federal Reserve. But who's counting :) 
kapouer		@mikeal as far as i understand v8/nodejs won't be officially supported by _debian_ security team. That doesn't mean security patches won't happen, though. 
mikeal		oh, by _debian_ security team. I though it was more of a general statement. My bad :) 
smikes		@kapouer is a javascript aficionado like the rest of us, see eg https://github.com/kapouer/node-webkitgtk  
SomeoneWeird		@smikes Not sure what relevance that has to this conversation, that was never disputed. 
saghul		> However, the fact that there are compile problems on MIPS and hurd is news to me and I think the libuv team would be interested in hearing this and working how how we can get some hardware to test this. cross-compile isn't very helpful because you still need to execute tests so we need some kind of hardware to run these.  FWIW, we have been in touch with the libuv Debian maintainer, and already fixed some issues with aarch64, IIRC. I'm not aware of any MIPS issues, but if there are I'm sure they'll get in touch, just as they've already done in the past. 
saghul		libuv is already packaged for Debian, officially: https://packages.debian.org/jessie/libuv0.10 and I do see MIPS packages there. About GNU/Hurd: patches are welcome :-) 
kapouer		@saghul i'm also in contact with libuv maintainer, the plan was to make nodejs debian package depend on it instead of using its private copy. That'll happen with version 0.12 / uv 1.0 
saghul		@kapouer unfortunately the Jessie boat has sailed, so Debian stable will not get libuv 1.0. 
kapouer		@saghul yep but as i said above, it's not that much of a problem. As soon as jessie is released, debian testing will roll latest versions of software again. 
bnoordhuis		> Currently new versions of node are breaking on MIPS and GNU hurd, mainly due to minor libuv problems (eg missing headers, undefined constants).  Can you file an issue at libuv/libuv for the MIPS build errors?  It probably requires just a few trivial fixes; and if not, I'll be happy to review patches.   I'm -1 on committing to Hurd support.  We don't support MINIX or Plan9 either.  Hell, we don't really support AIX either, except on a self-serve basis, and that platform actually has a sizable user base (and people running node on it.)  EDIT: To be clear, it's not that I'm against simple build fixes, it's that I don't want to make a promise of support. 
smikes		@bnoordhuis Sorry, I was incorrect about where the MIPS problem is.  It's not in `libuv` but rather in node; the failing test is `release test-buffer`, see   ``` assert.js:92   throw new assert.AssertionError({         ^ AssertionError: [0,252,0,98,0,101,0,114] deepEqual [252,0,98,0,101,0,114,0]     at /¬´PKGBUILDDIR¬ª/test/simple/test-buffer.js:339:10     at Array.forEach (native)     at Object.<anonymous> (/¬´PKGBUILDDIR¬ª/test/simple/test-buffer.js:336:42)     at Module._compile (module.js:456:26)     at Object.Module._extensions..js (module.js:474:10)     at Module.load (module.js:356:32)     at Function.Module._load (module.js:312:12)     at Function.Module.runMain (module.js:497:10)     at startup (node.js:119:16)     at node.js:906:3 Command: out/Release/node /¬´PKGBUILDDIR¬ª/test/simple/test-buffer.js ```  (ref : https://buildd.debian.org/status/fetch.php?pkg=nodejs&arch=mips&ver=0.10.29~dfsg-1&stamp=1402725731 ) 
bnoordhuis		@smikes If that also happens with io.js/v0.12, can you file an issue there?  We're not doing anything with the v0.10 branch at the moment.  (Aside, I think there were one or two endianness issues that have been fixed in the v0.11/v0.12 development cycle but I can't find the commits right now.) 
smikes		@bnoordhuis Will do. I don't know anything about the debian process (yet) but once there is an `io.js` package to try installing, I will bring feedback about build problems on debian platforms to https://github.com/iojs/io.js 
kenperkins		What's the status of this PR? The last explicit comment was @bnoordhuis with a :-1: on Hurd.  
bnoordhuis		I think we can close this.  Bug reports or pull requests for the MIPS build issues never materialized and no one's going to commit to Hurd support. 
smikes		Yes, if I get back to the debian thing I will circle back and create a new issue/PR.  
Fishrock123		Also, I think this is something we can only really do on the CI accurately. 
jbergstroem		Sounds like we'd want to perfect an uninstaller on each os for this to run smoothly. 
orangemocha		> Sounds like we'd want to perfect an uninstaller on each os for this to run smoothly.  .. or using VMs or containers that are reset at the beginning of each run. 
jbergstroem		@orangemocha that won't really solve the users problem which most likely tied to old stuff laying around. We should essentially test: - install version X then Y - uninstall version X then install Y - install version Y - uninstall version Y  Then rinse&repeat for a multitude of versions. 
orangemocha		I see what you mean. I was suggesting that we would need to be able to reset things to a clean state because if any of those uninstalls failed it could affect subsequent tests and, even worse, subsequent runs. 
Trott		@nodejs/build @Fishrock123 Should this remain open?
kfarnung		/cc @nodejs/build   I used #650 as a template, but it seems like there was some refactoring done since.  Are the playbooks under `setup/` still used?
kfarnung		Looks like the CC/CXX variables are set in the init.d template: https://github.com/nodejs/build/blob/master/ansible/roles/jenkins-worker/templates/freebsd.initd.j2#L31
gibfahn		>Are the playbooks under setup/ still used?  We're moving away from them as quickly as possible. 
kfarnung		@gibfahn Do you want me to attempt to update the older scripts?  @jbergstroem I took a stab at adding a config for the compilers and pushed an update.  I verified on my machine that it applies correctly on both FreeBSD 10 and 11 (doesn't change the latter).  As discussed in #723, it looks like `clang34` (3.4.2) and `clang35` (3.5.2) both crash when compiling V8 6.0.  Building with `clang38` (3.8.1) was successful.  Are we opposed to using `clang38` for FreeBSD 10 builds?
gibfahn		>@gibfahn Do you want me to attempt to update the older scripts?  I think we should be okay just using the newer scripts, if you've tested building/testing node after using them then that's good enough for me (idk what @jbergstroem thinks though).
mhdawson		I think we need to document somewhere which one should be used for which platforms.  For example the Raspberry Pi ones are not complete and I burned some time using the new one before realizing that.  Given that we can doc that somewhere prominently I'd agree that just have the latest be updated makes sense.
kfarnung		It sounds like this may no longer be relevant, feel free to close if that's the case.
kfarnung		Closing this out, feel free to resurrect if necessary.
jbergstroem		I've rebuilt 1 and 3. Putting them back on. Re-open if they start failing. 
jbergstroem		Looks like we're getting an incorrect cpu core value: https://ci.nodejs.org/job/node-test-commit-arm/5402/nodes=ubuntu1604-arm64/console  I don't have time to fix this; will remove from arm job for now. 
jbergstroem		Re-imaged them. Closing! 
refack		Sidebar: why do we keep the `POST_STATUS_TO_PR` arg? Why not try every time?
maclover7		ping @gibfahn 
maclover7		ping @gibfahn 
maclover7		ping @gibfahn 
rvagg		I also have this in ~root for synchronizing data from the old server:  ``` #!/bin/sh  rsync -avz --times --perms root@104.131.173.199:/home/dist/ /home/dist/ --exclude .ssh rsync -avz --times --perms root@104.131.173.199:/home/libuv/ /home/libuv/  #NOTE: this misses out on *access.log, they should be preserved on the old server until DNS gives up rsync -avz --times --perms root@104.131.173.199:/var/log/nginx/ /var/log/nginx/ --exclude access.log --exclude error.log --exclude iojs.org-access.log --exclude iojs.org-error.log --exclude nodejs.org-access.log --exclude nodejs.org-error.log --exclude libuv.org-access.log --exclude libuv.org-error.log ```  Will probably need to do that last one some time after conversion to pick up straggler DNS holdouts (thankfully cloudflare will handle most of that for us!). It's important we preserve as much of the log data for metrics even while we're serving from two hosts. 
rvagg		@nodejs/website @nodejs/benchmarking @saghul  We've set up a new server to host our various sites and have currently turned it on for benchmarking.nodejs.org and iojs.org. We'd also like dist.libuv.org updated to 138.197.224.240, @saghul are you the one to do that? AAAA is 2604:a880:400:d0::b2c:a001 if you have one of those.  We'll probably switch over nodejs.org to this new host (fronted by CloudFlare still) tomorrow, for now we're easing in. Please watch for problems, bug reports, people complaining, the usual signs of something wrong and let us know. Ping myself or @jbergstroem directly, IRC is a good place to get us both in a timely manner.  Will update here when we move nodejs.org over. 
rvagg		@jbergstroem I think I'm pretty much done with this PR, also added: - swap spdy for http2 - added metrics setup - extracted latest-linker to https://github.com/nodejs/nodejs-latest-linker  should be simpler to translate it all to your new work without all the additional config. resources/secrets/makesecrets.sh should make the secrets for you although there's stuff in host_vars that doesn't get made by that script, not sure how to deal with the extra stuff, I guess somewhere in the secrets repo? 
rvagg		- added svg and changed to `application/javascript` in `gzip_types` - added cron job to `npm update -g` @jbergstroem pls have a look - combined the 3 JS tools into a single tools.yaml 
jbergstroem		LGTM 
rvagg		Task list for transition (shortly): - [x] CloudFlare DNS update nodejs.org, direct.nodejs.org to 138.197.224.240 and AAAA to 2604:a880:400:d0::b2c:a001 (tho I don't know if this really does anything via CF) - [x] Same for above but iojs - [x] Update GitHub webhook endpoint for nodejs.org and iojs.org to the new server (if they have a hardwired IP then I'll change that to direct.nodejs.org) - [x] Run a test push to nodejs.org master to make sure it updates automatically - [x] Run a nightly build to check if CI release servers can access the new host - [x] Run a release build and have a releaser (who is not me) attempt to promote and check that they can at least get a list of builds available - [x] Point backup server to the new host - [x] Change rsnapshot config to fetch log files from subdirectories of /var/log/nginx (have moved each hostname to a subdir, we can exclude old/ but I'd still like those kept somewhere other than this server even if they are a static once-off) - [ ] Copy remaining logs off old server to new server as DNS drains (give it ~48 hours at least although most traffic is through CF so it'll be pretty quick)  @jbergstroem pls update my comment with any additional items you can think of 
rvagg		@nodejs/collaborators ping, we're just about to switch over nodejs.org to a new backend server, please stay tuned to chatter about problems with the site and let us know ASAP if there are any concerns. We don't expect any of course but you never know! 
saghul		Thanks for the heads up, I emails @piscisaureus since he handles the libuv DNS, hopefully that is adjusted shortly. 
piscisaureus		done  On Wed, Oct 12, 2016 at 10:54 PM, Sa√∫l Ibarra Corretg√© < notifications@github.com> wrote:  > Thanks for the heads up, I emails @piscisaureus > https://github.com/piscisaureus since he handles the libuv DNS, > hopefully that is adjusted shortly. >  > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > https://github.com/nodejs/build/pull/516#issuecomment-253420866, or mute > the thread > https://github.com/notifications/unsubscribe-auth/AANUkSEMsFk2-NG8rGBS2AYVD-nd_X8qks5qzceBgaJpZM4KUSIv > . 
rvagg		Just discovered the hard way that the promotion scripts were out of date and didn't handle `test` builds or even 0.10 and 0.12. I must have a pull request somewhere with an update that never made it in to master. Updated in latest commit. 
rvagg		Added coverage.nodejs.org as per https://github.com/nodejs/testing/issues/36 
jbergstroem		Since you're in here, can you add the unencrypted pubkey to the root user? I guess we should use iojs or something but that's at least how it works now. 
cjihrig		I am getting reports on IRC that the libuv downloads are only available over IPv4, and returning a 301 redirect over IPv6. I'm not sure if this is the right place to bring it up, or if this issue is even related. I'm just curious if anyone from the build working group can help shed some light. Thanks. 
jbergstroem		@cjihrig by looking at the config attached to this PR I would say that sounds about right: https://raw.githubusercontent.com/nodejs/build/a49c891f3b4180631a174e28056bd0395232dec8/setup/www/resources/config/libuv.org  @rvagg perhaps:  ``` diff --- libuv.org   2016-10-25 12:00:04.000000000 -0300 +++ libuv.org-ipv6      2016-10-25 12:01:09.000000000 -0300 @@ -1,5 +1,6 @@  server {      listen *:80; +    listen [::]:80;      server_name dist.libuv.org;       keepalive_timeout 60; ``` 
rvagg		k, have done that on http://dist.libuv.org/, http://coverage.nodejs.org/ (just the port 80 redirect) and http://benchmarking.nodejs.org/ (ditto port 80 redirect), if someone with a decent ipv6 setup could report back on how well that's working it'd be appreciated. 
txdv		``` txdv | Dani-hp: can you test now if it works correctly? Dani-hp | yep looking good now Dani-hp | thanks guys ```  The guy reporting the problem says it is working now. 
AMDmi3		I also confirm the fix :+1: 
rvagg		such pain ... we had to use svn in our internal CI system a while back and it was so painful that we ended up working around it completely. iirc Windows was the worst. 
jbergstroem		Do we want to run v8 on all our test machines or a limited subset? 
mhdawson		Unless there is a reason to avoid svn I'd think it would just be easier to enable on all machines as opposed to limiting to a subset. 
mhdawson		PR here to start adding https://github.com/nodejs/build/pull/330 
jbergstroem		The only difference I could find was a trailing npm process at 2 that's been running for a week and consuming all cpu at all cores. Sweet! 
Trott		Looks like it's fixed. Thanks! I'm going to close this (because, hey, it can always be re-opened). 
jbergstroem		The service window has come and gone. 
mhdawson		Received reminder today that this is still planned.
mhdawson		Time has passed, softlayer machines seem to be online. Closing.
thefourtheye		LGTM 
targos		LGTM 
joaocgreis		LGTM  The `which git` call was there only temporarily while debugging a problem with Ubuntu 12.04. We never know when we might need it again for some other problem, so it is good to have it. 
orangemocha		LGTM 
joaocgreis		Landed in 0aecff5b897bd3241468190554608e90725317e1. 
jasnell		What multi-provider options for CDN and hosting do we currently have (or can we have)?
jbergstroem		> @jasnell said: > What multi-provider options for CDN and hosting do we currently have (or can we have)?  As of now; here's a list of (viable; excluding sponsors that probably won't cut it) providers that either sponsor us or might do so in the near future (google?):  CDN:  - Cloudflare (current)  - Google? (cc: @MylesBorins)  Hosting:  - Digital Ocean (current)  - Joyent (suggested failover)  - Softlayer/IBM  - Rackspace  - Azure/Microsoft (only for testing, but perhaps there's room to step up?) 
MylesBorins		Google may be able to help with both CDN and hosting. We any even be able to find resources to help with implementation. If we want to move forward with that lmk and I'll ping some people internally to see what I can get  On Apr 5, 2017 3:33 AM, "Johan Bergstr√∂m" <notifications@github.com> wrote:  > As of now; here's a list of (viable; excluding sponsors that probably > won't cut it) > > CDN: > >    - Cloudflare >    - Google? (cc: @MylesBorins <https://github.com/MylesBorins>) > > Hosting: > >    - Digital Ocean (current) >    - Joyent (suggested failover) >    - Softlayer (IBM) >    - Rackspace >    - Azure/Microsoft ? > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/676#issuecomment-291704381>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecV2EZvCeaKRi6Se9bs47byMyZ6s9Eks5rsu9igaJpZM4Mzous> > . > 
rvagg		CloudFlare log component is @ https://github.com/nodejs/build/pull/679, gets us closer to removing those cache bypass rules in CloudFlare.  For the record @nodejs/build, I discovered that we didn't have bypass rules for *.7z and *.zip files so metrics until today won't be accurate for them.
rvagg		> Lets have it serve stale content if upstream fails.  FWIW I did try and configure CF to do this, the "always on" option is turned off but that seems to conflict with "bypass" so it never worked. I've also never been game to trouble-shoot to figure out how to make it work because that would involve taking the server offline .. or playing in a fresh zone that I'd have to set up. Thankfully we don't have to figure that out after this is all sorted out.  Also @jbergstroem has pretty much got the mirror for the site set up so we're closer to enabling load balancing. We just need to confirm that logs are in order first and then we should be good to go.
vielmetti		You have access to resources on Packet, and make sure you consider those for hosting too.
refack		> Lets let Cloudflare cache our everything; downloads, website and so on. Lets have it serve stale content if upstream fails. > Lets query Cloudflare about logs (preferably before third action, above) so we (still) can generate download statistics.  Do you need any help with configuring Cloudflare, I have extensive experience with them (and some insider connections).
rvagg		 ![cf_lb](https://cloud.githubusercontent.com/assets/495647/25265081/76667c52-26ae-11e7-81d3-898b443f1c93.png)   So @jbergstroem set this up and configured a 15 minute sync to pull from the DigitalOcean primary server to our Joyent backup server for /home/dist (downloads) and /home/www (everything else). We did a little bit of testing with a temporary hostname, it's tricky to get full confidence without sticking this into production unfortunately but we were comfortable _enough_ to turn it on and give it a go during a low traffic time.  So what we have now is basically the same as before, _but_ if the DO server flakes out for whatever reason (health check is just `GET /` for now), then it'll start pulling from the Joyent server until DO comes back. So we don't have an SPOF now, _but_ we're not done yet.  Next step is to turn off the bypass for download files, we're still forcing the Cloudflare edges to fetch from the origin server to get all of the download files (tarballs, installers, etc.) so we can collect metrics. You can see in the PR I submitted that we're now fetching logs from CF for /dist/ and /download/ every hour. I still haven't properly verified that they contain exactly what we need, I'm in the process of doing a comparison with the raw logs to gain confidence in the process. After that we need to do a switch-over for the metrics data so that at some specific point it switches from using the raw nginx logs to using these CF logs for the files @ nodejs.org/metrics/. I have a simple converter from JSON to the nginx log format we're using as an interim step, whether we end up without that intermediate transform or using the raw JSON can be decided later, for now I just want to make it work.  A couple of interesting asides - I experienced I/O problems with the DO block device we're using for /home/ while working on there just now, makes me concerned about filesystem stability but at least we have this failover in place. Also @jbergstroem introduced me to systemd timers to replace crontab, they're ugly and verbose as per systemd but since they can apparently detect whether a previous job is still running or not they'll solve some of the problems we've had with overlapping jobs on the DO server, so we'd better convert all of that stuff I suppose.  @jbergstroem we need a playbook for the new backup server work, is that going to be hard?
joaocgreis		`rm -rf .git` or even the whole job workspace is better, there have been issues with `git gc` hanging in the past. Jenkins will clone again and start in a clean state in the next job run.  We could add this to the end of jobs, running only if older than some time or bigger than some space. Or perhaps better, create a job that runs on all workers once a month that scans the jenkins workspace and deletes every job workspace (or just `.git` folders).  There are already some steps in this direction: - The `git-rpi-clean` job runs weekly to delete the workspaces on Raspberry Pis. Since they are slow, it clones the repos again right away to save time on the next test job. - There used to be a post-action in `node-test-binary-windows` to clean the matrix (jenkins-workspace) workspace, but it was deleted by a jenkins update (same as status posting to PRs). For now this is back to manual. - The `git-rebase` job has a `git remote prune origin` (not `git prune`) line that runs every time, because the refspec it uses was causing issues with fetching from repos of different users. This is not a problem with refspecs that don't fetch all branches. 
refack		Sort of resolved
fhemberger		@nodejs/build Is this something useful for you to work with? Then I'd add further tests. Please let me know if I missed an important test case in the list above. 
rvagg		@fhemberger yes, this is great, I probably wouldn't have chosen mocha but that's just personal bias and whoever starts the work gets to choose in my books 
fhemberger		What would you prefer instead of mocha? It was just the first thing that came to my mind. I'd just like to use a test runner with simple, readable output, where everybody can add tests easily. 
rvagg		it's fine, proceed as you prefer and we'll adapt 
jbergstroem		This could alternatively be written as part of the nginx test tool (It's perl and annoying though). Benefit being it actually running against the web server config instead of testing the live stuff. 
DavidTPate		I've been using [RunScope](https://www.runscope.com) for some of my Projects. They have a good method of configuration for tests against live systems as well as for running tests like CRON and also from multiple regions in the world with performance metrics and such.  They have a markup for importing tests which could be added to a repo and used to configure RunScope. 
fhemberger		Hmm, not sure that RunScope is the right match. @jbergstroem If you want to port it, please go ahead (sorry, I don't do perl). Just let me know if I should invest any more time into this. ;) 
jbergstroem		@fhemberger was just thinking that it'd be great being able to reuse the nginx config as part of the test suite. I won't pursue this and since I'm not driving I don't get to decide where to turn :)  
fhemberger		Closing this, there seems to be no interest in pursuing this idea further.
gibfahn		This is a CitGM issue, so you should raise it on https://github.com/nodejs/citgm. We definitely need to work out what's going on and fix it.
gibfahn		Thanks, I'll close this for now, we can reopen if it turns out to be an infra issue.
refack		> We definitely need to work out what's going on and fix it.  [question] Is it a real problem to have more than one worker per platform? For example https://ci.nodejs.org/label/fedora22/ one is off-line?
gibfahn		>Is it a real problem to have more than one worker per platform?  Do you mean "Is it a problem to have _only_ one worker per platform"? If so then the answer is yes.
gibfahn		@Trott I see you took [test-rackspace-fedora22-x64-1](https://ci.nodejs.org/computer/test-rackspace-fedora22-x64-1/0) offline as it was failing to build. Do you know if it's fixed?
refack		> Do you mean "Is it a problem to have only one worker per platform"? If so then the answer is yes.  Yes. Also is it a financial/human-bandwidth/whatevs problem to maintain redundancies? or an oversight? Again, if I can help in anyway, I'm pretty much available.
gibfahn		>is it a financial/human-bandwidth/whatevs problem to maintain redundancies? or an oversight?  Maintaining the infra in general is a bandwidth problem, and yeah, there's definitely lots that needs doing. I'll see if I can find some ways to get you involved, we're always keen to have more people help out, it's just there's a lot of specific stuff to learn about how we've ended up doing things in Node.
refack		I do devops too üòä , also don't need to see the "secrets"
Trott		> @Trott I see you took test-rackspace-fedora22-x64-1 offline as it was failing to build. Do you know if it's fixed?  @gibfahn Yikes! That was weeks ago! I have no idea if anyone fixed it or not. /ping @jbergstroem 
gibfahn		>I do devops too üòä , also don't need to see the "secrets"  The amount you can do without Jenkins or machine access is limited though. The [ansible work](https://github.com/nodejs/build/issues/712) is hopefully a good way in.
mhdawson		@orangemocha  any chance it was you ?  
orangemocha		@joaocgreis implemented node-stress-single-test, not sure who implemented the fips version. 
joaocgreis		I implemented it, to help with https://github.com/nodejs/node/issues/4200 . @mhdawson feel free to go ahead and update it 
mhdawson		Ok updated and passing run: https://ci.nodejs.org/job/node-stress-single-test-fips/4/ 
mhdawson		@gibfahn @richardlau FYI 
jbergstroem		How about adding a step after creating it instead? something like:  ``` console # either $ gunzip -t $foo.tar.gz && echo $?   # xz supports -t as well # ..or $ tar -zxf $foo.tar.gz &> /dev/null && echo $? ``` 
mhdawson		This looks like a similar issue that we did not notice for smartos as well https://github.com/nodejs/build/issues/585.  Having a regular job that validates we can download/unzip all of the current releases might help. 
gibfahn		Do we generate md5 checksums for the builds? Seems like something that might be worth doing if we don't.
gdams		@mhdawson I'd be happy to quickly script one of these up. I wrote a similar job to automate checking the md5 checksums of the IBM Node releases
mhdawson		@gdams that would be great.  Once you have the script we can look at setting up the ci to run it.    I can think of a couple of different levels we could valid to:  1) Download and validate unzip works 2) Actually run "hello world", this would mean we'd need to download/unzip on the machine where the version can run.  2) would be great, but 1 would also go a long way to making sure things are ok.
gdams		Okay so to start with I have written a simple bash tool that will scrape the download dir for a certain node version and check all of the sha sums. Where abouts would it make sense to store this script?
gibfahn		@gdams I'd put it in [`tools/`](https://github.com/nodejs/node/tree/master/tools)
mhdawson		Ok the script rocks. I've updated the CI job so that it can be used to:  Run tests on all current shipping releases (StreamandVersion = release) Run tests on all current shipping releases and nightly releases (StreamAndVersion = latest) Run tests on the lastest of a particular stream (ex v6) either shipping or nightly (StreamAndVersion = vX, were X is the stream), use 2nd selection box to select Nightly or Release Run tests on a specific level (shipping or nightly) (StreamAndVersion = vY, were Y is the specific version ex v8.4.1), use 2nd selection box to select Nightly or Release  https://ci.nodejs.org/view/All/job/validate-downloads/  Have also configured to run nightly.
mhdawson		The other thing we should setup is who gets emails on failures, I'm thinking the release team but we probably need to check with them if they are ok/want that.
mhdawson		Ok job is setup and scheduled to run nightly.  Now we need to make sure the release team is aware its a tool they can use and discuss who should be on the email notification list for when it fails.  What I suggest is that we add an email alias called  ``` download-validation-alert ```  and then we can add those interested/willing to get notifications of failures.  My initial thought is that it potentially makes sense to include people from the release team as well as some people from the build team.  The rational for adding the release is if a release needs to be fixed, then   one of them will have to be involved.  The rationale for the build team is that if there is an infra problem or a problem with the job itself then the build team may need to help with that.  I've opened this PR to create the alias and will update with those who would like to be added: https://github.com/nodejs/email/pull/54.  Goal is to get initial list landed early next week.  I've opened this PR as initial doc for the job: https://github.com/nodejs/build/pull/849  @nodejs/build @nodejs/release please let me know if you want to be added to the notification alias.
maclover7		ping @mhdawson -- can this be closed? It looks like the job has been running well and everything's working okay.
mhdawson		Yes, closing
nschonni		@joaocgreis rebased with the updated message. It might help if a CONTRIBUTING.md file is added, even if it just links to one in another repo if you don't want to duplicate the content.
joaocgreis		Landed in https://github.com/nodejs/build/commit/ffca4ecb8cb58bea8c49f336cf23a60c7c1051b0.  Thanks again @nschonni!
fhinkel		Thanks for opening in the right repo.
rvagg		Can't explain this one, but since some of the weirdest infra problems have been solved with a reboot I've done that! Updated the machine while I was at it, I'm not sure it'd been updated since first installed (I hope I haven't messed up the benchmarks in the process!).  Re-ran last job and it looks OK for now, let's keep an eye on it and see if the problem returns.  https://ci.nodejs.org/job/node-test-commit-v8-linux/1026/nodes=benchmark,v8test=v8test/
refack		It's still there https://ci.nodejs.org/job/node-test-commit-v8-linux/nodes=benchmark,v8test=v8test/1027/console
rvagg		OK, I've got build 1027's state copied and compiled and I can repeat the failure. I don't have time to figure it out but I have it sitting in a directory on that server that won't be deleted (automatically), so who wants to take a look? @nodejs/v8  ``` iojs@nodejsb1:~/v8test_temp_gh_build_936$ /home/iojs/v8test_temp_gh_build_936/deps/v8/out/x64.release/d8 --test --random-seed=1505382534 --no-stress-opt --nohard-abort /home/iojs/v8test_temp_gh_build_936/deps/v8/test/message/console.js default: 0.001000 abcd: 0.027000 b: 0.000000 a: 0.001000 log more warn 2 error debug info /home/iojs/v8test_temp_gh_build_936/deps/v8/test/message/console.js:25: Error: exception console.info({ toString: () => {throw new Error("exception");} })                                 ^ Error: exception     at Object.toString (/home/iojs/v8test_temp_gh_build_936/deps/v8/test/message/console.js:25:39)     at console.info (<anonymous>)     at /home/iojs/v8test_temp_gh_build_936/deps/v8/test/message/console.js:25:9 ```
rvagg		More:  ``` iojs@nodejsb1:~/v8test_temp_gh_build_936$ echo $? 0 ```  :shrug: someone should take a look though and see if you can make sense why this is failing in CI
fhinkel		Thanks @rvagg . Do you get consistent failure with the copy of 1027? Or is it flaky?
rvagg		@fhinkel well, I get consistent output that I pasted above, but exit status for the test is `0` to maybe that's not actually a failure?
rvagg		@fhinkel I've put your github keys onto that machine, `ssh iojs@50.23.85.254` and look in the `v8test_temp_gh_build_936` directory for that clone of 1027. You can run `/home/iojs/v8test_temp_gh_build_936/deps/v8/out/x64.release/d8 --test --random-seed=1505382534 --no-stress-opt --nohard-abort /home/iojs/v8test_temp_gh_build_936/deps/v8/test/message/console.js` to replicate the failing bit.
fhinkel		Thanks! Investigating. Will let you know when you can delete my keys again.
fhinkel		That build doesn't reproduce the error for me. Message tests are somewhat confusing:  The message tests are supposed to throw error messages. The test runner compares the output with the `*.out` files. So running just the test file with `d8` looks like an error, but the test actually passes because the output matches the expected output.    ``` iojs@nodejsb1:~/v8test_temp_gh_build_936$ ./deps/v8/tools/run-tests.py --arch=x64 --mode=release message/console >>> Running tests for x64.release No connection to distribution server; running tests locally. [00:00|% 100|+   1|-   0]: Done    ```  Pinging @hashseed as he has touched the test case in the past if he has any idea why this might be flaky on the CI. When the test on the CI fails, the output looks to my eye exactly like the expected output :confused: 
hashseed		Doesn't ring a bell. I have never seen this test fail before, and the expectation file seems to match the output too.
fhinkel		@georgneis figured it out :fireworks: There's a regex checking that the time is a number - with 1 decimal in front of the period. So anything longer than 10 milliseconds fails the regex check. We'll fix this on the V8 side. 
fhinkel		I guess we can close this? We can backport [v8/753322](https://chromium-review.googlesource.com/c/v8/v8/+/753322) once it lands to avoid further flakes.
rvagg		phew! what a relief, thanks @fhinkel & @GeorgNeis. I'll remove those keys from the server now.
mikeal		We may also want to get this benchmark modified.  https://github.com/TechEmpower/FrameworkBenchmarks/tree/master/frameworks/JavaScript/nodejs  For instance, are we sure these are the fastest database drivers?  https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/JavaScript/nodejs/hello.js 
rwky		It'd be a good idea to add more drivers instead of just the fastest one, for example the mysql driver used isn't the most popular (going by npm downloads) https://www.npmjs.org/package/mysql is. 
mhart		Well it seems to hold its own in some of the benchmarks:  http://www.techempower.com/benchmarks/#section=data-r9&hw=peak&test=update  I think a lot more progress can be made in HTTP/network performance ‚Äì libuv+C++ beats the pants off Node.js at the moment, and considering that raw v8 approaches, if not beats, C++ performance in a number of other benchmarks, there must be considerable gains to be made somewhere. 
mhart		Sorry, I meant C, not C++ ‚Äì was thinking of this:  https://github.com/kellabyte/Haywire 
mhart		Going to close this in favour of #44 but I'd love for there to be a focus on continued HTTP performance improvements among other things ‚Äì it does feel a little odd that we've hardly moved since 0.10.x 
rvagg		and yes I recognise the sheer hackiness of this, but it kind of _evolved_ and I don't have the inclination to refactor into a pure Node (awk, or whatever) program 
Fishrock123		So long as it being really hacky isn't going to bite us terribly in the future... :s 
rvagg		/cc @orangemocha @misterdjules 
jasnell		/cc @mhdawson  
Fishrock123		Pull me in if npm testing will be discussed, otherwise I don't have much to add. 
misterdjules		I've been sick for the past few days, I'll update the doodle when I feel better. Don't let that block any meeting though. 
rvagg		my schedule is spotty this week due to travel and CampJS on the weekend, next week and beyond is much more settled though because I'll be home finally; but the doodle should represent my estimated availability though 
Starefossen		I responded to the Doodle as well, as a representative from the Docker WG. I am also interested in helping out with build / ci wherever possible as well. 
rvagg		might be good to have a rep from @nodejs/website on this as well because of the overlap 
robertkowalski		yo, responded :) 
orangemocha		So it looks like Monday 9pm-10pm UTC (2pm-3pm PST) works for everybody (except for @misterdjules who will be on vacation). Not sure how to send an invite through github, so consider this it :) 
Starefossen		Where will the meeting take place today? Hangouts? 
mhdawson		I also need the meeting details  
orangemocha		https://plus.google.com/hangouts/_/g77deac75xhp62i62uzoa4oopea 
rvagg		Sorry for being so unhelpful this time around @orangemocha, I've been super disorganised with nearly a month away from home. Got back last night and only just tuned in to the fact that this meeting is happening now.  I've rallied folks in https://gitter.im/nodejs/build, hopefully we'll have a good turn-out. If you have thoughts on an agenda could you drop them in here? 
orangemocha		Sorry, I didn't have a lot of time to prepare either. But I jotted down a couple of agenda items. Perhaps we can take notes during the call and post them here afterwards. 
rvagg		# Minutes 2015-05-25  ## Agenda - Make io.js CI infra work with joyent/node   - Tests - Alexis   - Releases - Julien + Rod - Hardware   - PowerPC (IBM)   - Azure - Jenkins + Windows uncertainty   - Flakey tests? - Secrets and asset management - Group interaction going forward  ## Present - Alexis Campala - Forrest Norvel - Rod Vagg - Johan Bergstrom - Hans Kristian Flaatten - Jeremiah Senkpiel - Michael Dawson  ## Make io.js CI infra work with joyent/node - Alexis to make a test job work. Make a copy of iojs-any-pr-multi, to work with joyent\node - Rod to work with Julien towards releases - Rod: note ignore the build labels prefixed with ‚Äúiojs-‚Äù, we‚Äôre migrating to plain names, like ‚Äúubuntu1404-64‚Äù  ## Hardware - Michael requested two machines from IBM - PowerPC le and be - Build machines to be managed by the build team - Ideally make setup ansible - Johan to help out with Michael on Ansible setup - `next` branch on io.js has V8 4.3 - Would be good to have these machines testing that   create a list of hardware we would be interested in adding (additional architectures or improve parallelism) - Johan to work on updating README  ## Jenkins + Windows uncertainty - A few tests failing when Jenkins runs as a service . Sometimes it takes a few days before the issue surfaces. Can‚Äôt replicate outside of Jenkins . Environmental issue- Alexis to investigate -  File an issue in the build repo to collect details of consistent failures   - johan will collect all possibly ‚Äújenkins‚Äù-related failures   - johan to collect data on -J / parallel test failures on the machines - Flaky tests: bring node-accept-pull-request over to the io.js CI  ## Secrets and asset management - Rod to work on maintaining a secrets document / spreadsheet for io.js infra - Rod to work on maintaining an assets list for build infra - Rod to push forward with signing keys for Windows, Apple Mac Developer Program account - Rod to push forward with ssl certs / wildcard or otherwise for nodejs.org iojs.org   move jenkins to nodejs.org domain  ## Group interaction going forward - GitHub issues - https://gitter.im/nodejs/build  ## Next meeting  in 8 days, one hour earlier  ## Agenda items for next meeting - node accept PR (Alexis to integrate with io.js Jenkins for joyent/node)   - flakey tests - npm integration - how it‚Äôs being tested on both sides and how to improve it (Forrest, Jeremiah) - testing for website (Robert) 
robertkowalski		@rvagg oh sorry please remove `testing for website` - i meant: this will probably be a topic in the long term.   and i forgot to say: as we just build the website for one target we could use something like travis. 
jbergstroem		(work in progress) build flakiness status here: https://gist.github.com/jbergstroem/d16518d38ef6ddc0e697 
orangemocha		Hangout URL: https://plus.google.com/hangouts/_/g77deac75xhp62i62uzoa4oopea 
orangemocha		Meeting canceled today because of low attendance. Let's reschedule. 
jbergstroem		@orangemocha isn't the meeting in 24 hours (I also thought it was today)? 
Starefossen		No, I think we scheduled the meeting for today, Tuesday one hour before the last time. 
Fishrock123		@mhdawson @othiym23 I think it's mainly just us hashing out npm stuff.. Michael, will you be at Nodeconf Adventure next week by any chance? If so, we could probably hash some of it out there..  (That goes for the rest too, I'm definitely not expecting anyone to be there, but if people are.. \o/) 
mhdawson		@Fishrock123 Unfortunately I won't be there.   Does it make sense if I sent out a doodle to agree on a time to talk specifically about npm testing ?  I'm thinking we want you, @othiym23 and whoever else is interested.  @orangemocha.  Sorry if I was confused about the time. Is the plan every Tuesday at 4 EST, if so I'll add to my calendar. 
rvagg		sorry, I didn't make it either .. I live by my calendar these days and it wasn't on there 
orangemocha		Let's try with an iCalendar file. Does this work? https://www.dropbox.com/s/2kguf6kp5jjabtx/Reconciling%20CI%20meeting.ics?dl=0 
robertkowalski		hey - i am on vacations. i am not sure if you still need someone from the website team... 
misterdjules		@orangemocha I was able to add it to my calendar, but I don't think I could reply to accept the meeting, so you probably didn't get notified too. 
othiym23		I didn't see this until now because I'm way behind on my notifications (I've been on boarding a new npm CLI team member this week!), but I'm happy to meet with @mhdawson and @Fishrock123 separately from the rest of @iojs/build to talk npm testing. Mon-We'd next week work best for me, because as Jeremiah said Thu-Fri are NodeConf Adventure, and the wifi will be dodgy. 
Fishrock123		I might be around Tuesday next week. Monday and Wednesday are going to be primarily filled with travel. 
mhdawson		Would Tuesday 1 or 2 PM EST work for you two ? 
refack		/cc @nodejs/platform-aix 
mhdawson		I can increase the size of / but it may be better to figure out how to move it ?  Was it causing failures or did you notice this pro-actively ?
refack		Not sure about failures, but after a few commands you couldn't even `cd`: ```console # cd /home/iojs/build/workspace/node-test-commit-aix ksh: There is not enough space in the file system. ```
refack		So a quick look at the IPs in `failedlogin` looks like it was attempted to be brute forced. Multiple times. The sampled IPs are known abuse sources: https://www.abuseipdb.com/check/61.177.172.52 https://www.abuseipdb.com/check/218.65.30.53 https://www.abuseipdb.com/check/116.31.116.7  Maybe we need implement some brute force mitigation techniques...    
refack		It's already 18MB after < 24h: ```console bash-4.3# ls -la /etc/security/failedlogin -rw-r-----    1 root     system     18273600 Sep 06 14:01 /etc/security/failedlogin```
refack		I added the following two line to `/etc/ssh/sshd_config`: ``` PasswordAuthentication no ChallengeResponseAuthentication no ``` #### So now `publickey` is the only available authentication method. AFAICT this way brute force password guessing is impossible, and also does not generate entries in `failedlogin` @rvagg @mhdawson @joaocgreis PTAL
sam-github		only allowing public key auth seems like good security practice to me
sam-github		@gibfahn can you forsee any negative repercussions to automation?
refack		I'm asking as currently the AIX setup is semi-manual, and I didn't know if a password is used in the earlier stages.
gibfahn		>@gibfahn can you forsee any negative repercussions to automation?  I can't, but I don't have that much experience with the bootstrapping part of the setup. Previously @mhdawson put my key on them and then I did the rest.  So that'd be a question for @mhdawson .
joaocgreis		This sounds good to me, I can't foresee any issue with doing this. Any machine has to be prepared to run Ansible by adding some key (there might be ways to run Ansible with passwords, but that'd be more work for nothing).  We already have a step to [disable sftp](https://github.com/nodejs/build/blob/b5093ac2a7b6e918039580deee43657a071efcbe/ansible/roles/baselayout/tasks/main.yml#L27-L30), this could be added next to it.
mhdawson		Seems reasonable to me as well, although I'm not an expert at ssh configuration.  I don't think there is any step that needs to login with a password as opposed to ssh key, other than possibly when we first receive the machines and have to add the initial ssh key ( I can't remember as its been a long time since I did that)
refack		Great. So I'll try to "ansible"ize that.
gibfahn		@rvagg ping
rvagg		Yeah, needs the new macos stuff too now 
rvagg		ok, I done gone fixed up the outstanding comments in the rpi section, done some squashing and rebasing and merged into master, additional work can be done as PRs ontop of this, good work folks!
rvagg		on it
rvagg		_Both_ Pi1's we use for building seem to be completely borked, probably needing a rebuild, hopefully not an SD card replacement. I'm going to try and get them back online today but to let you get on with building I've just removed armv6 as a target completely while I sort this out. It'll just mean you won't get the `-armv6l.*` files and we'll have to run the job again after it's fixed. Don't let that stop a release or RC build though @MylesBorins.
MylesBorins		kicked off build for 6.11.2  Executors do not seem to be starting up... thoughts?
rvagg		As per IRC: I killed everything off and restarted Jenkins and I see you've successfully kicked off your build again.  The Pi1's turned out to be not as borked as I feared and it wasn't to much drama to get them back up and running. They are both connected again and are back in the iojs+release job and one is building your current job, so no need to redo later: https://ci-release.nodejs.org/job/iojs+release/1900/nodes=pi1-raspbian-wheezy/console
rvagg		buildbot is probably the most reasonable alternative, I actually started off with buildbot when we first spun this all up in node-forward but discovered that I was spending waaaaay to much time hacking Python than moving forward since that's how you have to configure it. It has come up in discussions on IRC this week though, we all have Jenkins fatigue (on top of existing Java fatigue for me). 
jasnell		Yep, truthfully this is getting to be far too painful. Jenkins is... not fun. If we could get a couple more folks helping with the python slinging, would it be reasonable to switch? @mhdawson ... what do you think? Any opinions here? 
mscdex		Let's write one in node! 
rvagg		See extensive discussion @ https://github.com/nodejs/build/issues/4 (which I'd rather have left open, but it did kind of go stale) 
Trott		@mscdex If you have a lot of free time to donate to such a project, there's this: https://github.com/fishin/ficion  (EDIT: ...which cjihrig already mentioned in the issue rvagg linked too...) 
mscdex		@Trott Just briefly looking at the code, the fishing theme is confusing, but I'm not even sure what state the project is in or how to really use it. 
Trott		@mscdex If you're serious about wanting to know more, you can email the author at lloyd.benson@gmail.com. (I wasn't sure if you're "Let's write one in Node!" was tongue-in-cheek or not.) 
mscdex		I was partially serious. 
mscdex		Do we have a list somewhere of what our needs and wants are in a CI solution? 
mikeal		It's kind of a amazing to me that nobody has written something like this yet being that Node.js is sort of ideal for this use case. 
misterdjules		@mikeal https://github.com/Strider-CD/strider. 
MylesBorins		Sounds like it could be a good foundation level project. On Nov 14, 2015 12:10 PM, "Mikeal Rogers" notifications@github.com wrote:  > It's kind of a amazing to me that nobody has written something like this > yet being that Node.js is sort of ideal for this use case. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/257#issuecomment-156743304. 
jbergstroem		I'm happy to be part of either setting buildbot up or writing a more formal outline for what would be needed to build our own. One approach could be reusing the buildbot (nine) architecture and replace as we go -- if replace is really necessary. The nine frontend is confusing to me, but slaves, masters and configuring them are pretty straightforward. 
mhdawson		@jasnell I don't know of a better alternative and I wonder if we'll just end up complaining about any other choice we make after having used it for a while. 
jasnell		@mhdawson good point ;-) I do think, however, it would be worthwhile, when time is available, to at least investigate a handful of options -- or, at the very least, get an outline of the requirements together. 
orangemocha		More options: - Contributing to Jenkins. As much grief as it gives us, I am not sure if starting from scratch would get us to a happy state quicker than working with with Jenkins. - Among the commercial options, this one looks promising: https://www.jetbrains.com/teamcity/ 
mscdex		@orangemocha Well, I would bet there are more node.js developers amongst us than Java developers, so I'm not sure how many contributions would be made to Jenkins (compared to a node-based solution).  As far as TeamCity goes, I saw that the other day. They do have an [Enterprise license for open source projects](https://www.jetbrains.com/buy/opensource/?product=teamcity) that is free, provided all of their requirements are met. Does someone want to play with a copy of TeamCity using the free license to get a feel for it and see how well it would work for us (andhow many modifications we'd need to make)? 
Fishrock123		I spent some time back at NodeConf.eu talking to @lloydbenson about https://github.com/fishin/ficion, but it's mostly just early stages.  I think it would be possible to build something much better than jenkins, maybe in node. I think we also talked about this at the collaborator summit.  I'd be willing to try to move a significant amount of my time to such an effort, if possible and if that was a decided on thing we wanted to try to do.  I think it's fit to be a foundation project. 
Fishrock123		Also I don't have two reliable computers at the current time, which makes trying to dev a CI-like thing difficult. That will change soon. 
mscdex		@Fishrock123 or use virtual machines :-)  I might be interested in pitching in as long as it's something we want to invest in and we know exactly what we want from a CI system. 
jasnell		ditto. happy to help on this. I can help provide some VM space for development. 
rmg		Building a better CI was literally the reason I came to node, just over 4 years ago... and I _still_ haven't had time. _mutters something about priorities..._ 
rmg		That is to say, "Count me in" 
Fishrock123		Perhaps we can re-use some of ficion but would prefer to make sure it is the way we like it?  In specific, I would prefer if it was as simple as possible, and attempting to be as real-time and as failure-proof (i.e. retries, multiple failure statuses, etc) as possible. 
jasnell		let's start by brainstorming/listing the requirements we'd have, then we can compare that against where ficion is currently at. 
yorkie		I guess https://github.com/travis-ci/travis-ci is a good referable example. 
rmg		@yorkie thanks, I honestly had no idea so much of Travis's infrastructure was actually open source! And MIT license, even :-) 
DavidTPate		I'd love to be involved in building it we just need to figure out what all we need out of the gate. 
Fishrock123		So, some steps for this: - Let's make a repo. Is `CI` a good enough name? or `new-CI`? - Possibly do some pre-investigations of requirements and potential candidates before we think too much about building our own.   - Personally, I'd like to keep it slim. - Schedule a meeting to discuss what we collect.  Thoughts? 
jasnell		- +1 for `nodejs/CI` - An Issue or requirements.md doc in the new CI ought to be sufficient for brainstorming requirements (+1 on keeping it absolutely minimal) - +1 for a meeting. Personally tho, my time will be limited over the next three weeks due to holiday and conferences.  
jbergstroem		come to think of it: `rsync` would probably be even quicker. 
jbergstroem		@rvagg do you see us generating docs from a windows box? I reckon rsync is baseline on every os we have as part of ci besides that. 
rvagg		> do you see us generating docs from a windows box  no no no, no 
jbergstroem		Ok, lets switch to rsync then. 
rvagg		I still don't see what's wrong with `scp` 
jbergstroem		Nothing wrong; its just the good 'ol :turtle: vs :rabbit2: :+1:  
jbergstroem		Let me run a test or two to quantify 
rvagg		when you compare it to the time it takes to run `PackageMaker` or watch the Windows linker at work, scp'ing docs is a trivial task and the slave that does that ends way before both the .pkg and .msi slaves, not to mention the arm slaves. 
jbergstroem		`real    0m4.394s`  I'm just going to go with the kids and say "lol". The reason I started witch-hunt was because I saw the release job 'freezing' at this line. It was likely doing something post that line (compressing with xz or whatever). Closing. 
jbergstroem		no objections. 
jbergstroem		Perhaps its time to pull all the sub-jobs to test-commit/test-pr? Ideally, the front page should fit all relevant jobs without scrolling. 
mhdawson		Moving the sub-jobs makes sense to me provide that the main job mostly passes.   Another option might be to have one tab for regression and then one for "nightly" jobs that would include the benchmark and coverage ones.    My main desire is to be able to look at a page and know that if something's not green it should be and needs to be investigated.    If there was a way that could rollup the other pages that would be even better but not sure jenkins can do that. 
mhdawson		Since there are no objections this was added to the page a while back.  Closing this and we can discuss a larger refactor when needed.
bnoordhuis		I can vouch for Gireesh.  > I would like to have access to BSD systems (net, free, open, and any other supported variants)  We have a FreeBSD buildbot but at the moment that's it.
gibfahn		SGTM
mhdawson		+1 from me
jbergstroem		No objections either if people from the libuv crowd like Ben says yes (which he did :). Does someone from the build team want to facilitate or should I do it?
gibfahn		I can add Gireesh's keys to a couple of the boxes, maybe [test-joyent-freebsd10-x64-1](https://ci.nodejs.org/computer/test-joyent-freebsd10-x64-1/) and [test-digitalocean-freebsd11-x64-1](https://ci.nodejs.org/computer/test-digitalocean-freebsd11-x64-1/), I'll discuss with @gireeshpunathil . 
jbergstroem		@gibfahn please mark which boxes he gets access to in this issue and walk him through the procedure of using it. If need be, perhaps disconnect them from ci while he uses them (I most often don't have to though).  General guidelines:  - since resources are shared with ci, please use them sparingly; copy a checkout, check disk/cpu usage etc.  - avoid using root unless you have to (the iojs user is available and what we use for testing)  - if you have to install packages (lldb, ..), remove them once done. if they don't exist in the package manager, let us know so we can manage state of the machine
gibfahn		I've added Gireesh's key to:  - the `root` user on [test-joyent-freebsd10-x64-1](https://ci.nodejs.org/computer/test-joyent-freebsd10-x64-1/) (72.2.115.15)  - the `freebsd` user on [test-digitalocean-freebsd11-x64-1](https://ci.nodejs.org/computer/test-digitalocean-freebsd11-x64-1/) (45.55.90.237)  We've been through all the above, and he should be good to go.
jbergstroem		Great! @gireeshpunathil: thanks for helping out improving libuv :) let us know when you're done using the machines.
gireeshpunathil		sure, will do, thank you too.
qbit		There is also this now: https://github.com/joyent/openbsd-kvm-image-builder  [Hopefully](https://github.com/joyent/freebsd-kvm-image-builder/issues/1#issuecomment-303732957) the images show up on Joyent's production env soon!
mhdawson		@gireeshpunathil do you still need access ?
gireeshpunathil		The test runs in [#1279](https://github.com/libuv/libuv/pull/1279#issuecomment-305798351) indicated that the darwin code cannot run as it is in *BSD* systems as it is, and needs proper porting.  At the moment, the discussion is on in the PR about viability of PR in v1 vs. v2, so I think I will delay further work on this system(s) until that is resolved. So you may please revoke the access. Thanks!
gibfahn		Keys removed.
orangemocha		Here's an idea... Jenkins already provides a (limited) job history, but the job parameters only include git references that can be deleted after the job is run. I am thinking we could push every change that gets submitted to a dedicated repo, with a tag name that includes: - The jenkins run id - The user id that started the Jenkins job. - Timestamp  Problems to solve: 1) The audit repo would get big. Is there a way to push to it without having to fetch the whole thing each time? 2) How to ensure this push is performed for any CI job, without doing it multiple times for the same change. Keep in mind that users can launch node-test-commit, node-test-commit-linux, or just any subjob. 
joaocgreis		Most of the changes will be just a tag to a commit that would be there otherwise, or with a very insignificant size after compressed. The problem here is binaries.  1) Not a problem, `git push` works with a url without fetching. But it would be smarter to fetch master before, or it will have to upload everything. 2) Without a better plan, we'd have to save on every job that has slaves (not the compile-only jobs, unless anyone can see a way of hijacking those). Most of them will be just a tag for the same object, so I don't see a problem there. The problem is the binaries, can we somehow restrict the binary-test jobs to be run only from the fanned counterpart? 
orangemocha		Restricting the binary jobs to be only invokeable from their intended parents sounds like the way to go.  I guess a good place to inject this would be in the rebase scripts. Do we still use the git-rebase job? 
joaocgreis		For both fanned jobs, yes. 
Trott		Is this still an issue? Should it remain open?
maclover7		Closing for now as something that would be nice to have, but seems to not currently be within our means. If someone wants to tackle this, please feel free to reopen.
MylesBorins		One way to check would be to regex the result of `node -p process.release.sourceUrl` for the string `release`.  It seems to me that doing this during the promotion stage might be too late as the tag will already be published.  As such I think it would make sense for this to happen in the release CI job... technically the job should simply fail if the bit isn't set
MylesBorins		ping @nodejs/build about how we could implement this
joaocgreis		This could be a check like and next to https://github.com/nodejs/node/blob/2f2f1cfa870c2153c811a7047d74c4a0bb8e5f2c/Makefile#L679-L683 . If DISTTYPE is release, `grep '^#define NODE_VERSION_IS_RELEASE 1$' src/node_version.h`.  We can also bake this into the release Jenkins script, but I believe it is better to have in the Makefile/vcbuild where it can be easily reviewed, if possible.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
evanlucas		LGTM This worked for v6.7.0 
mhdawson		LGTM 
mhdawson		Sorry been catching up after being away at Node Interactive.  Will try to come up with a good suggestion this week. 
Fishrock123		ping @mhdawson  
mhdawson		@Fishrock123 thanks for the reminder, will bring it up in WG meeting to day to see what kind of suggestions I can come up with.  @piccoloaiutante you may also want to join the meeting today to listen to get a feel with respect to the issues being discussed/worked. 
jbergstroem		I guess it depends on what the interest might be. I can see a few things between this and the github-bot group. 
piccoloaiutante		@mhdawson @jbergstroem i can join you in WG meeting in half hour so we can discuss about it 
jbergstroem		@piccoloaiutante lets keep the discussion post the meeting. I can stick around a bit longer and I'm sure @mhdawson can too! You're still welcome to join in and listen to our meeting. 
piccoloaiutante		@jbergstroem no problem, i'll follow it. Then i'm not sure that i'll stick around  after it since i'm based in italy, but anyway we can go on with this issue. 
joaocgreis		Welcome @piccoloaiutante ! Here are some ideas: - Adapt the [Windows Ansible scripts](https://github.com/nodejs/build/tree/master/setup/windows) to the [refactor by Johan](https://github.com/jbergstroem/build/tree/feature/refactor-the-world/ansible), integrating Windows as well as possible. Some things that would be nice to have:   - Handling reboots, when updating Windows or installing Visual Studio.   - Updating Windows before starting to install programs.   - Generating `.rdp` and/or `.remmina` files (I have a rough script for Remmina that can serve as a starting point).   - Handle Windows secrets better - passwords are used only for Windows and currently the file we have is not parsable, so we can decide for something better that Ansible can pick from the secrets (currently I have a custom script to generate `host_vars`). - `vcbuild.bat` support for https://github.com/nodejs/build/issues/387 , porting [what is currently in the `Makefile`](https://github.com/nodejs/node/blob/4b312387ead4ba11146b28b8ac05ed385919c4af/Makefile#L263-L292) (introduced initially in https://github.com/nodejs/node/pull/4704 ). 
piccoloaiutante		@joaocgreis I have a first draft of ansible script for supporting widows update and reboot. This should cover your first two points. I tested it on windows 10 and windows server 2012. Can you have a quick look at it before I submit a PR for it just to see that I'm on track of what you're expecting?  https://gist.github.com/piccoloaiutante/d6f0766235b63d44defd3d35bc1887c7 
piccoloaiutante		@joaocgreis i can get it Windows Server 2008R2 from msdn. Don't worry i'll be back with info about it. Thanks 
piccoloaiutante		@joaocgreis i spent a lot of time trying to install power shell 3.0 on windows 2008 R2 so i could run the setup [script](https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1) but i didn't succeed. Is there anything particular that i should do for provisioning this OS?
joaocgreis		@piccoloaiutante I've used the script at http://docs.ansible.com/ansible/intro_windows.html#getting-to-powershell-3-0-or-higher before, it worked for me. (Feel free to move this conversation to IRC or email, I don't mind interacting more to help you)
joaocgreis		I also had issues updating PowerShell in my local VM, the script didn't work. But the instructions at https://msdn.microsoft.com/en-us/powershell/scripting/setup/installing-windows-powershell#a-namebkmkinstallingonwindows7andwindowsserver2008r2ainstalling-windows-powershell-on-windows-7-and-windows-server-2008-r2 did it.
joaocgreis		As noted by @nschonni in https://github.com/nodejs/build/pull/750#issuecomment-307440878 , we don't have a `CONTRIBUTING.md`. We could add one pointing to https://github.com/nodejs/node/blob/master/CONTRIBUTING.md , with also a placeholder to mention any differences or additions. Furthermore, the templates (https://github.com/nodejs/node/tree/master/.github) would also be good to have. This could probably be a good first contribution.
mhdawson		@piccoloaiutante I think we can close this now as you are part of the WG :)
rvagg		Another item: - Security, multiple concerns:   - Infrastructure providers trust us to limit our activity to what we've said we're going to do and we need to ensure that we're responsible with that   - Some infrastructure is provided by non-infrastructure companies and we are even more constrained in who we can give access to. The OS X machines are behind Voxer's firewall so we have to limit access to them. The ARM machines are in my office, on my network, so I need to limit access to them.   - We are producing binaries that are widely used and _must_ be trustworthy. We have to limit access to the machine that are producing those binaries in order to retain that trust. There is a little bit of cross-over between test and CI machines but I've been trying to keep that to a minimum.   - We have to protect the signing keys for the Windows and OS X installers (currently owned by NodeSource)   - We have to protect the TLS certificates for both iojs.org (owned by @indutny) and jenkins-iojs.nodesource.com (wildcard cert owned by NodeSource).   - We have to be careful allowing arbitrary code to run on the full CI set because it's mostly not containerised (this is why the plan is to put PRs through containerised build slaves first). We need to set up structures to keep that separation in place and educate everyone who has access to escalate CI runs to the full set.  Part of the difficulty in scaling up the involvement of other people in the Build work I've been doing comes back to the points above, so I'd love help in exploring how we can expand the number of people involved while being cognisant of the security issues involved. 
bnoordhuis		I don't really have anything to contribute to the Docker WG but I can sit in as a libuv representative if needed. 
rmg		I'm definitely interested. 
jbergstroem		Yeah, I'd be interested as well. 
othiym23		I'm also interested. 
retrohacker		Also interested. Could you expand on  > Discuss the relationship between "Build" and "Docker"  Also, what requirements do we have for ARM donations? 
Qard		I'm super interested in ARM stuff. We might be able to get some help from [Online Labs](http://labs.online.net/). 
Fishrock123		> Longer-term plans to replace Jenkins with our own solution.  I'd be interesting in working on this in the long term. 
edouardb		Hi, I am from Online Labs, @Qard told us that you are in need of ARM-based CI nodes. We can provide you some nodes, ping us on IRC (irc.online.net/#onlinelabs) 
rvagg		@wblankenship:  > Discuss the relationship between "Build" and "Docker"  I put this on the agenda to discuss, I have no strong opinions on it but my working model has been something like this: the io.js Docker team is really a sub-working group of the io.js Build team with some overlapping responsibilities--mainly the build-containers repo. At the same time however, the Docker team should be autonomously responsible for the docker-iojs repo and not have its activities there dictated by anyone in the broader Build team. So in some ways the Docker team is a separate WG but given the small scope of its responsibilities I can't see it making sense to have it spin up as a separate WG, leaving build-containers purely in Build's scope of responsibility. But of course this is all open to discussion which is why it'd be great to have y'all join a meeting.  > Also, what requirements do we have for ARM donations?  No requirements at this stage except that I'd like to see any build assets under the control of the a subset of the Build team (what that subset is I don't know, currently just me in practice). I'll outline this in a new issue I think but the idea is this: there are lots of people using Node on ARM hardware and we can neither track that usage or keep up with it. So instead of going and actively hunting down all the hardware and operating system variants we can get our hands on to test with, lets have the community donate test hardware that we can use either as part of our CI infrastructure or in some way that informs that. The issue of BeagleBone Black is one of these, it's very popular for NodeBots but I don't have one and I'm not inclined to go out and buy one just for this because I don't see its usage as being anything near the rate of RPi for Node, but it comes with a quirky old Linux distro that makes it awkward and our current ARMv7 builds don't work on it because of glibc incompatibilities. Also consider ARMv8 support, I can't wait to have this as first-class in io.js, it'd be awesome to be able to build and distribute 64-bit binaries. But hardware is either quite expensive or comes in the form of Chromebooks at the moment (although [this](https://www.96boards.org/) looks to be changing that), so it'd be great if someone in the community, or a corporate sponsor, could provide hardware if it was important to them.  @edouardb (and @Qard), thanks for joining in the conversation! I actually have an Online Labs account (rvagg@nodesource.com, I was put on to you by @wolfeidau of Ninjablocks) and had a machine connected to the io.js CI for a while from there, however, the speed was significantly slower than the Odroid XU3 I have in my office and it impacting on the total speed of CI builds. _However_, since that time I've also put a couple of Raspberry Pis into the mix and they are obviously a lot slower again. I've also put ccache on each of the ARM machines and this has helped a lot. Currently the ARMv7 builds made available by io.js are made on the Odroid variant of Ubuntu 14.04 which has a much newer libc than some of the ARMv7 hardware out there (like the BeagleBone Black) so it's causing some pain and since you have a Wheezy distro available then perhaps that's a better place to make release builds. 
rmg		Has a date/time been established yet? If the Doodle is any indication this is going to be a small group. In the interest of collaboration, I added in some evening slots for myself and it looks like the result is exactly 1 time slot that works for all 4 of the people who have filled in the doodle. 
jbergstroem		I can probably extend my schedule as well if additional participants would opt for another time.  
jbergstroem		Do we have a date/time set yet? 
rmg		Looks like the first couple time slots on the doodle have already gone by..  /ping @rvagg  
rvagg		Sorry, terrible week, super-busy and unwell at the same time.  I've picked a time that was both popular and worked best for me, unfortunately we didn't have a time that worked for everyone and @wblankenship is the one missing out on the time that we missed. Thankfully I work with him @ NodeSource so I can 1:1 if need be before/after and we'll also have a recorded Hangout.  @jbergstroem @rmg @othiym23 @wblankenship: please see http://doodle.com/r5cz2dq6rcpd9b5e for the time in your timezone, it's ~11 hours from now. I don't have time right now to set up the Hangout and the rest but I'll do that in the morning after the TC meeting and post details here. Anybody else who's interested but didn't respond to the doodle can join too, I'll dump the Hangout link in here. 
retrohacker		It turns out that my last class of the day was canceled tonight, I have to commute back from campus but _should_ be able to make it. 
rvagg		- Hangout for participating: https://plus.google.com/hangouts/_/hoaevent/AP36tYdM5W875vX5xRYouREWZkfLnVzw9nkYGmibICbeim4W7J3Mcw - YouTube for public viewing live & recorded, not participating: http://www.youtube.com/watch?v=OKQi3pTF7fs - Doc for agenda and minutes: https://docs.google.com/document/d/1vRhsYBs4Hw6vRu55h5eWTwDzS1NctxdTvMMEnCbDs14 (empty so far, will attempt to put in an agenda, help from others is welcome!) 
rvagg		in 45 minutes from now if that wasn't clear 
feross		I can't think of any security issues caused by adding CORS headers. It'll just make it possible for javascript on any website to download the installer files and use the data.  In addition to setting the `Access-Control-Allow-Origin: *` header on responses to GET request, it's important to also support the `OPTIONS` preflight method.  When a client sets a custom header like `Range` (to make http range requests) the browser first sends a "preflight" request to ask the server if the request should be allowed. (That's because certain headers or methods (like POST), when done cross-origin, could be dangerous).  So, when a client sends an OPTIONS request, this would be a good server response:  ``` Access-Control-Allow-Methods: GET,HEAD Access-Control-Allow-Headers: Range Access-Control-Max-Age: 600 ```  This allows GET and HEAD requests, allows the `Range` header, and tells the client to cache this authorization for 10 minutes.
wmhilton		@feross That brings up an interesting point. Does the server have to support HTTP range requests to work as a webseed?  Because I noticed that some file extensions (tested in the https://nodejs.org/dist/v6.10.2/ directory) do *not* respect range requests: - .txt, .asc, .sig, .lib  These file extensions *did* respect range requests: - .gz, .xz, .7z, .zip, .msi, .exe  It also might be a file size rather than file extension thing. The text files and the .lib file that I tested were all smaller than 1.5MB.
wmhilton		> I can't think of any security issues caused by adding CORS headers.  That is correct. CORS is simply a way to bypass the same-origin policy, which prevents malicious pages from obtaining sensitive data on other sites by using AJAX and exploiting the fact the user is logged in with a cookie. Since the nodejs site, or at least the /dist folder, contains no sensitive data, there is absolutely no security advantage to not having CORS headers.
jbergstroem		@wmhilton: FWIW, we will shortly have cloudflare front our downloads; perhaps see how that's supported at their end as well.
wmhilton		I use the free level of Cloudflare for my sites, and while it adds a few headers I've never had a problem with it stripping or altering CORS headers. Cloudflare [passes Access-Control-Allow-Origin header through unaltered](https://support.cloudflare.com/hc/en-us/articles/200308847-Does-CloudFlare-support-Cross-origin-resource-sharing-CORS-).
jbergstroem		@wmhilton was rather talking about range requests; but yeah - the more testing the better!
wmhilton		Good thinking. I just tested, and I am able to do range requests on a file on my website that is cached by Cloudflare and hosted on github pages.  ```sh curl -X GET \   http://wmhilton.com/hubo-js/data/hubo-urdf/meshes/Body_Torso.stl \   -H 'cache-control: no-cache' \   -H 'postman-token: 8c90a4b6-99cd-06bf-0f8c-6577f6e1bd99' \   -H 'range: bytes=0-15' ```
jbergstroem		Ok, great that we have that confirmed. I have no issues with adding cors headers for downloads; just want to test out options configuration.
wmhilton		I still would love to see CORS headers added, but in the meantime (and because opening a Github issue with every website in the world isn't practical) I made a workaround service called [cors-buster](https://cors-buster.now.sh) that proxies HTTP range requests. I list both the canonical URL and the proxied one as webseeds in the output of [webtorrentify-server](https://webtorrentify.now.sh) so now my demo [download-with-webtorrent-button](https://wmhilton.com/download-with-webtorrent-button/) actually works!  The very first time it's used, this download button is much slower, because the file has to be downloaded once on https://webtorrentify.now.sh (if it isn't cached) to generate a torrent, and again through the https://cors-buster.now.sh proxy. However webtorrentify-server memoizes all its results, so the second time a link is clicked it should return the cached copy right away. When only one user is downloading node via WebTorrent, the only seed is the (cors-proxied) original URL so there is no bandwidth savings. But when you download on multiple browsers simultaneously, you start to see a huge speedup, because it starts downloading pieces from multiple neighbors in parallel. And most importantly, as the number of simultaneous downloads of the file increases, the bandwidth demand on the origin server actually *decreases*. In fact, the origin server could go down and nobody would notice.
maclover7		ping -- is this still something we want to do?
wmhilton		I'd still love it. :)
feross		Same ‚Äì it's just a single header change and it would make it easier to download Node.js in new ways :)
rvagg		`nodejs.org` nginx config file is @ https://github.com/nodejs/build/tree/master/setup/www/resources/config if you want to PR to that, I can't think of any objections that might arise.
wmhilton		It looks like it already has a CORS header for `.json` files on [this line](https://github.com/nodejs/build/blame/master/setup/www/resources/config/nodejs.org#L59). Here's the current config:  ```nginx     location /dist {         alias /home/dist/nodejs/release/;         autoindex on;         default_type text/plain;          location ~ \.json$ {             add_header access-control-allow-origin *;         }     } ```   @feross Can you help me figure out how we would change it? Here's my first draft:  ```nginx     location /dist {         alias /home/dist/nodejs/release/;         autoindex on;         default_type text/plain;          # Make it possible for javascript on any website to download the installer files and use the data.         # (As of Nginx 1.7.5, add_header supports an "always" parameter which         # allows CORS to work if the backend returns 4xx or 5xx status code.)         add_header 'Access-Control-Allow-Origin' '*' always;         add_header 'Access-Control-Allow-Methods' 'GET, HEAD, OPTIONS' always;         add_header 'Access-Control-Allow-Headers' 'Range,Content-Type' always;         if ($request_method = 'OPTIONS') {             # Tell client that this pre-flight info is valid for 10 minutes             add_header 'Access-Control-Max-Age' 600;             add_header 'Content-Type' 'text/plain charset=UTF-8';             add_header 'Content-Length' 0;             return 204;         }          # JSON files are now covered by the general CORS settings directly above         #location ~ \.json$ {         #    add_header access-control-allow-origin *;         #}     } ``` 
feross		@wmhilton The proposed config looks good to me.  Just wondering ‚Äì why is it necessary to allow the `Content-Type` header with `Access-Control-Allow-Headers`? It seems that since only GET, HEAD, and OPTIONS methods are allowed, the client will never need to set `Content-Type`, unless I'm missing something.
wmhilton		IIRC what happens is that some libraries automatically insert a `Content-Type` if you specify a particular body-parsing method (like `application/json` if you request JSON or `application/octet-stream` if you request binary) and that causes CORS errors. Because quote "The only allowed values for the Content-Type header are: application/x-www-form-urlencoded, multipart/form-data, and text/plain" ([source](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS))  You would think the client would just silently strip the Content-Type header from the CORS request, but IIRC it rejects the request before it gets a chance to leave the browser. :smh:
wmhilton		So its inclusion is to avoid getting "I use [insert name of AJAX library here] and it didn't work" issues.
feross		Yep, you're correct. In fact, this is what `simple-get` (used by WebTorrent) actually does.  So, this LGTM! Want to send the PR?
wmhilton		I guess I should!
maclover7		Closing in favor of the pull request, #1275
mhdawson		PR for ansible scripts to configure machines https://github.com/nodejs/build/pull/361 
mhdawson		Libuv tests now include linuxOne under the linux runs, all green 
mhdawson		PR to add ccache https://github.com/nodejs/build/pull/373 
mhdawson		Job for Node tests is ready here:  https://ci.nodejs.org/job/node-test-commit-linuxone-mdawson/  Today I can build it with my branch (mhdawson/io.js linuxone3) as that branch contains lkgr from google as of ~ March 25 + some later patches from the google repos for z along with an updated version of master from @ofrobots that was updated to work with that level of V8 and the recent changes for Node itself landed under: https://github.com/nodejs/node/pull/5941.  So I think that job is as ready as we can be until master or v6 can be built with the 5.1 version of v8.   Node that the core tests ran/passed - ie test was all green. 
mhdawson		Provide link to console/userid/password to @jbergstroem  who will add to the right place  Added the test keys to the 2 test machines 
mhdawson		So all that is left at this point until we need a release machine (which we can handle separately) are for the outstanding build PR's to land and  the Terms and Conditions from Marist so that myself, @jbergstroem and @rvagg can accept them 
ofrobots		Great! As someone who has worked a lot with zArch in the past, I am somewhat amused by the issue number this ended up at :smile:.   For others who might not have context: [link](https://en.wikipedia.org/wiki/IBM_System/360). 
mhdawson		Issue opened to track last issue of accepting terms: https://github.com/nodejs/build/issues/386 
mhdawson		All done, closing. 
rvagg		the apt install does the symlinks properly for ccache with no further action needed on 14.04? if so, then lgtm 
jbergstroem		Ye man, all ubuntu's seems to install everything (including symlinks) to `/usr/lib/ccache` 
jbergstroem		Merged in 6131d5cac044e52958c3b2da7597bf3cc34ce2e9. Thanks. 
joaocgreis		I won't be able to make it to this one.  I added https://github.com/nodejs/build/pull/809 and https://github.com/nodejs/build/pull/797 to the agenda, and they should be discussed even without me (I added because I can't take them further). I helped with the Ansible changes without realizing that we may not be able to land it because of compatibility (@seishun I'm sorry for this and thanks for all the hard work you've done on those PRs). If I understand correctly, the changes proposed will break 32-bit build on Linux. I don't think this is something we can do for current versions, but don't know if we can for upcoming versions or at all. This needs someone with more Linux experience to weight in.
seishun		@mhdawson Is it 6AM or 6PM?  @joaocgreis As far as I can tell, https://github.com/nodejs/build/pull/797 doesn't break anything. https://github.com/nodejs/build/pull/809 is "breaking" in the sense that it's not (easily?) possible to install a supported version of gcc on 32-bit CentOS 6, which means it would probably need to be dropped from CI. According to @rvagg (https://github.com/nodejs/build/pull/797#issuecomment-316963881), public releases are built on CentOS 6, so 32-bit releases would need to be built on a different machine or dropped altogether.  If these changes cannot land, then I'd like to see an alternative suggestion. Leaving it as-is isn't really an option since the CI machines in question (and many others that I haven't looked into yet) currently use unsupported compiler versions. (see https://github.com/nodejs/build/issues/762)
gdams		@seishun it's at 6PM
piccoloaiutante		I won't be albe to make it. I haven't done anything in the past three weeks since I've been mostly on vacation.
Trott		Super minor nit, but this will be at 6PM EDT, not EST, right? If so, I guess the title should be edited to change that... (perhaps even better, just update it to be UTC?)
mhdawson		link for participants https://hangouts.google.com/hangouts/_/ytl/AcMmruOezHj1vtB6EdFoQ4SvUMyS5otpKJAIVcqjiqc=?eid=100598160817214911030
rvagg		Thanks so much for the Keybox demo @gdams, that's actually pretty exciting! @jbergstroem you should review the video for this.
rvagg		oh, and @joaocgreis there's some discussion about Windows & 32-bit binaries in there that'll end up in an issue but you may want to listen to the discussion if you have time
skavanagh		@gdams  - so I somehow stumbled across this post..   An alternative is to set things up so users can login and set their own keys to the systems under the profiles they have been assigned (and you can then disable those keys anytime). That way you aren't adding the public keys for them.  https://github.com/skavanagh/KeyBox/issues/138#issuecomment-220196009   This way everyone handles their own keys to discourage sharing / reuse. And if you have keybox generate the key for the users, you can require them to set a good passphrase for the private (at least initially, I guess they could always remove it).  What's nice about setting it up this way.. is it makes it easy to rotate keys, since (after the key has been disabled) the user can just login to keybox and issue new ones for themselves.  Hope this helps! 
jbergstroem		Sounds good. I'd be honoured to have @bnoordhuis more active around here!  
joaocgreis		All OK for me as well! 
Starefossen		Very good news üëè  
jbergstroem		So, four of us should be enough. I'll add him to secrets and pass him the details. 
jbergstroem		I've now added Ben to the security group and he's verified that he can access hosts. 
retrohacker		:+1: the webupd8team PPA is the best way I've found documented to install official java repos on ubuntu.  I did find this script in the wild for java 1.5, will probably work here as well. It appears the installer looks for the presence of a file to ensure you have accepted the EULA, if the install fails you create that file and try again.  ``` DEBIAN_FRONTEND=noninteractive apt-get install -y java5-sun-jre || : debconf 'echo SET shared/accepted-sun-dlj-v1-1 true; echo $(read) >&2' apt-get install -y java5-sun-jre ``` 
rvagg		10.04 has been retired from our cluster so thankfully we don't need to care about this 
bnoordhuis		> sshfs  CPU is probably going to be a big bottleneck on those machines unless you use a cipher like blowfish, and maybe even then.  ssh and sshfs don't have null ciphers.  If you don't care about the abstractions NFS provides except storage (not files, locks, uids, etc.), you could configure a Linux machine on your network as a iSCSI target and have the Pis use it as block storage.  Cheap, fast, easy to set up and maintain.
Trott		Does it make sense to configure CI to not run on the Pi3's until this is all sorted out? Or would that just make it so we won't know when it's fixed?
rvagg		@Trott if you can confirm that it continues to be a problem we can take them out of the pool, I don't think we'd lose a lot in terms of coverage. It is useful knowing if the problem is persisting but I could reconnect them whenever I'm doing testing of changes on my end.
Trott		> @Trott if you can confirm that it continues to be a problem we can take them out of the pool,   All day, nothin' but failing Pi3's...I haven't seen a single Pi3 pass...
rvagg		Done: https://ci.nodejs.org/job/node-test-binary-arm/8670/  @joaocgreis can you sanity check for me -- I've just unticked pi3 from the labels list and it _seems_ to be enough to turn them off, tbh it feels too simple!
joaocgreis		@rvagg Looks good! Might be the only thing in Jenkins, but it is that simple.
refack		Hello @krytarowski,  Can you give some background on accomplishing this: 1. Do you know a hosting provider that hosts NetBSD images? 2. What is NetBSD's package manager, and how do we get `git` `python2` and a new enough `gcc` (If NetBSD uses glibc, then one that is build on `glibc >= 2.12`)  Optional: Best help you can provide us is writing an ansible playbook, that setups up a machine (like the ones in `/ansible/` - https://github.com/nodejs/build/blob/master/ansible/playbooks/jenkins/worker/create.yml or https://github.com/nodejs/build/blob/master/setup/freebsd/ansible-playbook.yaml
gdams		I know that [RootBSD](https://www.rootbsd.net/) hosts NetBSD images
krytarowski		1. amazon, gce should work; I use for personal purposes https://www.serveraptor.com/ 2. pkgsrc - the same as SmartOS;  ``` PKG_PATH=http://cdn.netbsd.org/pub/pkgsrc/packages/NetBSD/amd64/8.0_current/All/ pkg_add git python27 ```  8.0 ships with GCC 5.4, there is a newer version in pkgsrc like gcc7 (7.1), gcc6 (6.4)... The one from base should be preferred.  I'm not literate in ansible unfortunately.  Changes compared with FreeBSD:  ``` ansible_python_interpreter: "/usr/bin/env python" ```  ->  ``` ansible_python_interpreter: "/usr/pkg/bin/python2.7" ```   This replaced with the above command: `pkg_add`:  ``` pkg install -U -y {{ item }} ``` 
maclover7		Are there any providers willing to donate NetBSD machines? Otherwise I think this should be closed until a more formal plan be written up
krytarowski		What are the requirements?
krytarowski		I can run it myself, but I'm not ansible-literate.
maclover7		Closing for now as something that would be nice to have, but seems to not currently be within our means. If someone wants to tackle this, please feel free to reopen.
krytarowski		So, what's the option to make libuv/NetBSD a supported tier platform? @cjihrig
rvagg		1. people to maintain the code 2. infrastructure to integrate it into our CI 3. TSC agreement that we can bump it up into the supported list (likely contingent on 1 & 2)  Re infrastructure, we have some limitations:  * Trusted: so we try to run as much as possible in IaaS providers or in locations that we have some kind of direct connection with where we can establish trust  * Stable: because any time a platform goes down our entire CI infra suffers, as do collaborators who are trying to test code, they shouldn't have to think about the platforms it's running on until there's a test failure. We've even been trimming platforms to improve stability, so we really care about this.  * Gratis: we've built our entire infra platform on donated resources and our preference is to continue. This means that AWS is not an option until we can convince them to donate (a possibility but we haven't been able to do this yet. GCP is not something we have used at all but I know that Google is likely willing to donate resources if we asked nicely. DigitalOcean, Rackspace, Joyent, SoftLayer, Azure are all strong partners that we can potentially expand capacity on.  People resource can't be understated, however. It's probably not going to be enough to have 1 person to call on when something goes wrong and nobody else understands. We need people to maintain the infra and to help triage and fix code bugs when they arise. The fewer people we have and the less responsive they are the lower likelihood that we're ever going to get to supported platform status.
krytarowski		I cannot help with devops-like tasks myself, for this we need another volunteer - my role is restricted to keeping the code functional and tests passing.
gibfahn		@refack I don't think we'd want to land this until Node 6 goes out of support (2019-04-18, you're a little early üòú ), but quick work!
refack		> but quick work!  Did it mostly as an exercise
jbergstroem		@refack out of curiosity; did you test the ansible playbook somehow? It's currently pretty tricky; having virtualization/vagrant handy might make it easier.
refack		> @refack out of curiosity; did you test the ansible playbook somehow? It's currently pretty tricky; having virtualization/vagrant handy might make it easier.  WIP
mhdawson		Yes we can't remove the BE machines for quite a while yet.
jbergstroem		Seeing how we're three years away, how about we just close it?
gibfahn		Closing, we can always reopen when it's time üòÉ 
jbergstroem		On a plane this time again :/ last time for a while. 
rmg		I finally put this on to my calendar, so I'll be there unless something dire comes up in the next 25 minutes :-) 
rvagg		@Starefossen is already using this in his slave checker simply by authing with a github token presumably on his own github account. If we have some piece of infra that needs shared access then we should use a shared github account, I've made at least one but I can't remember what it was called (probably iojs-something), @joaocgreis has one too. We should pick one and put login credentials into secrets/infra. 
Starefossen		Yes, the slave checking is currently done with a personal GitHub token from my user account https://github.com/Starefossen/jenkins-monitor/blob/master/lib/jenkins.js#L26. The GitHub token needs at least `user:email  Access user email addresses (read-only)` permissions in order to authenticate successfully.  <img width="518" alt="screen shot 2016-02-02 at 22 23 58" src="https://cloud.githubusercontent.com/assets/968267/12765011/d181bc06-c9fc-11e5-9d7f-e08d66add4c6.png"> 
jbergstroem		The tricky part is also that we have different access levels. Collaborators in `nodejs/node` shouldn't be able to reload jenkins for instance which means that the user we tie to this setup must be infra level. I'm not sure how much the token ACL affects what Jenkins cares about seeing how the ACL in Jenkins mostly handles their own permissions based on user/group lookup. 
joaocgreis		The shared user @nodejs-ci is used for jenkins to access the temporary repo. It is controlled by the ci@iojs.org email (https://github.com/nodejs/email/pull/2) and the password and key are in the secrets repo. The key to be considered from there is `ci-iojs-org-id_rsa-github`, there is another key `ci-iojs-org-id_rsa-slaves` but it is only used for the cross compile server and will be deleted soon.  Feel free to use the credentials in secrets to create tokens. Right now the key is only for the temporary repo and compile server, but it can be moved to infra level if needed. 
jbergstroem		While doing the backup stuff I revisited this. What's actually required is 'roles' so jenkins can see that you are in the right group, but that's the only permission it needs. 
jbergstroem		I think we have everything in place with regards to tokens and apps to share access. Closing. 
gibfahn		Hmm, this is still happening, https://github.com/nodejs/node/pull/10517 (https://ci.nodejs.org/job/node-test-commit/6915/) for a CI run that's still in Jenkins.
jbergstroem		Can confirm. The jenkins script for notifying is likely to blame and I've looked at it a few times; can't quite figure it out. Still on it.
joaocgreis		Should be fixed. It was running the two scripts (for success and failure) unconditionally, so it would mark it as success and immediately mark it as failure. I tested with a success, I don't have a PR that fails to test.
joaocgreis		AIX was reporting success when the job fails, so I changed it as well but did not test (cc @mhdawson)
mhdawson		To test on AIX  is what we we need a PR that fails on AIX, and we run the CI job with the results posted to the PR and we then validate that it shows as failed ?  
gibfahn		I've confirmed that the AIX reporting is fixed.   | Before/After Fix | PR | CI Job | PR Reported | CI Reported | | --- | --- | --- | --- | --- |  | Before | [11852](https://github.com/nodejs/node/pull/11852) | [8483](https://ci.nodejs.org/job/node-test-commit/8483/) | üíö  | üî¥  |  | After | [9163](https://github.com/nodejs/node/pull/9163) | [8493](https://ci.nodejs.org/job/node-test-commit/8493/) | üî¥  | üî¥  |  | After | [11856](https://github.com/nodejs/node/pull/11856) | [8495](https://ci.nodejs.org/job/node-test-commit/8495/) | üíö  | üíö  | 
joaocgreis		@gibfahn thanks!
mikeal		I am +1 on this happening. However, I'm not familiar enough with the technical bits here to do a proper "review."
rvagg		@mikeal this is done now and is working, prefer the https version when handing it around or linking, sorry it's taken so long when it's actually very simple!
rvagg		I think we can just reuse the machines we use for general test runs, it's basically the same thing, see https://ci.nodejs.org/job/smoker/ for an initial attempt at putting citgm in there, feel free to take that and make it betterer and add additional test hosts. 
jbergstroem		@mhdawson whats the status on these machines? 
mhdawson		The last comment was that we could just use the machines that are used for general test runs.  ie no special machines would be required.  
jbergstroem		Thinking `iojs-softlayers-benchmark` is suitable for this? If so, are we done here? 
jbergstroem		Oops, wrong button. 
mhdawson		I think the suggestion was that it could just run on any of the existing machines. Unless there is disagreement on that point then we could just close this.   The main objection I could see is if the test will run for a long time. 
mhdawson		Based on suggestions I'm going to just close this 
bnoordhuis		Just curious, how are you going to deal with floating patches? 
jbergstroem		@bnoordhuis my view is that we should respect abi and not apply floating patches without a high amount of resistance. I for instance PR'ed removing building against a shared c-ares for this reason, but optimally we'd upstream patches and let them flow through there before patching them locally. I wouldn't really see this as a build problem, rather a problem with how the iojs WG/TC handles patches in `deps/`. 
bnoordhuis		Most floating patches aren't ABI breaks, they're bug fixes.  c-ares is an exception because upstream is no longer actively developed. 
jbergstroem		Regardless, the bug fixes should go upstream no? Lets run through them: - libuv: you and saghul. I'll assume that whatever we patch we want upstream - http_parser: indutny and more. Same thing? - openssl: I doubt we'll accept floating patches here that will affect shared vs bundled (for instance the cli patch that we might land to improve windows buildbot status). - v8: we might float for a version, but everything we've done so far afaik is upstreamed.  I think we should let whatever bug we fix in tree fail until it lands in upstream. With that said, having it part of the incremental build might not be what we want. We could definitely start by having it part of `iojs+any-pr+multi` until we end up getting discrepancies. 
bnoordhuis		I don't think it's that clear cut.  To take that OpenSSL CLI patch as an example, it fixes an issue for us but it's not the kind of patch that upstream will gladly (or ever) take.  Now what?  Also, I think you're overlooking the LTS aspect.  If we commit to longer-term support, we effectively become the maintainers of dependencies with a shorter support cycle, like V8.  We'll be carrying an ever increasing number of patches.  > I think we should let whatever bug we fix in tree fail until it lands in upstream.  You mean wait until an upgrade pulls in the bug fix instead of applying it ahead of time?  I speculate you're not going to find much support for a proposal like that, people like their bug fixes. 
mhdawson		I agree that we should aim to have all floating patches upstreamed, but particularly in the LTS releases that won't always be possible.  V8 is a good example were we can only contribute to bleeding edge, and beta/stable, and in terms of stable for example it would likely have to be relatively important to get it.  So we'll have to handle the case were we need to float a patch on the V8 version that is in the LTS stream while at the same time making sure that we contributed it back so that we don't have to float in later versions. 
jbergstroem		> @bnoordhuis > I don't think it's that clear cut. To take that OpenSSL CLI patch as an example, it fixes an issue for us but it's not the kind of patch that upstream will gladly (or ever) take. Now what?  It only went in because it didn't change the api or abi, just how we use the openssl cli to test on windows. I'm not saying that we should see this as a case-by-case, but I don't really see a scenario where we'd allow floating patches on top of things like zlib or openssl. If we look at what's been going on in `deps/`, we rarely touch anything but npm or v8 between upstream releases.  > @bnoordhuis > You mean wait until an upgrade pulls in the bug fix instead of applying it ahead of time? I speculate you're not going to find much support for a proposal like that, people like their bug fixes.  Yeah ‚Äì you're right. That's why I also think that down the road (or perhaps earlier) a shared builder is more about release conformance than testing every revision/branch. Perhaps toss it in when rc's are cut? Part of what I'm trying to accomplish is to let all packagers know we care about how they've chosen to use our configure options.  > @bnoordhuis > Also, I think you're overlooking the LTS aspect.  >  > @mhdawson  >  V8 is a good example were we can only contribute to bleeding edge, and beta/stable, and in terms of stable for example it would likely have to be relatively important to get it.  I agree. V8 is also one of the dependencies we don't allow building against a shared library because of its unpredictable development.  Just to be clear here: I'm talking about a shared openssl, zlib, libuv, http-parser and ‚Äì as of recent ‚Äì icu. I did a survey some months ago and found that pretty much all third party packagers (os distributions, package managers, ..) uses a shared zlib and most also a shared openssl. After that it diminishes somewhat, but there are occurrences of libuv. Actually, npm is also a common decouple. 
jbergstroem		I'm starting to feel that we need better coverage on `--debug` than just another bot. It currently fails to build on SmartOS. Perhaps another job that runs debug on all slaves every now and then? 
bnoordhuis		It's not a bad idea but there are probably going to be a lot of spurious failures because debug builds are so slow.  I guesstimate that a run on the rpi 1 will take three or four hours, maybe more. 
jbergstroem		@bnoordhuis yeah, it's brutal. We're unavoidably moving towards more build groups anyway (as in, different jobs for different type of problems; PR, pre-release, etc) -- so having debug part of something we run occasionally is most likely what we want. 
jbergstroem		Update: we now have a debug builder: https://ci.nodejs.org/job/jbergstroem-test-commit-debug/  This will be tested for a while and then integrated as a part of a recurring test runner. I aim to add freebsd and smartos vm's in there as well. 
orangemocha		We could also add a parameter to node-test-commit. 
rvagg		Came up recently @ https://github.com/nodejs/node/pull/16432, http2 debug builds not compiling at all and we didn't notice until after it was pushed out in a release
rvagg		Experimenting with shared library linking @ https://ci.nodejs.org/job/node-test-commit-linux-linking/nodes=ubuntu1604_openssl110_x64/3/ for testing the upcoming OpenSSL 1.1 support @ https://github.com/nodejs/node/pull/16130  Extends https://github.com/nodejs/build/pull/992 and adds a Docker container that downloads and compiles openssl 1.1.0g and puts it in `/opt/openssl`. Then in Jenkins it does like the FIPS job and does a custom `./configure` to link followed by `make test-ci`.  I'm thinking that we could expand this and put a bunch of shared libraries in /opt/ and have a pool of these containers that we pull in and run them all as part of the normal build & test process. Add in zlib and cares and we'd have good coverage.
maclover7		This is largely solved via https://ci.nodejs.org/job/node-test-commit-linux-containered, closing for now
maclover7		@mcollina Is this still needed?
mcollina		This might not be needed anymore, but I'll confirm this asap.
maclover7		Closing due to inactivity, please reopen if still needed
mcollina		can you please keep this open? thanks. I have very limited connectivity at the moment and GitHub does not load.  Il giorno mar 19 giu 2018 alle 23:47 Jon Moss <notifications@github.com> ha scritto:  > Closed #655 <https://github.com/nodejs/build/issues/655>. > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/655#event-1689929156>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AADL4188ZF0xVFPbQlq-0-Qts5gFZ_k6ks5t-Y2sgaJpZM4Mg48i> > . > 
mhdawson		I think it would be cool to have some stickers :)  We'd probably have to get the ok from somebody in the foundation for the design. 
gibfahn		I can't say we need this, but definitely want that sticker.   `<bikeshedAlert>`Maybe less text? Something like `Node.js Build WG`?`</bikeshedAlert>` 
MylesBorins		I'd be into something super minimal... something we can put on a hex sticker. What about just the gears or the shovel? 
jbergstroem		@gibfahn we could always make the text optional if stickers would be a reality. I just saw the symbol/text as a way to consolidate a readme/etc. 
jbergstroem		I'll prepare a hex shovel, the symbol and the text for use if we end up doing it! 
bnb		Love this idea! Would be super fun to start getting NodeSchool-esque logos for each WG. They have a [HexDex](https://nodeschool.io/hexdex.html) with all the logos - maybe the Org could do something similar, starting with the Build WG? 
jbergstroem		Well, our shovel aint got shit on those hexdex:es -- I'll see what I can do :) 
rvagg		@jbergstroem get us a hex ASAP and I'll get some printed before Node Interactive, I have some other hexes to print as well so may as well do it at the same time. Vector would be nice of course. 
MylesBorins		@jbergstroem I've made some hexes before so let me know if you want some support  edit: sticker spec for reference https://github.com/terinjokes/StickerConstructorSpec 
jbergstroem		@TheAlphaNerd no issues with dimensions; bigger issues with skill. 
jbergstroem		[wip.pdf](https://github.com/nodejs/build/files/606720/wip.pdf)  Here's the PDF is someone wants to chip in.
mikeal		Can someone send an email to the trademark alias to get this approved as derivative :)
mhdawson		A good opportunity to print them https://www.stickermule.com/ca/p/831c7240  $19 US
maclover7		ping -- should this stay open?
bnb		@maclover7 this issue has been referenced in discussion around https://github.com/nodejs/community-committee/issues/212 - I don't think it needs to stay open for that discussion to move forward, but just a heads up üòä 
gibfahn		>ping -- should this stay open?  What do you think? My view is that having a logo would be good, so it should stay open until we actually have one. But maybe we don't need one (in which case we should agree that and close it).  Maybe this should be on the wg-agenda for the next meeting if it's been forgotten. Probably needs a champion though (someone to work out next steps).
gdams		I'm going to try and re kindle the fire on this issue. I personally feel that having our own logo would be a great idea and as @mhdawson has mentioned, it would also be a good opportunity to create stickers which the WG members could have. It ties in nicely with some of the other discussions about how to raise awareness of the build WG and how to give them more recognition in the foundation.
mhdawson		I've mentioned it as a possible first one for the https://github.com/nodejs/badges initiative.  Having said that I think I could just ask for approval for the logo suggested originally if we all agree its good.
Trott		Anyone who wants should feel free to pursue this, but I don't think we need this as an open issue in the tracker at this time.
mhdawson		More of an FYI - https://github.com/nodejs/benchmarking/issues/18, please comment if you think we need a bigger machine.  My plan would be to onboard it just like any other machine in the CI.  Since I've now connected to the right people and have the ok to bring softlayer resources to the Node community, the other thing I wanted to discuss is whether we need additional x vms for the build infrastructure.  @jasnell mentioned wanting one to run the citgm regressions tests regularly so I'll co-ordinate with him to get the specs and bring that on-line and I could provide a few others from softlayer if we have a need.  
orangemocha		Ok, confirming our meeting for tomorrow.  **Meeting Time:** 8PM Tuesday August 29th UTC, or in your local timezone: http://www.timeanddate.com/worldclock/fixedtime.html?msg=Node.js+Foundation+Build+WG+Meeting&iso=20150929T20&p1=%3A&ah=1  **Hangout**: https://plus.google.com/hangouts/_/hoaevent/AP36tYfm47NYzCgUWeErtz5lpLlvMqM0yzG6URwoVdwEwQcA_2hwqQ?hl=en  **YouTube (view-only!)**:  http://youtu.be/Qo05XA9LxOc  **Minutes** (give me your google login if you don't have edit access):  https://docs.google.com/document/d/1pYpM_9LkhrvyUTOQZei1jdS1Y55RW7Z_i5STCWxzQM4/edit 
rvagg		Thanks for taking this @orangemocha, very much appreciated, I'm not sure I'll be able to make it because it's midnight here and I still haven't finished urgent work I need to get done for core.  One potential agenda item is https://github.com/nodejs/build/issues/164 re getting 0.10 and 0.12 builds out‚Äîis there anything else that's not on that list that we need to do and how can we chip away at the list to get some builds out asap? 
mhdawson		Updated link to watch http://youtu.be/Qo05XA9LxOc 
mhdawson		Seems like a reasonable idea to me.
ofrobots		Thanks for writing this up / working on this. It would be great to have a canary build of Node.js available so that we can flow node canary feedback to the V8 team before the V8 dev branches solidify into the beta/stable branches.  A question about the work-flow you proposed: how would the cherry-picking from `canary-base` work? It sounds like this would be an automated process. How would the tool know what commits should be cherry-picked?
bmeurer		I like this idea a lot! Looking forward to Node Canary.
hashseed		Looking forward to this!  Does it make sense to check in part of update-v8 into V8 itself? For the file list of dependencies it might make sense?
hashseed		I wonder whether it makes sense to port the update-v8 script to python. Requiring Node.js to be able to update Node.js feels weird. Especially if we are looking into checking it into V8 to be able to keep it in sync with V8's version: V8 would have a circular dependency on Node.js.
targos		@hashseed   I don't speak python. That's why I wrote it in JS in the first place :).  > Requiring Node.js to be able to update Node.js feels weird  It doesn't feel so weird to me. We already need Node.js to be able to lint Node.js with ESLint.  > Does it make sense to check in part of update-v8 into V8 itself?  I think it would make a lot of sense to have the list of dependencies in V8 (ideally as a JSON). It is currently here: https://github.com/targos/update-v8/blob/master/src/constants.js#L14-L49 I'm not so sure for the rest of the script. It is more coupled to Node than V8 (directory structure, commit messages). I don't know if other V8 users could benefit from it.
targos		@ofrobots   > How would the tool know what commits should be cherry-picked?  All the commits after the last V8 update. Something like: ``` git cherry-pick `git log canary-base -1 --format=format:%H --grep "deps: update V8"`...canary-base ```
gibfahn		We should try to get this up and running in the Collab Summit.  with @piccoloaiutante @targos @fhinkel   _**EDIT:**_ Will update with progress.  ### Subtasks:  - [x] A nightly job to update the [canary](https://github.com/nodejs/node-v8/tree/canary) node branch to the [Last Known Good Revision branch of V8](https://github.com/v8/v8/tree/lkgr) and then run `node-test-commit`.     -  [node-update-v8-canary](https://ci.nodejs.org/view/All/job/node-update-v8-canary/) will update [nodejs/node-v8:canary](https://github.com/nodejs/node-v8/tree/canary) and then run a `node-test-commit` equivalent: [node-test-commit-node-v8](https://ci.nodejs.org/job/node-test-commit-node-v8/) - [x] A trigger to the release CI to build the canary branch every night.     - Added `H H(0-7) * * * ` to the [node-update-v8-canary](https://ci.nodejs.org/job/node-update-v8-canary/) job, it runs every night.
targos		The nightly test job works fine and allowed to find and fix quickly some issues. [Latest run is almost green](https://ci.nodejs.org/job/node-test-commit-node-v8/30/) (known issue on FreeBSD: https://github.com/nodejs/node-v8/issues/1, https://github.com/nodejs/build/issues/723).  @gibfahn would you like to help me setup the nightly build?
gibfahn		>@gibfahn would you like to help me setup the nightly build?  So this would be adding a trigger to the release CI job to upload builds every night? I can definitely help with that, but I'll add this to the Build WG agenda tonight to make sure there are no objections.
targos		Yes. I'm thinking about a new https://nodejs.org/download/v8-nightly/ similar to https://nodejs.org/download/chakracore-nightly/
rvagg		fyi I have this on my plate, still working on it, it's not a trivial task unfortunately because it also involves the `Makefile` in core. In the meeting we talked about using "v8-canary" since it roughly aligns with the V8 in Chrome Canary. I'm fine with either that or "v8-nightly". Can we get a consensus here on naming?
gibfahn		I prefer `v8-canary`, I think it better conveys that it's V8 canary and Node master/nightly.
targos		v8-canary is fine for me
kfarnung		+1 on `v8-canary`.  `chakracore-nightly` tracks a relatively stable version of ChakraCore (just like normal node nightly builds).
piccoloaiutante		+1 for `v8-canary` also for me
rvagg		OK folks, here it is https://nodejs.org/download/v8-canary/  I did some pretty heavy refactoring of the Jenkins iojs+release job on ci-release to make this work nicely but in the process I cleaned up a bunch of old cruft and made it easier to trigger.  I haven't automated this yet, I'll need to do some small tweaks to nightly-builder to handle this nicely, that shouldn't take much effort though.  I wanted to get y'all thinking about how the messaging for this might go‚Äîusing these builds is going to be a huge mess if you're using addons. They may compile but you're not going to get ABI stability, even across v8-canary versions, and it could potentially break every 6 weeks. So perhaps we could think about a blog post that describes what this is, why you might tinker with them and what kind of breakage you should expect and how to deal with that.
rvagg		``` $ node -p process.versions { http_parser: '2.7.0',   node: '9.0.0-v8-canary20170609cd40078f1f',   v8: '6.1.101',   uv: '1.12.0',   zlib: '1.2.11',   ares: '1.10.1-DEV',   modules: '58',   openssl: '1.0.2l',   icu: '59.1',   unicode: '9.0',   cldr: '31.0.1',   tz: '2017b' } ```
rvagg		To add to what I said above about messaging‚Äîyou may not want to promote this too much, I can see people getting much more tangled in this than they would in our slower and more controlled `master` process that leads to `nightly` builds. Let's have a talk about limiting uninformed user problems first.
rvagg		https://twitter.com/mathias/status/873204105161961473  So much for taking it gently
mathiasbynens		My apologies, @rvagg ‚Äî I was unaware of your request to ‚Äúnot promote this too much‚Äù when I posted that.  I‚Äôd assume those opting in to canary builds know what to expect, analogous to Chrome Canary: you‚Äôre giving up stability for a more bleeding-edge feature set.  I agree that a blog post would help clarify the difference between nightly and canary builds. üëç
rvagg		Yeah, no biggie, I'll try and find time soon to draft up a post for you all to review 
rvagg		PRs to support this in the dist tooling we use:  * https://github.com/nodejs/nodejs-dist-indexer/pull/4  * https://github.com/nodejs/nodejs-nightly-builder/pull/1  * https://github.com/nodejs/build/pull/757  They're active on the server now and we have working [index.tab](https://nodejs.org/download/v8-canary/index.tab) and [index.json](https://nodejs.org/download/v8-canary/index.json) files and there's a cron job in to trigger a job for this every day _if_there's a new commit on nodejs/node-v8/canary. One's running now as I'm writing this and will show up eventually soonish.
targos		@gibfahn I have a problem with the `node-update-v8-canary` job on one of the fedora boxes. It fails consistently because "a cherry-pick or revert is already in progress": https://ci.nodejs.org/view/All/job/node-update-v8-canary/78/console I cannot reproduce this locally and when the job runs on the other box, it works fine. I don't understand how this can happen because we do `git reset --hard origin/master` before the update and I think that should abort any cherry-pick that is in progress...
gibfahn		@targos odd, it looks like something got messed up in the workspace. I cleaned it out and reran, and now I'm getting: https://ci.nodejs.org/view/All/job/node-update-v8-canary/80/console  ```bash  > git fetch --tags --progress git@github.com:nodejs/node.git +refs/heads/*:refs/remotes/origin/*  > git rev-parse refs/remotes/origin/canary^{commit} # timeout=10  > git rev-parse refs/remotes/origin/origin/canary^{commit} # timeout=10  > git rev-parse origin/canary^{commit} # timeout=10 ERROR: Couldn't find any revision to build. Verify the repository and branch configuration for this job. ```  Shouldn't it be looking for `canary-base` in nodejs/node?
targos		> Shouldn't it be looking for `canary-base` in nodejs/node?  Indeed. I changed the config. Trying a new run: https://ci.nodejs.org/view/All/job/node-update-v8-canary/82/console  Edit: back to normal. Thank you!
gibfahn		>Trying a new run:  Looks like that went better.  LMK if it happens again, I had a look on the machine and none of the directories seemed to have a cherry-pick in progress.   If you have the ability to see the `Wipe Out Current Workspace` button in the [`Workspace` tab](https://ci.nodejs.org/view/All/job/node-update-v8-canary/ws/#) of the job, I recommend wiping and rerunning as a first resort if something starts going wrong. Things seem to get corrupted fairly often in Jenkins.
targos		The daily update/test and nightly build work well. I think this can be closed. Thanks everyone for the help!
targos		Ah! We still haven't posted anything on the blog about this. @rvagg would you still like to do this draft?
aalexand		Would it be possible to publish the headers for the canary build? Currently trying to install a module with C/C++ code gives an error like below.  ``` $ npm link ../nodejs-profilers/  > profiler-min@1.0.0 install /some/path/nodejs-profilers > node-gyp rebuild  gyp WARN install got an error, rolling back install gyp ERR! configure error  gyp ERR! stack Error: 404 response downloading https://nodejs.org/download/v8-canary/v11.0.0-v8-canary201807140f69779e03/node-v11.0.0-v8-canary201807140f69779e03-headers.tar.gz ```
rvagg		In addition to this FYI: I'm keen to retire the ARM8 resources in my garage, they were only early prototypes of the final AppliedMicro X-Gene machines so they are not optimally set up, they draw more power than they could be, the fans are noisy and they take up a bunch of space on my "rack". The new test & release resources essentially make them redundant already, if all goes smoothly from here, but they do add Ubuntu 14.04 coverage to our ARM8 resources. IMO this is not critical at all, with CentOS 7 we do even older now. The ARM8 machines in my garage do serve some auxiliary purposes at the moment, including acting as a jump-host for SSH access to the cluster and serving the NFS shares that the Raspberry PIs currently use. I'm in the process of repurposing an Intel NUC that I have laying around that's not doing anything useful to serve these jobs so it means I might be able to turn them off at some point in the near future.  packet.net have FreeBSD and Debian on their roadmap for ARM8 so we should get even more coverage soon. They have Core OS, so I guess we could even be testing on some fancy Docker setups, I'm not sure what that would be that would add value though.  Additionally, Scaleway, an ARMv7 provider that sponsors a couple of test machines and a release build machine, have branched out in to ARM8 and reached out to me recently, so we'll probably be expanding some redundancy and additional distro coverage with them too!
rvagg		:+1:  
jbergstroem		Made it so f464251d3d17e361f4bbeadb61a1873668eca6e8. 
rvagg		see https://github.com/nodejs/build/blob/server-resources/doc/server-resources.csv for rendered version, quite nice 
Starefossen		Great work on this Rod!  Does the CSV spec require the file to be all on one line?  Also, there is no Mac OS X 10.12 version (yet). 
rvagg		thanks for picking up 10.12 @Starefossen, an Excel error.  re it being on one line, no there is nothing dictating that, it's just that I'm using Excel on OSX right now so if you're seeing it as one line then that'll be a linebreaking thing, I'll try to make it betterer 
Starefossen		Never mind the one line thing, I was just GitHub's view acting up that wouldn't let me comment on the Mac OS X version thingy here: https://github.com/nodejs/build/pull/120/files. 
jbergstroem		Looks good! The only feedback I have is that a markdown table would be more readable when editing (unless Excel) - but I fear that'd render much worse on github. 
rvagg		@jbergstroem yeah, Github is going to cap the width of it unfortunately, I do like the idea of it being readable as text though  another alternative would be to use CSV but space it out so it's easily readable as text too, I'm just not sure how GitHub is going to render it with extra spaces 
rvagg		Check that out, best of both worlds? https://raw.githubusercontent.com/nodejs/build/server-resources/doc/server-resources.csv the GitHub formatter strips extra padding whitespace.  Also, this is why I love Node:  ``` js > csv2 = require('csv2') > columnify = require('columnify') > listStream = require('list-stream') > fs.createReadStream('./doc/server-resources.csv')   .pipe(csv2())   .pipe(listStream.obj(function (err, list) {     fs.writeFileSync('doc/server-resources.csv',       columnify(list, {         dataTransform: trans = function (data) { return (data.indexOf(',') > -1 ? `"${data}"` : data) + ',' },         headerTransform: trans, showHeaders: false }))   })) ``` 
jbergstroem		looks great! columnify is my new best friend. 
jbergstroem		Suggesting we land this. 
rvagg		holding off till I update the rackspace ones, we're missing some non-windows servers there 
jbergstroem		Lets bring this up to date and land it. 
jbergstroem		In some distant dream I was hoping to be able to generate this as part of the ansible setup stack. This comment might remind our future selves to do just that. 
jbergstroem		How about we write a playbook for the refactor that just connects to each host and returns minimal facts for each? (cores, ram, disk) 
BridgeAR		Should this stay open? There was no progress for more than 1.5 years now.
gdams		I think that https://github.com/nodejs/build/blob/master/ansible/inventory.yml contains most of this information anyway
jbergstroem		Not sure what's going on here (tried building and got the same issues) -- restarting jenkins slaves and trying again seems to fix it. Great.. I'll close but please reopen if you see the same issue again. 
MylesBorins		I don't even see sunos as a machine in the ci-release infra  /cc @jbergstroem 
gibfahn		Looks like v6.9.1 and v4.7.1 were the last releases to have x64, none of the v7 ones do.  So missing on:  - v4.6.2 v4.7.0 v4.7.1   - v6.9.2 v6.9.3   - v7.0.0 v7.1.0 v7.2.0 v7.2.1 v7.3.0 v7.4.0 
jbergstroem		smartos machine are added with respective smartos-XX tags. 13 is used for 0.x (pre-1.x tag) and 14 for 1.x and forward (post-1.x tag; subject to change as of v8 55 -- using smartos15 jenkins tag for unknown reasons). They are later matched with [OSTYPE=solaris](https://github.com/nodejs/build/blob/master/setup/smartos/resources/jenkins_manifest.xml.j2#L21). So, I'm not sure why that conditional isn't triggering.
joaocgreis		The conditional was triggering fine, but there was a subtle error in the x64 portion of the script (line brake "`\`" with a space after it). Should be fixed now: https://nodejs.org/download/test/v4.7.1-test201701051035318070/ . Please reopen if not.
rvagg		Check the -tar slave and see what it's doing, e.g. https://ci-release.nodejs.org/job/iojs+release/475/nodes=osx1010-release-tar/consoleFull for 475, I think they might be switched and perhaps it was my fault. Will look into it shortly. 
rvagg		Ah, it's because the job it does depends on the OSTYPE env var on the machine so it's always running the tar job on the tar machine and pkg job on the pkg machine even though they both have both labels so it can look like it's switched around but it's consistently the same machine. I need to change it so that the job it does depends on the label the salve is used as instead.  But in summary, it still should be building properly, you just won't necessarily find the build job listed under the right osx slave. 
jbergstroem		I confirmed all artefacts being there as well; the logic is just a bit off. 
rvagg		``` rsync jenkins@ci.nodejs.org:/var/lib/jenkins/ /var/backups/jenkins/ \   -avz --times --perms --delete --exclude='workspace*/' --exclude='archive/' ```  I haven't actually put this in crontab yet, I _intend_ to, I've run it manually on the main webserver which has plenty of space (nodejs.org). 
orangemocha		Related thought: it would be nice to have the scripts config in source control. 
rvagg		+1, since nobody but admins can actually see, critique or suggest changes to them .. lots of xml though! 
jbergstroem		I think there's too many moving bits at the moment to have them committed. If we look back at our review pace we'd be waiting to get stuff done. Improve review pace or decrease tinkering pace? 
orangemocha		True, it's not that simple. We would almost need a development/staging jenkins to test the changes, spit out the scripts, then review those for merge into the repo and the production jenkins. I don't think that's a priority right now.   I plan to look into secondary masters to improve the availability, maybe at that point I will look into this workflow.  For now, it would be great if they could just be published somewhere (GithHub?), even if manually. 
jbergstroem		As of writing the backup is `~50G`. Digitalocean takes 4 rolling weekly backups of the vm. Perhaps set up a vm somewhere running `rsnapshot` with ~300G space? should cover us for a 7d, 4w, 6m routine. I guess we could also ask ourselves if we need logs that old ([there's a plugin for that](https://wiki.jenkins-ci.org/display/JENKINS/Discard+Old+Build+plugin)). 
orangemocha		Do you know what takes that much space? Is it mostly logs? I think the logs are not that critical, but the configuration/scripts are. It probably makes sense to treat them separately and have more up-to-date backups for the configuration only.   Ideally, I'd like to be able to replicate the configuration to a secondary master for improved availability. 
jbergstroem		It's pretty much only logs (99% of size) 
rvagg		I've just put this in the crontab on the nodejs.org webserver:  ``` */15 *  * * *   root    rsync jenkins@ci.nodejs.org:/var/lib/jenkins/ /var/backups/jenkins/ -aqz --times --perms --delete --exclude='workspace*/' --exclude='archive/' ```  perhaps we could just use it as a backup Jenkins? Have it installed and ready to start at a moment's notice and just change DNS to make the slaves point to it. Although I'm not convinced we need to be so prepared for this, being down for a few hours once in a blue moon isn't really a big deal. Just spinning up a new server, rsyncing those files onto it and going from there is probably good enough. 
jbergstroem		I'm ok with failover -- the current environment works well enough; just want to figure out the occasional gateway timeouts. Also, reducing how log we store logs would improve quality of life handling jenkins (cold storage?)  
orangemocha		Well, the master is a single point of failure right now and those outages are not so rare.  > Have it installed and ready to start at a moment's notice and just change DNS to make the slaves point to it.  I was thinking more of a server that would be always up and usable, like ci2.nodejs.org. That way there is no need to do anything specific to failover, and runs from each master their own unique URLs. 
orangemocha		It certainly sounds reasonable not to store logs forever. And if the backups for the logs are not so frequent I think that's ok too. 
rvagg		> I was thinking more of a server that would be always up and usable, like ci2.nodejs.org. That way there is no need to do anything specific to failover, and runs from each master their own unique URLs.  We'd have to move the slaves off using slave.jar to using ssh to make this workable, we have a constraint on running multiple instances of java on the smaller Pi's. I'm concerned about @jbergstroem's reports of finding a heap of zombie ssh-agent processes on the Jenkins server recently since they are likely from the ssh slaves that have been recently added (Windows I think). 
orangemocha		> we have a constraint on running multiple instances of java on the smaller Pi's  I see. @joaocgreis believes that ssh solves the disconnect issue (we don't have have it set up for the Windows slaves actually, and that's why those machines are disconnecting so often).  So this seems worth pursuing.. hopefully the ssh-agent zombie side effect can be addressed and we are not trading one issue for another :) 
jbergstroem		I killed all agents and restarted jenkins yesterday. Haven't seen any stray `ssh-agent` since. 
jbergstroem		So, I identified the issue with `ci` being the DO backup by comparing kernel stack trace timestamps with the DO backup window. We've now disabled DO image backups since we backup our jenkins anyway.  
jbergstroem		I've now enabled backups to the new ci server (well, from `iojs-www`), as well as renaming the old backup (lives in `/var/backups`). I also took the liberty of running one. Closing since we have a running backup. Reopen if I missed something. 
joaocgreis		SGTM. Do we also have fails in `arm-fanned`? I've only seen them on `arm`.  He's already on the Build WG and his work is related, I'm ok with permanent access.
joaocgreis		cc @nodejs/jenkins-admins 
gibfahn		LGTM (not sure if my vote counts...)  +1 to making his access permanent, I'd have thought the github bot would require more access in the future, and given that @phillipj is already in the build team, I think we can trust him üòÑ .
Trott		+1 
thefourtheye		+1
jbergstroem		@phillipj you cool with getting elevated access and working full-time as jenkins admin? üòÑ 
thefourtheye		![](http://i3.kym-cdn.com/photos/images/facebook/000/933/852/18f.jpg)
phillipj		> @phillipj you cool with getting elevated access and working full-time as jenkins admin? üòÑ  Full-time? Sure, who needs a paid job anyway..  In all seriousness, I cannot promise to do more than github-bot related things ATM. After hopefully fixing what is needed for ARM and citgm, I'm fine with the access being revoked if that sounds best for the current admins.
mhdawson		+1 from me.
jbergstroem		Cool; @mhdawson can you add him to the group?
mhdawson		@jbergstroem added.  
phillipj		Thanks!
jbergstroem		Assom - closing. Ping me if you need help/assistance. It might be good to copy jobs when you start modifying even though we have the config history plugin.
rvagg		done, now live on iojs.org with webhook active for the iojs.github.io repo 
Trott		o/ 
mhdawson		I'll be there. 
mhdawson		Collaboration summit came and went so closing this.
Trott		Hard to say. If someone bookmarks, do they want the specific version they're looking at or do they want the bookmark to always go to the latest version? I'm inclined to think the latter, but I don't really know.
Trott		I'm going to close due to long period of inactivity. Feel free to re-open if this is something that should be further discussed. Maybe ask that it get put on the Build WG agenda to make sure it doesn't get lost like it seems to perhaps have been the first time. Anyway, I'm not closing because this is a bad idea or anything. Just closing things that have been inactive for more than a year or so.
jbergstroem		I think the "issue" with `jenkins-admins` is that the majority of that groups inherits ownership as part of other groups, such as the TSC. The idea was to have a small circle have "full" access to jenkins since having that gives you access to things like ci-release and similar.   I think one way forward would be elevating control for the build group so they can configure jobs in CI; we just need to check that it doesn't violate any security limits we have (for instance node-private jobs that are related to security). 
gibfahn		@jbergstroem Does that mean that `jenkins-admins` should be a subset of [nodejs/release](https://github.com/orgs/nodejs/teams/release)?   I agree saying that `build` != `jenkins-admins` is a good idea, it means we can have more people in the build group! 
jbergstroem		@gibfahn `nodejs/release` have access to ci-release but cannot modify the jobs. `jenkins-admins` are the highest level of access we have.  Edit: the idea being that the release group can create releases but not necessarily meddle with infra. 
gibfahn		@jbergstroem Is there anything I can do to help out with this? Maybe check which jobs are node-private? 
jbergstroem		@gibfahn I'd hate for this to sstall progress, so I'll just add you to all citgm jobs; but we should probably still figure out userlevels in jenkins. We also have finer grain control over things like libuv so their team can access it, so it would make sense. 
mhdawson		I'm thinking we could do something along the following lines:  1) when necessary for a particular WG, set so that they can control who can run/configure etc along the lines of what we did for libuv 2) Ask that the follow something along the lines of: https://github.com/nodejs/build/blob/master/doc/process/special_access_to_build_resources.md in terms of ensuring a level of trust in those that are given access and for how long.  If necessary it might even include an audit after jobs have been created to sanity check that  they are not doing anything unexpected. 
mhdawson		I'll add I can see this being useful for the benchmarking WG as well. 
gibfahn		I know that @nodejs/citgm were interested in this as well (for giving non-build members access to the citgm jobs). 
mhdawson		@jbergstroem if you think what I've suggested could make sense, I'd volunteer to write up a first cut at what we might document as the policy/requirements for granting people access to be able to  - run a specific job - be able to configure jobs in a particular category. 
jbergstroem		I think we can be more generous with providing access to specific groups for certain jobs. It will probably even make more sense if they are named consistently (citgm, libuv, etc).  When it comes to specific people to for instance test-commit or test-pr, that would probably benefit to similar documentation as what we have for temporary ssh access. 
gibfahn		Fixed by https://github.com/nodejs/build/commit/1fcbba2ae6242588a1f7cb779a859d8734213097
rvagg		perhaps you should just include a bare git mirror like we do for arm machines, located in /home/iojs/git/io.js.reference, when a clone is done, any refs found in there will be linked straight to it and any not found in there are pulled from the remote. I occasionally update the mirror as upstream gets more bloated. The reference location is specified in the jenkins job setup under advanced git options, or something like that. 
jbergstroem		I left my comment in the originating issue. I'd prefer the following triage: 1. Is `-mx256m` required to solve issues? 2. Can we test setting the timeout locally or even globally [still machine only] to confirm that's the actual issue? 3. What @rvagg said. 
mhdawson		Doing 1) now https://ci.nodejs.org/job/node-test-commit-aix/120/ build without 12m  @jbergstroem In respect to 2) I'm not sure what you mean.  The timeout is being set for AIX machines only, and having made the change once ruling out 1), we would have confirmed that it does resolve the issue.  Since its the git plugin timing out I don't think changing anything locally on the machine would have an effect although I could be wrong.  Looking at the configuration options for git config here https://git-scm.com/docs/git-config.html I don't see what looks like the right timeout (core.packedRefsTimeout sounds like something else). 
mhdawson		@rvagg which jobs uses the bare mirror ? I looked at https://ci.nodejs.org/job/node-test-commit-arm/ and I did find references to io.js.reference but maybe I just did not look hard enough.   Never mind found the right setting now :) 
mhdawson		Ok was fine without change to heap size so removed that.  Change pushed to pr. 
mhdawson		Ok found this:https://randyfay.com/content/reference-cache-repositories-speed-clones-git-clone-reference  So looks like I create the mirror using  git clone --mirror  https://github.com/nodejs/node.git   /home/iojs/git/io.js.reference  And I also found that "Path of the reference repo to use during clone"  is already set   Build with mirror to see how much faster it is: https://ci.nodejs.org/job/node-test-commit-aix/121/ 
rvagg		bingo, and occasionally you can `git fetch` inside it to update it, but I suspect that won't be needed for a while in this case 
mhdawson		I added the mirror but the total execution did not seem to be any faster.  I'm thinking the problem may not be the remote traffic but slowness of the disk I/O.  At this point I think extending the timeout still makes sense.  
mhdawson		@jbergstroem you ok with landing this now or was there something else I should try out first ?  
jbergstroem		@mhdawson sure, LGTM 
mhdawson		landed as c7e244bcc73b2dc373bab973ebd725255e487f4f 
jbergstroem		any reason we don't do this for every setup?
mcollina		The other setups already have that folder created.
jbergstroem		> @mcollina said: > The other setups already have that folder created.  Ah. We should probably move it to the jenkins role instead then. 
piccoloaiutante		@jbergstroem so creating another folder under `ansible/roles/jenkins-worker/tasks/partials` for this task and adding files only for ubuntu. Then adding this task to `main.yml` similarly to what has been done for `tap2junit` ?
jbergstroem		@piccoloaiutante I would probably just add it to `main.yml` in the jenkins-worker role seeing how it's a oneliner all workers should be doing.
piccoloaiutante		@jbergstroem should be good ok now.
gibfahn		I think we're good to land this, it's been sitting for ages and it keeps causing issues with releases.  Of course merging this doesn't mean the tmp directories will magically come back unless someone reruns the playbook on the machines.
gibfahn		Copying @rvagg 's comment from https://github.com/nodejs/node/issues/4531#issuecomment-168903077  > Thanks for reporting this @igorklopov, I'm a little surprised this went unnoticed for so long. I can confirm your analysis and I've traced it to the fact that we're using Raspbian to get gcc 4.8 on the machine we're using to compile these binaries even though it's a vanilla Debian Wheezy armhf base, and of course Raspbian is armv6 so we get `#define __ARM_ARCH 6` from the compiler... >  > Doh! >  > Which means we need to go back to finding a Wheezy armhf gcc 4.8(+) source or build one ourselves (which I think I did in the past but that's such a nightmare). > 
gibfahn		@xadillax are you still working on this (see https://github.com/nodejs/node/issues/4531#issuecomment-309050134)?
rvagg		:man_facepalming: this is technically stalled and it's my fault. I have a server running on scaleway with custom compiled 4.9 gcc packages that I was going to install on those other boxes. I stalled on trying to figure out _where_ to store these packages so they can be reused when reprovisioning the boxes, I don't really want to check them in to this repo.
piranna		Don't worry @rvagg, all we have our private lives and sometimes it gets busy :-)
XadillaX		@gibfahn Yes but slow progress due to my poor PI3.
refack		ping ya'll
mhdawson		@rvagg does it make sense to put those on the ci under the download user  like we are for packages needed for other platforms ? 
piranna		> @rvagg does it make sense to put those on the ci under the download user like we are for packages needed for other platforms ?  I think it would be a good idea. Would it be possible to also have downloads of binaries compiled with other standard C libraries beside glibc, like musl?
gibfahn		>> @rvagg does it make sense to put those on the ci under the download user like we are for packages needed for other platforms ? > >I think it would be a good idea. Would it be possible to also have downloads of binaries compiled with other standard C libraries beside glibc, like musl?  To be clear, @rvagg was talking about putting the gcc versions he compiled himself on https://ci.nodejs.org/downloads/. If you're talking about Node compiled with musl, then that'd be a different thing, and probably worth a separate issue.  Separate question, other than glibc and musl, are there any other C standard libraries in common use?
piranna		> Separate question, other than glibc and musl, are there any other C standard libraries in common use?  NewLib used to be fairly popular, and although everybody is moving to musl, I think DietLib and uLibC are still popular enough. There are some other ones, but this ones are the first that came to my mind.
piranna		For your info, I have at https://github.com/NodeOS/nodejs/releases compilations of Node.js with musl automatically detected, compiled and tested each time a new version is published, if someone is interested :-)
rvagg		I uploaded binaries for gcc 4.9.2 armhf here: https://ci.nodejs.org/downloads/armhf/ Built using Jessie source and a debian/control patch I inherited from somewhere, improved for when we had need for it on Wheezy Intel nodes and then modified again for armhf use. (I did this ages ago but they've been sitting on the dedicated Scaleway host I built them on).  4.9.2 is below our recommended minimum still but it works fine, so does 4.8, it's just more likely we'll run into trouble in future (apparently) so we'll just have to deal with that when we get to it and presumably CI will pick up any problems beforehand.  Latest test runs using this new compiler on the 2 test armv7 wheezy hosts are green @ https://ci.nodejs.org/job/node-test-commit-arm/nodes=armv7-wheezy/  So I've gone ahead and upgraded the release host with the same process and started a rebuild of the latest nightly @ https://nodejs.org/download/nightly/v9.0.0-nightly20171016e2015b5347/ (in progress, no cached object files cause of the new compiler so it's quite slow, dates on those files should show when they land).  This also means that any binary produced from ARMv7 from now on will be ARMv7-native, there's potential for breakage here although the only plausible scenario I can think of is that someone is using the ARMv7 binaries on an ARMv6 host and that's easily fixable (i.e. download the ARMv6 binaries instead!).  More info on how this is all set up @ #923
rvagg		OK, this is not so great, checking on the build that was produced on an ARMv7 Ubuntu 14.04 machine:  ``` $ ./node-v9.0.0-nightly20171016e2015b5347-linux-armv7l/bin/node -p process.config.variables.arm_version ./node-v9.0.0-nightly20171016e2015b5347-linux-armv7l/bin/node: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by ./node-v9.0.0-nightly20171016e2015b5347-linux-armv7l/bin/node) ```  I'm going to have to roll back those changes on the release machine for now until I find a path forward, but it looks like we get locked in to glibc++ 3.4.20+ with this change so we're probably broken for Wheezy and Trusty era Linuxes. That's not a _huge_ drama because Trusty on ARMv7 isn't that prevalent and Raspberry Pi OSs are all Jessie+ based now, but it's still going to break for a lot of folks out there.
refack		@rvagg is everything stable for the 9.0.0 release cycle (asking for a friend)?
rvagg		back to status quo at least (James' RC0):  <img width="1278" alt="screenshot 2017-10-18 23 07 52" src="https://user-images.githubusercontent.com/495647/31717822-3155173e-b459-11e7-9a9b-6c7900fb6820.png">  sucks to be back at the 6 but I don't have a solution yet so we'll have to roll with that. 
rvagg		Some more discussion in https://github.com/nodejs/node/pull/16848 related to this, pretty messy. I did some more work on getting a custom 4.9 compiled on Wheezy but the stdlibc++ problem is hard to escape. If V8 forces us to 4.9 then we're going to have to abandon Wheezy.  But perhaps that's not so bad since Wheezy is EOL at the end of May 2018 and Ubuntu 12.04 public LTS is done. Ubuntu 12.04 is in "Extended" till early 2019, that's for paying customers.  We can do this move for Node 10 and that'd work out fine I think for all of this, but the hassle we have now is that we're looking at bringing in V8 6.2 to Node 8 LTS.
ghost		Any progress on this?
rvagg		OK, so status update on this. I'm afraid that we're probably going to give up on making this work properly for Node 4 - 9 but the good news is that Node 10 is going to come with a new set of minimum supported Linux and compiler versions. This _should_ mean that we get to build Node on Debian Jessie which has newer toolchains available for it.  As of today, I've switched on a new ARMv7 release build configuration that is used for Node 10+, and this includes what's in `master` today. So nightly versions of Node from now on should be built with a native ARMv7 compiler rather than the one we had to pull in from Raspbian which is for ARMv6+.  The [v10.0.0-nightly2018033083d44bee01 nightly](https://nodejs.org/download/nightly/v10.0.0-nightly2018033083d44bee01/) has been rebuilt using this new toolchain, so if you grab the ARMv7 build from there you should see the right thing: https://nodejs.org/download/nightly/v10.0.0-nightly2018033083d44bee01/node-v10.0.0-nightly2018033083d44bee01-linux-armv7l.tar.gz  ``` > process.config ...   variables:    { arm_float_abi: 'hard',      arm_fpu: 'vfpv3',      arm_thumb: 0,      arm_version: '7', ...      host_arch: 'arm', ...      node_byteorder: 'little', ...      node_tag: '-nightly2018033083d44bee01', ...      target_arch: 'arm', ... ```  We haven't locked in final distro, libc and compiler versions for Node 10 but I'm pretty confident this is how we're going to go live with it at this stage.
piranna		> We haven't locked in final distro, libc and compiler versions for Node 10 but I'm pretty confident this is how we're going to go live with it at this stage.  Could this be able to open the door to have oficial builds using musl in the build matrix?
rvagg		@piranna probably not, that's a different question and not dependent on a new major version, it could be done at any time. There's some discussion in https://github.com/nodejs/build/issues/1140, I wouldn't hold my breath but perhaps if there's enough of a call for it and someone can come up with a strategy that doesn't put us on a slippery slow to an explosion in the number of binaries we have to produce then _maybe_ it'll happen.
piranna		> @piranna probably not, that's a different question and not dependent on a new major version, it could be done at any time  My question was more related to the point that maybe the new toolchain would help about it. Anyway, thank you so much for your detailed explanation :-)
orangemocha		:+1:  
joaocgreis		/cc @nodejs/build  
rvagg		rubber stamp lgtm 
joaocgreis		I don't think `start.sh` should be in all machines. There should be some kind of mechanism everywhere to restart Jenkins when it crashes, which is (or was) a relatively frequent issue. So, if Jenkins does not start because of the restart mechanism, we should debug the restart mechanism. If Jenkins starts and we want it stopped for some reason, we must always disable the restart mechanism anyway. Having `start.sh` everywhere will be very misleading and may not be kept in sync with the other way Jenkins is launched.  `start.sh` was traditionally used with `monit`. Here, I see it used with `launchctl` and a `KeepAlive=true` config entry. If that does not work, then we should debug it, not start Jenkins manually and risk having two running.  About this PR, shouldn't the two files under `ansible/partials/` be in `ansible/roles/package-upgrade/files/` instead, since they're only required for `package-upgrade`?  This shouldn't land with the current inventory. A partial one is ok, or even no inventory entries. Also, Gibson raised a valid point about the duplicated `sudo` entry. After these are addressed LGTM. Thanks for doing this!
gibfahn		>I don't think start.sh should be in all  machines. There should be some kind of mechanism everywhere to restart  Jenkins when it crashes, which is (or was) a relatively frequent issue.  So, if Jenkins does not start because of the restart mechanism, we  should debug the restart mechanism. If Jenkins starts and we want it  stopped for some reason, we must always disable the restart mechanism  anyway. Having start.sh everywhere will be very misleading and may not be kept in sync with the other way Jenkins is launched.  I was thinking that maybe `~iojs/start.sh` could just call the relevant service restart command (to avoid people having to remember what it is for each platform). Might be confusing to call it `start.sh` in that case though.
joaocgreis		> I was thinking that maybe `~iojs/start.sh` could just call the relevant service restart command  That would be quite nice! The `jenkins-worker` role should probably be separated by the boot system into partials, perhaps with a common section. It would be nice to include a `start.sh` (or restart?) script that does the right thing there, different for each start system.    
mhdawson		Agreed it would be nice if there was a script that could be used to start/stop which calls the version for the specific platform.  Maybe "agent-control" with start and stop options ? 
refack		Can we land this, and iterate later? On just land the inventory?
refack		BTW the libuv jobs need `GYP` to be in `~/gyp` so meanwhile I added a curl call in the job: https://ci.nodejs.org/job/libuv-test-commit-osx/jobConfigHistory/showDiffFiles?timestamp1=2017-11-22_22-16-55&timestamp2=2017-12-01_15-30-59
mhdawson		I'm all for landing minus the inventory and then updating once we have additional changes (like adding in support for ccache)
gibfahn		>I'm all for landing minus the inventory and then updating once we have additional changes (like adding in support for ccache)  SGTM, but it'd be good to add some TODOs to the source code (like for adding ccache) so we've noted the gaps.
gdams		We should probably add the final bit for ccache and then I think this is good to land. It should just be a case of getting ccache from homebrew and then adding ccache to the path.
gdams		Okay ccache bits added
mhdawson		@gdams have you updated the existing machines after the update to add ccache ? If so I'll kick off a run to check the speed and then we should move forward in adding the rest of the machines that don't have test failures.
gibfahn		@gdams we should enable putting core dumps in the working directory instead of in `/cores`, that way they'll get cleaned up when jobs finish, rather than being owned by root and filling up the machine.  https://apple.stackexchange.com/questions/108861/can-os-x-dump-cores-in-the-working-directory-instead-of-cores  
mhdawson		@gdams after holiday ping to see if I should check the speed and then add machines to builds.
maclover7		ping @gdams @gibfahn @mhdawson 
maclover7		ping @gdams @gibfahn @mhdawson
gibfahn		>ping @gdams @gibfahn @mhdawson  Looks like this needs a rebase from George.  We should probably get this landed, as AIUI we're already using these scripts.
mhdawson		It was just waiting on a fix up for cache. But I'm +1 for landing and then fixing that
gdams		So we are able to easily install ccache. My only remaining issue is adding ccache to the path of the jenkins node. I added the path to ccache in `etc/paths` and this works locally but I am unable to make this extra path appear in jenkins. (I have even tried restarting the machine)
gibfahn		>So we are able to easily install ccache. My only remaining issue is adding ccache to the path of the jenkins node. I added the path to ccache in etc/paths and this works locally but I am unable to make this extra path appear in jenkins. (I have even tried restarting the machine)  @gdams is `/etc/paths` definitely added even when things are launched directly (not through a terminal)?
gdams		>is `/etc/paths` definitely added even when things are launched directly (not through a terminal)?  @gibfahn I'm not 100% sure. I'll do some investigation.
rvagg		I needed the macstadium hosts in my ssh config and @Trott couldn't figure it out to go fix some broken hosts so I've pulled out the inventory.yml and plugin fixes here and pushed them to master as 0495bc2. It'd be good if we could get this merged soon, it can be iterated on once merged if it's not fully working yet.
mhdawson		I'm +1 on merging and then updating if we need to. @gdams can you rebase and then we'll do that?
gibfahn		>It'd be good if we could get this merged soon, it can be iterated on once merged if it's not fully working yet.  Yep, we an just leave TODOs where things aren't working yet (or need manual setup).
gdams		updated PTAL
gdams		@mhdawson I think those merge issues are now cleared up
mhdawson		Ok, will try to land by the end of week unless there are objections, comments from other @nodejs/build team members.
mhdawson		@gdams can you squash your commits.   git am is failing with conflicts even with -3.  I suspect that if you squash that will get rid of the issue.
gdams		@mhdawson rebased and squashed 
mhdawson		Landed as eb68f591fdd5e07fdd705d1e9fa5316f85d87335
maclover7		Should this be combined with #317 or closed?
gibfahn		Yep, closing as a duplicate of #317 as that one has more info in it.
indutny		Done in https://github.com/node-forward/node/commit/6bcea4ff932144a5fd02affefd45164fbf471e67 
bnoordhuis		LGTM FWIW 
jbergstroem		@mhdawson: Suggesting we add @joaocgreis as well since he is in our infra team (meaning he has access to it)? 
mhdawson		@jbergstroem.  Makes sense.  We could add everybody who has access but I thought that would be unmanageable so was thinking it would be reasonable if we just had a specific subset.  Having said that adding  @joaocgreis  as the infra team makes sense to me. 
joaocgreis		@mhdawson I won't be able to attend the WG meeting today, let me know if you need something else from me. Thanks! 
mhdawson		@joaocgreis got your email, it looks good to me.  
jbergstroem		I've sent my email as well. 
mhdawson		Received/forwarded for everybody now.  
gibfahn		I think if @rvagg releases an 8.1.2 today/tomorrow we'll be fine.
rvagg		as @refack suggested on IRC, it doesn't have to be permanent, my main concern is with our serious redirect bloat but this can be a temporary one (don't know how long but everyone will move on from this pretty quick I reckon). I've implemented it for now.
gibfahn		Hmm, maybe the machine can't handle that much console logging.
gibfahn		Probably caused by https://github.com/nodejs/build/issues/720#issuecomment-334953619
refack		AFAICT it happened only one (and https://github.com/nodejs/build/issues/720#issuecomment-334953619 is a genius idea!). RE: "that much console logging" shall I remind you of the 130MB console output for even Windows build ;)
refack		I think there's an stdout race of some sort between `cat` and Jenkins. I just rebuilt that job (https://ci.nodejs.org/job/node-test-linter/12355/console) and got a different truncation: ``` ok 35 - /usr/home/iojs/buBuild step 'Execute shell' marked build as failure ```
refack		Got an incantation - replace `cat` with ~`grep -v -P "^(ok|1\.\.|TAP|$)" test-eslint.tap"`~ (no GNU options on smartOS/BSD machines ü§¶‚Äç‚ôÇÔ∏è ), and maybe add an `echo ---------` for good measure.
refack		@Trott to the point of job 12318's failure, see https://github.com/nodejs/node/pull/15805#issuecomment-335013975
gibfahn		Yeah, it's probably due to the speed at which this spewed out 5000 lines of text.  This should be fixed by https://github.com/nodejs/build/issues/720#issuecomment-335012924. I'll close this, reopen if it reoccurs.
joaocgreis		First time this happened in CI was February 1, was very rare but seems to be becoming more frequent.  Ref https://github.com/nodejs/node/pull/5224 , the environment should be the same for every build, I'll double check. EDIT: it is. 
orangemocha		Investigating... 
joaocgreis		For reference, here's the error: `c:\workspace\node-compile-windows\label\win-vs2015\deps\openssl\openssl.targets(28,5): error MSB4175: The task factory "XamlTaskFactory" could not be loaded from the assembly "Microsoft.Build.Tasks.Core". Could not find file 'C:\Users\Administrator\AppData\Local\Temp\1\3jvoaqze.dll'. [c:\workspace\node-compile-windows\label\win-vs2015\deps\openssl\openssl.vcxproj]`  Last time this happened on CI was on [March 2](https://ci.nodejs.org/job/node-compile-windows/label=win-vs2015/1585/). Chakracore builds were reproducing this very frequently, but now are all green too. So this might be fixed, one suspect is Windows update.  I wasn't able to understand the root cause of this, but seems to be related with the `Temp` folder. Possible things to try if we ever get a consistent repro is to clean the `Temp` folder before the runs or using a different folder by setting the environment variables that point to it. 
orangemocha		We did some investigation, but so far it hasn't been conclusive. Given that this hasn't happened any more since March 2nd, I think we should abandon the investigation for now and resume it only if the problem reappears. 
MylesBorins		it is also worth noting that these are the tarball machines, not the .pkg machines... which are no producing the .pkg or headers either 
jbergstroem		Is this still a problem? 
jbergstroem		Closing. Reopen if this still is an issue. 
gibfahn		>nit: alphabetical order perhaps?  @jasnell whoops, should probably have checked that.  Okay, it's not ASCII sorted (i.e. what `vim` does when you do `:sort`). I think this is the way to go, but I'm open to:  1. moving `Starefossen` down to `s` 2. changing it to `starefossen` 3. leaving it as is because it's what most tools will do if you ask them to sort the list.
rvagg		@jbergstroem yep, although I see a benchmarking & perf working group in our near future too, would love @trevnorris and @brycebaril to get involved in that too but I'm pretty sure they aren't as interested in infrastructure concerns. Build's responsibility should be providing and maintaining the hardware.  Let's discuss bootstrapping the effort in #43  
trevnorris		@rvagg I'm interested in helping where I can. While I can offer a better benchmarking suit then what io.js currently has, not sure how much I can help in terms of recording/reporting that data longitudinally. 
jbergstroem		We now have hardware available through https://ci.nodejs.org and a working group active at https://github.com/nodejs/benchmarking/. Awesome! :+1:  
jbergstroem		smartos didn't have an exception in the jenkins config. Added dependencies to hosts and fixed the config. New run here: https://jenkins-iojs.nodesource.com/job/libuv+any-pr+multi/82/ 
jbergstroem		Seems like we get a linking error at the smartos bots. I'll have a look today. At least the build seems to be executing properly -- closing. 
saghul		Thanks Johan! On Apr 11, 2015 1:52 AM, "Johan Bergstr√∂m" notifications@github.com wrote:  > Seems like we get a linking error at the smartos bots. I'll have a look > today. At least the build seems to be executing properly -- closing. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/iojs/build/issues/70#issuecomment-91721129. 
santigimeno		I've tested the scripts on a `freebsd11-x64` box and it looks they work fine (I've only tested that the ansible scripts run without problems not that the box ends in the expected state)
maclover7		Moving this to #1277 
jbergstroem		Ok, I will open a PR tomorrow.
targos		ping
jbergstroem		Truly sorry :( $wrk, etc etc. I will get to it as soon as I can; likely tomorrow morning.
targos		ping @nodejs/build (no worries @jbergstroem. I totally understand that you can be busy)
Trott		@gibfahn Did you mean to add the `wg-agenda` label or is `ctc-agenda` correct? If `ctc-agenda`, can you (or someone else) provide a short description of what you hope CTC will do? Thanks!
joaocgreis		Was `wg-agenda` and was already discussed in https://github.com/nodejs/build/issues/766
kfarnung		I'm going to be taking a look at it this week.
kfarnung		@targos I've taken a look at this and it's not too straightforward.  I have opened #772 for the first step, installing the `clang34` package, but there's a bit more subtlety to the builds on FreeBSD.  I'll keep working on this throughout the week.  As a side note, are the builds still failing with 3.4.1?  I don't see any indication of the original build issue in the CI or in my local builds.
targos		@kfarnung Thank you for looking into this. The builds are failing with V8 6.0. You can try to reproduce with this branch: https://github.com/targos/node/tree/v8-6.0  The latest failure on CI is here: https://ci.nodejs.org/job/node-test-commit-freebsd/9863/nodes=freebsd10-64/console
kfarnung		Thanks @targos, that was the key.  I'm now able to repro the crash.  Unfortunately I'm still seeing the crash on 3.4.2, see attached, so I'm going to try 3.5 (EDIT: also failing) and 3.8 (EDIT: passing) as well, just to see if any of the newer versions resolve the issue.  ### Attempts - [freebsd10-clang34-output.txt](https://github.com/nodejs/build/files/1106655/freebsd10-clang34-output.txt) - **FAILED** with SegFault - [freebsd10-clang35-output.txt](https://github.com/nodejs/build/files/1106873/freebsd10-clang35-output.txt) - **FAILED** with SegFault - [freebsd10-clang38-output.txt](https://github.com/nodejs/build/files/1106987/freebsd10-clang38-output.txt) - **PASSED**
kfarnung		@jbergstroem Is there any objection to upgrading clang to 3.8.1?  That's approximately the same version as FreeBSD 11 (which uses 3.8.0).
jbergstroem		@kfarnung I will have a chat with package maintainers in FreeBSD seeing how they should/would run into this as well.
jbergstroem		> @jbergstroem said: > I will have a chat with package maintainers in FreeBSD seeing how they should/would run into this as well.  I've emailed with @bradleythughes (one of the node maintainers in ports/freebsd) about this and their build system can reproduce the same issues. The idea is to spend a bit of time seeing how feasible it is to fix this upstream (v8).  In general, is the v8 team moving forward with raising the lowest required clang version? @targos do you perhaps know?
targos		@jbergstroem I don't know if there is a lowest required clang version. Note that the crash cannot be reproduced with V8 6.1 so you might be able to bisect V8 to find the fix.
MylesBorins		Can we get this upgrade? This is blocking us from the V8 6.0 release
gibfahn		>Note that the crash cannot be reproduced with V8 6.1 so you might be able to bisect V8 to find the fix.  Sounds like we need to bisect and then have that fix backported. Upgrading clang sounds good too, but a backport sounds quicker.
MylesBorins		@targos is doing this bisect something you can do? @ofrobots perhaps you can help?
targos		The error message from https://github.com/nodejs/node-v8/issues/1#issue-227257916 mentions: ``` 3.	../deps/v8/src/wasm/wasm-module.cc:2656:5: Generating code for declaration 'AsyncCompileJob::CompileTask<0>::CompileTask' c++: error: unable to execute command: Segmentation fault (core dumped) ```  The code for AsyncCompileJob moved in https://github.com/v8/v8/commit/c7892d3577f53a707128bad7a392e05c2d904b60. IMO it's highly probable that this is what "fixed" the crash. I will try to backport this commit.
targos		I give up. This file had too many changes between 6.0 and the refactoring...
jbergstroem		So, have we established a version we're going with?
kfarnung		If we want to stay with current OS packages then it's 3.8.1 (which is slightly ahead of the FreeBSD 11 machine which uses 3.8.0).
gibfahn		3.8.1 SGTM
jbergstroem		Ok, I can do the PR.
kfarnung		You can base it on https://github.com/nodejs/build/pull/772
MylesBorins		@targos I dug in a little bit more and it would appear the biggest change (aside from the giant refactor) was making `AsyncCompileJob::CompileTask` no longer a template class. I'm thinking that the bug in Clang is related to this
MylesBorins		I think I have a fix  https://github.com/MylesBorins/node/commit/70b4ec5249e7aa120f90e25188e0de636b4ae905
bnoordhuis		@MylesBorins We updated the baseline to clang 3.4.2 for precisely the reason you describe in the commit log.
MylesBorins		I landed a fix upstream to V8 ... So we are good either way üòÇ  On Jul 26, 2017 2:20 AM, "Ben Noordhuis" <notifications@github.com> wrote:  > @MylesBorins <https://github.com/mylesborins> We updated the baseline to > clang 3.4.2 for precisely the reason you describe in the commit log. > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/723#issuecomment-317998562>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecV522HdXUmeggtI1ldft4j9HHoOdpks5sRwTqgaJpZM4NcDmN> > . > 
ignisf		@MylesBorins could you please link to your change in upstream V8?
ignisf		Ah, nvm, found it https://chromium-review.googlesource.com/c/v8/v8/+/585979
mhdawson		Seems to be consistent, also don't anything that looks like it changed in terms of config or timeouts although I do see that the timeout configured for git only seems to apply to some of the operations but that is not new.  Looking at the disk usage while the checkout is in progress is does look like git is progressing and just runs out of time.  The libuv jobs still seem to be ok as well, so does not seem like fundamental problem with checkouts 
mhdawson		Will try a reboot to see if that helps 
mhdawson		Still timed out even after reboot.  Will contact David (at IBM) who provided this temporary machine to see if he can ask the provider if there are issues.    @jbergstroem in the mean time do you think it would be ok for me to bump the global git timeout to see if that helps as the local one does not seem to apply to the git step that is timing out. 
jbergstroem		@mhdawson you mean on the machine? Feel free (you're referring to `git config --global`, no?) imo. 
mhdawson		I was not thinking about git config --gobal but instead the jenkins level config for timeouts but I'll see if the --global works  
mhdawson		Ready more I was pretty sure it was jenkins/plug-in timing out.  Tried adding this when starting the slave:  <PRE> -Dorg.jenkinsci.plugins.gitclient.Git.timeOut=30 </PRE>  and also upped the memory for the jvm (hoping this speeds things up)  The combination seems to have resolved the issue.  Will validate and then update the ansible files 
jbergstroem		Did you test them separately? It'd be nice to avoid having to bump ram since we likely want to use the same over all machines (so we can put it at a shared variable file at some stage). 
mhdawson		I'll try to do that next week 
mhdawson		Build with heap size changed back to 128m https://ci.nodejs.org/job/node-test-commit-aix/120/ 
mhdawson		The build with the smaller heap size was ok, so removed that change.  See PR for rest of discussion. 
mhdawson		Have extended the timeout under https://github.com/nodejs/build/pull/383 closing 
gibfahn		Obviously this isn't complete RHEL support, but seems like a good first step.
chrislea		There should be no functional difference between supporting CentOS and RHEL at the same version. IE, if you support CentOS 6.x, then you already support RHEL 6.x.
gibfahn		@chrislea presumably you might have to configure your yum repos to pull from a different server right?
chrislea		@gibfahn we certainly don't at NodeSource. Our `rpm` packages, repos, and instructions are identical for CentOS 6 / RHEL 6 and also CentOS 7 / RHEL 7.
refack		Does RHEL6 have a better `devtoolset-6.i686`? And what about RHEL5 is it easier to find than CentOS5? (If they are easier to maintain can we use them instead of CentOS?) 
chrislea		CentOS 5 / RHEL 5 has reached production EOL and we shouldn't think about that distro.  The `devtoolset-*` packages are identical for CentOS and RHEL, so there's no better or worse.  It's probably worth quickly explaining the relationship between CentOS and RHEL to clear up confusion. The basic zen as I understand it of the GPL is "if you distribute a binary, you also have to make the sources for that binary available". So that makes it harder to add value as a binary distribution vendor. The way Red Hat gets around this is surprisingly simple. When they make a release, they make their source code packages available everywhere, for free. But if you want a complete binary distribution from those sources that Red Hat supports, then you have to pay them to get those binaries.  CentOS is a group of people that grabs all of the freely available RHEL source packages and builds an entire binary distribution from them. So when I say they are functionally equivalent, I'm being pretty literal. They are the same thing modulo the amount of support you can get from Red Hat (IIRC you actually can pay Red Hat to support your CentOS install, but I think maybe it's more limited and some of their online management tools won't work without the full license).  If I'm being picky, CentOS also has to swap out some branding stuff that Red Hat has trademarked, but that's just for legal reasons. From an engineer's standpoint, CentOS and RHEL are the same thing. Same kernels, same `libc++`, same compilers, same ABIs, etc.
mhdawson		@jbergstroem I added the 2nd test and release machine so I think this is ready to go 
jbergstroem		LGTM 
mhdawson		Landed as b8a9401b5d5e384b21f09d4a6da71cfb1df69946 
Trott		Looks to me like it *might* only be failing on test-rackspace-win2008r2-x64-4? Going to try taking that one offline and running again...
Trott		Oh, nope, that's totally wrong, it's a bunch of hosts, but it's strange (to me) that some fail consistently and others don't.   
Trott		Seeing what happens if I take all the failing nodes offline because why not? It's not as if anyone can get a CI run out of them right now.  Took these offline:  * test-rackspace-win2008r2-x64-4 * test-azure_msft-win10-x64-3 * test-azure_msft-win10-x64-1 * test-azure_msft-win2012r2-x64-3 * test-azure_msft-win2012r2-x64-2  And here's a run to see if it fixes things or not: https://ci.nodejs.org/job/node-test-commit-windows-fanned/12947/
refack		Cleaned and brought online: - [x] test-rackspace-win2008r2-x64-4 - [x] test-azure_msft-win10-x64-3 - [x] test-azure_msft-win10-x64-1 - [x] test-azure_msft-win2012r2-x64-3 - [x] test-azure_msft-win2012r2-x64-2   @Trott if you see a stalled job you can go to it's job page (for example  https://ci.nodejs.org/job/node-test-binary-windows/COMPILED_BY=vs2015-x86,RUNNER=win2012r2,RUN_SUBSET=3/ ) if it's the top job you can click `workspace` and wipe it.  As for intermittent failures, I think it's because this is triggered by git's auto-GC logic...
Trott		Results are better, but still some failures...took this one offline:  - [x] test-azure_msft-win10-x64-5  There are two more with consistent build failures but they're in the middle of doing something right now so I don't want to take offline until they really fail again.  https://ci.nodejs.org/computer/test-azure_msft-win10-x64-1/builds https://ci.nodejs.org/computer/test-rackspace-win2008r2-x64-5/builds
Trott		Build history is too convincing. Took these offline too:  - [x] test-azure_msft-win10-x64-1 - [x] test-rackspace-win2008r2-x64-5
Trott		Trying again to see if we can get a green Windows build now: https://ci.nodejs.org/job/node-test-commit-windows-fanned/12952/
Trott		Looks like it's gonna be green this time. Not sure how to fix the offline hosts but at least CI isn't perma-red.  
Trott		Took test-azure_msft-win10-x64-5 offline too. Build failures, obviously.  - [x] test-azure_msft-win10-x64-5
refack		- [x] ` test-requireio_davglass-debian7-arm_pi1p-1` back (was errcode `failed-to-remove-.nfs` https://github.com/nodejs/build/issues/951)
refack		All windows machines are back. 3 windows-fanned CI: https://ci.nodejs.org/job/node-test-commit-windows-fanned/12986/ https://ci.nodejs.org/job/node-test-commit-windows-fanned/12987/ https://ci.nodejs.org/job/node-test-commit-windows-fanned/12988/
refack		`test-azure_msft-win10-x64-5` was a little stubborn, but should now be Ok. Also all PIs are back online.
joaocgreis		Some workers were still failing. The `node-test-binary-windows` job is set up to only fetch the git branch with the binaries, so the problem is not the size of the binary repo but the local git repository. The automatic `git gc` would take too long.  I created a new job `git-clean-windows` similar to `git-clean-rpi` set to run evey week, should prevent this from happening again. I'll close this issue after I confirm it is working as expected.
refack		@joaocgreis it seems like the local `git` repo does accumulate binaries even the branch was deleted from the remote. I rerun a `node-test-binary-windows` job and some workers were able to find the revision: https://drive.google.com/file/d/0Bz0LZMH4OpErbVZkSm9YSFhCTVU/view?usp=sharing
gibfahn		>@joaocgreis it seems like the local git repo does accumulate binaries even the branch was deleted from the remote. I rerun a node-test-binary-windows job and some workers were able to find the revision:  Does `git fetch --prune` help? That removes the tracking branches from remote repos (which are not otherwise removed).  I also run this locally to delete local branches which had an upstream, but the upstream was deleted:  ```bash # Delete orphaned local branches. git fetch -p && git branch -vv | awk '/: gone]/{print $1}' | xargs git branch -D  ```
refack		I see several optimization we could take for `node-test-binary-windows` 1. Use shallow checkout 2. Use git LFS 3. Add a prune step ![image](https://user-images.githubusercontent.com/96947/32417421-8704d486-c227-11e7-914b-068ba0c3281e.png)  I'll clone the job and try this out. 
joaocgreis		I never noticed that prune step before, it sounds promising. Some jobs already do it on the scripts, if it works feel free to add it all over (also the clean before *and* after, added it to `test-binary-windows` and seems to be working quite well).  As long as the weekly job to clean the workspaces does its job, we should not see this issue again. Note that `git fetch --prune` is not `git prune`, it will not delete objects. A commit can be kept around even if all branches pointing to it are deleted (IIUC because they are still pointed to by the reflog - `git reflog`). To delete objects, we have to run `git gc`. But I've had plenty of issues with that as well (tried it in the arm jobs for some time). It is not rare for `git gc` to hang and I suspect it can leave the repo in an inconsistent state. So the best option is to delete and clone again, it's not pretty but has been working decently well in the Raspberries for a long time.  About a shallow checkout: by having the full history in the test machines, git can transfer only what is new. Long time since I've tried this, feel free to double-check, but I think a shallow clone will have to transfer everything everytime, or at least much more if there are several commits in between. 
gibfahn		@mcollina Obviously there may be extra things we have to do later, but let's start by getting a job up and running.
mcollina		üëç 
mcollina		@gibfahn any update on the work you started during the Collab Summit?
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
MylesBorins		@ghostbar there have been new individuals (such as myself) added to the release team and all their public key's are available in the README. Have you updated your keys? 
ghostbar		@TheAlphaNerd I can see that, I did indeed update it, but then again: who signs those changes?  That's why I said the signature is becoming meaningless, basically the chain of trust is being broken and that's the basis of gpg signatures.  Therefore, my proposal, of a known signature, and only somebody already trusted can make changes on that key when giving subkeys. Turns out, there have been attacks before on GitHub, so I can't blindly trust on a change in a github repo README. 
MylesBorins		@ghostbar I have mixed feelings regarding that statement. There is ample documentation as to how individuals are on boarded to the release group. Perhaps we should be signing any commit that modifies that list though.  If I am following you correctly, you would prefer to see a single master key, and that all signatories are signing the releases using a subkey of that?  /cc @nodejs/ctc @mikeal  
ghostbar		I see that as a good idea, specially for letting CI tools do their job with less intervention from individuals.  And solves the issue of trust, I can see your key and Evan's got created at this year and at the end of 2015 respectively and have no signatures on it. I don't know you, how can I be sure it's your key if someone else from the release team haven't signed yours?  This would be solved as well by using a master key for the release team and giving subkeys of it to the people that can do releases. Not only I don't need to check on each release if a new key got added but I know the individual signing the release can be trusted.  I know it may seem paranoid, but that's how gpg keys should be used (creating a network of trust), so I see a win on using it right. 
bnoordhuis		I forgot the issue number but master keys have been discussed before.  IIRC, the reason we didn't go with that is because no one was sure who should manage the master key.  (Saying "The Node.js Foundation" is mu; at the end of the day, it's still one or more individuals that hold the master key.) 
ChALkeR		Why would we want master keys? The chain could be fixed just by signing the list of new keys with keys from the old list each time a new key is added to the list.  More specifically, can we host the list of trusted keys somewhere, signed by each key from that list (and, perhaps, the keys that were used before)?  @ghostbar would that fix your concerns? 
ghostbar		Yes, indeed that would solve the trust issue @ChALkeR, as I said in my first message the first alert I got was: this release is signed with a key I do not know and is not signed by the other keys I have trusted in the past (and in fact, isn't signed by anyone, at all). And as the title says, I would like to have signatures from a trusted key, which means someone else that's already trusted signed it. (The more, the better)  The master key would be an ideal solution for less human interaction when new members get added to the release team. 
rvagg		fwiw I've signed the keys of the other members of the release team and uploaded them to the main keyserver pool, they should be propagated already if you want to `gpg --refresh-keys --keyserver pool.sks-keyservers.net`.  > can we host the list of trusted keys somewhere  We host a list of fingerprints on the README, this should be considered a trusted source, changing the README requires jumping through the various hoops we have.  ``` curl -sL https://raw.githubusercontent.com/nodejs/node/master/README.md | grep ^gpg ```  will get you all of the keys you need to trust at the moment in order to be able to verify the signed SHASUMS files. These even get hardwired into the Dockerfiles maintained by the default `node` images: https://github.com/nodejs/docker-node/blob/master/4.4/Dockerfile  The release team shouldn't change very often but when it does, a sig will be added to the README, that's the authoritative location to get them. 
jbergstroem		We could alternatively create a new file in the repo that contains the keys and always commit to it signed (`git commit -S`). That would avoid the 'github hacked' or 'who just force pushed' scenario. 
ghostbar		Thank you @rvagg; having the keys signed already, the releases are definitely signed by a trusted key, so I'm gonna close this issue. 
phillipj		@nodejs/build understand this isn't trivial to review, unless taking the time to test the guide. You okey with me merging anyhow, as this doesn't affect anything runtime in any way?
jbergstroem		Get it done. Sorry for holding up progress.
phillipj		Thanks üëç 
gibfahn		I notice that the equivalent for CentOS 5 links to the specific release rather than the latest. Is there any reason to think we would want to do that? I'd have thought keeping with the latest would be better, just checking.  https://github.com/nodejs/build/blob/0d149d7fecaeb84cec33a9804d8b7391b5f08ee1/ansible/roles/baselayout/tasks/partials/repo/centos5.yml#L17  Also while you're at it, that CentOS 5 link is broken [because](https://dl.fedoraproject.org/pub/epel/5/README):  >ATTENTION >====================================== > The contents of this directory have been moved to our archives available at: >  > http://archives.fedoraproject.org/pub/archive/epel/ >  > If you are having troubles finding something there please stop by #epel on irc.freenode.net  So while you're at it, maybe change:  ```diff -    name: "http://dl.fedoraproject.org/pub/epel/5/{{ ansible_architecture }}/epel-release-5-4.noarch.rpm" +    name: "http://archives.fedoraproject.org/pub/archive/epel/5/{{ ansible_architecture }}/epel-release-5-4.noarch.rpm" ```
rvagg		I think the centos5 thing is the way it is because I don't believe there was a non-specific URL when it was set up. These new URLs are new I think.  I've gone and fixed up the centos5 URL and added cento6 for good measure (not tested so crossing fingers on that one!). PTAL @gibfahn 
joaocgreis		Approved in today's meeting (https://github.com/nodejs/build/issues/660). I'll move forward with adding to secrets.  @kunalspathak can you file a PR adding yourself to the list in README.md?
joaocgreis		Can someone with permission add @kunalspathak to https://github.com/orgs/nodejs/teams/build ? (@jbergstroem @mhdawson perhaps)
jbergstroem		Added!
joaocgreis		Thanks! This is done then!
jbergstroem		Seeing how we've had our share of issues with Jenkins I say we let it brew for a while. The pipeline stuff is pretty interesting, looking forward to migrating towards that. 
ChALkeR		From the security point of view, I would prefer not to see auto landing from Jenkins. 
jbergstroem		Borderline gossip-y, but there's some valuable information in this thread with regards to 2.0: https://news.ycombinator.com/item?id=11362058 
rvagg		FYI our release Jenkins, which is pretty well protected from public access, was inadvertently updated to Jenkins 2.7. They marked that branch as "lts" and started pushing it out via their apt repo so we magically went from 1.5 to 2.7 without warning. @jbergstroem and I agreed to let it be and experiment with it a little. It seems to work OK and has pushed out nightlies and the last v4.5 RC. @jbergstroem has even gone all out and applied a custom theme to it .. yay?  We'll experiment with this for a while and maybe roll it out to ci.nodejs.org when we're comfortable. So far so good, but ci-release.nodejs.org is significantly less complex than ci.nodejs.org is!  heads-up @nodejs/release folks 
jbergstroem		We're now running jenkins 2.x at both ci and ci-release. 
jbergstroem		Hey! What are your thoughts on the matter? Outlining how your solution would look could be a great way to get involved. 
mhdawson		I'm not aware of any work on that front but do think its important/interesting.  As jbergstroem mentions a good first step would be to document what you think the next steps would be.   
DavidTPate		Cool, I'll do some additional digging into the testing pipeline that's running right now. IIRC it uses Python to orchestrate everything, so that's likely where instrumentation would be added. It seems like it will take 2 different solutions to determine the coverage of the Javascript & C++ code. 
rvagg		@chrisdickinson began some work on code coverage, perhaps we can pester him for his code. 
balupton		I'm also wondering about this. Would be very insightful to know how much of node.js is actually tested. Including the deps.  Has there been a reason this has not been tackled yet? From my understanding, test coverage is an indicator of stability and quality, perhaps my understanding is wrong, or there has just been more important factors for Node.js world. 
evanlucas		Last I checked, the js was ~72% covered. It was a nightmare to setup, but I'll see if I can do it again 
jbergstroem		@balupton the reason it hasn't been tackled is because no one has stepped up and written any code. Feel free to chip in if you have the time! 
evanlucas		Woah I was way off. More like 85% https://nodejs-coverage-sduffwsopa.now.sh/  That's really old though. I'll try and post the changes I had to make tomorrow.  
rvagg		This might be a good thread to move over to https://github.com/nodejs/testing, it may even be something already on the radar for them, also the Testing Working Group is a great place to get involved and contributing some coverage tooling would be an excellent start. 
santigimeno		> This might be a good thread to move over to https://github.com/nodejs/testing, it may even be something already on the radar for them  Yes, it was something we had on the radar, but no work has been done yet.  > Testing Working Group is a great place to get involved and contributing some coverage tooling would be an excellent start.  Yes, it would be great having more people involved! :D 
evanlucas		For anyone interested, https://github.com/evanlucas/node/commit/1445e3d7fda9fc46f45a1183f75fa9263b24cf18 is what I used to generate that. You'll also have to install `istanbul`. https://nodejs-coverage-kvhhcskvtz.now.sh/ is the results for current master branch.  EDIT: It is **really** ugly by the way. 
santigimeno		This is great. Thank you! 
mhdawson		Coverage work progressed in an issue over in the testing WG:  https://github.com/nodejs/testing/issues/36.  Closing this issue.  Please re-open if you disagree. 
refack		Ref: https://github.com/nodejs/node/pull/13969 I originally removed it from `vcbuild.bat` because it was set in a redefined subroutine (which seemed strange), and I couldn't find any reference to it in our code base. But is there any chance any downstream consumer still uses it (i.e. NVIDIA, Microsoft, or anyone who builds their own `node.exe`)?
joaocgreis		@refack that is possible, any staging server could be used in [`vcbuild.bat`](https://github.com/nodejs/node/blob/ef28d85611bcb1b0d91b6525b7c16517bf21020c/vcbuild.bat#L337-L346). But I have no idea, never heard of a concrete case of anyone using our build scripts with their own infra.
refack		http://blog.sec-consult.com/2017/04/application-whitelisting-application.html NVIDIA build `node.exe` and sign it with their own certificate ü§¶‚Äç‚ôÇÔ∏è 
joaocgreis		For that they only need to run `vcbuild sign` in a machine with the certificate installed. Uploading to a staging server goes a bit further :slightly_smiling_face: 
refack		AFAICT the nightlies have been built (I looked in the `.xz` and the new `vcbuild.bat` is there)
richardlau		>  But I have no idea, never heard of a concrete case of anyone using our build scripts with their own infra.  IBM does to produce https://developer.ibm.com/node/sdk/.
joaocgreis		@richardlau in case you need to keep your infra in sync, my change above only adds `set DISTTYPEDIR=%DISTTYPE%` before calling `vcbuild.bat`.
richardlau		@joaocgreis I think `DISTTYPEDIR` is one of the things we don't use.  cc @gibfahn 
mhdawson		@joaocgreis the change in the job likes like the right replacement for what was removed.  Having said that I don't quite understand why it was removed from the build script if we need it and other teams that build releases might need it as well.
gibfahn		>I originally removed it from vcbuild.bat because it was set in a redefined subroutine (which seemed strange), and I couldn't find any reference to it in our code base.  For posterity, according to [StackOverflow](https://stackoverflow.com/questions/31528295/batch-goto-label-with-duplicate-labels) this will only be used for [`getnodeversion`](https://github.com/nodejs/node/blob/v8.x/vcbuild.bat#L508-L552).   >Having said that I don't quite understand why it was removed from the build script if we need it and other teams that build releases might need it as well.  It was removed accidentally, and is re-added in https://github.com/nodejs/node/pull/13969.
gibfahn		Just came across https://github.com/nodejs/node/issues/2084, which seems relevant. @joaocgreis I think your change still makes sense in that context.
joaocgreis		I believe this can be closed now. Please reopen if there's any issue.
rvagg		LGTM but why turn errors off? Wouldn't caching be just as good for those? 
jbergstroem		I don't see a point in getting a lot of "file not found" from the cache backend. 
joaocgreis		I approve.
mhdawson		Added user temporarily and install key for jbarz  Going to disable in CI temporarily as well when necessary.
mhdawson		Access is no longer required, removed account. Closing.
mhdawson		First machine added (https://ci.nodejs.org/computer/test-osuosl-aix61-ppc64_be-1/).  A few tweaks to the instructions were required. PR for those here: https://github.com/nodejs/build/pull/461  built/ran tests, 2 new failures.  Failures covered by this issue:  https://github.com/nodejs/node/issues/7973 
mhdawson		Build/test time seems to be ~30 mins on the new machine, which is about the same as the slowest platforms so once we get the second machine on-line will plan to add to regular regression runs. 
jbergstroem		@mhdawson is that with ccache? 30min tests sounds awfully slow. 
mhdawson		ccache is installed and I think it should be being used but I was planning to double check 
mhdawson		Good news is ccache was not properly in the path, so we should do better, will fix that up. 
mhdawson		Ok ccache is working now and down 19 mins. It seems that the node-test-commit-linux job seems to typically run 20 mins as do a few of the sub jobs I looked at so seems in the right ballpark. https://ci.nodejs.org/job/node-test-commit-aix/288/nodes=aix61-ppc64/ 
jbergstroem		Are you using JOBS? 
mhdawson		Second machine almost ready, just needs to be added to firewall https://ci.nodejs.org/computer/test-osuosl-aix61-ppc64_be-2/) 
mhdawson		@jbergstroem the parallelism was previously set to 1 but I changed that to 5 as part of the change since it looked like that was how many cpus were available on the new machines. So right now it has -j 5 
mhdawson		I think this means we have 5 procs  bash-4.3# /usr/sbin/lsdev -C -c processor proc0  Available 00-00 Processor proc4  Available 00-04 Processor proc8  Available 00-08 Processor proc12 Available 00-12 Processor proc16 Available 00-16 Processor bash-4.3# 
jbergstroem		On the phone, will add shortly! 
jbergstroem		(added) 
mhdawson		Second machine in and working now.  Will plan to stitch AIX into the regular regression runs on Monday.  There are still some failures (AIX was green but there are issues related to malloc bnoorhuis is working on and one new issue seen on the new machines), but I think its still good to be able to tell if commit introduce new failures. 
mhdawson		Ok 2 test machines are up and running.  They have been running ok for the last few days and seem to run about 20 mins which is consistent with other linux jobs.  Will plan to to change to run test on AIX as part of standard job to test PRs as opposed to nightly some time later today. 
mhdawson		Next step will then be to setup/add the release machine to the release CI. 
mhdawson		Added AIX to node-test-commit.  Run here to validate: https://ci.nodejs.org/job/node-test-commit/4475/ 
Trott		node-test-commit-aix is red every run. So now all of our CI runs are red. Any chance we can remove it until that is sorted out?  
joaocgreis		@mhdawson I disabled the job in `node-test-commit` for now as per @Trott 's request. Your test run was red so I'm not sure what you intended, of course we can discuss this further. Our CI infra is full of known issues that we must dismiss (or, when in doubt, re-run), but a permanently red job is not an advantage for collaborators, so I opted to disable it for now.  EDIT: CI back to green: https://ci.nodejs.org/job/node-test-commit/4489/ 
Trott		Thanks @joaocgreis!  If this needs to be re-added quickly for some reason, and if the same tests are failing each time, I suppose they could be marked flaky in the status file, although obviously that's not as good as fixing whatever the issue is. 
mhdawson		AIX was green until a little while back until it was broken by some new changes, which then covered up other changes being made that cause more problems.  This is bound to happen when its not run as part of the regular job.  Now that we have adequate hardware to run AIX for each run, I was hoping we could add AIX even though it was red as it would help more quickly find regressions (even if submitters don't notice because it was already red) and would help the triage if one did sneak in.  The failures are consistent and there are 2 issues open to cover the existing failures.  There has been enough red in the past I did not think it was going to be a major issue but maybe that's changed now.  I can look to see if I can mark them as flaky for just AIX but not sure its the best thing to do. 
Trott		> There has been enough red in the past I did not think it was going to be a major issue but maybe that's changed now.  Definitely. We've been mostly-green since December and I'd hate to go back to a world where red CI was shrugged off and code landed.  There are two ways to mark the tests as flaky, I think. I believe one way results in green and the other way results in yellow. I'd be OK with a yellow CI indicating there's tests that need addressing but not anything that should hold up code landing. /cc @orangemocha in case I'm wrong about that. I'm pretty sure he set that stuff up and I don't actually know how it works. I'm just a happy user. 
joaocgreis		I see there are already two tests in https://github.com/nodejs/node/blob/ab3306ad/test/parallel/parallel.status for AIX, it would be great to add the missing ones as well and re-enabling the job. Please test v4 as well (if AIX is to run for v4).  @mhdawson if you expect the tests to be corrected soon, you can add them as `PASS,FLAKY`, that will make every job yellow until the tests are fixed. If it might take long, add them as `PASS,FAIL` or just `FAIL` if it fails reliably, making the job green. 
mhdawson		They should be fixed relatively soon.  I will add then as PASS,FLAKY so that it shows up as yellow and are still visible. 
mhdawson		ok PR here to mark as flaky https://github.com/nodejs/node/pull/8065 
mhdawson		Build for 4.x https://ci.nodejs.org/job/node-test-commit-aix/335/ 
mhdawson		There are 2 failures on 4.x:   not ok 659 parallel/test-regress-GH-1899   not ok 730 parallel/test-stdio-closed  The second is one that we are actively investigating in master which was only seen after we moved to the new AIX macihnes.  The first seems to have been fixed in libuv as per https://github.com/nodejs/node/issues/3676 which likely has not made it back to 4.x.  I'm thinking that given that people can get 4.x AIX builds from the IBM developerworks site with any required fixes for these issues, and 6.x LTS is not that far away that the goal should be having AIX in the community releases for 6.x and just leave 4.x as it is.  We can revisit/confirm this once we have AIX downloads for 6.x (as stable) available on the community download page. 
mhdawson		One additional comment, if we think the failures will be an issue in the CI when people do runs against 4.x I'm happy to submit a PR to mark those 2 tests as expected to fail in 4.x.  @Trott, @joaocgreis  what's your views on that 
joaocgreis		If v4 does not support AIX, then CI should not run it for v4. Currently, CI detects node versions v0.x and arm is not run on those. I'll have to extend this mechanism soon for v4 (easy but not much).  @mhdawson my view is that for now you're welcome to mark those as flaky. When we have the mechanism to run only on v6, you can keep supporting it or not, your call I guess. 
mhdawson		It would be nice for runs against 4.x to catch any new regressions since we do ship 4.x binaries even if the community does not.  I'll submit a PR to mark those 2 as flaky 
mhdawson		build to validate my branch before creating PR for v4.x-staging https://ci.nodejs.org/job/node-test-commit-aix/346 
jbergstroem		The makefile change would look something like this (in nodejs):  ``` diff diff --git Makefile Makefile index 401464c..7e7395b 100644 --- Makefile +++ Makefile @@ -167,8 +167,13 @@ test-all: test-build test/gc/node_modules/weak/build/Release/weakref.node  test-all-valgrind: test-build         $(PYTHON) tools/test.py --mode=debug,release --valgrind  + +ifdef JOBS +TEST_OPTS=-j $(JOBS) +endif  test-ci: | build-addons -       $(PYTHON) tools/test.py -p tap --logfile test.tap --mode=release --flaky-tests=$(FLAKY_TESTS) \ +       $(PYTHON) tools/test.py $(TEST_OPTS) -p tap --logfile test.tap \ +               --mode=release --flaky-tests=$(FLAKY_TESTS) \                 $(TEST_CI_ARGS) addons message parallel sequential   test-release: test-build ```  ..and we'll do something like `JOBS={{ ansible_processor_cores }}` or `JOBS=$(getconf _NPROCESSORS_ONLN)` in init scripts for architectures that detects it properly. For the other ones we'll override. 
jbergstroem		Landed in node a few days ago: https://github.com/nodejs/node/commit/f49a1d050178cbaab7732e8643c4db33c4b81ede. The next step is to add `JOBS` to all build environments. 
rvagg		bleh! I already opened this PR @ https://github.com/nodejs/build/pull/298, how about I go and resolve that one? 
rvagg		How does the `curl` for build status get authenticated for this? I'm out of the loop on build status stuff but I'm not seeing anything that's stopping arbitrary hosts from updating build status if they have the right URL.  Rest of it looks pretty good, I'm +1 on getting this hooked up into Jenkins. Great work!
gibfahn		@jkrems would appreciate a review from you if you have the time.
maclover7		@jkrems thank you for the neat git clone/checkout snippet!!  @gibfahn updated PR, for right now +1 on keep the shared bits inside this pipeline file, and then as we need to we can extract out to helper/utility files
maclover7		@refack @gibfahn updated this to take advantage of `post-build-status-update`, is this okay to land? (Someone will also need to create job on Jenkins since I'm not an admin)
maclover7		@gibfahn updated, would you be able to create the http parser job on Jenkins?
gibfahn		Job: https://ci.nodejs.org/view/All/job/node-test-http-parser/  Can you move this into `jenkins/pipelines/`?
maclover7		> Can you move this into jenkins/pipelines/?  Fixed upon landing, landed in d094879b513beebff067617b08dc3f2b06364746.
mhdawson		Outage occurred, was about 20 minutes at around 3:20 I think.  Nighties are green and so have test builds so closing. 
MylesBorins		ping... this is quite annoying... seems like it is failign on v7 too
gibfahn		I can add something to the combination filter to skip, but do we know why it's failing on 4/6/7 but not master?  Example of failing build: https://ci.nodejs.org/job/node-test-commit-linux/nodes=ubuntu1204-clang341-64/9384/console  ```cc /usr/bin/../lib/gcc/x86_64-linux-gnu/4.9/../../../../include/c++/4.9/cstddef:51:11:  error: no member named 'max_align_t' in the global namespace   using ::max_align_t;         ~~^ ```  _**EDIT:**_ Some comments by @bnoordhuis and @MylesBorins which suggest this isn't a new thing, https://github.com/nodejs/node/pull/12392#issuecomment-293944040 https://github.com/nodejs/node/pull/12412#issuecomment-295170293 https://github.com/nodejs/node/pull/12104#issuecomment-289944039
targos		I think this is because of the upgrade to gcc 4.9: https://github.com/nodejs/build/pull/650 It doesn't fail on master because this particular build is skipped.
gibfahn		Lots of people running into this elsewhere: https://github.com/philsquared/Catch/issues/334 https://askubuntu.com/questions/523613/upgrade-to-gcc-4-9-broke-clang http://stackoverflow.com/questions/23462950/clang-only-compiles-c11-program-using-boostformat-when-std-c11-option-i  Suggested fixes are "use `libc++` instead of `libstdc++`" and "upgrade to clang 3.5". Not sure why it's intermittent, but it may be to do with `ccache`.  Given this, are there any objections to changing the existing master skip to include Node 4/6/7?   ```bash # clang is only supported in Node versions 7 and lower MAJOR_VERSION=`cat src/node_version.h |grep "#define NODE_MAJOR_VERSION" | awk '{ print $3}'` RUN_TESTS="RUN" echo $SMARTOS_VERSION if [[ "$nodes" =~ clang && ${MAJOR_VERSION} -gt "7" ]]; then # change to -gt "4"   RUN_TESTS="DONT_RUN" fi ```  Alternatively we could just remove `ubuntu1204-clang341-64` from node-test-commit-linux.
MylesBorins		I have confirmed that the ubuntu1204-clang341 executor is skipped on master and failing on all release lines. I am disabling it from the main CI for now.  I'm unsure who originally set it up, but I've changed this issue to track the progress of adding this executor back to CI  /cc @nodejs/ctc @nodejs/build
bnoordhuis		For posterity: it was added (by Rod?) in response to nodejs/node#8323.
jbergstroem		I updated GCC on the box as a response to @targos on newer V8 requirements too. Then there's [this](https://github.com/nodejs/build/issues/688#issuecomment-297474239).
bnoordhuis		I'll close this out, the buildbot has been removed.
MylesBorins		Last successful build was July 11th
rvagg		restarted both of them, one had a bunch of unnecessary stuff on disk that was pushing it into "not enough disk" making it effectively offline, both working again now, thanks @MylesBorins 
rvagg		+1 from me for @joaocgreis, he's become indispensable for operations and already has access to many of the machines in question 
orangemocha		Absolutely agree! 
jbergstroem		Great. I'll add him to the secrets repo and have this in mind when doing some work on ansible next. 
joaocgreis		Thanks! 
Trott		@nodejs/build @skomski Should this remain open?
gibfahn		This should be relatively straightforward now we have [`node-test-commit-linux-containered`](https://ci.nodejs.org/job/node-test-commit-linux-containered/).
maclover7		@skomski If you are able to provide the `CONFIG_FLAGS` necessary to pass to `make run-ci` to make this work, then we can move forward with this. The Build WG does not have many resources available right now
maclover7		Closing due to inactivity, please reopen if you would still like to work on this
mhdawson		Ok, our suggestion was accepted as good, going to close
rmg		LGTM. 
rvagg		thanks! I think I already added gz on the server but forgot to add it here. will sync now. 
Fishrock123		Works now. Cheers. :tada:  
wanghaiquan		mask 
bnoordhuis		Can you report that over here: https://github.com/nodejs/node/issues?  This bug tracker is for CI infra and such, not for problems with building node (although I can understand why you'd think that.)  I will say that Android is not a supported platform so you'll probably be asked to investigate yourself. 
gibfahn		cc/ @nodejs/build 
kunalspathak		LGTM
joaocgreis		A suggestion, to be discussed: For minutes (only for minutes!), we could make these corrections directly using GitHub's edit feature, adding commits to the PR. When landing, @mhdawson would simply have to interactive rebase on top of master, reword the first commit to add metadata and apply all others as fixups.  @mhdawson since you are writing the minutes, I guess it's up to you to decide how you prefer. When you open the next minutes PR you could explicitly invite this in the opening comment, if you want.
mhdawson		@joaocgreis your suggestion sounds good to me.
mhdawson		Landed as d8afb4d95d609c9fba5359548bb1729146ccfab8
mhdawson		I've created the youtube meeting, and will add the link for participants a few minutes before the meeting starts.
mhdawson		sorry I'm a bit late: https://hangouts.google.com/hangouts/_/ytl/WnAkVlNCy469TJma6aMz8G74fekbMND1M9SNl9eJ0hE
kfarnung		@mhdawson I'm not seeing the meeting participant link.  EDIT:  Nevermind üëç 
piccoloaiutante		i can't enter, it seems unauthorised
mhdawson		Let me cut/paste again 
mhdawson		https://hangouts.google.com/hangouts/_/ytl/WnAkVlNCy469TJma6aMz8G74fekbMND1M9SNl9eJ0hE=?eid=100598160817214911030
mhdawson		For those watching will start broadcast in a few mins, just giving people a bit more time to join.
refack		You going on air? I love watching you guys üòÑ 
rvagg		getting a 403 for the latest url
gibfahn		@rvagg Odd, it works for everyone else.
rvagg		nevermind .. copy/paste doesn't work for the url from github FOR SOME STUPID REASON
mhdawson		PR for minutes: https://github.com/nodejs/build/pull/745
mhdawson		Minutes landed, closing.
refack		+1
rvagg		Removed & retired, I'll hang on to the hardware for a little while just in case we decide this was a bad idea!
bnoordhuis		Sounds reasonable to me. 
rvagg		kill 
mhdawson		Sounds reasonable to me too. 
jbergstroem		Gone! 
jbergstroem		Oops; perhaps a bit premature. It's gone from jenkins, but we still need this to be merged: #408. 
jbergstroem		Just merged the PR but forgot to reference this. Closing. 
gibfahn		Maybe a question for @joaocgreis or @refack 
refack		I'm not sure, but I believe the certificate is kept in the release server's Certificate Store, which is "FIPS 140-2 Level 2" compliant, as long as the hardware is kept on premise.
refack		Ohh, and as a second level of validation we publish the hashes of the released files in a publicly readable location (i.e. https://nodejs.org/download/release/v8.4.0/SHASUMS256.txt) which is anyway the most cross-platform way to validate the integritiy of binaries.
Daniel15		>  which is anyway the most corss-platform way to validate the integritiy of binaries.  That's true, however Windows shows scary warning messages unless the installer is Authenticode signed, so it's basically a requirement for any Windows app. For all non-Windows platforms, Yarn uses GPG signing.
joaocgreis		Our certificate is stored in the Windows Certificate Store. Signing is currently automated with `signtool`: [`EXE`](https://github.com/nodejs/node/blob/640b20616d2cdc46bc3df8703cdc1395578ff1b3/vcbuild.bat#L270), [`MSI`](https://github.com/nodejs/node/blob/640b20616d2cdc46bc3df8703cdc1395578ff1b3/vcbuild.bat#L352) (using [this `bat` file](https://github.com/nodejs/node/blob/640b20616d2cdc46bc3df8703cdc1395578ff1b3/tools/sign.bat) to try multiple timeservers).  However, our certificate expires next month, and if DigiCert has also moved to USB tokens, we have the same issue.  @rvagg I believe you the current certificate in 2015. How do we renew it? Can you renew it or check if DigiCert still provides the `.p12` file?
Daniel15		I just asked DigiCert about it and their support rep wasn't even aware of these changes üòõ . It sounds like they're still issuing regular certificate files at the moment. Maybe we should switch Yarn to a DigiCert certificate if they can still issue regular certificate files. Hmm.  > Signing is currently automated with signtool: EXE, MSI (using this bat file to try multiple timeservers).  Cool, that's pretty similar to what we do with Yarn at the moment (using osslsigncode rather than signtool, but it's pretty similar).  
rvagg		https://github.com/nodejs/TSC/issues/347 thanks for expiry heads up @joaocgreis! I have nothing else useful to add here; I didn't know about this new requirement and hope that we can get away with renewal as is, and the description of how we do it is accurate.
joaocgreis		@rvagg let us know when you have the key, if it turns out to be USB we don't have much time to find a solution. Thanks!
rvagg		Got the cert paid for. @joaocgreis is going to be installing it on our release machines. Will report progress back here as that's done.
joaocgreis		Certificate is still being processed by DigiCert. I'll keep checking and install it when ready.
joaocgreis		Certificates installed and good until 2020.  @Daniel15 in response to the original issue here, we have a `.p12` file. The process is documented here: https://www.digicert.com/code-signing/installing-code-signing-certificate.htm , the last step is particularly relevant.
Daniel15		Thanks! I actually just got a code signing cert for Yarn from DigiCert as well. I'm glad they're still using regular certificate files because it means I don't have to change Yarn's signing process just yet üòÉ 
jbergstroem		what does "broken" mean? 404?
targos		Looking at https://nodejs.org/metrics/summaries/total.csv It seems that there is an issue collecting the metrics. There is no data between 2016-10-13 and today apart from one point on 2016-11-01.
rvagg		logrotate wasn't working, fixed in https://github.com/nodejs/build/pull/552, the effect of this is that the unrotated access.log file was getting too big to process effectively‚Äîthere were a few processes running on the server trying to complete it to no avail.  I've fixed it all up now and split up the big file manually into one per day and it's now processing and should look kind of normal again soon.
jbergstroem		My idea is to get this as part of the github bot and have that run on all commits to all PR. Travis is great but we have the horsepower to pull this of with our own infrastructure and I kind of like the idea of having that independence.   Btw, all for increasing checks on all commits. I think linting and commit message checks should be done on a per-commit basis for all pr's; CI results would come in second. 
jbergstroem		Just an update here: We have a job that we will be integrating somehow shortly: https://ci.nodejs.org/job/node-test-commitmsg/ 
jbergstroem		If you're referring to the icu download we already have a local setup for all machines -- perhaps have a look at the other test jobs. 
MylesBorins		@jbergstroem how is that being done?  For the CI job's I'm seeing  ``` bash NODE_TEST_DIR=${HOME}/node-tmp make run-ci -j $(getconf _NPROCESSORS_ONLN) ```  and in the make file  ``` make run-ci:     $(PYTHON) ./configure $(CONFIG_FLAGS)     $(MAKE)     $(MAKE) test-ci  ``` 
MylesBorins		@jbergstroem I just dug into the build script for `test-commit-osx` and `test-commit-linux` and both are doing the same setup as mentioned above.  AFAIK there isn't anywhere that is testing ICU in CI. 
jbergstroem		Sorry -- poor memory. In release jobs we do stuff like:  ``` bash CONFIG_FLAGS="--download-path=${HOME}/node-icu/" mkdir -p ${HOME}/node-icu/ ... ``` 
MylesBorins		so we are doing that. but we are still fetching on everything make 
jbergstroem		If so, it should only be fetched once. 
MylesBorins		nailed it this time... was putting the config_flags in the wrong place :smile:  I'm going to audit the other code and make sure we are not making the same mistake there too 
jbergstroem		Thanks for this! Will review.
mhdawson		@jbergstroem can you remind me if for releases we build with a later compiler or stick to the one supported by the release.  I'm just thinking this could affect the compiler on the release machines for PPC and s390 which in turn could affect the binaries for 4.x and 6.x
chrislea		I hate to be the bearer of bad news @seishun and @jbergstroem, but I'm pretty sure if you use the `ubuntu-toolchain-r/test` repository to install `gcc 4.9`, it will update `libstdc++` on the target machine when you do it. The result will be binaries that don't (or at least may not) work on a Trusty machine unless the end user has also added that PPA and installed the new compiler themselves. I'd **strongly** suggest checking on this before putting a lot of effort in going down this road.
rvagg		^ that should be pretty easy to test inside Docker if someone has time, run a 14.04 container and mount your local `node` source directory, install buildessential and the ubuntu-toolchain-r PPA and compile. Then quit and run it again and see if the `./node` executable works inside the new container without the PPA attached. If you need slightly more advanced testing then install buildessential and curl and do a `curl https://deb.nodesource.com/test | bash`, that's my node testing tool that'll run through some basic functionality, including compiling a native addon and give you a success or fail mark.
chrislea		I'm going to be up late waiting on other compiles to finish, so I went ahead and tested this on ec2 nodes. Unfortunately I was correct. You can essentially see the problem from this: ``` ubuntu@ip-172-30-25-156:~$ apt-cache policy libstdc++6 libstdc++6:   Installed: 4.8.4-2ubuntu1~14.04.3   Candidate: 7.1.0-5ubuntu2~14.04   Version table:      7.1.0-5ubuntu2~14.04 0         500 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu/ trusty/main amd64 Packages  *** 4.8.4-2ubuntu1~14.04.3 0         500 http://us-west-2.ec2.archive.ubuntu.com/ubuntu/ trusty-updates/main amd64 Packages         500 http://security.ubuntu.com/ubuntu/ trusty-security/main amd64 Packages         100 /var/lib/dpkg/status      4.8.2-19ubuntu1 0         500 http://us-west-2.ec2.archive.ubuntu.com/ubuntu/ trusty/main amd64 Packages ``` I went ahead and compiled 8.2.1 on a Trusty instance with `gcc = 4.9` from the `ubuntu-toolchain-r/test` PPA, then I moved the build tree over to a "pristine" Trusty instance and ran `make install`. After it was installed and I tried to invoke node I got: ``` ubuntu@ip-172-30-31-229:~$ which node /usr/local/bin/node ubuntu@ip-172-30-31-229:~$ node -v node: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by node) ``` So, I don't think using the `ubuntu-toolchain-r/test` repository is a viable option here.
seishun		Are Ubuntu 14.04 machines used for building public releases? If they are used just for CI, then why does it matter?
rvagg		sorry, should have thought more clearly about this, we build public releases for 8.x+ on CentOS6 using devtoolset3 (I think!), and I don't believe we've had to upgrade the compiler there (have we?)
seishun		CentOS6 machines do indeed require an upgrade too (they use gcc 4.8 I think). I was going to work on that but am blocked by https://github.com/nodejs/build/issues/801.  Are release machines separate from CI machines?
seishun		Rebased on master now that https://github.com/nodejs/build/pull/735 has landed.
joaocgreis		(@seishun yes, release machines are separate from test CI machines. They run connected to an instance of Jenkins that's private for releasers and jenkins-admins.)  Change seems good, I will run a test.  The question about CentOS6 looks pertinent though. We have committed to this table: https://github.com/nodejs/node/blob/master/BUILDING.md#supported-platforms-1 (change to version branches to see the table for other node versions) . So, if we update gcc, will we break support? Even if we add a new machine to build the next semver major version of node, we'll have to keep the old machines until the versions that they build go out of LTS support. Should this PR still land if we can't update CentOS6?
seishun		@joaocgreis  >Should this PR still land if we can't update CentOS6?  Can't we? I just built node with gcc 6.3.1 on CentOS 6 (using my PR https://github.com/nodejs/build/pull/809), then moved the binary to a pristine CentOS 6 instance and it ran successfully.
joaocgreis		Tested, this works perfectly. This can land as soon as the issue with compatibility is resolved.  @nodejs/build I don't feel qualified to decide about Linux compiler updates alone, can you please take a look? According to @seishun , the update does not break compatibility, but I'm not familiar with all the details.
chrislea		Sorry I'm a little late coming back to this thread @seishun and @joaocgreis.  I've tested this *extensively*. Building Node with `gcc = 6.3.1` from `devtoolset-6` works just fine. Meaning, the resulting binaries run just fine on a "pristine" CentOS 6. The same binaries run just fine on every other distro I've tested on, which is every currently supported version of CentOS / RHEL, Fedora, Debian, and Ubuntu with the exception of Debian Wheezy which I think we've decided we don't worry about anymore.  There are two (really 1.5) concerns here.  The 0.5 is the concern about building a binary add-on with an older compiler may have some incompatibility issue due to the C++ ABI not being stable. So if you take those built binaries, put them on say CentOS 6, and then build an add-on there with whatever the native compiler there is, could there be a problem. I've been told by people who know a ton more about compilers than myself that this isn't a concern practically.  The other concern is that `devtoolset-*` is 64bit only. If this is the new plan, we drop 32bit support I think. Personally I'm just fine with that, but I thought I should bring it up.
seishun		>The 0.5 is the concern about building a binary add-on with an older compiler may have some incompatibility issue due to the C++ ABI not being stable. So if you take those built binaries, put them on say CentOS 6, and then build an add-on there with whatever the native compiler there is, could there be a problem. I've been told by people who know a ton more about compilers than myself that this isn't a concern practically.  Not an issue according to @bnoordhuis https://github.com/nodejs/node/pull/13466#issuecomment-306164694  >The other concern is that devtoolset-* is 64bit only. If this is the new plan, we drop 32bit support I think. Personally I'm just fine with that, but I thought I should bring it up.  No need to drop 32-bit _support_ I think, we just wouldn't test node on 32-bit CentOS 6. +1 for dropping public 32-bit _builds_.  (Alternatively, we could build 32-bit builds on newer OS, but I suspect that such builds wouldn't work on CentOS 6, which would be kind of confusing)
gibfahn		Result of WG discussion was that this PR is fine to land, we don't build on Ubuntu so this only affects test boxes.
gibfahn		>I've tested this extensively. Building Node with gcc = 6.3.1 from devtoolset-6 works just fine. Meaning, the resulting binaries run just fine on a "pristine" CentOS 6. The same binaries run just fine on every other distro I've tested on, which is every currently supported version of CentOS / RHEL, Fedora, Debian, and Ubuntu with the exception of Debian Wheezy which I think we've decided we don't worry about anymore.  @rvagg you said you were going to confirm this, but it sounds like we _should_ be okay here in terms of devtoolset-6 builds running on stock CentOS 6 machines.
gibfahn		>The other concern is that devtoolset-* is 64bit only. If this is the new plan, we drop 32bit support I think. Personally I'm just fine with that, but I thought I should bring it up.  @chrislea are you saying we can't build 32-bit binaries at all with devtoolset-6?
chrislea		@gibfahn That is correct. All of the `devtoolset-*` packages for RHEL 6 / CentOS 6 are 64 bit **only**. So if we go down this road, it means we are dropping support for 32bit x86.
seishun		@chrislea Could you clarify why this requires dropping support for 32-bit? Looking at https://ci.nodejs.org/job/node-test-commit-linux/12372/, there are a bunch of other 32-bit Linux machines where node is being tested. Among them is Ubuntu 14.04, which definitely can run gcc 4.9.4.
chrislea		@seishun sorry I should be more explicit.  The current strategy as I understand it for building the released binary tarballs for x86 is to build things on CentOS 6 since that's the "oldest" distro we support. Those binaries will then work on any "newer" distro because we have forward compatibility with `libc6` and `libstdc++`. If you build 32bit x86 binaries on say Ubuntu 14.04, they won't work on anything "older" than that, which would include CentOS 6.  So if we're willing to drop support for 32bit CentOS 6 and Debian Wheezy, and also build the 32bit x86 binaries on a different distro than we build the 64bit x86 binaries on, then it can happen. At that point, things are somewhat messier and personally I'd be wondering if it was worth the effort, but I also don't have to do the work to make it all happen so  ¬Ø\_(„ÉÑ)_/¬Ø .
gibfahn		So the discussion in the Build WG yesterday was operating under the impression that we could continue to build 32-bit binaries with this change, as we can't we'll have to rethink. I don't think dropping CentOS 6 is likely to just happen, in fact I suspect that dropping 32-bit builds is more likely. I'll be raising a separate issue to discuss that anyway.
chrislea		Sounds good @gibfahn, please let me know if there's anything I can do to help.
refack		If there are no objections, I will land this and run it on: * test-digitalocean-ubuntu1404-x64-1 * test-digitalocean-ubuntu1404-x64-2 * test-osuosl-ubuntu1404-ppc64_be-1 * test-osuosl-ubuntu1404-ppc64_be-2 * test-osuosl-ubuntu1404-ppc64_le-1 * test-osuosl-ubuntu1404-ppc64_le-2 * test-softlayer-ubuntu1404-x64-1 * test-softlayer-ubuntu1404-x64-2  Not sure about the ARMs but I'll try.
seishun		@refack No objections it seems.
gibfahn		:shipit: @refack 
rvagg		Not directly relevant here but I got caught by the libstdc++6 problem when trying to pull in a custom 4.9 build for armv7 builds on Wheezy. Still scratching my head trying to figure out a way forward on this one. Happy to have thoughts from others if you have any! https://github.com/nodejs/build/issues/829
seishun		@refack status?
rvagg		ok, so I've run this on the x64 and x86 machines and merged this commit into master  _However_ we have a bunch of non-x machines that I'm not comfortable running this on. I started doing it on the ppc machines but there were too many changes being made (like setting hostname) that make me think that the new ansible scripts haven't been run on those before, so I'm not comfortable pushing forward. Also there's the xgene machines and I'm going to bet that ubuntu-toolchain-r doesn't have a gcc4.9 for arm64 ubuntu14.04 because that's a pretty unique combo (I've had a heap of trouble with Java on these). Plus I'd really like to yank those xgene machines anyway so ignoring them is probably OK for now.  So, @mhdawson (or someone else IBM?), how safe do you think it is to run the current ansible scripts against the ppc machines? Is adding the ubuntu-toolchain-r ppa actually going to give us what we need on ppc le & ppc be? Launchpad says that these are built for ["powerpc"](https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/ppa/+build/6539173) but is that even what we need? Should we just exclude ppc ubuntu14.04 from all of this?  I've had similar problems with the Java stuff I did in #964, I haven't put an arch restriction on that but I'm guessing webupd8 isn't available for ppc either, like it's not for arm64 ubuntu 14.04, I just haven't even bothered running it on those.
chrislea		Tossing my $0.02 back in here again (if this is getting annoying somebody please tell me and I'll stop).  I don't think using the `ubuntu-toolchain-r` stuff should ever be an option. It's a big nasty Pandora's box and will cause many, many more problems than it solves.  I am increasingly feeling like it's completely reasonable to just stop shipping 32bit Intel binaries as of Node 9.x. All of the major distros are moving away from even providing 32bit builds, and just discussing it is draining a lot of time for people on the build WG.  I'm not saying that I think we should stop supporting 32bit Intel builds. I'm just saying that "supporting" for that arch should simply mean that we make sure the build works and passes tests on newer distros where people don't have to jump through a bunch of hoops just to get a modern compiler toolchain installed and working.  I suspect most of the live 32bit Intel things out there that are running Node are devices like NAS appliances and the like, and I've never seen any of them that don't build their own binaries. So as long as we know that Node can be built on a reasonably modern 32bit box, I don't think we're short changing anybody.  / $0.02
seishun		@rvagg  >ok, so I've run this on the x64 and x86 machines and merged this commit into master  Finally! Thanks for taking the time.  >Should we just exclude ppc ubuntu14.04 from all of this?  I'm fine with that as long as it's also excluded from CI jobs on master.  @chrislea   >I don't think using the ubuntu-toolchain-r stuff should ever be an option. >I am increasingly feeling like it's completely reasonable to just stop shipping 32bit Intel binaries as of Node 9.x.  Could you clarify how these two statements are related? gcc needs to be updated on 64-bit Ubuntu 14.04 as well.
refack		P.S. Also a huge boon would be if I could grab the binary artifacts from `node-compile-windows` /cc @joaocgreis 
joaocgreis		The compile jobs now archive artifacts - this will use some disk space in the CI host but should not be an issue.  Having the nightlies latest link might take some work, I suspect adding support to https://github.com/nodejs/nodejs-latest-linker and testing it well.
rvagg		latest-linker uses simple sorting to figure out the latest. One reason I didn't add nightly symlinks in the first place is that it's technically possible to have them out of order, two with the same date but commits sorting differently. But that'd only happen if we had some weird extended compile-duration problem with them or they were manually triggered so I guess it's not a big deal. The other reason is, latest what? There are usually multiple branches going on in there, do we just do the latest master?
joaocgreis		>  possible to have them out of order, ... I guess it's not a big deal.  I agree.  > The other reason is, latest what? There are usually multiple branches going on in there, do we just do the latest master?  What makes sense to me is having exactly the same as for releases. `latest-v8.x` would be the nightly generated from `v8.x-staging`, `latest-v9.x` would be `master` now and become `v9.x-staging` at some point. `latest` would always be `master`, we could even call it `latest-master` instead of just `latest` (if the code for distinguishing nightlies from releases is easy enough, else just ignore this for now).
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
rvagg		https://ci.nodejs.org/view/All/job/node-test-commit-private/79/ is an example of a (cleanly) failing build, only the Windows fanned job is in play here but this is why it fails (well, I'm not _entirely_ sure this is the reason but it seems like a fair guess). 
joaocgreis		@rvagg I just moved @nodejs-ci to infra level, so you can use it. 
rvagg		@joaocgreis is @nodejs-ci the one known as `ci@iojs.org` in Jenkins? GitHub says it already has access to node-private. 
rvagg		And it fails with the same problem 
joaocgreis		The problem was a corrupt git workspace. It's quite rare but has happened in the past. Should be fixed now. 
jbergstroem		@rvagg it's part of the [backup job](https://github.com/nodejs/build/blob/master/backup/backup_scripts/remove_old.sh#L14). I can lower it to 20? 
MylesBorins		@rvagg should we take this opportunity to rename the smoker?  edit: missed the comment to me as I didn't see the bullet.  Cleaning it up now  edit 2: looks like there is already a job called "smoker"... @rvagg are you cool with renaming to smoker-archive or just straight up nuking it?  I have moved forward with renaming thealphanerd-smoker to citgm-smoker for now 
joaocgreis		Removed old jobs, now I only have 6, all work in progress. Will keep this in mind.  @jbergstroem +1 for 20. 
orangemocha		I deleted my jobs, they were no longer useful. Thanks! 
mhdawson		Delete the 2 with my name on them.  Had been on the edge of doing that and this was  the last push I needed. 
rvagg		@jbergstroem 20 sounds good for now 
rvagg		@nodejs/testing in case you weren't aware, we clean out Jenkins builds after 30 days and are now talking about going down to 20, simply because Jenkins doesn't really scale in the way we need. If you're linking to build output in issues and you think you may want to retain the output for future reference you should probably be copying and pasting directly into issues rather than relying on them being available next time you need them. 
santigimeno		@rvagg Thanks for the heads up. 
mhdawson		Just seeing if we can close this one out. @jbergstroem @joaocgreis do you still need the remaining ones with your names ?  
jbergstroem		@mhdawson yes i want mine (but feel free to close from my end) 
joaocgreis		I still need the ones with my name as well. I'll go ahead and close, please reopen if I missed anything. 
indutny		NOTE: I'm not sure if the lldb-3.6 is available on all of these platforms. Please re-check! 
jbergstroem		Does it specifically require 3.6 or is it the lowest req? 
indutny		I don't think it is the lowest required version, but I know for sure that it works starting from it. 
indutny		Additional note: while not absolutely necessary, it will make things easier if the version of lldb will be the same across the machines. I'll need to supply path to `lldb` when running `llnode`'s build script, and this path depends on the version. 
MylesBorins		@indutny why is only ubuntu included? Smoke testing also runs on fedora, osx, and ppc 
indutny		@TheAlphaNerd I'm sure that this thing works on OS X and Ubuntu, and OS X doesn't need any additional packages. I may take a look at fedora, but don't have access to ppc. 
jbergstroem		@indutny that's kind of why I asked. Can only assume that mileage will vary between the different systems. I know FreeBSD only have 3.7 or 3.8. 
indutny		So anything actionable on this? 
jbergstroem		@indutny yeah; likely finding out all packages on the platforms we support and adding them to the playbooks. I can look at this, likely end of week. Is something held up by this at the moment? 
maclover7		ping @indutny -- is this still needed? If yes, this will need to be updated to change `ansible`, not `setup`
BridgeAR		This should likely be closed. There was no progress for more than 1.5 years now.
mmarchini		Closing since this has been inactive for quite some time now and https://github.com/nodejs/build/pull/1025 downloads `lldb` before testing llnode. Feel free to reopen if you believe this should still land though.
Trott		I think this is primarily maintained by the IBM folks, so... /ping @mhdawson @gibfahn
gibfahn		>I think this is primarily maintained by the IBM folks, so... /ping @mhdawson @gibfahn  Yep, we're looking at it (that's who @nodejs/platform-ppc is!)
gibfahn		The jobs are hanging here:  ```bash out/Release/cctest --gtest_output=tap:cctest.tap [==========] Running 48 tests from 6 test cases. [----------] Global test environment set-up. [----------] 2 tests from Base64Test [ RUN      ] Base64Test.Encode [       OK ] Base64Test.Encode (0 ms) [ RUN      ] Base64Test.Decode [       OK ] Base64Test.Decode (0 ms) [----------] 2 tests from Base64Test (0 ms total)  [----------] 3 tests from EnvironmentTest [ RUN      ] EnvironmentTest.AtExitWithEnvironment [       OK ] EnvironmentTest.AtExitWithEnvironment (375 ms) [ RUN      ] EnvironmentTest.AtExitWithArgument [       OK ] EnvironmentTest.AtExitWithArgument (354 ms) [ RUN      ] EnvironmentTest.MultipleEnvironmentsPerIsolate ```  Looks like the hanging test is `EnvironmentTest.MultipleEnvironmentsPerIsolate`.
gibfahn		`ps -ef` on the machine shows nothing out of the ordinary:  ``` iojs      7205 29041  0 05:31 ?        00:00:00 make test-ci iojs      9930  7205  0 05:32 ?        00:00:01 out/Release/cctest --gtest_output=tap:cctest.tap iojs     29038  1005  0 05:30 ?        00:00:00 /bin/sh -xe /tmp/hudson6064430299175217652.sh iojs     29041 29038  0 05:30 ?        00:00:00 make run-ci -j 2 ```
mhdawson		Looking at the recent history I only see a single failure that looks like that hang.
refack		Did anyone add an auto-termination? This one seems to have stoped on it own https://ci.nodejs.org/job/node-test-commit-plinux/10156/nodes=ppcbe-ubuntu1404/ ``` [----------] 3 tests from EnvironmentTest [ RUN      ] EnvironmentTest.AtExitWithEnvironment [       OK ] EnvironmentTest.AtExitWithEnvironment (369 ms) [ RUN      ] EnvironmentTest.AtExitWithArgument [       OK ] EnvironmentTest.AtExitWithArgument (359 ms) [ RUN      ] EnvironmentTest.MultipleEnvironmentsPerIsolate make[1]: *** [test-ci] Terminated make[1]: Leaving directory `/home/iojs/build/workspace/node-test-commit-plinux/nodes/ppcbe-ubuntu1404' make: *** [run-ci] Error 2 Build step 'Execute shell' marked build as failure Run condition [Always] enabling perform for step [[]] TAP Reports Processing: START Looking for TAP results report in workspace using pattern: *.tap Did not find any matching files. Setting build result to FAILURE. Checking ^not ok Jenkins Text Finder: File set '*.tap' is empty Sending e-mails to: michael_dawson@ca.ibm.com gib@uk.ibm.com Notifying upstream projects of job completion Finished: FAILURE ``` 
Trott		> Yep, we're looking at it (that's who @nodejs/platform-ppc is!)  Oops, I missed that @refack had already pinged that team. Sorry!
mhdawson		The hangs seems to have occurred on both be-1 and be-2 from the history.
mhdawson		Jobs seem to be running/completing on both be-1 and be-2 now.  The failures are related to https://github.com/nodejs/node/issues/14177 which is not specific to PPC
gibfahn		I killed the `out/Release/cctest --gtest_output=tap:cctest.tap` process on both machines, this seems to have fixed it, I have no idea why. It was definitely hanging before.
mhdawson		Ok I guess we can just keep an eye on it for today.
mhdawson		Just noticed same failure on arm job as well: https://ci.nodejs.org/job/node-test-commit-arm/10826/nodes=armv7-wheezy/console
refack		https://ci.nodejs.org/job/node-test-commit-plinux/10165/nodes=ppcbe-ubuntu1404/console
mhdawson		No instance of cctest running this time on the machine. 
Trott		Happening here too: https://ci.nodejs.org/job/node-test-commit-plinux/10166/nodes=ppcbe-ubuntu1404/console
mhdawson		This is the test  ```cc TEST_F(EnvironmentTest, MultipleEnvironmentsPerIsolate) {   const v8::HandleScope handle_scope(isolate_);   const Argv argv;   Env env1 {handle_scope, isolate_, argv};   Env env2 {handle_scope, isolate_, argv};    AtExit(*env1, at_exit_callback1);   AtExit(*env2, at_exit_callback2);   RunAtExit(*env1);   EXPECT_TRUE(called_cb_1);   EXPECT_FALSE(called_cb_2);    RunAtExit(*env2);   EXPECT_TRUE(called_cb_2); } ```  From test/cctest/test_environment.cc 
jBarz		I will investigate this and provide an update
mhdawson		@jBarz thanks :)
refack		https://ci.nodejs.org/job/node-test-commit-linux/11117/nodes=centos5-64
refack		https://ci.nodejs.org/job/node-test-commit-plinux/10172/nodes=ppcbe-ubuntu1404/console
mhdawson		Given that we now have seen this on centos and ARM in addition to PPC, this should likely be moved to the normal repo as a problem with the test across platforms.  @refack any objections ? 
mhdawson		My guess is that disabling snapshots has shifted the timing to expose and existing problem either in the test or v8 itself.
refack		No objection, just a suggestion to implement some sort of timeout on jobs spawned by `node-test-commit`, they should not run (actual time on runners) more the 2h...
refack		Closing in favor of https://github.com/nodejs/node/issues/14206
joaocgreis		+1  I remember some discussion around this, but I can't find it. @evanlucas do you remember? If it was simply not ready at the time, is it now?
gibfahn		The job is there, it just needs to be included (as long as it's working). I see that @evanlucas has access to it.  https://ci.nodejs.org/job/node-test-commitmsg/
gibfahn		>@Trott thanks for the suggestion. Have installed it now. Do you use it when reviewing as well or just as a sanity check for the final land ? > >I also wonder if we could add running this to the node-test-pull-request in addition to the linter ?  @mhdawson in reply to https://github.com/nodejs/node/commit/d5b397c9b63507a5dcf6d1da47df2a090d3d73a0#commitcomment-23061905  I use it as a sanity check before landing. It'll fail before then anyway as it won't find the `PR-URL` or `Reviewed-By` lines. I think @refack has it set up as a pre-commit hook (so you can't commit without it being correct), though I could be misremembering.
mcollina		I am -1 with this change as proposed. This will fail for all the PRs on the first go, because they lack reviews and a PR url. It will just increase the amount of nit-picking, as we will likely require to fix those details by the author, while we currently fix them on landing.  If we can disable those checks for the job, then I am +1.
refack		> I think @refack has it set up as a pre-commit  `pre-push`, and it has a `if ($2.includes('/nodejs/node'))`. I don't want to tie my hands completely.  P.S. if it could be added as a _warning_ to the linter job that might be better. Or if at some point we make a `node-land` job (could take a forced-pushed PR or an arbitrary branch in the landers repo) that's the more appropriate place.
evanlucas		> It'll fail before then anyway as it won't find the PR-URL or Reviewed-By lines.   You can pass the `--no-validate-metadata` flag and it won't fail from a missing PR-URL or missing Reviewed-By.  The job that is there works pretty well. It can be run against a single commit or all of the commits for a PR. It becomes tricky with fixup commits though...
fhinkel		`core-validate-commit` is great! But iIrc it complains about Chromium backports because a line with the URL is too long. 
evanlucas		@fhinkel I‚Äôll try to get around to fixing that. 
joaocgreis		According to https://wiki.jenkins.io/display/JENKINS/Git+Plugin , it's the git plugin that sets `GIT_COMMIT`. The `*-fanned` jobs don't use the git plugin because they don't need to (running it would make them slower). This is why that variable does not exist, not because `git-rebase` clears it.  The `git-rebase` job is needed to rabase (which is done by a build step in other jobs) and push the commit to test to the temp repo. This rebase step could be part of the compile jobs, but it would make them more complex than they already are, so I'd prefer not to go that way. What we can do to make this work is add a git checkout in the fanned jobs and accept the delay. 
gibfahn		Couldn't the node-test-commit job just pass `GIT_COMMIT` down to the subjobs?
joaocgreis		I've added the parameter `GIT_COMMIT` to both fanned jobs, and logic in `node-test-commit` to pass it down. Seems to be working as expected, please reopen if not.
rvagg		@saghul sorry, a borked redirect put an https in front and it'll work (will fix it now). A bunch of the bots have been down and I've been taking the opportunity to redo them all, changing the name from "node-forward" to "iojs" and documenting the setup procedure in this repo for each of them. The main Windows machine went down and I can't get it back (stuck in a reboot and I haven't got a response from Rackspace about it) so I've spun up 2 more and was going to document the setup procedure properly as I do them again but I haven't got to it yet!  The ones that should be working are the containers which have been our priority recently, we've redone the way they work and now have 2 large machines running container builds for 3 x Ubuntu versions and 2 x Debian versions so far. They _should_ work for libuv but I haven't tested that yet.  Will keep you posted here. 
saghul		Thank you so much Rod! :heart_eyes:  
saghul		Since I'm here, looks like the parametrized build thing is no longer working, see: https://jenkins-iojs.nodesource.com/view/libuv/job/libuv+any-pr+containers/4/ 
rvagg		fixed: https://jenkins-iojs.nodesource.com/view/libuv/job/libuv+any-pr+containers/nodes=iojs-containers-ubuntu-trusty/5/ 
saghul		Works, thanks! 
jbergstroem		@misterdjules we changed [the logic for what smartos version builds what node version](https://github.com/nodejs/build/issues/601) recently. It seems that the user is trying to use a toolchain version we're not "supporting" (our official support table is yet to be merged). How about we skip 6.x altogether for smartos14 then? 
misterdjules		Building node binaries with GCC 4.8 won't be supported out of the box (i.e without installing any additional package from pkgsrc) on any SmartOS image. This was an oversight from me, and due to miscommunication between myself and the team building SmartOS images and packages.  However, I'm not sure moving from GCC 4.8 to GCC 4.9 is an option within a given major version number due to potential ABI compatibility issues, which is why I was suggesting to move to GCC 4.9 for node >= v8.x.  This is a summary of my understanding of the current situation on SmartOS:  ## Node.js building requirements on SmartOS  Node version | SmartOS image | Compiler | Notes -------------|---------------|----------|------ \>= v4.x <= master | \>= 14.x | GCC 4.9 | The v7.x branch still builds on SmartOS 14.x images because https://github.com/nodejs/node/pull/11029 floated [a patch that fixes compilation on these images](https://github.com/nodejs/node/pull/11029/commits/fd04af18d34f4b4315ac30b19e28ce3d21efaff0). master | \>= 15.x | GCC 4.9 | the master branch relies on [a GCC bugfix](https://github.com/joyent/pkgsrc/issues/270) that hasn't backported to the pkgsrc repository that comes with SmartOS 14.x images  ## Node.js runtime requirements for SmartOS  Node version | SmartOS image | Notes -------------|---------------|-------- \>= 0.10.x | \>= 1.7.0         | SmartOS images >= 1.7.0 < 14.4.0 are not supported though, so one should really use images \>= 14.4.2 \>= v4.x <= v8.x | \>= image version used to build node binaries (is it 14.x?) and < 16.4.x | The gcc4.8-libs package needs to be installed, because node binaries have been built with GCC 4.8, for which runtime libraries are not installed by default. For these node versions, the recommended binaries are the ones available in pkgsrc, not the one available from nodejs.org. Note that the binaries downloaded from the pkgsrc repositories are not officially supported by the Node.js project, and instead are supported by Joyent. SmartOS images >= 16.4 are not supported because GCC 4.8 runtime libraries are not available in their pkgsrc repository \>= v8.x | >= 15.4.0 | node binaries should be built with a GCC 4.9 that includes [fixes that haven't been backported to 14.x images](https://github.com/joyent/pkgsrc/issues/270)  @jbergstroem Does the above look correct? Also, on what SmartOS images are node binaries for versions >= v4.x and <= v8.x currently built? That would determine the minimum SmartOS image version required to run those (with the caveat of needing to install GCC 4.8 runtime libraries, as noted in the table above).  Also pinging @jperkin for feedback.
chorrell		Looks good!   I think it might be useful to add the SmartOS image type (e.g., base-64-lts or base-32-lts) as well as the image version (e.g., 15.4.x).  I think it would also be good to spell out the pkgsrc release version the given image version corresponds to. So for example:   | Node version  | SmartOS image | pkgsrc release | Compiler  | Notes | | -------------- | --------------- | --------------- |---------- | ------ | | >= v4.x <= master | >= base-64-lts 14.4.x | 2014Q4 |  GCC 4.9   | The v7.x branch still builds on SmartOS 14.x images because nodejs/node#11029 floated a patch that fixes compilation on these images. | | master            | >= base-64-lts 15.4.x  | 2015Q4 |  GCC 4.9   | the master branch relies on a GCC bugfix that hasn't backported to the pkgsrc repository that comes with SmartOS 14.x images |  For the image type, I'm assuming 64-bit pkgsrc (base-64-lts) is what is required. Also, for image and pkgsrc versions I would generally recommend sticking with the Q4 (14.4.x, 15.4.x, 16.4.x etc) release since those are our LTS releases. 
Trott		AFAIK, two years later, we're still using GCC 4.8. Or at least that's what we say in https://github.com/nodejs/node/blob/master/BUILDING.md#supported-platforms-1. Is that correct? And if so, should we do something to change it?
richardlau		>  >  > AFAIK, two years later, we're still using GCC 4.8. Or at least that's what we say in https://github.com/nodejs/node/blob/master/BUILDING.md#supported-platforms-1. Is that correct? And if so, should we do something to change it?  https://github.com/nodejs/node/pull/25684
gibfahn		You're right, we're passing the output to a TAP file, but not actually running the TAP parser on it. I tried enabling it, but the TAP plugin fails to parse it as it isn't valid TAP.  ``` Looking for TAP results report in workspace using pattern: test-eslint.tap Saving reports... org.tap4j.parser.ParserException: Error parsing TAP Stream: Duplicated TAP Header found. 	at org.tap4j.parser.Tap13Parser.parseTapStream(Tap13Parser.java:230) Caused by: org.tap4j.parser.ParserException: Duplicated TAP Header found. 	at org.tap4j.parser.Tap13Parser.parseLine(Tap13Parser.java:320) 	at org.tap4j.parser.Tap13Parser.parseTapStream(Tap13Parser.java:224) 	... 15 more TAP parse errors found in the build. Marking build as UNSTABLE ```  The problem with the generated TAP output is that it's actually multiple TAP files concatenated together, so there are multiple TAP headers.  ``` TAP version 13 1..1 ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/vm/run-in-context.js  TAP version 13 1..1 ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/vm/run-in-this-context.js  TAP version 13 1..2 ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/util/normalize-encoding.js ok 2 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/v8/get-stats.js ```  In the short term you can view the file directly (assuming you have the permissions) by going to [the workspace](https://ci.nodejs.org/job/node-test-linter/ws/test-eslint.tap/*view*/).
gibfahn		Looks like we can definitely blame @jbergstroem for this üòú   @mscdex https://github.com/nodejs/node/pull/5638#issuecomment-194689171  >@jbergstroem Does the tap output have to all be in one single group of results (e.g. only one TAP header in the output file) or does it not matter?  ---  @jbergstroem https://github.com/nodejs/node/pull/5638#issuecomment-194694304  >@mscdex I recall globbing files ("collect *.tap") working just fine. 
gibfahn		So basically we need to update [tools/jslint.js](https://github.com/nodejs/node/blob/master/tools/jslint.js) to remove all but the first instance of `TAP version 13`, I think that should work.
refack		I'm on it.  P.S. the `workspace` will only show that last run, right?
gibfahn		>P.S. the `workspace` will only show that last run, right?  Yes, it's overwritten every time.
gibfahn		>The plan cannot appear in the middle of the output, nor can it appear more than once.  I lied, the plan [can't be repeated](https://testanything.org/tap-version-13-specification.html), I tested it and it still breaks the TAP parsing plugin. The easiest solution is to use Subtests that Jenkins can understand (see the [plugin docs](https://wiki.jenkins-ci.org/display/JENKINS/TAP+Plugin)):  I've confirmed that this works in a [test job](https://ci.nodejs.org/view/All/job/gibfahn-tmp/4/console):  ```tap TAP version 13   1..9   ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/legacy-vs-whatwg-url-searchparams-serialize.js   ok 2 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/legacy-vs-whatwg-url-serialize.js   ok 3 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/url-format.js   ok 4 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/url-resolve.js   ok 5 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/url-searchparams-iteration.js   ok 6 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/url-searchparams-read.js   ok 7 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/url-searchparams-sort.js   ok 8 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/usvstring.js   ok 9 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/whatwg-url-idna.js ok 1 Second set     1..4   ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/url/whatwg-url-properties.js   ok 2 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/util/format.js   ok 3 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/util/inspect-proxy.js   ok 4 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/util/inspect.js ok 2 Third set     1..1   ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/vm/run-in-context.js ok 3 Fourth set   1..1   ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/vm/run-in-this-context.js ok 4 Fifth set   1..2   ok 1 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/util/normalize-encoding.js   ok 2 - /usr/home/iojs/build/workspace/node-test-linter/benchmark/v8/get-stats.js ok 5 sixth set 1..5 ```
refack		I started writing the PR... ‚úçÔ∏è 
gibfahn		I changed   ``` gmake lint-ci ```  to  ```bash gmake lint-ci || { cat test-eslint.tap && exit 1; } ```  , so if the test fails you'll get the test output catted to the console.  It's not the best solution, but I think it solves the problem in the immediate term.
gibfahn		New solution (to hopefully avoid #906) and make it easier to see what's going on:  ```bash #!/usr/bin/env bash -ex # If lint-ci fails, print all the interesting lines to the console. # FreeBSD sed can't handle \s, so use gsed if it exists. which gsed &>/dev/null && SED=gsed || SED=sed gmake lint-ci || {    cat test-eslint.tap | grep -v '^ok\|^TAP version 13\|^1\.\.' | $SED '/^\s*$/d' &&    exit 1; } ```  Filters out the passing tests, the `TAP version 13` headers, and the `1..10` lines.
refack		`grep -v -P "^(ok|1\.\.|TAP|$)" test-eslint.tap`
gibfahn		Updated comment above, the script got a bit more complicated because:  - `/bin/bash` doesn't exist - POSIX `sed` doesn't support `\s` for whitespace. - The linter runs on two boxes, one is `test-rackspace-freebsd10-x64-1` which has `gsed`, the other is`test-joyent-freebsd10-x64-2` which doesn't.  So at the moment on the `joyent` box it doesn't trim whitespace, and on the other `rackspace` box it does. If we could: - Work out the POSIX equivalent of `sed '/^\s*$/d'` is (and test it on the `joyent` box) - Install gsed on the joyent box  That would improve the situation.  At the moment the `freebsd` box trims blank lines, but the `joyent` one doesn't. It's better than before, but not ideal.
gibfahn		This is done for now, better solutions always appreciated.
jbergstroem		LGTM. Perhaps open a new issue/PR that edits the root readme about ansible requirements? 
jbergstroem		I can set it up unless anyone from the build group has opinions on how it should be done. I'll wait around for a day or three.
mikeal		The initial request has been sitting for like 20 days already, so the quicker the better ;)
jbergstroem		So, I looked at it (over at cloudflare) and it's not handled through there -- rather in our [nginx config](https://github.com/nodejs/build/blame/master/setup/www/resources/config/nodejs.org#L163).  We should probably add SSL redirects for both as well.
mikeal		Unfortunately the newsletter listing page isn't available in TLS. However, the link to signup which actually accepts personal information you may not want to leak is TLS.
jbergstroem		@mikeal was just referring to capturing `https://newsletter.nodejs.org` in our redirect rule as well. It would land in `default_server` otherwise; likely implying `nodejs.org`.
mikeal		ahhhhh
mikeal		poke :)
rvagg		pls review @mikeal @jbergstroem https://github.com/nodejs/build/pull/652
rvagg		2 new machines  https://ci.nodejs.org/computer/nodejs-scaleway-armv7-wheezy-1/ https://ci.nodejs.org/computer/nodejs-scaleway-armv7-wheezy-2/  taking offline  https://ci.nodejs.org/computer/iojs-online_net-armv7-wheezy-1/ https://ci.nodejs.org/computer/iojs-online_net-armv7-wheezy-2/ 
joaocgreis		Thanks for adding monit! I'm doing the ansible for the cross compiling machine, already included it. I changed it to be a little more generic by using the server_user variable, you can include it if you want. The file `resources/monit-jenkins.conf` became:  ``` check process jenkins   matching slave.jar   start program = "/bin/su {{server_user}} -c 'cd /home/{{server_user}} && /home/{{server_user}}/start.sh'"   stop program = "/bin/su {{server_user}} -c 'pkill jenkins'" ```  And added this after the `Copy monit config` in the playbook:  ```     - name: Monit | Copy server user name to monit config       replace: dest=/etc/monit/conf.d/jenkins regexp="\{\{server_user\}\}" replace="{{ server_user }}"       tags: monit ```  I did not test run your changes here, but the text LGTM. 
rvagg		@joaocgreis we should put that into ansible-tasks/monit.yaml cause it's used in a few places now, do you want to do that? 
joaocgreis		@rvagg Sounds good, I'll do it. 
jbergstroem		(re suggestion above) Config probably needs to live elsewhere though since some machines are into init scripts. Also, newer upstarts and systemd afaik handles restarting/keepalive for you. 
jbergstroem		..and back up. 
jbergstroem		Great post, thanks for the update. 
aqrln		Sites hosted on other Node.js domains that I tried loaded successfully with a new wildcard certificate expiring on November 21, 2019, so it is only an issue with the Jenkins server which still has an outdated certificate.
gibfahn		>You cannot visit ci.nodejs.org right now because the website uses HSTS. Network errors and attacks are usually temporary, so this page will probably work later.  Yeah, this is an urgent issue.
gibfahn		Probably related to https://github.com/nodejs/build/issues/812, cc @rvagg @mhdawson @joaocgreis @jbergstroem (I assume it's an infra thing).
rvagg		Sorry, this didn't come up in my mentions list so I'm a bit late to the party! I think because you added the mentions in an edit @gibfahn? Edits don't go out as emails so I don't have a mention in the email I have for this so got to it later than my more urgent queue of mentions. Best to do mentions in a new comment in future to be sure.  Done now, on both ci.nodejs.org and ci-release.nodejs.org. FYI these certs and keys are in secrets/build/infra/, encrypted for build/infra eyes only of course.
mhdawson		Can we close the issue ? 
aqrln		@mhdawson yes, absolutely. I forgot about it, sorry.  @rvagg thanks for resolving the issue!
joaocgreis		This is not a but, it's just a "feature" of Jenkins that is confusing in our use case. When the build is skipped, the green icon is shaded (`<img ... style="opacity:0.5"`), it should look like this: ![image](https://cloud.githubusercontent.com/assets/134460/23863911/e40bc1d2-0808-11e7-999c-387ae0355b1b.png) Compare with a successful build that should look like this: ![image](https://cloud.githubusercontent.com/assets/134460/23863945/fdeacd0a-0808-11e7-9ca1-f32ec85becd7.png)  I don't think there's much we can do about this. Unless perhaps split the `node-compile-windows` job, but that would make it harder to maintain and we'd have to figure out how to trigger the correct one.
targos		I see. Thank you for the explanation.
gibfahn		Sounds good to me
jbergstroem		SGTM
jbergstroem		(just adding so i don't forget: I need to introduce the `benchmark-` naming in the ansible refactor so we can make an assumption about ssh keys)
mhdawson		Ok team created and jobs configured to allow teams appropriate level of access.
jbergstroem		@mhdawson we need ssh keys as well, right?
mhdawson		Yes figured we'd cover that under https://github.com/nodejs/build/issues/642
jasnell		:+1: sounds like a solid approach. 
mkdolan		@rvagg did you get the email with our LF point of contact on the DNS? Please also sync with @misterdjules as well b/c we wouldn't want anything to break.  
rvagg		@mkdolan yep, I have that email, and yes, @misterdjules should definitely be part of the process, I'm not suggesting a radical break with the past or anything and want to make sure that any legacy concerns are taken care of while we're doing this.  I'll get iojs.org set up first so I can use that as a concrete proposal and we can assess from there. 
mkdolan		Great, just wanted to make sure all the connections are there.   > On Jun 26, 2015, at 8:43 AM, Rod Vagg notifications@github.com wrote: >  > @mkdolan https://github.com/mkdolan yep, I have that email, and yes, @misterdjules https://github.com/misterdjules should definitely be part of the process, I'm not suggesting a radical break with the past or anything and want to make sure that any legacy concerns are taken care of while we're doing this. >  > I'll get iojs.org set up first so I can use that as a concrete proposal and we can assess from there. >  > ‚Äî > Reply to this email directly or view it on GitHub https://github.com/nodejs/build/issues/124#issuecomment-115667038. 
jbergstroem		Since we're using cloudflare I don't see this change in the foreseeable future. Closing. 
rvagg		Added the new batch of Pi's, also added the full NFS configuration, used the shared monit task (imported @joaocgreis PR), also made the git and ccache tasks idempotent(ish) in that they test the version that's installed and skip if it's the one that's been requested to be installed.  SO, the upshot of this latest batch of changes is that I can run this playbook against the Pi cluster and it's relatively quick (in Pi terms) and doesn't mess up existing config for Pi's that are already set up. How it _should_ be. 
jbergstroem		2 questions: - Is it possible to check for an existing binary once and bail instead of at every task? (`ccache_version.stdout.find('{{ version }}') == -1`, etc) - can we improve the monit check to look for slave name? That way we could use it out of the box for jobs where we run multiple slaves (centos5-gcc41 comes to mind)  I'm guessing the checks are for packages that are already installed through a package manager or similar? How about attempting to uninstall them? 
rvagg		> Is it possible to check for an existing binary once and bail instead of at every task? (ccache_version.stdout.find('{{ version }}') == -1, etc)  I looked for this but didn't find it, perhaps you can find something better?  > can we improve the monit check to look for slave name? That way we could use it out of the box for jobs where we run multiple slaves (centos5-gcc41 comes to mind)  ``` $ ssh iojs-ns-pi1p-1 ps auxww | grep java iojs      2090  1.0  9.2 394412 41208 ?        Sl   Sep10  18:07 java -jar slave.jar -jnlpUrl https://jenkins-iojs.nodesource.com/computer/iojs-nodesource-raspbian-wheezy-pi1p-1/slave-agent.jnlp -secret secret_omitted_from_paste ```  So yes, it could be done for multiple slaves, just match the jenkins id, in this case `iojs-nodesource-raspbian-wheezy-pi1p-1`, currently we're just doing a check for `matching slave.jar`.  For the purpose of the Pi's this is fine, perhaps the next round of changes can make it more flexible. 
jbergstroem		Regarding version check;  the simplest improvement would be using ansibles [`register`](http://docs.ansible.com/ansible/playbooks_conditionals.html#register-variables) and check against that. The other option would be combining above result with a [fail check](http://docs.ansible.com/ansible/fail_module.html).   Anyway, above suggestions are improvements, not requirements; LGTM for progress. 
seishun		I would like to work on this, but I couldn't find documentation on how this whole thing works and how I can test it locally. I would be grateful if someone can point me in the right direction.  /cc @nodejs/build 
gibfahn		@seishun old ansible scripts are in [`setup/`](https://github.com/nodejs/build/tree/master/setup), new ones are in [`ansible/`](https://github.com/nodejs/build/tree/master/ansible), we're trying to move over to the new scripts, but they're not completely tested yet.  The [`README` in the ansible directory](https://github.com/nodejs/build/blob/master/ansible/README.md) should have the info you need to get started, and the scripts should be documented. If you run into any issues with it let us know.
seishun		@gibfahn I'm a complete noob here, would https://github.com/nodejs/build/blob/master/setup/TESTING_LOCALLY.md still work for testing new scripts?
refack		@seishun I tried to follow the guides once but got distracted... You doing it for the first time is a good chance to document missing pieces and hidden assumption, so it would be great if you open a `doc` PR or pass the info to me and I'll PR it.
gibfahn		@seishun in theory yes. Actually that document should really be ported over to `ansible/`.
seishun		There are 5 remaining platforms in CI with outdated gcc:  * centos7-64. This should be simple, just need to figure out if the `ansible/` scripts work well for CentOS 7 or the old scripts in `setup/` should be changed instead. * rhel72-s390x. It's not listed in [ansible/roles/baselayout/vars/main.yml](https://github.com/nodejs/build/blob/master/ansible/roles/baselayout/vars/main.yml), so presumably it was deployed using [setup/rhel72-linuxonecc](https://github.com/nodejs/build/tree/master/setup/rhel72-linuxonecc). I'm also not sure how one can test the changes. @refack mentioned that you can get trial access [here](https://developer.ibm.com/linuxone/) - will that work? * aix61-ppc64. Same as above - is [setup/aix61](https://github.com/nodejs/build/tree/master/setup/aix61) the right script? Is there any way I can test this? * cc-armv6 and cc-armv7. These two run on a machine called `node-msft-cross-compiler-1`, which is absent in both [ansible/inventory.yml](https://github.com/nodejs/build/blob/master/ansible/inventory.yml) and [setup/ansible-inventory](https://github.com/nodejs/build/blob/master/setup/ansible-inventory). Which OS does it run? Are there Ansible scripts for it?
mhdawson		For rhel72-s390x, @jbarz is working on this.  The problem is that new ansible templates were created which did not included full support for all platforms.  The old templates were, still are used to deploy to the existing machines.  Please don't update without co-ordinating with @jbarz For aix61-ppc64 - I think @gibfahn is looking into that, please don't update without co-ordinating with him.
seishun		>For rhel72-s390x, @jBarz is working on this. >For aix61-ppc64 - I think @gibfahn is looking into that  Are there issues for tracking these?
mhdawson		AIX is coverd in https://github.com/nodejs/build/issues/925  rhel72-s390x, possibly not, I more recently asked jBarz to start working on that.  @jBarz can you open an issue for tracking the work on linuxOne.
jBarz		Done for linuxOne https://github.com/nodejs/build/issues/1023
joaocgreis		@seishun about the cross compiler, there is a PR that never landed: https://github.com/nodejs/build/pull/244
seishun		@joaocgreis Looks like it uses `arm-linux-gnueabihf-gcc` and `arm-linux-gnueabihf-g++` from https://github.com/raspberrypi/tools. These files haven't been updated in 4 years. Any idea what could be used instead to have gcc 4.9.4 or newer?
seishun		Or is someone already working on upgrading gcc on that machine?
mhdawson		@rvagg I seem to remember you already mentioning doing some work for 4.9.4 on the raspberry PIs
rvagg		No, I was working on 4.9 for the armv7 wheezy release machines but couldn't come up with a solution for it that didn't rely on a newer libc that screwed up release builds, we suffer from the same problem with the release pi's too. This is all on Wheezy which we are using on release armv6 and armv7. We're using the same compiler for both, which is bad because we get armv6 binaries on armv7 but I don't have a solution yet. If this is a _must_ then I think we're going to have to consider building release builds with on Jessie, or Ubuntu 14.04 and dropping official support for Wheezy-era distros (Ubuntu 12.04 included) for the Node versions that require this level of compiler.
seishun		>If this is a must then I think we're going to have to consider building release builds with on Jessie, or Ubuntu 14.04 and dropping official support for Wheezy-era distros (Ubuntu 12.04 included) for the Node versions that require this level of compiler.  Support for Wheezy ends on [31st May 2018](https://wiki.debian.org/LTS), which is shortly after Node 10.x release, so dropping it at least on master is reasonable.
rvagg		> Support for Wheezy ends on 31st May 2018, which is shortly after Node 10.x release, so dropping it at least on master is reasonable.  +1 but how should we document this? tbh I'm not at all happy with this approach of documenting kernel versions, nobody thinks in terms of kernel versions. libc versions are a tiny bit more relevant but perhaps we need to use distribution names or release timeframes, or at least make note of it in our supported systems documentation.
seishun		>or at least make note of it in our supported systems documentation.  The documentation already mentions that EOL distros are not supported: https://github.com/nodejs/node/blob/master/BUILDING.md#supported-platforms-1
seishun		It looks like things have changed since 29 Nov 2017 (https://github.com/nodejs/build/issues/762#issuecomment-347976392).  * cc-armv6 and cc-armv7 are gone. @joaocgreis is this intentional? * A test commit that requires gcc 4.9 fails on centos7-arm64: https://ci.nodejs.org/job/node-test-commit-arm/12930/nodes=centos7-arm64/ Was this machine introduced, re-enabled or modified recently? How is it deployed? Is someone working on upgrading gcc there?
joaocgreis		@seishun the `arm-fanned` job was not available because of cluster maintenance. It's back now.  About the compiler, I don't know what other compiler we can use. If someone wants to give it a try, we can probably give access or work out some way.
seishun		@joaocgreis  Access would be great. I also happen to have a Raspberry Pi 3 (I just need to buy a cable and a memory card, I think) so I can tinker with it locally, but I would need help setting things up. For instance, what's the environment on Raspberry Pi on CI? How do I make sure the cross-compiled binary works? Is it enough to just copy the binary to the Raspberry Pi and run it, or are there more sophisticated steps?  Also, is there a particular reason why it uses binaries under https://github.com/raspberrypi/tools/tree/master/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin? Is there something special about them? For example, why not use binaries under https://github.com/raspberrypi/tools/tree/master/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin instead? (4.9.3 < 4.9.4, but it should work as a stopgap measure)
mhdawson		Instructions for setup are here: https://github.com/nodejs/build/tree/master/setup/raspberry-pi (at least that is what I used when I setup the one for the release CI a little while ago)
seishun		Are these instructions meant for the cross-compiling setup? The installation of GCC makes me suspect it isn't. And it doesn't mention the OS.
mhdawson		No, for compilation on the machines themselves. Maybe @rvagg know about cross-compiling.
joaocgreis		About the folder with 4.9.3 binaries, it did not yet exist when I set up the cross-compile machine. I gave those a try locally and they seem to work, but they can't compile V8 6.5. The cross-compile machine should be very easy to update if that's the way forward, but the release machines compile natively and I don't know what compiler we can use there - it doesn't make sense to update the test CI if the releases would fail.  About the CI setup, cross-compiling is essential to reduce the time it takes to compile (up to 1 day in some conditions), but there's no other reason to use it. Thus, the Raspberry Pi workers connected to test CI don't compile node, but compile the addons - in `node-test-binary-arm` the `addons` row should be the only one that needs a compiler installed. The cross-compiler was deployed (once, more that 2 years ago) with `setup/cross-compiler` and the Raspberry Pis with `setup/raspberry-pi`. The releases are compiled natively to ensure that node can still be compiled on the devices and to have a last line of defense against any issue with cross-compilation.  Testing this locally is not very hard, at lease should not require Ansible. To compile just follow and adapt the script of `node-cross-compile`, commands are echoed to the console in any run. To run tests use `node-test-binary-arm`. This boils down to: clone the `raspberrypi/tools` repository and adapt [our script](https://github.com/nodejs/build/blob/master/setup/cross-compiler/resources/cc-armv6.sh) to the correct location. Then source it on the shell where node will be compiled and run `CONFIG_FLAGS='--dest-cpu=arm' make build-ci -j 10`. Use the tar command at the end of the script to pack the binaries. Then, on a Raspberry Pi (can be emulated with qemu, there are instructions online), check out the same revision of node, unpack the binaries and run `make test-ci-js` (the `--run` argument used in CI is to divide the tests by several runners).  If there is no supported compiler that we can use for Raspberry Pi, shouldn't we be talking about dropping support for it in node v10 instead? Are there sill supported OSs for the Pi 1? 
seishun		>I gave those a try locally and they seem to work, but they can't compile V8 6.5.   What was the error?  >but the release machines compile natively and I don't know what compiler we can use there  Support for Wheezy ends on 31st May 2018, I assume it will be dropped in Node 10. Jessie seems to have gcc 4.9.2 in the repos, can that compile V8 6.5?  >Are there sill supported OSs for the Pi 1?  According to [Wikipedia](https://en.wikipedia.org/wiki/Raspbian), Jessie works on Raspberry Pi 1.
rvagg		> Support for Wheezy ends on 31st May 2018, I assume it will be dropped in Node 10. Jessie seems to have gcc 4.9.2 in the repos, can that compile V8 6.5?  But we are stuck with it for previous versions of Node, we need to be careful not to drop support for distros, compilers and libc that we are still going to be shipping in 4, 6, 8 even if we're moving to deprecating them for future versions. We are running Wheezy on the Pi1 and Pi2's and so far have no plan to migrate to Jessie for them. I'm not really happy with this push to move our compiler versions up without a solid plan to continue to test and support our older LTS branches that we have locked in minimum libc version commitments to.
seishun		>But we are stuck with it for previous versions of Node, we need to be careful not to drop support for distros, compilers and libc that we are still going to be shipping in 4, 6, 8 even if we're moving to deprecating them for future versions. We are running Wheezy on the Pi1 and Pi2's and so far have no plan to migrate to Jessie for them.  Is there an issue with upgrading gcc on the Pi3s and leaving the Pi1s and Pi2s as-is?  >I'm not really happy with this push to move our compiler versions up without a solid plan to continue to test and support our older LTS branches that we have locked in minimum libc version commitments to.  Can you suggest a better plan, then?
joaocgreis		> > I gave those a try locally and they seem to work, but they can't compile V8 6.5. > > What was the error?  The error is exactly the same as the one in https://github.com/nodejs/node-v8/issues/35. Strange thing there is that it happens while compiling native code, but I tried it in a machine with gcc 5.4.0 and it still happens.  
seishun		Then it was a bug in V8 that has been fixed upstream: https://github.com/nodejs/node-v8/issues/35#issuecomment-361385469.
vsemozhetbyt		Refs: https://github.com/nodejs/node/issues/14930 (2 cases)
refack		@maclover7 thanks for following up on this. 1. IMHO this should be done in code, as a test in `/test/sequential` so that we could not only check sanity, but also validate properties of the resulted docs. 2. Meanwhile I see no harm in adding `make doc-only` to `node-test-linter`
maclover7		Looks like this happened again (see https://github.com/nodejs/node/issues/14930#issuecomment-334582142 for details). Personally, I still think compiling docs via node-test-commit (or node-test-linter) is the best solution. I see @gibfahn and @mhdawson gave a +1 to @refack's comment above... what's the best way to move forward here?
gibfahn		>what's the best way to move forward here?  So assuming we're doing this:  >IMHO this should be done in code, as a test in /test/sequential so that we could not only check sanity, but also validate properties of the resulted docs.  that would just be a PR to nodejs/node that added a test in `test/sequential/` that runs `make doc` and verifies that things work okay.
rvagg		Cool, lgtm.  Does the install process for ccache on smartos not make symlinks itself? I'm finding for newer linux distros it's all taken care of automatically.  re restarting Jenkins, I think you have access to do that via the Jenkins interface, otherwise ping me and I can do it manually on the server. 
jbergstroem		@rvagg nope -- symlinking on SmartOS has to be taken care of manually (FreeBSD also sets up a libexec directory). I've updated to the newest pkg repo on my local smartos machines and it seems like we might have to do some path changes forward as well. No biggie though.  As for Jenkins, thanks for letting me know. I'd rather have you do it this time since it's somewhat voodoo. He's probably already annoyed with me.  I'll merge this and #89 if you think that looks good as well and then we'll update/restart. 
jbergstroem		Merged in 46d79036f09a488e13422300b30599fe007117cb . Thanks for reviewing (and doing it so quickly). 
jbergstroem		Found a typo. Crap. `s|/usr/local/libexec|/opt/local/libexec|g`..  
jbergstroem		Great summary, Rod. We should get evangelists a summary somehow. Minor correction: If I recall correctly, both smartos jails are 64-bit but we pass arch=ia32 to one of them. 
Fishrock123		> This week I upgraded this machine to a 60G from a 30G because we filled up the disk with nightly, next-nightly and release builds. We'll need to come up with a scalable solution to this in the medium-term.  Maybe we don't actually need to keep every nightly beyond a certain point? 
cjihrig		What about keeping the last 30 (or some other number) of nightlies and just tagging each nightly on GitHub? I'm not sure if you get anything by tagging nightlies except the historical aspect. 
mikeal		Wow, this is amazing. Thanks Rod! 
silverwind		Great writeup on the situation, appreciated. 
rosskukulinski		Awesome writeup @rvagg.  I see that you've already linked to iojs/evangelism, so this should go out this week's update. 
retrohacker		Amazing writeup @rvagg :heart:  Would like to take a look at setting up something like ganglia to monitor our servers and monit to monitor our service health. Thoughts? 
rvagg		@wblankenship sure, I have no opinions on tooling here, they all seem equally bad and outdated (ganglia lives on sourceforge still for instance; is it better than nagios?). If you want to put in the effort then go for it! We have a basic status page @ https://jenkins-iojs.nodesource.com/computer/ and I have a basic status tool @ https://github.com/nodejs/build/tree/master/tools/jenkins-status that I run manually occasionally but having emails or push notifications for machines and/or slave processes that go down would be amazing, that's what I want. 
retrohacker		So what I'm thinking is that we use ganglia for trending and nagios for alerts. There are tools to integrate the two. Will start looking at deploying ganglia now. 
jbergstroem		My personal needs for monitoring/health would only go as far as "is the vm/machine up?" and "is jenkins running [potentially: is it running properly]". Both is covered by the jenkins status page. 
jbergstroem		Closing -- nothing actionable here. We now have a monitor script if slaves go down (or up). 
rvagg		Sorry for not dropping this earlier but I have a conflict today that's stopping me attending. Some updates from me tho: - Still working on OSX resources, delays are from the company that is _nearly_ ready to commit, I also have a backup macmini nearly ready for my garage (just cause I have a spare one not in action atm!) - New nodejs.org server nearly ready, really needs to be done this week but I've had delays on my end due to new commitments this week - Back up to 14 Pi1's so we have 12 in the test cluster, a complete set finally - Supported platforms list was accepted, I just need to tweak the PR for it to land, we need to adjust it for landing on v4 as well 
mhdawson		PR for minutes: https://github.com/nodejs/build/pull/517 
mhdawson		Minutes landed closing 
rvagg		nice, lgtm 
jbergstroem		Landed in 72a7038ca868e6879c15f2034b81014402bbb40e. Thanks for the review. 
jbergstroem		Is this still an issue? 
MylesBorins		Closing, I think everything is sorted 
jbergstroem		I hear what you're asking, but its not that simple. We've done a lot of profiling of jenkins in the past, and one way to speed it up is to avoid using tap. This is not about the UI being slow; it's about waiting for jenkins to parse hundreds of XMLs. If you search for tap in the issue tracker you'll find a few issues about performance and additionally a stub to a solution (ask gibson if you're at the berlin summit!).  Going to close this, but if you have insights into the fact that it is indeed the UI, please reopen!
refack		Right now the build queue is empty but the UI is still sluggish ü§∑‚Äç‚ôÇÔ∏è (~1 minute from submit until new pages loads, and I got 500-timeouts several times looking at completed jobs) We are all web devs here, and it's hard to believe Jenkins it that badly designed üòï   ![image](https://user-images.githubusercontent.com/96947/27957871-4d6a5072-62ee-11e7-8089-010bdb2032fb.png)  Let's write a new Node.js based CI üòâ 
jbergstroem		I personally still think that moving to buildbot would be the most mature move, but seeing how I have little time to put in at the moment I'm happy with any solution that is more performant.
rvagg		/cc @ljharb 
rvagg		need to remember:  > Moving unpacked Windows binaries to win-x64 and win-x86 directories, with redirects in place for old locations (node-gyp will care, even after we "fix" it, older versions will be in the wild)  someone remind me if we go live and I haven't updated this issue for that or it'll be broken for windows users who aren't using the new npm/node-gyp 
ljharb		> someone remind me if we go live and I haven't updated this issue for that or it'll be broken for windows users who aren't using the new npm/node-gyp  @rvagg you haven't updated this issue and node 4 has gone live 
ljharb		@rvagg @Fishrock123 friendly ping :-) 
rvagg		Ticked everything off, I haven't done anything with win-x64 vs x64, just leaving it as it is for v0.10 and v0.12.  The one change is that we're now including the -headers.tar.gz file from 0.10.41 and 0.12.9 and onward. Unfortunately it's bloated in those versions so not very helpful. It'll be fixed for the next releases thanks to https://github.com/nodejs/node/pull/4149 and I was thinking of _maybe_ putting a hardwired check in node-gyp to start using it for `^0.12.10` and `^0.10.42`.  I believe this is ready to be closed without anything else, @ljharb? 
ljharb		looks good - in general all the release process changes you've put in are enormously helpful for `nvm`, the only requests i continue to have is filling in old releases with new binaries and whatnot whenever possible :-)  close away! 
rvagg		@ljharb how far back in Node.js versions are you getting requests for support? Are there people still caring about pre-0.8 that you're in contact with? 
ljharb		I only very rarely see mentions of pre-0.8 come up (although 0.8 still does often). However, it would be nice to have them all consistent, and allow nvm to shed a lot of its legacy code without dropping any version compat. 
gibfahn		SGTM
targos		Actually I'm not sure I need it anymore.
gibfahn		Reopen if you do.
gibfahn		Do you know what OS it's running? We'll need to make sure it's covered by our existing ansible scripts.  >nearForm is operating that machine from our HQ.  What does that mean exactly? Intel are paying for it but it's your hardware?
mcollina		The HW is Intel's, we are paying for the operating expenses and hosting.
mhdawson		In terms of hooking it up to the CI, for other machines the approach is that you provide the root password to the build WG who will then manage the machine.  In terms of things like reboots etc all existing machines either have some sort of control panel that we can use to reboot when necessary.  I'm guessing that won't be the case in this instance so we'll need contacts/procedures to be able recover in the cases were the build team members can no longer access the system.  We'll need to discuss based on the system/specs but unless there is some type of virtualization that we believe will not affect performance I'm assuming they will be a single OS instance running on each of the boxes.
jbergstroem		Better understanding who has physical access (and intended os level access) would also make it easier for us to understand how the machine can be used.
mcollina		Our staff has physical access, but you can refer to me for any physical maintenance. I can forward account credentials for a root-level account. You can refer to me for any physical maintenance, I'll forward the request to our team.  As it is a dedicated server, it should be used for benchmarks.
jbergstroem		@mcollina benchmarks of course, it's just mostly about what code we can benchmark so to speak. For instance, adding it to a release process where we benchmark prior release to avoid any regressions could include things that comes from security-related release stuff.
mcollina		@jbergstroem that's ok. Also check benchmark regressions on PRs, maybe?
rvagg		Fixed @ #800   Temporary use is simple test running, more sophisticated benchmarking use is left as an exercise for the ... _someone else_. Let me know if there's anything I can help with in advancing that cause though.
mcollina		cc @AndreasMadsen
AndreasMadsen		@mcollina anything specifically I will comment on?  Primarily, I wanted a machine so we could run a Jenkins job like that proposed in  https://github.com/nodejs/benchmarking/pull/58. The biggest problem with the benchmarks right now, are that the benchmarks take too long to run on a personal machine and developers tends to use it for other things while it's benchmarking. The latter creates systematic noise that no statistics can't handle.  Running the entire benchmark suite on each release (maybe just major/minor) sounds nice. However, it will take a very long time, we are talking multiple days. Also, when running a single benchmark test there is a small risk of a false positive, this is not a problem when running just a few benchmarks but when running them all (1000+) there will be many false positives, thus it doesn't have much value. There is no theoretical nice way around this, the only thing that can really be done is to make the benchmarks run longer. This is one of the reasons why the Large Hadron Collider costs 7+ billion dollars :p
octaviansoldea		Following some previous communication with colleagues, I would like to indicate that the Node.js benchmark servers provided by Intel have the following details:  The machines are Wildcat Pass 2U 8x3.5" HDD 10Gbe Xeon DP v4 Server [R2308WTTYS-IDD], 1x1100W, equipped with Intel¬Æ Xeon¬Æ Processor E5-2600 v4 Family, Socket R3, Intel¬Æ Server Board S2600WTTR,  Board Chipset: Intel¬Æ C612 Chipset. They are equipped with 8 slots of memory where each one is of type 8GB 2400 Reg ECC 1.2V DDR4 Kingston KVR24R17S8/8I Single Rank. Moreover, each station is equipped with integrated LAN 2x 10GbE, the number of LAN Ports is 2, and has hard disk of 1 TB, SATA, 6Gb/s, 7200 RPM. This type of server aims Cloud/Datacenter. 
mhdawson		@AndreasMadsen I think https://github.com/nodejs/benchmarking/pull/58 is the first thing we should aim to get running on these machines.  Once we get experience with that we can see if we think regular runs (like on releases) might make sense.
mhdawson		I'll see if @gareth-ellis has time to get together in the next few weeks to see if we can push nodejs/benchmarking#58 forward.
rvagg		@octaviansoldea thanks for the specs! I might link to your comment from the nodes on Jenkins.
bnoordhuis		@santigimeno Aren't you about to be onboarded?  You'll have full access after that. 
orangemocha		I think the question was about having job write access to Jenkins, to allow for experimenting with the scripts. Currently, that is restricted to a smaller set of collaborators, due to security considerations.  @nodejs/build @nodejs/jenkins-admins not sure if we have a well defined criteria for granting trust. Thoughts? 
jbergstroem		The build group has access to create jobs but not to credentials/jenkins admin. Would that suffice? I mean, we could give similar access to the testing group. 
orangemocha		I believe it would suffice. Still, we need to answer the question about trust. Tagging for wg-agenda. 
orangemocha		We had a long discussion about Jenkins access and security at the last [Build WG meeting](https://github.com/nodejs/build/issues/336). Regrettably, at this time we are not prepared to grant the requested level of access.   Being able to edit jobs gives a user more ability to screw up our CI infrastructure (whether maliciously or inadvertently). This is hence restricted to a small group of users (nodejs/jenkins-admins). While it would be great to have more people who can help manage the infrastructure, we need to consider the security risks and maintain somewhat strict criteria for granting access. Even though not formally defined, those criteria could be summarized as follows (this is mostly my take on it): 1. How long have you been a Node contributor / collaborator? 2. Are you also participating in the Build WG? 3. Is your work on Node affiliated with a company / would you lose your job if you purposely tried hacking our infra?   One idea we discussed was to create a separate instance of Jenkins (with a smaller number of slaves), which could be used for experimentation by a broader group of people. As a side note, this would also be useful for current jenkins-admins to test global configuration changes (e.g. Jenkins or plugin upgrades) before rolling them out to the main Jenkins. The jenkins-admins could then review changes made in the experimental instance and integrate them into the main Jenkins. A GitHub workflow is also imaginable.   Due to resource constraints - mostly people's time - at this time the Build WG has not agreed to move forward with the experimental Jenkins instance. However if anybody (@nodejs/testing ?) wanted to set this up and maintain it, we would be glad to offer mentorship and a few machines. 
mhdawson		@joaocgreis 
refack		If there is nothing secret there I'd suggest adding a copy of the rules file to this repo for version management. Else keep it in `secrets` So the procedure will be: 1. Local: edit `X/Y/iptable_rules` 2. Local: commit & push 3. local: `scp X/Y/iptable_rules ci:~/` 4. Remote: `iptables-restore ~iptable_rules`  Maybe add a (0) step of `iptables-save > iptable_rules` and local `scp ci:~/iptable_rules X/Y/iptable_rules` to verify rules are in sync with git
joaocgreis		The exact firewall rules we use should be private. I don't think there's anything sensitive there at the moment, but we should be able to change them quickly and secretly if needed.  We could keep them in the secrets repo but I don't think we need to move forward with this at the moment, there is some work in progress at https://github.com/nodejs/build/blob/master/ansible/playbooks/jenkins/host/iptables.yml to generate the rules from the inventory. The documentation here can be later updated or integrated with the docs in `ansible/`, but for now this is good to have.
jbergstroem		(I already have a ansible playbook that can update the firewall. I would revisit it though)
gibfahn		Added @joaocgreis 's links to the commit message, seems good to document the WIP ansible playbook in the commit message.
joaocgreis		Did not run this, LGTM if it works. 
jbergstroem		LGTM. A note for the future: once we require ansible 2.0 we should use the iptables module. 
mhdawson		Landed as f1af626a20ce24cac94cd96cb3b89c1d1cf8b658 
Trott		LGTM
mhdawson		Landed as de9831395be8ddd1af9d9b4e52bb5bf1050542fc
leachiM2k		+1 
fhemberger		Maybe it would also be a good idea to put that long list of redirects into an external file and include it into  nginx.conf, to keep the main config short and clean. 
Trott		@nodejs/build @fhemberger Should this remain open?
fhemberger		@Trott I still think this would be useful in the light of the planned relaunch, but in the end it depends on the needs of the build team.
maclover7		Closing for now as something that would be nice to have, but not currently within our means. If someone wants to tackle this, please feel free to reopen.
gibfahn		@phillipj thoughts on using this? It'd work pretty well with the automatic build status stuff in https://github.com/nodejs/citgm/issues/358.
jbergstroem		I'm `-0`. Not a badge fan and/or user myself.
gibfahn		Having looked at it in greater detail, this wouldn't work (at least for the citgm use case) as it would  just show the status of the last build run. The last build run will usually have been on a PR, so it won't actually show us the current status of `master` unless we duplicate the citgm job (which is a bad idea imo).  @gdams I'll close this, comment if you find a way to make it work and we can look at it again.
Trott		I killed the `gmake` processes and this immediately appeared in the Jenkins interface right under the line that it was stalled at for hours:  ``` gmake[2]: *** Deleting file '/usr/home/iojs/build/workspace/node-test-commit-freebsd/nodes/freebsd10-64/out/Release/obj.target/openssl/deps/openssl/openssl/crypto/pkcs12/p12_add.o' gmake[2]: *** Deleting file '/usr/home/iojs/build/workspace/node-test-commit-freebsd/nodes/freebsd10-64/out/Release/obj.target/openssl/deps/openssl/openssl/crypto/pem/pvkfmt.o' gmake[1]: *** [Makefile:68: node] Terminated gmake: *** [Makefile:347: build-ci] Terminated Terminated Build step 'Execute shell' marked build as failure Run condition [Always] enabling perform for step [[Execute a set of scripts, Execute a set of scripts]] Archiving artifacts TAP Reports Processing: START Looking for TAP results report in workspace using pattern: *.tap Did not find any matching files. Setting build result to FAILURE. Checking ^not ok Jenkins Text Finder: File set '*.tap' is empty Notifying upstream projects of job completion Finished: FAILURE ```  Looks like the problem is the `gmake` hanging (deadlocked, perhaps, somehow?) and not a problem with Jenkins communication?  If deadlocking is a reasonable possibility, maybe it makes sense to use `-j 1` for a while and see if that makes the problem go away? At least as a step for confirming the deadlocking? 
jbergstroem		When you've looked; has it always been in openssl? `-j1` would confirm the theory that their makefile is racy. 
bnoordhuis		We don't use openssl's Makefile, we generate our own through gyp. 
jbergstroem		@bnoordhuis Ah, that's right. I've seen this for the last month or so; tried cleaning ccache to no avail. Looking at [recent commits to our openssl repo](https://github.com/nodejs/node/commits/master/deps/openssl) I don't see anything that would corroborate that timeline. Suggestions on how to capture more info? 
bnoordhuis		Is cc gcc or clang?  Are the gmake processes really blocked when you attach a debugger and if so where? Do `-j1` and `-j2` work? 
jbergstroem		It's clang. We already run `-j2` (`-j$num_cores`) -- idea is to try `-j1` next. I can try and attach next time as well. 
jbergstroem		So, I saw a freeze on one of the fbsd11 boxes:  ``` iojs           6288   0.0  0.1   13144   2416  -  I    16:45       0:00.01 /bin/sh -xe /tmp/hudson4600675383322005305.sh iojs           6290   0.0  0.1   10612   2732  -  I    16:45       0:00.02 gmake run-ci -j 2 iojs           6344   0.0  0.1   10612   2796  -  I    16:45       0:00.02 gmake iojs           6387   0.0  0.6   20852  12896  -  I    16:45       0:00.83 gmake -C out BUILDTYPE=Release V=1 iojs          11595   0.0  0.0       0      0  -  Z    16:45       0:00.01 <defunct> iojs          11596   0.0  0.0       0      0  -  Z    16:45       0:00.00 <defunct> ```  bt is mostly optimized out:  ``` (lldb) bt * thread #1: tid = 100596, 0x0000000800b453da libc.so.7`_pselect + 10   * frame #0: 0x0000000800b453da libc.so.7`_pselect + 10     frame #1: 0x000000000041a15c gmake`jobserver_acquire + 156     frame #2: 0x0000000000413f74 gmake`new_job + 1172     frame #3: 0x0000000000420fab gmake`??? + 3739     frame #4: 0x0000000000422464 gmake`??? + 164     frame #5: 0x00000000004206f5 gmake`??? + 1509     frame #6: 0x0000000000422464 gmake`??? + 164     frame #7: 0x00000000004206f5 gmake`??? + 1509     frame #8: 0x0000000000422464 gmake`??? + 164     frame #9: 0x00000000004206f5 gmake`??? + 1509     frame #10: 0x0000000000422464 gmake`??? + 164     frame #11: 0x00000000004206f5 gmake`??? + 1509     frame #12: 0x000000000041fea2 gmake`update_goal_chain + 418     frame #13: 0x000000000041787a gmake`main + 8154     frame #14: 0x0000000000406aff gmake`_start + 383 ```  ..but forcing it to continue seems to point to job server management:  ``` (lldb) n Process 6387 stopped * thread #1: tid = 100596, 0x0000000000413ef5 gmake`new_job + 1045, stop reason = instruction step over     frame #0: 0x0000000000413ef5 gmake`new_job + 1045 gmake`new_job: ->  0x413ef5 <+1045>: cmpl   $0x0, 0x221574(%rip)      ; job_counter + 7     0x413efc <+1052>: je     0x413fb3                  ; <+1235>     0x413f02 <+1058>: callq  0x41a0b0                  ; jobserver_pre_acquire     0x413f07 <+1063>: xorl   %edi, %edi (lldb) n Process 6387 stopped * thread #1: tid = 100596, 0x0000000000413efc gmake`new_job + 1052, stop reason = instruction step over     frame #0: 0x0000000000413efc gmake`new_job + 1052 gmake`new_job: ->  0x413efc <+1052>: je     0x413fb3                  ; <+1235>     0x413f02 <+1058>: callq  0x41a0b0                  ; jobserver_pre_acquire     0x413f07 <+1063>: xorl   %edi, %edi     0x413f09 <+1065>: xorl   %esi, %esi (lldb) c Process 6387 resuming ```  ..after which the build kept going. 
bnoordhuis		I suspect it's http://savannah.gnu.org/bugs/?49014.  Savannah lists comments in reverse order so start at the bottom if you want the discussion to make sense. 
jbergstroem		Ouch. Lets look at downgrading gmake to 4.1 for now then. 
jbergstroem		I've updated all 10 hosts, will have to hunt for a 11 package (or install ports).  Edit: since 4.1 isn't available for 11 I tried installing the 10 package; seems to work fine (tried compiling node). Hopefully we can avoid the stalls for freebsd.  
jbergstroem		Haven't seen this occur since I downgraded gmake. Thanks for helping out, @bnoordhuis. I'll close this once I figure out how to pin packages in FreeBSD (or a newer version of gmake hits the shelves). 
jbergstroem		Fixed:  ``` console ansible -m shell -a 'pkg lock -y gmake' "*freebsd*" test-joyent-freebsd10-x64-1 | SUCCESS | rc=0 >> Locking gmake-4.1_2  test-joyent-freebsd10-x64-2 | SUCCESS | rc=0 >> Locking gmake-4.1_2  test-digitalocean-freebsd10-x64-1 | SUCCESS | rc=0 >> Locking gmake-4.1_2  test-digitalocean-freebsd11-x64-1 | SUCCESS | rc=0 >> Locking gmake-4.1_2  test-digitalocean-freebsd11-x64-2 | SUCCESS | rc=0 >> Locking gmake-4.1_2  test-rackspace-freebsd10-x64-1 | SUCCESS | rc=0 >> Locking gmake-4.1_2 ``` 
rvagg		``` $ ccache -s cache directory                     /home/iojs/.ccache cache hit (direct)                  1113 cache hit (preprocessed)               0 cache miss                          1120 called for link                       11 called for preprocessing              12 unsupported source language           16 files in cache                      3363 cache size                          39.1 Mbytes max cache size                       8.0 Gbytes ```  working well, built in less than a minute the second time with same configuration. 
mhdawson		@nodejs/build any comments and if not can I get an approval so I can land.
refack		@mhdawson I added my nit fixes as a `[suggestion]` commit. Obviously feel free to push it out.
gibfahn		I'll review properly later today, but as an initial thought I'd rather we didn't start having separate documentation of these jobs, as in my experience it just gets out of date.  I'd rather we document the script clearly with comments, and then have the parameters just be the arguments to the script. If the way the parameter names map to the script is clear (it's either an environment variable, or it's passed in directly as an argument to the script) then that should be self-documenting. We could then have a single file that has an overview of each of the jobs to document what we have (i.e. the first paragraph of this doc).  I'll try to go through this in the next couple of hours and come up with something concrete.
mhdawson		@gibfahn waiting on your follow up input.
gibfahn		So I'd suggest having a single file `jenkins/jobs.md` with an entry for each job, and a link to the job. Then we document the job parameters in the job itself (otherwise we have to duplicate that info, people probably won't think to look here). I updated the job config to add some of the parameter info already.  #### `jenkins/jobs.md:`  <br/>  ```markdown # Jenkins Jobs  ## [validate-downloads](https://ci.nodejs.org/view/All/job/validate-downloads/)  This job validates that the downloads on nodejs.org are good. It is scheduled to be run nightly and can also be run manually after a release is made. **Note:** there is delay between when releases are generated and when they will be available on nodejs.org (up to 60 minutes) so releasers may have to wait a bit be able to complete the validation.  If validation fails an email notification is sent to the `release-validation-alert` email alias. If you would like to get these notifications submit a PR to have your email added for that alias in https://github.com/nodejs/email/blob/master/iojs.org/aliases.json.  This job needs to be updated each time a we roll over to a new Current release. ``` 
refack		@gibfahn can land?
gibfahn		Okay, pushed a commit to my branch that does what I suggested in earlier comments.  LGTM.
mhdawson		Landed as aed300d66bab013e50829ef1e62ddce792461597
gibfahn		SGTM, but we should make sure this is documented (probably in the post-mortem repo README).
mhdawson		@gibfahn is there the plan to do the equivalent for citgm.  If so I'd like to just copy the text to do what we need for the post-mortem README.
mhdawson		teams created, added Richard C to admin team initially as I think he'll be the first person from the post-mortem team that may need to modify the post-mortem jobs. The post-mortem WG can add additional people as needed.
mhdawson		Jobs are in place, just leaving this open until we close on the doc. @gibfahn @gdams can one of you update the CITGM readme to doc the jobs used there and then I'll replicate that to the node-report repo.
maclover7		Job was created: https://ci.nodejs.org/view/post-mortem/job/nodereport-continuous-integration/
jbergstroem		fedora24 runs gcc 6.1.1:  ``` [root@test-rackspace-fedora24-x64-1 ~]# cc --version cc (GCC) 6.1.1 20160621 (Red Hat 6.1.1-3) ```  Do we need something newer? What suitable distribution? 
targos		There is already a machine with fedora 24 in `test-commit-linux`and this system has GCC 6 by default. 
addaleax		No, 6.1.1 is what I have, too. Could something like that be included in `test-commit-v8-linux` runs? 
jbergstroem		@addaleax done.  
jbergstroem		I need to redeploy one of the fedora24 slaves seeing how we need a beefier vm to run tests on. I'll update this issue when done. 
jbergstroem		Done! Part of the suite as of now: https://ci.nodejs.org/job/node-test-commit-v8-linux/ 
bnoordhuis		What purpose would that serve? 
orangemocha		To raise awareness with collaborators that our CI is not a fully-sandboxed system, and that malicious code in a pull request could compromise the infrastructure. @rvagg has suggested that there might be a lack of such awareness with most collaborators. This is a reminder at the right place and time. 
rvagg		+1 if we make it a required check. @bnoordhuis because CI is so easy, everyone's just used to throwing code at it, often without even looking at the code being submitted but just because it's _something that needs to be done_. This is just an attempt to remind them that when submitting code to CI you are taking responsibility for what is sent to our build slaves for executation. 
MylesBorins		@rvagg how does that apply to smoketesting? Should we be doing things in a more sand boxed fashion for citgm? 
rvagg		@TheAlphaNerd smoke testing opens a vector via the packages being smoke tested, basically anything you can get into one of those packages you can get onto our test infrastructure. So it's our responsibility to make sure the list of packages is _reputable_ at least. It's hard to sandbox smoke testing without a lot more server resources to run them on but that would be an ideal. Another option we can consider is to separate out a smoke testing user on the servers we're using so that the impact is somewhat contained. 
MylesBorins		@rvagg I would feel a lot better about that if it were possible. 
rvagg		possible .. but someone has to do the work 
MylesBorins		That is something I would be open to exploring... although I may need some mentorship with figuring out how we orchestrate our systems 
rvagg		You'd probably need to lean on @jbergstroem who has overhauled the setup of all our slaves, I think he's a bit busy though 
jbergstroem		I'm busier than normal but lets try and schedule something. Ping me on IRC and we'll suss it out. 
orangemocha		The change is now live. @nodejs/collaborators : there is an additional checkbox parameter when launching [node-test-pull-request](https://ci.nodejs.org/job/node-test-pull-request/) or node-test-commit. It reads:  [ ]  `I have reviewed *the latest version of* these changes and I am sure that they don‚Äôt contain any code that could compromise the security of the CI infrastructure.`  ...and it's intended as a reminder for you to make sure that the change that you are testing doesn't contain any malicious code that could compromise our Jenkins slaves. 
orangemocha		Closing as the original issue has been resolved.  
orangemocha		Whatever reminds us to keep it updated would be best. A single issue is likely to fade into oblivion.  How about 4) creating a section for it in the README in the secrets repo? We can remind people of the policy and have the list of temporary accesses there.  Also, I suggest that if we have to give access to non-collaborators, we take those machines out of the pool and redeploy them / add new ones in their stead. 
mhdawson		In terms of redeploying that can take some time from the build team and in the case were we have limited resources (ex AIX box for now) potentially affect builds.  Is the problem that containment per user id under the os is not strong enough ?  I was thinking that we'd give access by creating an individual user id, and then fully deleting the user/directories when cleaning up. 
orangemocha		Isolate by user might be feasible in theory, but I don't think that we have configured the build machines with that goal in mind. So additional work might be required.   In case where redeployment is undesirable (eg AIX) we should probably simply avoid giving access to non-collaborators. 
mhdawson		Depending on the main concern, another option might be to setup separate machines which are not connected to the ci ?   While not an option for AIX in the short term I think it would be later on and could be feasible for other platforms.   
jbergstroem		I've temporarily given out access to test machines prior. I did so by asking for a pubkey; telling the user what to think about (since it might be running a job) such as copying their own environment, not downloading and installing stuff, etc. This has been to collaborators and I think the trust has been just fine. I've also revoked their key and when they said they were done (note: no one has had access days through, just <24h:ish). I guess we could record this in a log somewhere but I think the mechanism works well enough. 
mhdawson		The process I've used follows what @jbergstroem mentions, except that access has been longer than 24 hours. Part of what I want is to make sure we clean up periodically.  The other case I'd like to cover is one were we have a collaborate who can vouch for the person.  For example, if there is a platform specific issue that only occurs in the machines in the CI (which did occur for AIX due to a missing APAR), I'd like to be able to grant access to team members here at IBM which may not be collaborators as I trust them to do the right thing.    It would still be nice to have a solution for non-collaborators as well as that allows them to investigate/resolve issues across platforms instead of potentially just the one platform they have access to. 
mhdawson		Proposal discussion being covered here: https://github.com/nodejs/build/pull/354 
mhdawson		I believe this is now documented in https://github.com/nodejs/build/blob/master/doc/process/special_access_to_build_resources.md 
orangemocha		Folks, due to other pressing work priorities I most likely won‚Äôt be able to attend these meetings on a regular basis. I haven‚Äôt been doing much work in build lately, and @joaocgreis has been doing a great job at handling the Windows stuff (and beyond). Secondarily, I also need to cut back on the number of late night meetings on my calendar.  Would someone else be willing to facilitate these going forward? 
phillipj		I'm on vacation until august, so I won't be able to join the meeting.  On Friday, 15 July 2016, Alexis Campailla notifications@github.com wrote:  > Folks, due to other pressing work priorities I most likely won‚Äôt be able > to attend these meetings on a regular basis. I haven‚Äôt been doing much work > in build lately, and @joaocgreis https://github.com/joaocgreis has been > doing a great job at handling the Windows stuff (and beyond). Secondarily, > I also need to cut back on the number of late night meetings on my calendar. >  > Would someone else be willing to facilitate these going forward? >  > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > https://github.com/nodejs/build/issues/450#issuecomment-232965973, or mute > the thread > https://github.com/notifications/unsubscribe-auth/ABLLE2wgUluwzmLy-8NDPtNeClsX6qbkks5qV5jzgaJpZM4JNct- > . 
jbergstroem		I'll be around. Although @phillipj won't be here I would still like to vote him in (unless anyone raises objections that is). 
jbergstroem		@orangemocha I don't have access to the nodejs youtube or google account; perhaps Rod can do it? 
rvagg		Yeah, I'll try and roll out of bed for this one @jbergstroem, hangout and live feed links are in OP, I haven't done document, if you have a link to the last one perhaps you can copy it, along with permissions, and make a new one for us? 
jbergstroem		I'll set it up. 
mhdawson		In the future I can also help setup the meeting as I have access to the accounts. 
jbergstroem		@mhdawson sounds good.  
jasnell		+1. FWIW, the bump inside nodejs before http-parser was because, at the time, we didn't have http-parser-private, which I've since fixed :-) ... in the future we can do the fixes in http-parser-private first. 
maclover7		This is a thing now: [http-parser-test](https://ci.nodejs.org/view/MyJobs/job/http-parser-test/) üéâ 
rvagg		There's two approaches we could take here: 1. set up `benchmarking` account and point benchmarking.nodejs.org to at directory in the home for that and you could put what you need in there 2. replicate what we do for the website, i.e. watch a repo (or some other trigger) for changes and rebuild the site within a container  The nice thing about #2 is that we isolate the server more than if we make a new account that can be used. Personally I'd rather us keep non-build-core users off the server so the more we can do at a distance the better. Could the data be posted back to a GitHub repository that can be used as a basis for building the site?  For context, this is how we do it with the website, nobody from the website group needs any kind of direct access and any activity performed by their scripting is done within a container so it's mostly isolated: https://github.com/nodejs/build/blob/master/setup/www/resources/scripts/build-site.sh#L36-L59 
jbergstroem		@rvagg you're right; the second approach is probably better. 
mhdawson		@rvagg just to confirm I understand.  For 2 we'd publish the contents for benchmarking.nodejs.org to a specific directory in the nodejs/benchmarking repository on github which we'd update daily and then this would be automatically replicated onto the website.  In this case we'd setup a public key so that change could be pushed from the benchmark machines to the nodejs/benchmarking repo on github, right ?  If so,  do we do anything special when setting that up or is it just a dummy github user (for example node-benchmarking) that we add and give access to the repo ?  
mhdawson		Ok so based on this and discussion with @jbergstroem I think the plan is to do the following:  1) mirror from https://github.com/nodejs/benchmarking/tree/master/www to the www site 2) make nodejs.org/benchmarking point to the mirrored contents  3) at least daily have the www server pull all of the pngs from  /home/benchmark/charts on the benchmark data machine (currently 50.97.245.4) as user benchmark and put  then into the charts directory in the mirror from https://github.com/nodejs/benchmarking/tree/master/www on the www server.  This would allow the benchmark team to update the static www pages and also make the latest charts available on the www server for reference by those static pages.  For more info on the planned overall flow/benchmarking infrastructure see https://github.com/nodejs/benchmarking/blob/master/benchmarks/README.md 
rvagg		sounds good, let us know when this is ready to happen and we can generate an ssh keypair for it and get it rolling. 
mhdawson		@rvagg I'd like to get the mirroring going now as the graphs should have been generating for while and getting them onto the website even with the index will let us see them and provide the basis that we can start creating the index.   
mhdawson		Anything I can do to help on this one ?  
mhdawson		Have temporarily setup jobs to push to here http://nodejs.devrus.com:8080/ - cronjob on benchmark machines pushes updated charts nightly (as opposed to pull which is planed for node webiste) - repo is pulled every 15 mins to update www content 
rvagg		Done, I think. Server setup including 6-hourly sync (I don't know what "overnight" means tbh so this is will get it done!) plus github webhook for the benchmarking repo so when you update it there it'll get synced on the server and put in to place.  https://github.com/nodejs/build/pull/344  https://benchmarking.nodejs.org/  You might want to ask `@nodejs/website` to have a go at your design since this is now a public presence. 
jbergstroem		Design? This is perfect! :tada:  
mhdawson		@rvagg, Thanks, will do 
mhdawson		Issue in website WG repo here https://github.com/nodejs/nodejs.org/issues/540.  I think we can close this issue now. 
MylesBorins		kicked off release again and things seem fine
orangemocha		Hypothesizing that this PR was updated around the time the job started.  
orangemocha		cc @nodejs/jenkins-admins  
orangemocha		Hypothesis not validated, given https://github.com/nodejs/node/pull/2404#issuecomment-136723262 
jbergstroem		Haven't seen this since. @orangemocha reopen if you have. 
rvagg		Also, `xz` is needed on these machines as we discovered today #258  
jbergstroem		Is this how we juggle gcc? 
rvagg		``` if [[ "X$PRE_1_PATH" != "X" && $NODE_VERSION =~ ^[0] ]]; then   export PATH=$PRE_1_PATH fi ```  yep 
jbergstroem		its almost like we should invert this? i mean, we do the exact same thing to "enable" gcc48. 
rvagg		maybe but I've taken the approach that >=v4.x releases are going to be more frequent and will soon be the only releases we're pushing and that <=v0.12 releases are the exception 
jbergstroem		Yeah, its just that double negation is kind of awkward. Perhaps create functions available through env that's called `gcc48` and `gcc41` we call regardless of default? 
rvagg		we can easily add `POST_1_PATH` and do the same there, I'd be happy to do that. Not so sure about using gcc version numbers because they are not consistent across the *nix environments we have. 
jbergstroem		@rvagg i didn't see this being used everywhere, just the exceptions which are centos5 and 6(gcc44, right?). 
rvagg		mm, in practice it's turned out to be the only thing needed for <=0.12 
jbergstroem		How about we do this instead:  ``` bash if [[ $NODE_VERSION =~ ^[0] ]]; then   export PATH=${PATH//\/opt\/rh\/devtoolset-2\/root\/usr\/bin:/} fi ```  ..then remove all the PRE_ variable stuff? I don't see above path changing anytime soon, and if it doesn't match we should be fine anyway. 
rvagg		fine, I guess. would be nice to be testing 0.10 and 0.12 with vanilla compilers on wheezy but since I don't think we're doing that then whatever. 
jbergstroem		Just implemented this. If things blow up for 0.x you know who to blame. 
gibfahn		cc/ @mhdawson 
jbergstroem		SGTM as long as we don't forget about identifying the cause as well.
mhdawson		This sounds good to me as well.  We have seen a number of cases were somebody runs the job on a PR, it exposes problems with the test which are later fixed, but in the mean time we have processes hanging around.    When you say it would be run as a job, is the idea that it would run at a particular interval (ex hourly) or do you know of some way we can tie it to the execution of other jobs ?  The other nice thing about a separate job is that we could have it "fail" if there were processes that needed to be killed, include that info in the job output and also email interested parties so that we'd know when processes were being left around and be notified with at least some amount of info. In the case when when the same problem occurred repeatedly we would likely catch  that from the the job failures/emails.
joaocgreis		Windows jobs already kill processes as part of the script, before and after running, including `run-tests.exe` of libuv, that shares the same machines.  I would also like to put more emphasis on finding out why and fixing this. Perhaps find a way to mark the build as unstable (yellow) when processes are left behind?
gibfahn		I've been thinking about this some more (not least because we've seen quite a few problems due to this).   I agree with @joaocgreis that we should consider processes left behind as a separate test-case (unless we can work out which processes came from which tests). Processes left behind should be considered a test-case failure.  I think the best place to do it would be in `tools/test.py` rather than individual jobs. The python [test runner](https://github.com/nodejs/node/blob/master/tools/test.py) should be able to work out which node processes are from the current test run and kill them.
mhdawson		@gibfahn sounds good, we should make sure the info about processes being killed is output to the job and the idea of it reporting as at least yellow would make sense to me so that we can identify tests that are leaving jobs behind and fix them.
richardlau		Related: https://github.com/nodejs/node/pull/11246
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
gibfahn		``` ps -ef | grep citgm | awk '{print $2}' | xargs kill ```  seems to have fixed it. We should look at adding cleanup to the citgm jobs.
gibfahn		@Trott btw it's worth cc'ing @nodejs/platform-aix 
gibfahn		First build that ran on it seems to be going okay, so closing.  https://ci.nodejs.org/job/node-test-commit-aix/nodes=aix61-ppc64/8923/console
refack		BTW: @gibfahn did you clean the ramdrive? I think the jobs go zombie when the ramdrive if full.
gibfahn		@refack it was only 10% full.
refack		> @refack it was only 10% full.  Puff goes my hypothesis ü§î 
refack		So how about adding `ps -ef | grep citgm | awk '{print $2}' | xargs kill` unconditionally to the end of the job?
gibfahn		>So how about adding ps -ef | grep citgm | awk '{print $2}' | xargs kill unconditionally to the end of the job?  SGTM
refack		@Trott and I just found 88 unterminated node processes on `test-osuosl-aix61-ppc64_be-1` I'm reopening, since we need to either kill the processes on cigtm job end, or take the AIXes out of the citgm roster.
mhdawson		@gdams I know you did a lot of the work to get AIX into citgm, do you think you could add the kill at citgm job end ? 
maclover7		These jobs are able to finish now
rvagg		lgtm, nightly building crontab job lives in tasks/tools.yaml if you want to add that too, might need to adjust https://github.com/nodejs/nodejs-nightly-builder too I suppose 
joaocgreis		Rebased and added changes to start the nightly build with `nodejs-nightly-builder`.
joaocgreis		This has been live on the server for some months now, I believe we should land even if only to make Ansible consistent with what's in the server. This is likely to be improved in the future, but is stable as is.  @nodejs/build if there are no objections I'll land this tomorrow.
gibfahn		+1 on landing.
joaocgreis		Thanks! Landed in https://github.com/nodejs/build/compare/9774dae...e5d81e9
rvagg		removed the node-ci user from node-private and turned back on the 2fa requirement there but we're still trying to figure out how we can get node-test-commit-private and sub-jobs access to node-private when required, ideally permanently so it still may require 2fa support.  /cc @joaocgreis FYI
rvagg		@joaocgreis @MylesBorins @gibfahn here's some of the basics of the problems were dealing with:  * CI needs access to nodejs-private/node-private for use in node-test-commit-private * All of the sub-jobs also need access with node-test-commit-private defers to them, ugh * We have a "node-private" deploy key in CI that we can use to access nodejs-private/node-private * git-rebase uses a different key, that's associated with a user (node-ci) so it can write to the node_binary_tmp repo, this is used for the fanned jobs * You can't select which SSH key to use for any particular operation, if you add both keys to the SSH Agent then it'll just use the first one in there for git@ operations, unless we store the key on disk (where we could load it with `GIT_SSH`) I can't find a way to mess with this, so git-rebase can only use one key to do both its read from node-private and its write to node_binary_tmp  So, we could consolidate everything into the single node-ci user and give it access to write to both node_binary_tmp and read from node-private. We did that for this week's security release.  Unfortunately, when we give node-ci elevated permissions then we open up the ability for CI to read arbitrarily from node-private, potentially giving anyone with CI access (or who can get code submitted in to CI without it being picked up by a collaborator beforehand) to sensitive information we store there. e.g. you could do some `git` CLI work to fetch stuff and spit it to console which is public. It's awkward and you'd have to be very motivated to get anything actually useful, but the avenue exists.  So, here's a suggestion for solving it at a higher level than just figuring out how to propagate a sensitive SSH key from node-test-commit-private downward:  * Make it much easier to flip Jenkins into "protected" mode, which could just be nodejs/security and nodejs/jenkins-admins having access, everyone else locked out * Any switch should also involve giving the node-ci user into the node-private repository, we'd have to add shared 2fa access to this user I think * When you flip Jenkins out of protected mode, we also remove the node-ci user  Perhaps this could be automated, but we could at least document the process so it's not hard to follow. We could flip it on any time we need to do a bit of testing, not just the few days before a release (ideally).  A couple of other options: * Figure out how to make the an SSH key propagate downward from node-test-commit-private _only_ so that you don't have access to that key unless you're starting from that root job, then we have a new user, maybe node-private-ci, that can access both node-private and node_binary_tmp, it might make git-rebase awkward but not impossible (I think). I just don't know if this is even possible to do without making holes for it to be exploited people with general collaborator access. * Set up an entirely new Jenkins, but that means new testing nodes and that's probably not practical * Set up node-test-commit-private so that it doesn't use sub-jobs, remove the fanned stuff and do the compile/test directly on the windows & arm machines and just accept the extended build time, it also might mean removing some nodes that are a bit too complicated (fips maybe?) * Set up our Release Jenkins to be able to run tests too and just do it all on there. We'd only have a limited set of machines we can test across but at least it'd be fully protected.  Anything we do has to deal with the fact that we don't do this kind of work often enough and so any config tends to get out of sync with our standard test setup. I noticed that node-test-commit-private doesn't currently have the full range of subjobs in it even now!
mhdawson		It would probably be good to open an issue in the build repo listing the sub-jobs which are not on node-test-commit-private so we can see if that's on purpose or by accident.  Your suggestion seems the most practical.  It would be nice to be able to test on the release ci, but I doubt that is practical for arm/windows that require the fans.  I assume other platforms would be ok.
jasnell		automating the generation of 2FA codes is quite trivial once you have the key and the cycle time. I've got an old (likely quite crappy) bit of code here that shows how it is done: https://github.com/jasnell/tfa. It should be rather trivial to work this into any auth workflow automation that required specifying a 2FA value.
joaocgreis		@rvagg @MylesBorins (and everyone else) there is a very important detail about the private jobs that you might be missing: only the private jobs are private, subjobs launched by them are not. For example, try to open the following links (while they are still available) in an incognito window (logged out of GH):  - https://ci.nodejs.org/view/All/job/node-test-commit-private/173/ - https://ci.nodejs.org/job/node-test-commit-freebsd/12615/  The second one was launched by the first and is perfectly accessible. There are even windows binaries available from https://ci.nodejs.org/job/node-compile-windows/12806/label=win-vs2015/ (there should be arm binaries as well, but the private job is out of sync and does not have arm-fanned).  Jenkins must always be locked for security work. Even if we manage to fix the issue above, I do not trust Jenkins (or our knowledge of it) to do security work without locking.  The private jobs might still have a reason to exist: to provide default values. But they should only pass the parameters to the regular jobs, not try to duplicate. 
MylesBorins		With that in mind would it perhaps make sense to create new private jobs that simply wrap the main job with sensible defaults?  I would like to see us so some research on having an approach that doesn't have to shut down CI. Having to rush the testing in to the last 24 hours is not only stressful, but a massive risk.  On Oct 26, 2017 11:10 AM, "Jo√£o Reis" <notifications@github.com> wrote:  > @rvagg <https://github.com/rvagg> @MylesBorins > <https://github.com/mylesborins> (and everyone else) there is a very > important detail about the private jobs that you might be missing: only the > private jobs are private, subjobs launched by them are not. For example, > try to open the following links (while they are still available) in an > incognito window (logged out of GH): > >    - https://ci.nodejs.org/view/All/job/node-test-commit-private/173/ >    - https://ci.nodejs.org/job/node-test-commit-freebsd/12615/ > > The second one was launched by the first and is perfectly accessible. > There are even windows binaries available from https://ci.nodejs.org/job/ > node-compile-windows/12806/label=win-vs2015/ (there should be arm > binaries as well, but the private job is out of sync and does not have > arm-fanned). > > Jenkins must always be locked for security work. Even if we manage to fix > the issue above, I do not trust Jenkins (or our knowledge of it) to do > security work without locking. > > The private jobs might still have a reason to exist: to provide default > values. But they should only pass the parameters to the regular jobs, not > try to duplicate. > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/943#issuecomment-339697594>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecVz1C94E1I2X4LP_0kVu_VTWkzdi4ks5swKCZgaJpZM4QFPsM> > . > 
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
mhdawson		One machine looked ok, but the other had a lot of old npm-* directories .  Thinking those might be due to cigtm runs.  I deleted them and we'll see if that fixes things up or if something has changed and we need more /tmp space 
mhdawson		Noticed that deleting npm-* only freed up 2%.  Looks like this one is the bigger culprit:  ``` bash-4.3# ls -l  /tmp/f081cbbd-2b39-4e5a-8ca2-948db962eace total 424 drwxr-xr-x    4 iojs     staff           256 Apr 03 12:47 npm_config_tmp drwxrwxr-x    7 iojs     staff          4096 Apr 03 12:46 uglify-js -rw-r--r--    1 iojs     staff        209974 Apr 03 12:46 uglify-js-2.8.21.tgz ```
mhdawson		deleting that freed up about 50% 
mhdawson		@gibfahn are the directories in the form of 0629edd6-daca-4544-ac99-2ffc3a3f0052 tied to citgm, they seem to be taking a good portion of the space on test-osuosl-aix61-ppc64_be-1 and there are a number of them from yesterday. 
mhdawson		Build to see if I've freed up enough space: https://ci.nodejs.org/job/node-test-commit-aix/4800/
gibfahn		@mhdawson yes, those directories are citgm's temp directories. A build was run without the temp directory settings properly configured, which is why they filled up. I thought I'd cleaned up both machines, but I guess not.  You can usually get rid of everything with: `rm -rf /tmp/*-*-*-*-* /tmp/npm-*`
mhdawson		can you take a look and clean up the rest.  When I looked yesterday I only cleaned up enough to get the machines working again.  
mhdawson		@gibfahn happened again.  Saw a failure over the weekend and tmp was full on test-osuosl-aix61-ppc64_be-2.  Lots of directories with watchify-xxxx but those did not contribute significantly to the space.  Issue still seemed to be the directories associated with citgm. Cleaned up all the directories with the command above.  I guess we'll see now if it continues to pile up. 
richardlau		https://github.com/nodejs/build/issues/682
Trott		Just ran `rm -rf /tmp/*-*-*-*-* /tmp/npm-*` again now and that seems to have cleared things up. Obviously, needing a more permanent solution...
gibfahn		See https://github.com/nodejs/citgm/issues/396#issuecomment-293217118 , this shouldn't be happening if jobs are configured correctly.
gibfahn		Sounds good to me
misterdjules		Modified the `node-test-commit-smartos` job to add a `TEST_CI_ARGS` parameter with a default value of `--abort-on-timeout`, and deleted the test job `node-test-commit-smartos-abort-on-timeout` that had been used to test this change.
bnoordhuis		This is breaking the CI for non-master branches, see e.g. https://ci.nodejs.org/job/node-test-commit-smartos/6865/nodes=smartos14-32/console which is a run of v6.x-staging.  If this can't be fixed easily and speedily, it should be rolled back for now.
bnoordhuis		To save people from clicking through, the error is this:  > test.py: error: no such option: --abort-on-timeout  Which makes sense because https://github.com/nodejs/node/pull/11086 hasn't been back-ported yet.
misterdjules		My apologies, I disabled the new flag in `node-test-commit-smartos` for now, and I'll work on a backport asap. Thank you for investigating, and sorry for the trouble!
bnoordhuis		Thanks, Julien.
thefourtheye		Is it generally okay to backport, given that it hasn't been released in "Current" release stream?
misterdjules		> Is it generally okay to backport, given that it hasn't been released in "Current" release stream?  I would think so. Is there any indication of the contrary somewhere? FWIW, this change is part of the v7.6.0 release proposal at https://github.com/nodejs/node/pull/11185. 
gibfahn		>> Is it generally okay to backport, given that it hasn't been released in "Current" release stream?  >I would think so. Is there any indication of the contrary somewhere?   cc/ @MylesBorins @nodejs/lts 
bnoordhuis		I don't see why not, it's not a user-visible change.  It's a change to the test harness, not to node itself.
thefourtheye		@bnoordhuis I agree, this specific case is okay. I am wondering, generally, if a commit lands in master, then it is good enough to be backported.
bnoordhuis		@thefourtheye Is that a question or a statement? =)  If it is a user-visible change (bug fix, feature, anything that changes the node binary or how it's built) it should bake for some time in Current first.
thefourtheye		I was just thinking out loud :D Cool. That makes sense. @bnoordhuis Thanks :-)
misterdjules		The changes to support that new `tools/test.py` command line flag were back ported to v6.x and v4.x branches with https://github.com/nodejs/node/pull/11354 and https://github.com/nodejs/node/pull/11351.  My intention is to re-enable this flag on SmartOS asap. Any objection, or any branch that I missed?
gibfahn		@misterdjules sounds good to me. If you get any failures you can always conditionally enable it for v4/v6/v7/master.
misterdjules		Enabled again for the `node-test-commit-smartos` job. Thanks all for the feedback, and sorry for the breakage.  I'll watch builds closely for the next couple of days to make sure it's not breaking anything, and I'll react as quickly as possible if it breaks.
orangemocha		In the past I had researched a few git-based solutions, in particular: 1. [git submodule](https://git-scm.com/docs/git-submodule) 2. [git subtree](https://git-scm.com/book/en/v1/Git-Tools-Subtree-Merging) 3. This simple script: http://blog.rusty.io/2010/01/24/submodules-and-subrepos-done-right/  Both submodules and subtree seemed to introduce more problems than they solve. The script approach at 3. looked promising. 
panuhorsmalahti		What exactly are the problems with git submodules? Requiring 3rd party scripts for version control seems like the worst option of all alternatives. 
bnoordhuis		Everything takes more work: `git submodule foreach git grep ...`, `git pull && git submodule update --recursive`, etc.  Bisecting across submodules is cumbersome as well.  I, for one, like our big monorepo, size be damned. 
MrRio		Google have their entire infrastructure, every product, in one repo.  It's easier to branch and make sweeping changes. 
panuhorsmalahti		@bnoordhuis Having to make sync commits manually also takes work. I work with submodules daily and I find them trivial, especially if the depth of submodules is only one and there are no complex relationships/duplicates of submodules.  @MrRio I support monorepositories if the monorepository doesn't contain a duplicate of another "master" repository. Atomic commits across multiple codebases is an attractive feature, but from what I gather not that useful in Node.js. 
Artoria2e5		As far as I know both 'solutions' 2 and 3 still involves checking the whole thing into git, and always fetching them when cloning.  ---  In solution 3 the changesets are stored twice which introduces some redundancy: 1. The `.subgit` directory (you still have to find somewhere to host it). 2. The big repo directory. Every time you do `subgit pull`, you get a big set of changes which is essentially a combined result of the changes made since the last `pull`. People want to keep the deps up to date, so you will end up having trunks of big commits (a bit 'shallower' indeed).  ---  Solution 2, the `subtree` solution (including the git/contrib/git-subtree wrapped one) doesn't show significant difference from the current method, since you are still essentially pointing the trees as if they are put here normally, only with some extra caution so they can still correspond to somewhere else in this `.git` dir or some other repos in the future.  https://schacon.github.io/git/git-read-tree.html#_sparse_checkout looks interesting though.  ---  Submodule is the right way to go IMHO. All those extra mess is simply caused by you trying to having more control over what to fetch. But if you simply want to fetch everything, you can just go for `git clone --recursive`.  There is even `git submodule update --depth 1` which allows you to save some time loading the dependencies.  Oh, and.. The [git-subtree](https://github.com/git/git/blob/master/contrib/subtree/git-subtree.sh) script can be quite handy for spliting new repos from existing directories. For example, I don't think chakrashim is hosted independently anywhere else and you may still want to split it out so everything looks like submodules.  ---  @bnoordhuis That's a valid point, but since ~~nodejs~~ your topic is now focusing on submodule-ing out dependencies which is not a main focus of bisecting, I'd rather say this problem is not that big.  `git bisect` does checks out `.gitmodules` and these `Subproject commit <SHA1>`  info of the commits being tested, and one can still figure out which commit _nodejs developers_ made included something bad from dependencies by running tests after doing submodule 'updates'. Careful bisecting in that specific submodule with the outside source being unchanged can be done afterwards.   If the developers of those dependencies are doing their job well, this actually filters out some noise.  * * *  P.S. You can shallow-clone git submodules if you don't like the space waste. See https://stackoverflow.com/a/17692710/3770260.
jbergstroem		@Arthur2e5 as far as I know nodejs hasn't taken a decision on whether we should use submodules or not (or am I missing something?). 
Artoria2e5		Yes, just prefix everything with _suppose_‚Ä¶ And I am just passing by. 
boopathi		Could something like gclient (https://www.chromium.org/developers/how-tos/depottools) help ? 
Fishrock123		Also keep in mind that we may have to float patches on our deps (optional or not) 
MylesBorins		One thing to consider about submodules, they do not necessarily help with size.   Perhaps they do help take down the burden of the initial size, but the eventual download is much higher if you are cloning a submodule with the entire history of the dependency.  For context, the ChakraCore git module is about 140 megabytes, it goes down to 100 if you simply copy the assets without the .git folder (70 of which is in the tests folder).  This discrepancy is only going to get larger as the project moves forward (for example the v8 repo is over 4GB and only 88 MB in the node.js repo. Obviously there is more than 88 MB worth of information tracked in the repo, but I doubt we have 4 GB as the node repo is 1.5 GB  All of this is still missing when this is often done in practice, which is during compilation, which can often happen in a tar ball with no .git folder. 
Trott		@nodejs/build Should this remain open?
Trott		I'm going to close this for now because it seems like we're not going to change anything. If I'm wrong about that and/or this should be open for another reason, feel free to re-open or comment. 
indieisaconcept		@rvagg I have a B+ I've never really used which you can have. It's just been sitting in a draw. It doesn't have the enclosure listed though - its one of [these](http://www.modmypi.com/raspberry-pi/cases/modmypi-single-colour/modmypi-modular-rpi-b-plus-case-black). 
rvagg		@indieisaconcept great! Enclosure doesn't matter too much, it makes them easier to stack and mount if they are consistent but it's not a big deal. We even have a Pi 2 that's running without an enclosure at the moment (I have a TODO on that). Are you going to be at Node.ninjas on the 7th? I've been thinking about heading up for that one. Either way, you're on my chase-down list now and I'll get it from you one way or another! 
indieisaconcept		@rvagg should be there - happy to post if you need it sooner. 
mhdawson		I'm in for providing 1 pi, whichever type is the highest priority.   
williamkapke		I'll get these sent out as soon as I get the address: - 3 x (Raspberry Pi 3 + memory + enclosure) - USB power, USB cables and network cables for 10 devices - Network switch  EDIT: This are on the way!!  FYI:  [Raspberry Pi 2 Model B](http://au.rs-online.com/web/p/products/8326274/)  > On back order for despatch 19/08/2016, delivery within 5 working days from despatch date.  [Raspberry Pi Model B+](http://au.rs-online.com/web/p/products/8111284/)  > On back order for despatch 27/05/2016, delivery within 5 working days from despatch date. 
jasnell		@rvagg ... I have an extra Pi 2 I'm not using. I see you're going to be at collab summit in two weeks so I'll give it to you then when I see you. 
chriswiggins		@rvagg I've sent you an email about donations :+1:  
rvagg		I've updated the OP with approximations on what have been pledged so far. I'm not willing to be firm until I actually get them but it's going well. I also have received a large open cash donation which we can use to fill in gaps or purchase some of the incidentals around the outside (like proper cable ties instead of the random items I've been using instead). And to acknowledge @williamkapke point about backordering for older Pis - yes, that's unfortunate and I was aware of this but it's because none of the downstream suppliers are stocking them in any meaningful quantity, my normal supplier (element14) even requires orders of >150 to get them in at all. It seems that RS are batching up orders to the Raspberry Pi Foundation so they can get them in the bulk lots that they are available in and not have to sit on unused stock. That's OK because we have a _working_ cluster atm but we need additional hardware in the pipeline at least.  Still keen to hear from new folks who would like to join the donors list! 
rvagg		fwiw I've started tracking all donations, including funds and in-kind donations of hardware, in a double-entry spreadsheet along with an inventory of what we've been given. I've shared with core members of the build team. I'm not sure yet if it's appropriate to make it public but I did want to let everyone know that there is _some_ level of accountability here and that it's not pouring into a black hole. 
jbergstroem		@rvagg suggesting you add a UPS to above list. If there are monetary donations I'd see that as a pretty good investment. 
rvagg		@jbergstroem did that yesterday! Got a sizable cash donation from @davglass and have so far used it to buy a UPS, some power and cable management equipment and another Pi 3. Got a couple more Pi 3's donated this morning so we're starting to have a nice cluster on the way in. @williamkapke is donating extra networking and USB power equipment too. Backlogs on all Pi hardware though so we'll have to be patient.  One thing that I'm looking forward to is Raspbian Jessie testing with the Pi 3's. So far we only have Raspbian Wheezy coverage on Pi. 
notthetup		@rvagg Would love to donate a RPi3. Emailing you. 
piccoloaiutante		@rvagg i'm happy to provide money for Raspberry Pi 3. I've just emailed to you :-).  
andineck		I think this is a great thing. That you open it up to the community, so that everyone can be involved (even though it might be less work to just buy it with the foundation money).  I would love to donate 1 PI (what ever is still needed). 
rvagg		@andineck if you want to go again then I'd recommend either a Pi 1 (2 or 3 more would be nice) or Pi 2 (1 more to round out to 12), we're getting close to having the upper target for Pi 3's already. Email me if you want to do it via $$, or you can just use the same info you used previously to order and have the delivery go to me, I'm still at the same address, follow the links above to RS for the older Pi's. 
andineck		@rvagg sorry for my very late reply. If the Pi 1 is still required, I would cover the costs via $$ (otherwise I would go for a Pi 2). If you can send me the Amount, and paypal Account, that would be great. 
rvagg		Hey folks, quick update, there's still back-order in place for older pi's and some of the newer ones but so far we have: - Networking, power and cable management miscellaneous components c/o @davglass financial contribution - Pi 1 B+ c/o @indieisaconcept in place and _nearly_ in CI but there's a hiccup with NFS that I haven't resolved yet - Pi 2 c/o @jasnell in place and in CI - 6 Pi 3's c/o @davglass, 2 from Pivotal Agency (https://www.pivotalagency.com.au/), 3 from @williamkapke, in place and hooked up to CI but not doing anything yet, we need to figure out how to construct the Pi 3 build job yet (/cc @joaocgreis who will probably need to assist in figuring out cross-compile).  Will keep you informed as we progress. @andineck I'll ping you when I have a better handle on the Pi 1 situation, we have Pi 2's up to 12 now so it's just the 1 B+'s that I'm concerned about but we may be close to 12 already with incomings. 
rvagg		oh, and also to that, we got: - UPS (!) c/o @davglass - USB power and networking components c/o @williamkapke  
joaocgreis		@nodejs/build @nodejs/jenkins-admins   I added the Pi 3 to https://ci.nodejs.org/job/node-test-binary-arm/ . It tests the same armv6 binary that is tested on Pi 1 and 2. The cross compiling server is relatively prepared to compile different binaries for each, if we want to do that. 
rvagg		@joaocgreis can we do armv7 on the Pi2 and Pi3's? Hopefully we'll be able to switch to arm64 on the Pi3's when that whole drama gets sorted out. 
joaocgreis		Done. Compiling with `-march=armv6zk` to run on the Pi1 and `-march=armv7-a` to run on the Pi2 and 3. 
rvagg		Received the outstanding 5 Raspberry Pi 3's today, bringing the total up to 11. I'll be setting them up this week. They are care of securogroup.com, @notthetup + @sayanee, @piccoloaiutante and @kahwee. We still have 4 outstanding Pi 1's on backorder and a network switch still to be received. 
piccoloaiutante		excellent @rvagg  
rvagg		Outstanding Pi3's are all set up now, we have a cluster of 11 so far: - https://ci.nodejs.org/computer/test-nodesource_davglass-debian8-arm_pi3-1/ - @davglass - https://ci.nodejs.org/computer/test-nodesource_kahwee-debian8-arm_pi3-1/ - @kahwee - https://ci.nodejs.org/computer/test-nodesource_notthetup_sayanee-debian8-arm_pi3-1/ @notthetup & @sayanee  - https://ci.nodejs.org/computer/test-nodesource_piccoloaiutante-debian8-arm_pi3-1/ - @piccoloaiutante  - https://ci.nodejs.org/computer/test-nodesource_pivotalagency-debian8-arm_pi3-1/ - @ksflyinghigh / @wearepvtl - https://ci.nodejs.org/computer/test-nodesource_pivotalagency-debian8-arm_pi3-2/ - @ksflyinghigh / @wearepvtl - https://ci.nodejs.org/computer/test-nodesource_securogroup-debian8-arm_pi3-1/ - @chriswiggins / @securogroup  - https://ci.nodejs.org/computer/test-nodesource_securogroup-debian8-arm_pi3-2/ - @chriswiggins / @securogroup  - https://ci.nodejs.org/computer/test-nodesource_williamkapke-debian8-arm_pi3-1/ - @williamkapke  - https://ci.nodejs.org/computer/test-nodesource_williamkapke-debian8-arm_pi3-2/ - @williamkapke  - https://ci.nodejs.org/computer/test-nodesource_williamkapke-debian8-arm_pi3-3/ - @williamkapke   An additional Pi 1 B+: - https://ci.nodejs.org/computer/test-nodesource_indieisaconcept-debian7-arm_pi1p-1/ - @indieisaconcept   An additional Pi 2: - https://ci.nodejs.org/computer/test-nodesource_jasnell-debian7-arm_pi2-1/ - @jasnell  With additional resources (networking, power, cables, UPS, cable ties, replacement & spare MicroSD cards) provided by @davglass and @williamkapke.  Still pending 4 Pi 1 B+'s on backorder, c/o @mhdawson, @chriswiggins / @securogroup and @chrislea. While RS Components still has these listed as on backorder with a deliver date mid-June, they have pushed that date back twice now and I tried to order another and they rejected that immediately. I assumed they were waiting for enough orders to warrant a batch of 1000 from the Pi Foundation but perhaps that's wrong and we're going to have trouble getting them. What sucks about this is that the Pi 1 B+'s are the ones we have the fewest of so are the biggest bottleneck and we have the least redundancy, they are also the ones we build ARMv6 binaries off for nodejs.org (we don't even build binaries on the 2's or 3's). I've found a small supplier in AU that say they have a few in stock still but they are selling them at a slight premium. I've contacted the Raspberry Pi Foundation directly to see if they might be kind enough to just donate a bunch to us. Will wait for a response from them before I act. I'll also need to contact RS to get an answer about the backordered ones.  Take-away for everyone in this thread - put the word out that if folks have Pi 1 B+'s laying around then we'd love for them to be donated to the project! We don't need original Pi 1's, they are too slow, these are the second generation B+'s that we need. This is what they look like: https://www.element14.com/community/servlet/JiveServlet/downloadImage/102-78141-4-227912/1362-900/b%2Bmarking.jpg 
rvagg		Woohoo! RS just got the B+'s back in stock and they are apparently now shipping the ones I'm without. I'm also thinking about using some of the additional funds we have to get a few more since these are a bottleneck for us and we have 2 out of action for the release cluster. 
notthetup		Hey @rvagg,  I have a couple of RPi B and one RPi B+ that I have no use for. They belong to @coffeesam and he's willing to donate it to the build cluster. Do you think it will work if I ship them to you? Do you need them?   I have shipped stuff from SG to AU before. It's not that expensive. 
rvagg		That'd be great @notthetup, we can't use the B's, we had two in the beginning but had to retire them because they are too slow to be useful. I'll send you the address. 
rvagg		OK, it's been a while but I've got 3 new Pi B+'s set up and running in the cluster: - https://ci.nodejs.org/computer/test-nodesource_mhdawson-debian7-arm_pi1p-1/ thanks to @mhdawson! - https://ci.nodejs.org/computer/test-nodesource_securogroup-debian7-arm_pi1p-1/ thanks to @chriswiggins / @securogroup - https://ci.nodejs.org/computer/test-nodesource_securogroup-debian7-arm_pi1p-2/ thanks to @chriswiggins / @securogroup  Bringing our total to 12 B+'s, which is great. _But_, since we have two of those out of general use thanks to being partitioned off for making release builds, we're effectively at 10. I'm supposed to have a 4th donation thanks to @chrislea but for some reason RS hasn't got a record of that order so I'm going to have to dig up the transaction to see if they took my money for it! I think I'll use some additional @davglass money to purchase another which will bring our total to 14!  We've also had an SD card failure from one of the existing B+'s and I'm all out of spares so I'll have to go and order a bunch more of those (I'm sure there will be plenty more failures in our future!). 
rvagg		Phew, got @chrislea's B+ today along with another one I ordered on behalf of @davglass. https://ci.nodejs.org/computer/test-nodesource_chrislea-debian7-arm_pi1p-1/ & https://ci.nodejs.org/computer/test-nodesource_davglass-debian7-arm_pi1p-1/, bringing the total of B+s to 14! 2 of those are isolated from the rest for doing release builds (the armv6l binaries on nodejs.org) leaving 12 in the test cluster for running tests in groups of 4 at a time so we can do 3 separate test jobs simultaneously. So we now have 14 B+'s, 12 2's and 11 3's. It'd be nice to add the extra 3 but mainly for symmetry, since they run so much faster than the rest they are not the bottleneck by any means.  Below are some photos that I took recently (without the additional 2 B+'s), for your viewing pleasure! Most of them have the names of the donor on them to help with identification, fwiw. From left to right we have the B+'s, 2's, 3's, 3 x Odroid XU's and 3 x AppliedMicro X-Gene's (armv8) with a UPS on top of them.  I'll get updated details and perhaps photos onto the [README](https://github.com/nodejs/build) soon to reflect all our new donors and the state of things. I also think a page on nodejs.org with similar information about our cluster and all our wonderful donors, individual and corporate, would be good too. I'll leave this issue open until the documentation is all done, but that's the last thing left to do!  ![node_arm_cluster_201609-6](https://cloud.githubusercontent.com/assets/495647/18666345/b882a3dc-7f6e-11e6-863b-a437fc86c104.jpg) ![node_arm_cluster_201609-4](https://cloud.githubusercontent.com/assets/495647/18666347/b88919b0-7f6e-11e6-86dd-c53705a33573.jpg) ![node_arm_cluster_201609-5](https://cloud.githubusercontent.com/assets/495647/18666346/b886a7f2-7f6e-11e6-9cf7-4d0723f0f3cc.jpg) 
phillipj		Very cool to see how these boxes are organized, thanks for sharing! 
joaocgreis		Updated. Now also tested in Windows 2008R2 and 2012R2. 
joaocgreis		This has had plenty of testing over the last year, was improved several times, and is now a very stable version. Even if we update to Ansible 2 soon, it would be good to land this version to keep it in history. @nodejs/build do you mind if I go ahead and land this? 
jbergstroem		@joaocgreis All for it. I have a few suggestions but lets focus our work on the 2.x branch instead. LGTM 
joaocgreis		Landed in https://github.com/nodejs/build/commit/23d3246014732b9325a687c3c9d10f1b11bf9ccf 
mhdawson		Ok have added test le machines:   https://ci.nodejs.org/computer/test-osuosl-ubuntu14-ppc64_le-5/ https://ci.nodejs.org/computer/test-osuosl-ubuntu14-ppc64_le-6/ 
jbergstroem		nitpick but perhaps we can rename them back to lowest unique id once done? 
mhdawson		Sure once, we have the final set I can do the renames 
mhdawson		new release machine(s) created and configured through ansible:  https://ci-release.nodejs.org/computer/release-osuosl-ubuntu14-ppc64_le-2/ https://ci-release.nodejs.org/computer/release-osuosl-ubuntu14-ppc64_be-2  @jbergstroem can you: - [ ] replace ssh test key with release key - [ ] Add firewall entries - [ ] add required keys etc needed for publishing releases.  Once you do that I'll temporarily disable the current machine and run a nightly type build to validate it works ok. 
jbergstroem		Ok, I'll do it after the meeting. 
mhdawson		Sorry IP for release-osuosl-ubuntu14-ppc64_le-2 is 140.211.168.66 
jbergstroem		`release-osuosl-ubuntu14-ppc64_le-2` is ready to go. 
mhdawson		Ok just added and ansible is running on them:  @jbergstroem can you add these to the firewall as well  TEST     test-osuosl-ubuntu14-ppc64_be-3/140.211.168.74         test-osuosl-ubuntu14-ppc64_be-4/140.211.168.75  RELEASE          release-osuosl-ubuntu14-ppc64_be-2/140.211.168.76 
mhdawson		@jbergstroem  trying to check out   release-osuosl-ubuntu14-ppc64_le-2/ release-osuosl-ubuntu14-ppc64_be-2 They don't seem to be connected and I can't log into them anymore to see if the agent is up or if it would be a firewall issue 
jbergstroem		Fixed. They disconnected when I restarted `ci-release` earlier today and referenced `https://ci.nodejs.org` in the init scripts. 
mhdawson		Thanks, ran a separate job to validate and seemed to generate/push ok so will move regular release job over to using the new machines 
mhdawson		Ok, have rename all of the test machines.  @jbergstroem since I don't have access can you rename the release ones ?   Once that's done I'll submit the ansiable config file changes to reflect the new ips. 
jbergstroem		ok, both release -2 machines have been renamed. 
mhdawson		As last step PR to update ips for PPC machines in ansible config: https://github.com/nodejs/build/pull/507 
MylesBorins		LGTM! 
mhdawson		LGTM 
joaocgreis		The master workspace needed to be cleaned up. Done, seems to be ok now. Please reopen if it's not, or if this happens again. And thanks for reporting!  So far this "workspace needs to be cleaned" failures have been rare and I haven't seen a pattern, so I guess  we have to keep cleaning manually when we see this. 
Trott		Hooray! Thanks for fixing that! After 10 consecutive days of failed node-daily-master jobs, we finally got a green one again this morning! 
jbergstroem		@joaocgreis can we create a cronjob for this?  
joaocgreis		@jbergstroem Jenkins should not be running when the workspaces are cleaned. Can we do this in the script that launches Jenkins? (where is that?) The command should be something like (untested):  ``` find /var/lib/jenkins/jobs -mindepth 2 -maxdepth 2 -name workspace\* -type d -print -exec rm -rf '{}' \; ``` 
jbergstroem		@joaocgreis the init script is provided by upstream, you can probs find it in `/etc/init.d` but it will likely be replaced on update. Seeing how we rarely restart host (well at least thats the ideal scenario) - is there another way? 
joaocgreis		@jbergstroem this is rare enough that just when we restart jenkins should be enough. I've been looking for a way to run things when nothing else is running, but no luck. (Also wanted to clean the temp repo).  But for the slaves this might be doable with a job, I'll investigate. 
joaocgreis		Found a way to run a script in master after the job, that does not make the job fail. Will delete `.git` if it is >=5Gb, so it is created again in the next run in a clean state.
bnoordhuis		I believe it only builds PRs from people that are whitelisted, i.e. known contributors. 
rvagg		insight in to how this whitelist is maintained would be great, if it can be automated then :thumbsup: 
bnoordhuis		Not 100% sure but I believe it's just a text file (a rather small one) with no automation whatsoever.  Perhaps a better approach is how [bors](https://github.com/graydon/bors) is used in the Rust project: a reviewer does a quick review of the pull request, then tells bors to build and test it ([example](https://github.com/rust-lang/rust/pull/11885#commitcomment-5238690)).  No doubt there's a Jenkins plugin with similar functionality.  It still requires a whitelist of reviewers but that list will be fairly static. 
tjfontaine		A small proxy sits in front of jenkins written in node, affectionately called jankins. If routes are not for jankins they are proxied to jenkins. It also accepts github webhooks.  Prs are only run on unicies, not windows, feature branches run on all platforms.  New prs are compared against a whitelist managed by a SQLite db, and automatically built if pushed by a collaborator on that list.  A chrome plugin with your jenkins API and github API key injects in pr pages recent build information if available. You may also trigger a rebuild of the pr  Jankins also listens for resyncs, if the pr has been previously built by someone with credentials subsequent pushes to the pr branch will automatically trigger a rebuild.  The jenkins job itself curls and applies the patch to the target branch so no manual rebasing is required. It either applies or doesn't.  The first step beyond that is always to run lint.  The end of the job always notifies the proxy of it finishing, but matrix jobs will notify for every axis completion, so the proxy needs to watch for the build being done.  Improvements on this process could be to manage the whitelist by checking against the github collaborator list.  Also automatic creation of jobs and management of webhooks in jankins would be ideal.  Extending a bot to be able to trigger from irc would be great, you could also make it listen to github comments to trigger it (github plugin does this but in the most annoying and verbose ways)  Potentially also using a complete throw away container zone would be ideal for sand boxing, but not enough of the platforms offer real isolation to support that. 
rmg		Since I had to write tooling for dealing with the 212 (and growing) jobs on our Jenkins instance, I added whitelist updating to it so that PRs are built automatically for anyone who has already landed a commit on master. 
bnoordhuis		Closing, we're effectively using a whitelist now.  If someone feels strongly about this, please reopen. 
refack		AFAICT checking nightlies (and canary nightlies) at least, is blocked by https://github.com/nodejs/build/issues/908
targos		For canary nightlies, I don't think we should block the download if a platform is missing.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
mhdawson		Just ran ok on release-voxer-osx1010-x64-2.  The 3 failures were on release-voxer-osx1010-x64-1 so good chance its related to the machine.   
mhdawson		Ran again on release-voxer-osx1010-x64-1 and failed so almost certainly something on that machine. @jbergstroem   @rvagg
rvagg		OK, I'll take it out of the pool until I can figure out what the deal is
rvagg		two of the machines failing .. different reasons but both for signing, have taken them offline for now until I figure it all out and test that they work, we still have a third that seems to work but needs to do a double shift to get the osx assets built.
maclover7		ping -- is still needed?
gibfahn		Don't think this is still failing.
jbergstroem		I'd prefer having an init script, but I can add it later since I'm already on to that. The hosts file seems to suggest a user called `pi` while ansible contains iojs. Not sure if intentional but LGTM either way as long as it works. 
rvagg		- yes init scripts would be awesome for these machines in particular because java regularly dies in the constrained environment, I just haven't proritised yet but am happy to have assistance! - `pi` user is the default user on raspbian and has sudo access, it's used to set up the boxes, the `iojs` user is unprivileged and is created by the setup procedure to run the slave. 
jbergstroem		Thanks for the reply. Happy to merge.  
rvagg		We are still building 4 and 6 binaries with CentOS 5 and we can't change that so we should continue to support it there, it's unlikely to cause too much trouble with the small incremental changes going in to 4 and 6, especially when 6 heads into Maintenance. CentOS6 is used for binaries of 8+. Another problem we have is that we can't even deploy new CentOS5 machines so we're stuck with the ones we have for the life of 6. If we were to need to replace them for whatever reason then we'd have to hack together some workaround like a Docker container or a VM in a VM or some other hackery. I'm +1 on removing it from the 8+ test mix though since we don't rely on it, leaving it for <8 test runs. Any objections to doing this since it's causing some grief?
Trott		@rvagg If I'm understanding you correctly, we can't remove CentOS 5 machines completely until Node.js 6.x is unsupported which won't be until April 2019. Is that right? Yikes! But, yeah, I get it.
rvagg		yerp, that's correct, kind of tied ourselves to a bit of a donkey with that one but it's a bit hard to change now
gibfahn		>I'm +1 on removing it from the 8+ test mix though since we don't rely on it, leaving it for <8 test runs. Any objections to doing this since it's causing some grief?  We should definitely do this.
refack		Seems like if push comes to shove we can at least provision a new CentOS5 machine on AWS (either a comunity AMI or [install our own](http://www.nixhat.com/2011/01/create-custom-centos-5x-ami-page1/))
Trott		Another CentOS 5-only flaky (and thus a probable argument for removing CentOS 5 from CI): https://github.com/nodejs/node/issues/14982
gibfahn		Okay, so I've replaced this code in https://ci.nodejs.org/job/node-test-commit-linux :   ```bash # clang is only supported in Node versions 7 and lower MAJOR_VERSION=`cat src/node_version.h |grep "#define NODE_MAJOR_VERSION" | awk '{ print $3}'` RUN_TESTS="RUN" echo $SMARTOS_VERSION if [[ "$nodes" =~ ubuntu1204 && ${MAJOR_VERSION} -gt 7 ]]; then   RUN_TESTS=DONT_RUN   SKIP_MESSAGE="ubuntu1204 is not supported for version 8 and above, skipping" elif [[ "$nodes" = fedora22 && ${MAJOR_VERSION} -gt 7 ]]; then   RUN_TESTS=DONT_RUN   SKIP_MESSAGE="fedora22 is not supported for version 8 and above, skipping" fi ```  ```bash # clang is only supported in Node versions 7 and lower MAJOR_VERSION=`cat src/node_version.h |grep "#define NODE_MAJOR_VERSION" | awk '{ print $3}'` RUN_TESTS="RUN" echo $SMARTOS_VERSION if [[ ${MAJOR_VERSION} -gt 7 && "$nodes" =~ 'ubuntu1204|fedora22|centos5' ]]; then   RUN_TESTS=DONT_RUN   SKIP_MESSAGE="$nodes is not supported for version 8 and above, skipping" fi ```  Please let me know if this is wrong!
refack		"if" sample: https://ci.nodejs.org/job/node-test-commit-linux/12908/nodes=centos5-32/console ``` centos5-32 is not supported for version 8 and above, skipping ``` "else" sample: https://ci.nodejs.org/job/node-test-commit-linux/12910/nodes=centos5-32/parameters/ ``` python ./configure  creating ./icu_config.gypi * Using ICU in deps/icu-small ... ``` SGTM
mhdawson		One other alternative might be to have the scripts transfer the resources to the local machine where ansible is running first and then transfer from there to target machine (assuming I can do that in ansible).  That would avoid having to transfer the ssh key to the target machines.
gibfahn		+1 on this, I think it makes sense (and we discussed it in a Build WG meeting a while back, IIRC there were no strong objections).  >One other alternative might be to have the scripts transfer the resources to the local machine where ansible is running first and then transfer from there to target machine  I'd rather we go with your first suggestion than this, transferring everything twice seems like it'd take a lot longer and be more error-prone.  >- generate a new keypair for that user > >- have the ansible scripts render the key onto the test machines. It would have to have been extracted from the secrets repo and put into a known location by the individual running ansible  Could we do it with the `nodejs_build_test` key? If the `resources` user just used the test key to authenticate, then ansible could just copy the key from `~/.ssh/nodejs_build_test.pub`.
rvagg		As per our discussion in the meeting this week: https://ci.nodejs.org/downloads/ and the insecure http://ci.nodejs.org/downloads/ are now publicly accessible for resources we need to share for our installations.  The files are served from /home/downloads/www/ from the user account `downloads`. This account has its own key which is available to build/infra folks at `secrets/build/infra/downloads@ci.nodejs.org.key` just upload files into the www directory for that user and they'll be available. This key only has access to the `downloads` account but once you're on the machine you can get access to anything we haven't fully secured in the jenkins config so we probably shouldn't be _too_ liberal passing it around but I think we can be a bit broader than just build/infra. I have no problem sharing it with team members who actually have resources to upload, like I imagine @gibfahn does.  Remember to preference `https` in your ansible scripts wherever possible, `http` only makes sense for machines that can't download via `https` such as the zOS (or is it AIX?) machines.
gibfahn		@mhdawson could you give me access to the account?
mhdawson		I'm not sure what the plan was for giving out the key since its stored in the infra secrets.  If you have some binaries you want me to upload just send them over. 
mhdawson		I see the comments above about sharing the key @gibfahn lets connect so I can give you the key.
mhdawson		I added zos/gyp.tar.gz  (edited had wrong name), thinking it may make sense to have directories for platforms  
mhdawson		My thought is we would like the job to allow  1) validation of the latest versions of the LTS can Current streams, this would be the default 2) validation a specific version passed in as a parameter (for example 8.1.4)
mhdawson		Duplicate of https://github.com/nodejs/build/issues/513, closing
mhdawson		Work in progress - https://ci.nodejs.org/job/node-test-commit-linux-fips/, note still builds non-fips just starting by cloning existing linux job and stripping down. Will then modify to build in fips mode 
mhdawson		Ok build is configured to build in FIPS capable mode and tests are passing.   https://ci.nodejs.org/job/node-test-commit-linux-fips/,  @nodejs/build, @nodejs/crypto  I'd like to add this so that its run as a sub-job as part of the regression tests as part of node-test-commit   Feedback I'd like: - agreement to go ahead and add as subjob - platforms we should run this on.  So far I have it set up to run only on ubuntu 1404. Since we are not shipping releases in FIPS capable mode I don't necessarily think we should run across all of the linux platforms as that would potentially double the runtime/workload.   
jbergstroem		@mhdawson is it compiled with ccache? looks slow. 
bnoordhuis		One problem is that FIPS mandates minimum key sizes for (EC)DH that are quite slow to generate.  See https://github.com/nodejs/node/issues/3881 and https://github.com/nodejs/node/pull/3902 - the ARM buildbots were affected most but even on comparatively beefy hardware it can take tens of seconds.  I suggest we try it and see how well it works in practice. 
mhdawson		@jbergstroem  I did not do anything to prevent it from being compiled with ccache.  There is an extra step where is need to compile fipscanister.  The actual job to compile/test took ~7 mins.  The parent job shows as taking a lot longer because it had to wait for a machine. 
mhdawson		Any objections to stitching in what I have now (ubuntu14 only) and then we can expand the platforms covered once we have agreement in this issue ?  
jbergstroem		One is better than none. I'd like to keep it as a separate job from the -linux one (similar to what you have at the moment). 
mhdawson		Agreed, I'll add what I have now and mark this for discussion in the next build meeting to discuss what the full set of platforms we'd think it makes sense to cover.  
mhdawson		@nodejs/collaborators just added nod-test-commit-linux-fips as subjob to node-test- commit. Let me know if you see any issues  
mhdawson		Note that even if the top level jobs says it took 16 mins, the job itself only took 10 because it was waiting for a free machine.  I think @jbergstroem was adding an additional ubuntu 14.04 machine that would address that 
mhdawson		Based on discussion in the build workgroup meeting yesterday we agreed that running on a single platform provides the required coverage. At this point we think any regressions caught by these tests will be across platforms.  Since we don't ship in FIPS capable mode on ay specific platforms running on the single on already enabled is therefore good enough. Going to leave as is unless we get additional input.  
rvagg		actually a lot of it is here already: https://jenkins-iojs.nodesource.com/computer/  all of the connected hosts are being used in some way and their names should indicate what they do and who is sponsoring them. I can tweak if you come up with an initial list. Unfortunately I don't have much time for this at the moment as I'm preparing for a flight tomorrow and full week (at least) ahead of me. 
jbergstroem		Ok, I'll use that as a baseline. 
jbergstroem		I think what we want to do here is an ansible script that collects info (through gather_facts) and updates the README.md programatically. 
gibfahn		I think this can/should be closed. We have a basic list, and if we want more info we should open a more targeted issue.
Trott		I used üòï for "I might be able to make it."  (Nit: Change `Sep` to `Oct` in the issue text.)
joaocgreis		I see this will be on a Thursday instead of Tuesday as usual. I can attend this one, but if the intention is to keep on Thursdays I won't be able to attend after DST ends.
gibfahn		>but if the intention is to keep on Thursdays  I think this is just a one-off change due to Node Interactive.
piccoloaiutante		This is still a bit late for me (midnight), so I'll skip it.
MylesBorins		Can you please add https://github.com/nodejs/build/issues/873 to the agenda
mhdawson		It was a one-off change.
mhdawson		I created the hangout but I'm now only 50% likely to be able to make it.  Hoping we have somebody else who can start/run the hangout as even if I make it I won't be in a good place to be making sure the hangout is up and running.
MylesBorins		I unfortunately can't make it  On Wed, Oct 11, 2017 at 1:37 PM, Michael Dawson <notifications@github.com> wrote:  > I created the hangout but I'm now only 50% likely to be able to make it. > Hoping we have somebody else who can start/run the hangout as even if I > make it I won't be in a good place to be making sure the hangout is up and > running. > > ‚Äî > You are receiving this because you are on a team that was mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/902#issuecomment-335889372>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecVw2RrNvxquGozfoPHm092_GWznYfks5srPzlgaJpZM4Ptbuz> > . > 
rvagg		Doh! Sorry, just saw this conflicting with another meeting I have in this slot. Won't be able to make it. 
mhdawson		link for attendees: https://hangouts.google.com/hangouts/_/ytl/hWIYjh1xCx6xMT_pHYhdzJxLl6gVzZtRasf_GYfAfcU=?eid=100598160817214911030
joaocgreis		Minutes document: https://docs.google.com/document/d/1Av6B74HokoewMPaziEncRFVLkqnbUCKoa56yLmCXrVU/edit?usp=sharing
refack		RE using "projects" as task lists: I've created two projects "action items" - https://github.com/nodejs/build/projects/1 and "access" https://github.com/nodejs/build/projects/2 AFAICT all "access request" issues are triaged on the access board. The "action items" board is more complicated. Both are na√Øve and arbitrary, so your feedback is appreciated.   H/T @MylesBorins 
rvagg		The website WG wanted it enabled on nodejs.org and never managed to get agreement to enable third-party apps on the org. There's a bot that does it now, @phillipj could probably help getting it running here too: https://github.com/nodejs/nodejs.org/issues/355#issuecomment-189020365 
MylesBorins		awesome... that's definitely enough to run with 
jbergstroem		LGTM 
mhdawson		Landed as https://github.com/nodejs/build/commit/3bca161439037b895da0f25fbdd8e99059313378 
rvagg		lgtm, thanks @Starefossen  
jbergstroem		Few minor nits. "Present" seems to be somewhat inconsistent -- my suggestion would be to use Full Name (@username) everywhere. 
Starefossen		@jbergstroem I have added all the docs I have access to from my Google Docs account. I have also re-formatted any existing documents to mach the the current format. 
rmg		It shows my horrible attendance record, but otherwise LGTM :blush:  
jbergstroem		Thanks for doing this. Minor nit but LGTM. I haven't cross-referenced this with the documents at google docs though. 
orangemocha		Rubber stamp LGTM. Thanks for doing this! 
mhdawson		Anything holding up landing this ? If not I can go ahead and land it.  I'm thinking that getting the minutes in even if there are a few inconsistencies is better than not having them at all. 
Starefossen		I don't think there is anything in particular holding up this PR other than consistent formatting between meetings. 
jbergstroem		..there's likely a few more to be fixed as well. 
jbergstroem		..if anyone is up for it, obviously. Still +1 to land this. 
mhdawson		If looks like @Starefossen had fixed up the consistency of the attendees.  I'd vote we land and then try to be more consistent going forward 
mhdawson		Landed as 6ae85b8ec977dfd198a4f376d7353b7db970046a 
Starefossen		@mhdawson PR is up for the remaining meeting minutes at #388 
mhdawson		Just confirming what type we need as I don't see flow in the options for a new job.  Maybe its not under NewJob ?
mhdawson		Guessing is the "Pipleline" option but will confirm before creating.
mhdawson		Job created here: https://ci.nodejs.org/job/citgm-flow/
gibfahn		Name changed to https://ci.nodejs.org/view/Node.js-citgm/job/citgm-continuous-integration-flow/
Trott		+1
mhdawson		As per https://github.com/nodejs/build/blob/master/doc/process/special_access_to_build_resources.md since @mscdex is a collaborator + 1 and we probably only need this issue to document/remind us to remove access once he's done.
jbergstroem		@mhdawson exactly; I prefer having an open issue so we can close once access is removed.
gibfahn		+1
jbergstroem		(I provided access at my previous comment)
mscdex		You can remove access now. I've fixed the bug! Thanks!
jbergstroem		Done, closing.
MylesBorins		dupe
joaocgreis		This happened because the Jenkins update fixed https://github.com/nodejs/build/issues/265 (note this has been around for so long because subsequent plugin versions also had issues, let's hope this one really fixes it).  So, we now have to disable all the checkboxes that say "Build only if SCM changes", under advanced, in multi-job jobs. This is still an issue on many jobs in Jenkins. @nodejs/build I plan to go over and fix it as soon as I can, meanwhile please do it for any jobs you notice failing.
joaocgreis		All switches flipped. Should be done.
jbergstroem		I'm an horrible admin :cold_sweat: I'll agree on sharing the responsibility but with our current type of WG I don't think we have to meet too frequently. 
rvagg		yeah, ok, perhaps we should just find a slot this time around and agree to meet every two weeks or every month at the same time and just lock it in. 
jbergstroem		..with the possible addition of if there's not enough things on the agenda we can jump past it? We should probably have had a meeting between this and last, but not much more. 
rvagg		yep, that sounds like a plan, but at least having something in my calendar is going to help me 
rvagg		OK, let's lock in tomorrow 8am AEST, 3pm PST  http://www.timeanddate.com/worldclock/fixedtime.html?msg=io.js+Build+WG+Meeting+2015-04-15&iso=20150415T22  I'll update this issue with a hangout link and some agenda ideas but @jbergstroem and @rmg and anyone else please add bits.  @bnoordhuis @geek @kenperkins if you can make it that'd be great, otherwise we'll just try and organise another one soonish. 
rvagg		- Google+ Event: https://plus.google.com/b/101986715696875566237/events/c00nhulargraen0jl5l4tr49qfo - Google Hangout: https://plus.google.com/hangouts/_/hoaevent/AP36tYfqwFTDFrz-xoI9lmFn6mssBGyAcsnrYYQUKg5PQNjNpEOcnA - YouTube: http://www.youtube.com/watch?v=801v7zcUOBE 
rvagg		Sorry, that timeanddate link should be: http://www.timeanddate.com/worldclock/fixedtime.html?msg=io.js+Build+WG+Meeting+2015-04-15&iso=20150415T22 (it was one day out) 
bnoordhuis		I'll let this one pass, it starts at midnight for me. 
rvagg		ouch, sorry @bnoordhuis, we'll do better next time 
kenperkins		I pickup my kids at 3:00pm pacific, so if there's a call in number I can do it, or I can join ~15m late. 
rvagg		@kenperkins install Hangouts on your phone perhaps? I use it on mine all the time. 
kenperkins		Not going make it today... I'm so sorry... Kid meltdown  Sent from my iPhone  On Apr 15, 2015, at 2:49 PM, Rod Vagg <notifications@github.com<mailto:notifications@github.com>> wrote:  @kenperkinshttps://github.com/kenperkins install Hangouts on your phone perhaps? I use it on mine all the time.  ##   Reply to this email directly or view it on GitHubhttps://github.com/iojs/build/issues/74#issuecomment-93578845. 
rmg		Ooops. I'll need to adjust my mail filter.. I didn't see this until now :-( 
rvagg		@jbergstroem and I had a productive chat, video here: https://www.youtube.com/watch?v=801v7zcUOBE mostly informative, no groundbreaking decisions but some action items. I'll PR in meeting notes soon.  Optimal time for these meetings is going to be tricky, two aussies, a few west-coast north America, a central US and an NL makes for awkward overlaps. It's either early mornings for me and @jbergstroem, late nights for @bnoordhuis or late nights for @rmg and @kenperkins and and very late nights for @wblankenship. We might have to cycle times on this one. 
rosskukulinski		@rvagg a comment while listening: - I know that you can image/snapshot running Windows VMs with Rackspace and spin up clones of that image.  You can even share that image with other accounts.  [More information](http://www.rackspace.com/knowledge_center/article/sharing-images-in-the-cloud-control-panel) 
jbergstroem		Going on a limb and stating that this meeting has been had. 
rvagg		fine by me, I think @joaocgreis is the one impacted here most since some of his setup is the reason we're holding off 
jbergstroem		I recall part of reason for avoiding future updates was issues related to updating the multijob plugin. The sad part here is that there seems to be 10+ issues filed upstream and main author hasn't done anything about it in months. I think we have other issues with the multijob plugin at the moment since a lot of the older jobs seems to have issues (see 'old data' through jenkins settings). Basically Choose Your Null Pointer Exception. 
jbergstroem		Here are the plugins I want to update: <img width="879" alt="screen shot 2016-02-01 at 12 07 00 pm" src="https://cloud.githubusercontent.com/assets/176984/12706576/5888d832-c8dc-11e5-93ba-81e1cac2ee21.png"> 
rvagg		I don't recall any concerns with those ones if you want to upgrade selectively 
jbergstroem		Yes, that'd be the plan. I just did a full backup and will try updating/restarting shortly 
jbergstroem		..and updated. Doesn't seem to be any issues at least: https://ci.nodejs.org/job/node-test-commit/2017/ 
jbergstroem		Still no issues. Closing. Please reopen if you find anything related to git. 
jbergstroem		They were indeed not consistent. I've fixed it in the jenkins job. Closing; reopen if you still see it!
rvagg		FYI if someone wants to tackle this, ansible is the place to contribute.  As of yesterday Fedora 24 was retired, so we're now running 3 out of date versions - 22, 23 and 24, and we really should get rid of those. Does someone with Jenkins access want to disentangle them from the various jobs?
jbergstroem		My contribution hours will change in roughly a month so expect me to vacuum the bug tracker unless anyone beats me to the punch (anon: i know you can do it!).
Trott		We currently have Fedora 27 in CI now and https://github.com/nodejs/build/issues/962 is an open issue to retire 22, 23, and 24.  I'm going to close this, but please re-open if that's not the right thing to do.
refack		> 27 has slipped a bit but is due on the 7th if I'm reading this right:  That's what I'm reading, but https://bugzilla.redhat.com/showdependencytree.cgi?id=F27FinalBlocker&hide_resolved=1 shows quite a few blockers still open. So ü§û 
rvagg		So I've added 26, one on DO and one on Rackspace. Landed Ansible updates @ 326ca9d (pretty trivial, added to inventory and extended 3 fedora24 & fedora25 special cases in ansible/roles/bootstrap/tasks/partials/fedora.yml).  I've also added a couple of new labels in Jenkins: `fedora-latest-x64` on `fedora26-x64` and `fedora-last-latest-x64` on `fedora25-x64`. I've then gone through all of the job config files (on the server, not via the web interface) and done some tweaking: where fedora25 exists, I've added fedora26 as well (mostly the node-test-commit-linux jobs and its copies/variants). Where only fedora22 and fedora23 exists (a lot of jobs) I've switched them to fedora-last-latest-x64 and fedora-latest-x64. This is the case for most of the citgm jobs, they've been running on very out of date Fedora and I've gone ahead and made the assumption that we'd rather them run on actively supported Fedora and not have to manually upgrade the labels in all of those jobs.  Fedora 27 isn't available yet but when I remember I'll add it.  /cc @nodejs/jenkins-admins @MylesBorins 
refack		And `node-gyp` (which I just noticed today don't receive statuses). We'll need to do some changes in the bot for `libuv` to receive these as well.
gibfahn		> We'll need to do some changes in the bot for libuv to receive these as well.  If we're going to do libuv, we'd probably need to add an `org` parameter to this PR (or change `REPO` to accept `nodejs/node`. But I assume that'd require changes to `github-bot` as well.
refack		> But I assume that'd require changes to github-bot as well.  1. Need to verify the bot has permissions to the `libuv` org. 2. We could do a simpler hack in the bot so that `libuv` == `libuv/libuv`
maclover7		Going to land this for now-- as this job continues to be used by others, we can keep making changes as necessary :)
maclover7		Landed in 6b050dc70bd4aa4e7914f694927ef01b7b1d48a9
jbergstroem		Suggesting you bump the ram from 128m to 192m and see if it changes anything. 
mhdawson		I did not look at the reason, as its already 8PM my time but I had to restart the service on test-osuosl-ubuntu14-ppc64_be-1, test-osuosl-ubuntu14-ppc64_le-1 and   test-osuosl-ubuntu14-ppc64_be-2   We'll have to dig deeper if this is required again as we've not had any issues like this for a long time.  
phillipj		Seeing this again today on test-osuosl-ubuntu14-ppc64_be-1.  Refs https://ci.nodejs.org/job/node-test-commit-plinux/6073/nodes=ppcbe-ubuntu1404/console
phillipj		FYI just bumped test-osuosl-ubuntu14-ppc64_be-1 to 256mb and restarted Jenkins service.
mhdawson		Thanks for the heads, up, lets watch for a few days.  If this seems to have resolved it I'll go ahead an d update the ansible configs.
mhdawson		Ok have not seen issues since change, created https://github.com/nodejs/build/pull/605 to modify ansible configs.
jbergstroem		@saghul specific version or just $latest? Do they LTS? 
jbergstroem		We'd have to put it inside of a docker instance or run virtualisation inside of a vm seeing how we're pretty short with barebones hardware. 
jbergstroem		Actually, come to think of it, I think Joyent might offer Alpine. I'll have a look. 
saghul		@jbergstroem no specific version. I understand there is already a bot for Node? That one should do. 
jbergstroem		@saghul ok; I'll look at hooking the docker version up today 
saghul		@jbergstroem excellent, thanks! 
jbergstroem		We have a bot now running native alpine34 on joyent: - node: https://ci.nodejs.org/job/node-test-commit-jbergstroem-alpine34/3/nodes=alpine34-x64/console - libuv: https://ci.nodejs.org/view/libuv/job/libuv-test-commit-linux/nodes=alpine34-x64/  @rvagg reckon we should do this route instead of the container thing? (ref #437) If so, I'll create a PR with playbook. 
phillipj		Newbie question; what's a bot in this context? 
jbergstroem		@phillipj apologies. A vm/jenkins slave. 
saghul		Closing, @jbergstroem already fixed it! üéâ  
valscion		This seems to have started working again. Wonder what was the cause ü§î 
jbergstroem		~~Thanks. Do you have a rough timestamp so I can correlate with server logs?~~ doh, its in the screenshot.
jbergstroem		looks like underlying disk issues: ``` [8438055.952070] INFO: task jbd2/sda-8:1111 blocked for more than 120 seconds. [8438055.952884]       Not tainted 4.4.0-45-generic #66-Ubuntu [8438055.953346] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message. [8438055.953774] jbd2/sda-8      D ffff88040923fad8     0  1111      2 0x00000000 [8438055.953781]  ffff88040923fad8 ffff88040923fb00 ffff88040d6aa940 ffff8800db875280 [8438055.953786]  ffff880409240000 ffff88041fcd6d00 7fffffffffffffff ffffffff8182dfc0 [8438055.953789]  ffff88040923fc30 ffff88040923faf0 ffffffff8182d7c5 0000000000000000 [8438055.953793] Call Trace: [8438055.953804]  [<ffffffff8182dfc0>] ? bit_wait+0x60/0x60 [8438055.953808]  [<ffffffff8182d7c5>] schedule+0x35/0x80 [8438055.953812]  [<ffffffff818308e5>] schedule_timeout+0x1b5/0x270 [8438055.953817]  [<ffffffff810caa91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20 [8438055.953822]  [<ffffffff8106425e>] ? kvm_clock_get_cycles+0x1e/0x20 [8438055.953828]  [<ffffffff810f5aac>] ? ktime_get+0x3c/0xb0 [8438055.953832]  [<ffffffff8182dfc0>] ? bit_wait+0x60/0x60 [8438055.953835]  [<ffffffff8182ccf4>] io_schedule_timeout+0xa4/0x110 [8438055.953839]  [<ffffffff8182dfdb>] bit_wait_io+0x1b/0x70 [8438055.953843]  [<ffffffff8182db6d>] __wait_on_bit+0x5d/0x90 [8438055.953847]  [<ffffffff8182dfc0>] ? bit_wait+0x60/0x60 [8438055.953851]  [<ffffffff8182dc22>] out_of_line_wait_on_bit+0x82/0xb0 [8438055.953856]  [<ffffffff810c3e10>] ? autoremove_wake_function+0x40/0x40 [8438055.953861]  [<ffffffff81244c42>] __wait_on_buffer+0x32/0x40 [8438055.953866]  [<ffffffff812ecdaf>] jbd2_journal_commit_transaction+0x10cf/0x1870 [8438055.953870]  [<ffffffff810caa91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20 [8438055.953876]  [<ffffffff810ec99e>] ? try_to_del_timer_sync+0x5e/0x90 [8438055.953879]  [<ffffffff812f116a>] kjournald2+0xca/0x250 [8438055.953883]  [<ffffffff810c3dd0>] ? wake_atomic_t_function+0x60/0x60 [8438055.953887]  [<ffffffff812f10a0>] ? commit_timeout+0x10/0x10 [8438055.953891]  [<ffffffff810a0928>] kthread+0xd8/0xf0 [8438055.953895]  [<ffffffff810a0850>] ? kthread_create_on_node+0x1e0/0x1e0 [8438055.953899]  [<ffffffff81831c4f>] ret_from_fork+0x3f/0x70 [8438055.953903]  [<ffffffff810a0850>] ? kthread_create_on_node+0x1e0/0x1e0 ```  will chat to support.
jbergstroem		Got a reply (sorry for reporting back late): Digitalocean support confirmed that there were issues with latency to block storage in NYC1 during this time that have since been fixed:  > Upon some additional review, we've potentially identified an issue that affected volume latency this morning in NYC1. We're working to investigate further, and will keep you updated.   > I've confirmed that the most recent disruption was likely due to an issue with reaching block storage volumes, which has been resolved, and we are continuing to work to ensure there's no future disruption. If you have any other questions or concerns, please let us know. 
valscion		Thank you for following up with the source of the error, I appreciate it a ton üíï 
williamkapke		Same problem?  https://github.com/nodejs/nodejs.org/issues/1191 
jasnell		Yes, same problem but it definitely is not resolved. Still having issues.
mobiletradingpartners		https://status.digitalocean.com:  NYC1 - Storage Volumes Our engineering team is actively investigating Storage Volume availability in NYC1. We apologize for any inconvenience that this causes for you.  maybe it's time to have a failover on another provider? 
geek		Thanks for raising this issue.  We absolutely should rename them. I am in favor of naming them SmartOS. 
misterdjules		What impact would this have on `nvm` and other version managers? For instance, when`nvm` runs on SmartOS, it downloads binaries whose names match the pattern `*sunos*`. If binaries are renamed, some versions of `nvm` running on SmartOS won't be able to download binaries.  It would be interesting to get @ljharb opinion on this. 
ljharb		Renaming the existing ones would absolutely break many existing nvm users.  What would be ideal is _duplicating_ them to be named "smartos" on all existing instances, and then only using "smartos" moving forward (after i've updated nvm and bumped it in travis-ci, ofc). 
jbergstroem		I'm just tired of our false advertising -- it's not like the binary will work on sunos anyway. Lets focus on the tooling that assumes it's called sunos on smartos and solve those scenarios. Which tools/repos/packagers do we need to talk to?  - [x] nvm :) 
misterdjules		@jbergstroem @ljharb One problematic use case I had in mind is a user who has already installed `nvm`, and who would want to install new SmartOS releases that would be available only under the `*smartos*` name.  What would the failure look like? Is there a way with the current and older versions of nvm to display a human readable error message that would suggest these users to upgrade to a newer version of nvm that supports these new `*smartos*` names?  I'm not sure if that's worth the effort depending on the number of users and how closely they follow nvm's and node's changes, but I think it's worth it to think about it, if only to avoid users confusion and a number of issues in nvm's issues tracker. 
jbergstroem		@misterdjules it sounds like an nvm problem though (which doesn't mean I don't care, just that its out of scope for this group). I think we should identify tools/repos/packagers and mention we're renaming; set up a timeframe when we can allow a "both will work" and then kill sunos. 
ljharb		@jbergstroem that's pretty harsh - it's a problem for anyone who has bookmarked that URL. Cool URLs don't change, and invalidating _any_ URL on the internet is a breaking change for somebody. Please don't minimize the damage that this could do. 
ljharb		@misterdjules there is no way to alter what current nvm users see - only what new ones see. 
jbergstroem		@ljharb but this would only be for future releases, right?  
jbergstroem		`nodejs-v7.0.0-sunos.tar.xz` would in time become `nodejs-v7.0.0-smartos.tar.xz` throughout the ecosystem. 
ljharb		In time it would, and if v7 was the first one to not have `sunos`, such that some nvm users wouldn't be able to download it (but could continue to download all the same sunos versions they already were), such that they needed to upgrade `nvm` - then that'd be fine!  The only breakage I'm concerned about is that existing `sunos` files must remain working forever. 
jbergstroem		@ljharb: existing files will work forever, this is just about new releases. 
ljharb		In that case, just give me a week's notice and I'll update `nvm`, and bump it on travis-ci, and I'm fully in favor.  @jbergstroem it would simplify my code _a lot_ if you backfilled all existing `sunos` files with `smartos` ones - is that a possibility? 
jbergstroem		@ljharb: I'll get back to you on that, but seeing how this affects a lot of legacy stuff I think no one is inclined on touching it (0.x releases, iojs, etc). I wouldn't place my bets on it. This'll likely be one of those xz vs gz things. 
jbergstroem		@geek, @misterdjules, @ljharb what are your thoughts on doing this for 7.x and forward? 
ljharb		I'm on board, but would like as much concrete notice as possible (and an example index.tab) to try to ship the change beforehand :-) 
jbergstroem		@ljharb cool. We just need to start somewhere and a major makes more sense. 
misterdjules		@jbergstroem Can we summarize what the current plan is in the original comment of this issue so that we can make sure we're on the same page?  Also, my apologies for not being responsive today, I'll be offline most of the time. 
jbergstroem		@misterdjules sure. Give me a few minutes.  Edit: done. 
ljharb		üëç OP LGTM 
jbergstroem		/cc @nodejs/build @nodejs/release  
misterdjules		@jbergstroem Thank you for outlining the current plan in the original comment! The plan looks good to me. 
jbergstroem		aside: 1. we should rename @nodejs/platform-solaris and the documentation around the same time we do this 2. I've chosen to leave above team. Although I do a lot of tinkering/build on them, I just don't have the day-to-day experience to help out with any proper debugging. With @geek being more active (and @misterdjules great effort) my contribution is pretty irrelevant anyway :) 
misterdjules		@jbergstroem Where is the platform-solaris team mentioned in the documentation? I can't find any occurrence.  
jbergstroem		@misterdjules I'm probably hallucinating; had this faint recollection of a document that listed teams and when to cc them. 
Trott		You are probably recalling https://github.com/nodejs/node/blob/master/doc/onboarding-extras.md but that team is not listed there. (PR to add it if you want.) 
jbergstroem		@Trott on spot as always -- thanks. Since we don't mention other platforms I think we'll be fine for now. 
jbergstroem		So, I'd like to proceed with this but I haven't heard much from the build group and/or release group. The next step would be writing logic in the smartos section of the release job, get some tests out. 
evanlucas		Let's just not forget to update https://github.com/nodejs/build/blob/master/setup/www/tools/dist-indexer/transform-filename.js when this happens. +1 from me 
jbergstroem		@evanlucas just pushed in https://github.com/nodejs/nodejs-dist-indexer  (note: this doesn't mean it's decided, just that we're preparing for it) 
mhdawson		LGTM 
jbergstroem		@rvagg, @joaocgreis: since its only us three that mainly will be affected by this I'd appreciate if you weighed in any opinions. 
joaocgreis		LGTM, thanks for the direct mention. I've only created linux machines in Azure (the CC server), there the key is added when creating the machine so I'm ok with just removing this. 
rvagg		fine by me 
gibfahn		+1 to giving @gabrielschulhof access to the AIX machines.  I'll put his key on a machine and send him instructions once someone else +1s.
refack		+1
gibfahn		@gabrielschulhof please send me your public key (`gibfahn` on Twitter, or `gibfahn@gmail.com, whatever's easier). I'll send you instructions.
gabrielschulhof		@gibfahn I have sent you my public key.
gibfahn		@gabrielschulhof I've given you access to `root@test-osuosl-aix61-ppc64_be-1`, you can find the IP here:  https://github.com/nodejs/build/blob/d7fc4a7ce3597d8f3db134872363d0a0dade6cdb/ansible/inventory.yml#L127  A couple of things to remember (some specific to this machine): - Work as `iojs` not `root` (`su iojs` as soon as you log in, everyone forgets this) - The default shell is `ksh` rather than `bash` (so do `bash` after the `su iojs`) - I created you a homedir at `/home/gabrielschulhof`, keep stuff in there where possible, and delete when you're done. - Feel free to copy workspaces from Jenkins (jenkins workspace is at `/home/iojs/build/workspace`) but don't work in those directories or you'll mess up jobs. - This machine is quite resource constrained, sorry about that! - If you don't have a keepalive setting in your `~/.ssh/config` you'll get kicked off quite quickly, I recommend putting this at the top:  ``` Host *    ServerAliveInterval 120 ```
gibfahn		Let us know when you're done with the machine, or if you need any help.
gabrielschulhof		Thank you! I'll let you know.  On Sun, Sep 24, 2017 at 11:41 PM, Gibson Fahnestock < notifications@github.com> wrote:  > Let us know when you're done with the machine, or if you need any help. > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/891#issuecomment-331738818>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AA7k0dRoQNhtpWlXNYQYRlCXUNDn2iSLks5slr6KgaJpZM4PgR57> > . > 
mhdawson		belated +1
mhdawson		@gibfahn just for reference, I've most often create a new user for people when I've given access like this.  I create the user and the ssh key.  Then when they are done delete the user and remove their directory.  Its and easy way to know what to clean up.
refack		@gabrielschulhof was the issue resolved?
gabrielschulhof		Yes, thanks, sorry, closing :)
refack		> Yes, thanks, sorry, closing   Removed key.
gabrielschulhof		@refack @gibfahn I actually didn't know that closing this issue would result in the removal of my key from the machine. Could you please put it back? Now that https://github.com/nodejs/node-addon-api/pull/103 is starting to work for Linux and OSX, I would like to also make it work on AIX.
gibfahn		I'll add you back in.
gabrielschulhof		@gibfahn Thanks!
gibfahn		@gabrielschulhof okay, I've created you a user on the machine as Michael suggested, you can get to it with: `ssh gabriel@test-osuosl-aix61-ppc64_be-1` or `ssh gabriel@140.211.9.101`.  For future reference, I did:  ```bash useradd -m -s /bin/bash -g staff gabriel ```
gdams		@gabrielschulhof do you still need access or can we now remove your user?
gabrielschulhof		No, I do not. Please remove my access! Thank you for your help!  On Mon, Aug 27, 2018 at 5:20 PM, George Adams <notifications@github.com> wrote:  > @gabrielschulhof <https://github.com/gabrielschulhof> do you still need > access or can we now remove your user? > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/891#issuecomment-416372527>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AA7k0UKmH61VFgXVoXOjRX5T5j4NuVxPks5uVGKPgaJpZM4PgR57> > . > 
gdams		removed user from test-osuosl-aix61-ppc64_be-1  For future reference, I did: ```bash userdel gabriel && rm -rf /home/gabriel ``` 
mhdawson		Sounds like a good idea to me.
rvagg		I'd like to see https://github.com/nodejs/build/projects/1 used as much as possible for triaging. Perhaps we could also start using build@nodejs.org for 911-style things too?
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
rvagg		done 
rvagg		Have a look in admin_logins.md in secrets/build/infra, most of them are in there. I think softlayer might be another one you need an individual account for but I'm not sure how to do that (I haven't looked yet, will try and remember to do so when I'm at the computer next)
rvagg		Servers:  * MacStadium (shared) * Digitalocean (individual) * Rackspace (shared) * Joyent (shared) * linuxOne (shared) * Scaleway (shared) * packet.net (individual) * SoftLayer (individual) _(@gibfahn can you talk to @mhdawson about getting a login here? I can't see an option to do so which I think means I don't have the access)._  Certificates:  * Apple (shared) * DigiCert for Authenticode (I've just added this to admin_logins.md) (shared) * GoGetSSL for SSL (I've just added this to admin_logins.md) (shared)  Other infra:  * Cloudflare (shared) * Mailgun email (accessible with Rackspace login) 
gibfahn		@rvagg @mhdawson I've added the accounts you mentioned, with links culled from Google.  PTAL.
rvagg		lgtm other than that single nit
gibfahn		@rvagg created issues for those two things, do either of them require changes to this PR?
rvagg		@gibfahn I think the Rackspace one can be done here, just note individual accounts here and we'll use the other issue to make sure each of us have it set up. 
mhdawson		@nodejs/build minutes from meeting yesterday. Please review and comment. 
Starefossen		Minutes looks fine, just wondering about the outline (if you look at some of the previous minutes I have transcribed). I think the meeting agenda should be in the minutes. 
mhdawson		@Starefossen pulled agenda from doc and added to minutes. 
jbergstroem		LGTM  
mhdawson		@Starefossen  look good to you now ?  
Starefossen		LGTM üëç  
mhdawson		Landed as c1533ba5bc63978b57cf93b6b881dbec5816c661 
rvagg		see also #963, we might want consider ditching 25 in this round too since _I believe_ it'll go EOL in about 5 weeks from now (EOL = "GA of next-but-one release plus one month" https://fedoraproject.org/wiki/Fedora_Release_Life_Cycle, F27 is due on the 7th.)
refack		There was a discussion about this subject a few months ago https://github.com/nodejs/node/pull/12672 AFAIR it was decided that we'll do "best-effort" CI to test that we don't pull the rug under anyone still using a node LTS version even on an EOL platform... So probably best to exclude from v8 and v9 matrix, keet for 4 and 6. Also possible reduce stock to 1 of each?
gibfahn		Same as https://github.com/nodejs/build/issues/961#issuecomment-341252191,   >AFAIR it was decided that we'll do "best-effort" CI to test that we don't pull the rug under anyone still using a node LTS version even on an EOL platform...  I think the conclusion was that there's no reason to force ourselves to drop everything as soon as it goes EoL. My vote is to leave them until they're the slightest bit of effort to maintain, at which point we remove them.  Of course if removing them frees up resources we need for something else that's a good argument for removing.
rvagg		See #964, on DigitalOcean we can still `yum update` but it's failing elsewhere. We can no longer run our Ansible scripts on them because of this. Since Fedora moves so fast I'd be keen to boot the older ones as soon as they present any difficulty, like they are now.
gibfahn		>like they are now.  If we're going for an "as long as they're no trouble" policy, and they're already causing trouble, then now is the time to remove them IMO.
rvagg		We're running out of space on digitalocean and rackspace is getting pretty full too, I'd like to drop off fedora 22 when we can run 27 (released yesterday); right now 27 isn't available on our providers but I'd expect it to be soon. Any objections?
rvagg		I've agreed to formulate a position to bring back to the group for discussion at the next meeting. For now I'm going to remove 22 when I put 27 in (sometime soon) but the proposal will be something like: no more than 8 machines allocated to Fedora at a time. We have 10 now so that would mean dropping 23 as well. In our discussion we also noted that Fedora should probably be treated differently since it's primarly a desktop distro and doesn't have an LTS strategy so our strategy should move with it. Ubuntu, Debian and CentOS (and _maybe_ Alpine) are more important for us since they are primary deployment targets.
mhdawson		Waiting on @rvagg for proposal, removing from agenda until that's ready.  No general objection to removing unsupported versions based on discussion.
rvagg		TODO here: details about what we're supporting and planning on supporting and some kind of calendar for distro updates that we expect.
Trott		Is there a projected timeline on this? There've been some Fedora 23 and 24 build issues but I won't bother surfacing them if we think they're going away from CI in the next few weeks.
maclover7		It looks like (unbeknownst to the build team as far as I can tell) the nodejs.org has been updated to remove several version of Fedora: https://github.com/nodejs/nodejs.org/pull/1560.  > Is there a projected timeline on this? There've been some Fedora 23 and 24 build issues but I won't bother surfacing them if we think they're going away from CI in the next few weeks.  My recommendation would be to mark then as offline now, and then physically the machines (from Ansible inventory and from the DigitalOcean/Rackspace/etc admin portals) later. Like @Trott said, to avoid spending time on things that are going away shortly.
maclover7		Will be fixed by #1163
maclover7		These machines have all been removed from the CI cluster
mhdawson		@gibfahn as requested.  If there are others I'll add to this list.  The installation of the python version that supports PIP is the most urgent.  Is that something you can look at this week ?   We'll want to look at the dependencies carefully as I ran into problems in previous updates and stage to one machine at a time. 
gibfahn		I'm pretty sure that there's an issue with curl on the AIX boxes, not sure if it's a LIBPATH issue or if something's wrong with the install. We don't have it on IBM machines, so it's definitely fixable, just annoying (it means things have to be scp'd rather than just curl'd.  ```bash $ curl exec(): 0509-036 Cannot load program curl because of the following errors:         0509-022 Cannot load module /usr/lib/libcurl.a(libcurl.so.4).         0509-150   Dependent module /opt/freeware/lib/libcrypto.a(libcrypto.so) could not be loaded.         0509-152   Member libcrypto.so is not found in archive          0509-022 Cannot load module curl.         0509-150   Dependent module /usr/lib/libcurl.a(libcurl.so.4) could not be loaded.         0509-022 Cannot load module . ```  cc/ @sxa555 who is looking at the AIX Ansible stuff.
sxa555		It should work if LIBPATH is pointing at /usr/lib (although that could well cause other issues of course!) - the libcrypto.a in /opt/freeware/lib doesn't have a non-version-suffixed version of libcrypto.so in it but the one in /usr/lib does. I'm not sure why the one in /opt/freeware/lib doesn't ... You'd expect it to work with the version of curl in there. You could just override it for the execution of curl for now.
gibfahn		@sxa555 Yep, that works. I'll add the LIBPATH setting to the manual install instructions for now (it's not a problem in CI AFAIK).
mhdawson		Ideally we could automate the removal of temporary access.  Maybe something that would delete additional users 1-2 weeks after they are created. Alternatively something that would centrally the temporary additions so it would be easy to review/delete those that are no longer required 
rvagg		yep, shared keys are fine by me, the secrets repo seems to be working well for sharing 
rmg		There should be a mechanism for refreshing the keys pulled from github in the event that a user has removed one due to it being compromised. Ideally they would notify everyone, but considering these keys are publicly available it's basically impossible for them to notify everyone we should adopt a good netizen approach ourselves if possible. 
jbergstroem		@rvagg If we're running with shared  we should create different subfolders in the secrets repo to delegate trust, no? - build-test - build-release - build-infra? (current)  Also, I'm guessing an even smaller subset of people have access to infra? 
rvagg		Currently we have secrets/build and secrets/build/release, both are managed with different keys, the subfolders seems to work OK with dotgpg, is there anything here that doesn't fit within those two layers? Non-release stuff just goes in secrets/build and is shared with a wider group. 
jbergstroem		Just wanted to make sure we didn't have issues with recursion. I'm happy with whatever  
rvagg		I'm pretty sure we don't, although it does feel a little awkward whenever I dive in there so I wouldn't be opposed to a reorg. So far we haven't had a use beyond _build_ for the secrets repo so I'm fine with spreading out in there more than we currently are. 
jbergstroem		Should we move the reorganising to the other repo?  I see four set of keys now: - release access keys (only accessible by people that handle release-related business) - release keys (stored on machines so they can be trusted when sending artifacts to machines that hosts them) - test access keys (accessible by people in the build group and likely also the upcoming test wg) - infra access keys (accessible by people doing devops for the build wg, such as node-www and ci.nodejs.org) 
jbergstroem		One benefit of having infra access keys is that people in that trust chain can be part of automating setting up new slaves (adding to ci firewall, etc). 
jbergstroem		This is now work in progress. If you're part of the build group you know what repo to access in order to fetch the keys.  @mhdawson brought up that having a benchmark group for handling access to the benchmark machines could be a thing. I'm +1. Anyone else? 
jbergstroem		Will create a benchmark group once we look into redeploying the machines. 
gdams		If you are looking for a nice solution for SSH key management, we are using http://sshkeybox.com/ in AdoptOpenJDK. It works really nicely as we can add all of our hosts, add users keys to machine groups and really easily add/revoke access. If you are interested I'd be happy to demo it to you!
maclover7		I believe this can be closed since we have https://github.com/nodejs-private/secrets/tree/master/build now. Please reopen if I'm wrong.
rvagg		Question @ the Board for @nodejs/build: what is the level of concern re bus-factor for people and redundancy for build resources required for releases. The question is specifically around the ARM cluster and the planned outage we had there (I clarified that releases were not impacted), but from our perspective I think macOS redundancy is much more of a problem right now and the people bus-factor might be something else we should be putting our attention on.
jbergstroem		I'd put macOS redundancy and perhaps my ability to a degree as well. I definitely haven't been as responsive and/or active this quarter as last quarter.
rvagg		@mhdawson I don't think the 3pm one is right, that's 6am for me but I haven't ended DST yet, that'll happen next week and this slot will move to 5am, it's never been that early for me, it's always been 7am during DST and 6am during the rest of the year. I suspect that calendar entry was set to a US TZ and got shifted when the US hit DST when it should have been tied to UTC. Either way, can we do this at 4pm Eastern? Otherwise I'm won't make it.
williamkapke		I added this to the calendar with EST timezone. I usually always add them as UTC but this one was - specifically stated at EST. I should have clarified. Happy to update to UTC - just LMK!  ![image](https://cloud.githubusercontent.com/assets/739813/24388573/897cf88e-1330-11e7-933d-901658741f57.png)  I'm confused at what time it's really happening now- the title of this issue says 4PM EST, the OP says 3PM EST.
piccoloaiutante		I should be able to make it.
mhdawson		Ok, based on the input from @rvagg lets agree its 4pm EST.
gibfahn		@williamkapke is that calendar linked from nodejs/node (or somewhere else, I haven't found the link yet)?
richardlau		@gibfahn There's a link at the bottom of the TSC meeting minutes, e.g. https://github.com/nodejs/TSC/blob/master/meetings/2017-03-09.md
williamkapke		@mhdawson I've updated it to use 8PM GMT: ![image](https://cloud.githubusercontent.com/assets/739813/24415575/befca0f2-1396-11e7-8a31-d9860e2c8775.png)  - http://www.wolframalpha.com/input/?i=8pm+UTC,+Mar+28,+2017+in+local+time - https://www.timeanddate.com/worldclock/fixedtime.html?msg=Build%20WG%20Meeting&iso=20170328T20 
jbergstroem		Sorry for my late notice but I won't be able to join. My efforts since last time: - jenkins master security updates, twice - play around with the tap2junit parser -- it will bail on citgm stuff; needs to be refactored - fix disk issues on joyent freebsd workers which seems to have changed how disk is mounted which led to disk issues. - housekeeping on a few workers  ..as for my input: - I'd like to bring up MacOS workers for discussion. We had an interesting opportunity which led into [this](https://github.com/nodejs/build/issues/539) and back to sleep. After we've sorted that we can revisit the opportunity and increase both redundancy and os versions tested. - Bus factor: I like Michaels idea of primary/backup for positions. Which would these positions be though? My ambition is that everybody in the build group would help out with the level of access they have -- meaning all of our members should feel free to look at restarting/debugging individual workers. As for the higher tiers; we need to start by defining these positions. - Adding Kunal: SGTM üëç  - I believe we were to remove the agenda label from a few of the raised topics as a result of previous meeting. Not sure that happened.
mhdawson		Link for participants: https://hangouts.google.com/hangouts/_/ytl/KOIfKbVSfs8CMc3OB3XsoxoKlDtgpIGkbsfAvJbRIe8=?eid=100598160817214911030&hl=en_US&authuser=0
williamkapke		I asked a question in IRC
gibfahn		Minutes here: https://github.com/nodejs/build/issues/665
gibfahn		Minutes landed, closing.
jbergstroem		Anyone with interest in some java digging should perhaps have a quick look in the $source and file an upstream bug? 
maclover7		ping -- is this still needed?
gibfahn		@refack did your jenkins-admins upgrade work okay? If so then I think this can be closed.
refack		Mine, and I assume @maclover7 and @joyeecheung's as well. So closing
rvagg		@Fishrock123 the .xz files are now in staging for v5.1.0 but you'll have to promote them so they are properly signed. Can you take care of that when you are able please? 
Fishrock123		Done. 
MylesBorins		/cc @jbergstroem  
jbergstroem		I'd prefer if @rvagg chipped in here seeing how I haven't done much work in this area. 
rvagg		I can't think of a good reason not to collapse it now. It was put there because at the time I was attempting to keep as much logic out of the Jenkins scripts as possible so we don't build up too much reliance on this beast, but that day has well passed now. Originally IIRC this was freeform text so it could point to anywhere. We've narrowed down the use of it a lot more now. Even the `custom` -> `rc` thing could be scripted away, the `custom` was a compromise I had to make when getting approval from others to land the code in `Makefile` that controls this. 
MylesBorins		@nodejs/build can we move forward on this? 
MylesBorins		This has happened and I am happy
mhdawson		@jbergstroem  can you review ?  
jbergstroem		suggesting we just add it on all machines while at it? you should be able to access the one's you've omitted such as freebsd or smartos through the test key to check package names. 
mhdawson		@jbergstroem  updated to add the rest of the platforms that made sense.  (Excluded raspberry pi as I don't think they will be powerful enough to run the v8 tests any time soon) 
jbergstroem		LGTM 
mhdawson		Landed as 12db79f166656da301138539fa2c5dac73992eb8 
gibfahn		@rvagg I assume this can be closed now, reopen or retarget to `ansible/` if not.
rvagg		eh, no not really, there's still things that use the old ansible scripts, retargetting is going to require fully reimplementing the github-bot scripts and I'm not really up for that, I barely have a grip on how the new ansible stuff works
gibfahn		@rvagg okay, so maybe just land this?
rvagg		yeah, I guess so!
ChALkeR		@mikeal Btw, is SQLite really a no-go? It's a mostly static site as I understand it, and proper server-side caching should reduce the load on the database to an absolute minimum. Or am I missing anything? 
jbergstroem		Ghost seems to support SQLite out of the box! 
maclover7		Is this okay to close?
refack		Since then there was talk about either using hosted Ghost or even WP ( :shudder: ) - https://github.com/nodejs/nodejs.org/issues/1349
jbergstroem		Just going to put this thread here; shows how important it is for us to start testing on more versions: https://github.com/nodejs/node/issues/5731 
ChALkeR		Do we really want to support OS X 10.7 and 10.8? Google does not support those from Chrome 50, that's v8 5.0 which is in Node.js 6.0. Also, 10.7 and 10.8 do not receive security updates from Apple. 
jbergstroem		@ChALkeR we still have 4.x to care about. 
jbergstroem		I'm happy to revisit requirements; but at this stage we need to define them and uphold them. We kind of do neither :) 
orangemocha		As stated in this issue, our ability to test Node.js on OSX is at risk. The Build WG would like to reach out to any companies interested in providing support for OSX resources - before asking the Foundation to allocate budget for it, but we are unsure about what channels to pursue. Escalating to @nodejs/tsc . 
orangemocha		...and @mikeal  
doug-wade		We here at [Redfin](https://github.com/redfin) have identified ~10 machines that we'd be able to allocate for node.js testing.  How do we discuss whether this meets the need and how the logistics might work?  Should we just call in to https://github.com/nodejs/build/issues/417? 
jbergstroem		@doug-wade Amazing news -- lets have a chat! Whats your preferred medium? I'm irc (jbergstroem@freenode) or [email](mailto:bugs at bergstroem dot nu). We can do Skype too. 
doug-wade		@jbergstroem I sent you an email about scheduling some Skype time 
jbergstroem		@doug-wade sweet, will get right back to you. 
Fishrock123		Any progress here? 
rvagg		nothing worth reporting 
jbergstroem		Taking this off the wg-agenda for now. Not that it isn't important, but lets bring it back once we have updates 
Trott		Should this remain open?
rvagg		ibm folks are helping with the new osx infra, we're still not in a great place with our osx infra but at least we're moving in the right direction
orangemocha		Thanks for reporting this. I removed `which git` from the scripts a few days ago. 
joaocgreis		Fixed by the removal of that line from the script. If we ever need to add it again, 0aecff5b897bd3241468190554608e90725317e1 enables it. 
rvagg		Please let me know what your google login is if you can't edit that doc and can only suggest and I'll add you. Also note there is a calendar entry for this that we are sharing, if you don't have it then please speak up. 
rvagg		Some items while I think of them: - nodejs.org domain is in the hands of the LF, I'd prefer it to be delegated to a Build WG managed Route53 account so it can be transparently managed (and scripted like https://github.com/nodejs/email) - nodejs.org MX is routed through Google Apps, managed by the LF, I'd prefer it to eventually move to management by the Build WG as a simple redirect service (like https://github.com/nodejs/email) rather than Google Apps but that might be for discussion with the eventual Executive Director. - Still haven't managed to get certs (SSL and code signing) purchased by the foundation, spending is probably in a difficult phase right now but I'd really like to free this up a bit so we can get on with the job. - Website WG now have http://new-nodejs.iojs.org/ as a placeholder for http://new.nodejs.org/ for which they will work with the community on a new design. DNS updates need to happen, wild-card SSL cert needs to happen. Currently using a mirror of the same [github-webook](https://github.com/rvagg/github-webhook) magic that deploys https://iojs.org but from https://github.com/nodejs/new.nodejs.org. Website WG is now converged, managing both main web properties. Evangelism WG is now (apparently) managing the Node.js blog but they may need access. - Would like to discuss website architecture, DigitalOcean, Joyent, redundancy, how do we move forward from thought-bubble to architecture planning. Also includes the possibility of hosting binaries on a redundant large-file / object store (S3, Cloud Files, Manta). - There is now a Build WG **Secrets** Google Spreadsheet that a limited number of people on this team have access to, we should (eventually?) figure out the rules for who can access this. - Server resources have been documented in a CSV file here: https://github.com/nodejs/build/pull/120 - @rvagg TODO still pending: document release process of io.js, this is changing somewhat with https://github.com/nodejs/io.js/pull/1938 and https://github.com/nodejs/io.js/pull/1975 so I'd rather wait. - io.js nightlies now have the ARMv7 binaries built on a Wheezy machine, making them much more compatible with older Linuxes, this still needs to happen to releases - I'm pushing for putting header tarballs with all of our releases: https://github.com/nodejs/io.js/pull/1975 (not just io.js preferably), coupling this with node-gyp will make a much more pleasurable experience but there's lots of work to be done downstream there. Getting header files up will be the first step. Note that the layout of these tarballs is different to the source tarball layout so node-gyp needs to configure its include directories differently but I've been testing it end-to-end and it's an easy fix to make it work. https://github.com/nodejs/io.js/pull/493 has the work I started on what node-gyp needs in core, I still have to get back to this but it should enable integration with nightly and rc builds as well as io.js and Node.js all at the same time when it's done (all with a tiny header tarball download which will ultimately be optional because most installed versions of will have the headers with them anyway!). - @mhdawson is making good progress on the PPC machines aparently, they are now listed in Jenkins and have their own project but won't be hooked up to any-pr+multi until he is satisfied they are good to go. - @orangemocha has made excellent progress on a converged Jenkins project(s), need update on this work and what's left to be done. - @jbergstroem has been tending to the slaves, lots of init scripting, moving on to Ansible refactoring I believe (yay!). 
misterdjules		I'll be traveling for work and thus won't be able to attend this meeting. 
mikeal		> nodejs.org domain is in the hands of the LF, I'd prefer it to be delegated to a Build WG managed Route53 account so it can be transparently managed (and scripted like https://github.com/nodejs/email) > nodejs.org MX is routed through Google Apps, managed by the LF, I'd prefer it to eventually move to management by the Build WG as a simple redirect service (like https://github.com/nodejs/email) rather than Google Apps but that might be for discussion with the eventual Executive Director.  The LF can point the DNS at whatever you like, we just need to find the best way to do this with no downtime. I've spoken with Mike about this and they are willing to make any changes you request they are just very afraid of taking action that is not at your request and being responsible for nodejs.org going down :)  The best way forward here is probably to come up with a transition plan and execute on it. 
kenperkins		I'm phone screening two candidates at this time today, I can't make it _again_. #fail 
mikeal		> Still haven't managed to get certs (SSL and code signing) purchased by the foundation, spending is probably in a difficult phase right now but I'd really like to free this up a bit so we can get on with the job.  @mkdolan who should we talk to at Let's Encrypt about getting certs? 
Starefossen		@rvagg Is it still the intention to version control the meeting minutes in the [/doc/meetings](https://github.com/nodejs/build/tree/master/doc/meetings) dir? 
rvagg		yep, they should, you're welcome to try and organise them, it's one of those things in the back of my mind that needs doing .. one day 
orangemocha		I wanted to make sure that @nodejs/collaborators are aware of this limitation. 
trevnorris		> Preserve existing PR-URL / Reviewed-By and don't add any such fields  I think this is the only viable solution if it is meant to be used to cherry-pick commits from master onto stable release branches. Those both already have metadata and we'll probably be cherry-picking many different commits onto a single branch/PR. 
trevnorris		Added tsc-agenda to discuss whether we're planning on using Jenkins to land patches on major release branches. 
orangemocha		The simplest approach seems to be to never remove anything that's already in the commit messages, and only add metadata from the form when non-empty values are entered/selected. 
trevnorris		Yeah. That would be perfect. Though I don't want to be the solo voice here, so want to briefly discuss it in the TSC meeting.  @orangemocha How does a new release branch get into Jenkins? e.g. when v5 is cut will that branch need to be manually added for rebase? 
orangemocha		It's a very simple change: deleting a line from a script. I guess I was worried about duplicate information, but there's definitely less harm in duplicate metadata than deleted metadata.  No extra steps are required for handling future branches. PRs against them will work, and you can specify any branch to node-merge-commit. The initial creation of the branch will have to be done manually. I hope I understood your question...   It should be noted that the changes to support flaky tests are not in the v3.x branch at the moment. 
orangemocha		I won't be attending the TSC meeting today. Please let me know what comes out of that discussion. It should be pretty safe to make the change. 
thefourtheye		@orangemocha How about having a checkbox which says "Remove metadata"? 
cjihrig		I don't think we should ever allow Jenkins to remove metadata. 
orangemocha		> How about having a checkbox which says "Remove metadata"?  That's also easy to do. It would be unchecked by default. 
orangemocha		Ok, to move forward with the proposed change? That is, to make Jenkins only add metadata when specified, and never remove it? I tried to parse the contents of the TSC meeting notes but I would prefer a +1 by someone who was actually there :) 
jasnell		+1 On Sep 2, 2015 6:18 PM, "Alexis Campailla" notifications@github.com wrote:  > Ok, to move forward with the proposed change? That is, to make Jenkins > only add metadata when specified, and never remove it? I tried to parse the > contents of the TSC meeting notes but I would prefer a +1 by someone who > was actually there :) >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/179#issuecomment-137294616. 
orangemocha		Made the change and updated the [wiki](https://github.com/nodejs/node/wiki/Merging-pull-requests-with-Jenkins). @nodejs/collaborators : the automated merge jobs have been changed to never remove any existing metadata lines (PR-URL:, Reviewed-By:, etc) from commit messages. Anything you specify in the form, if not empty, it's simply added at the end of the commit message.  Note that node-accept-pull-request always adds the current PR-URL. I don't think that this will conflict with the scenario of porting / cherry-picking multiple commits. If it's done with a PR, then the additional PR-URL will be added. 
trevnorris		> Note that node-accept-pull-request always adds the current PR-URL.  @rvagg Are we adding the additional PR-URL to the cherry picked commits for release branches? 
trevnorris		Or, I might be thinking of the wrong Jenkins job. 
rvagg		This is a policy that will adapt over time but currently I think we're at: if the cherry-pick is clean or easy then it can go straight in, where it requires a lot of massaging and you're not the primary and/or you need review then it should be done via PR. In the case of a PR cherry-pick I'd like to have the two sets of metadata with the original indented. i.e. I'd like us to preserve existing metadata (PR-URL, Reviewed-By, Fixes, Closes) to be indented and any new metadata to be appended to the end. This is also interesting for the tooling we use for releases. We use https://github.com/rvagg/commit-stream to parse this data in both https://github.com/rvagg/changelog-maker which produces changelogs for us and https://github.com/rvagg/iojs-tools/tree/master/branch-diff which I use to determine what needs to be cherry-picked. So we require predictable metadata formats so our tooling works properly into the future. 
trevnorris		So regardless, there is a path where we need to be able to land commits with existing metadata, and without a PR-URL appended. /cc @orangemocha   @rvagg Can we say that regardless of whether the cherry-pick landed in a PR, if it applies cleanly then it doesn't need the additional metadata? Also, are you implying we can land clean cherry-picks w/o opening a PR? If this were the case, and the PR is labeled for landing on the stable branch, then it would be much easier for developers to move commits over to stable.  I'm trying to think of ways we can make the cherry-picking easier for everyone to participate. My original plan, when I proposed it, was that PRs would have tags like `cherry-pick-stable` and `cherry-pick-LTS`. If the PR was labeled with on of these then the committer could land it immediately on the given branch if it was clean. Or if not then another PR could be opened, and a branch made on `nodejs/node` with a predictable pattern like `refs/cherry-picks/pr42` where the patch would be migrated. Then before release we could check the outstanding issues.  These steps are straight forward and easy enough for any collaborator to follow. If it got to the point that resolution of the conflict was hard then they could hand over the work in the PR they opened. But at least it would be there to work on. I'm finding it difficult to help out with the cherry-picking PRs today because they have gotten so massive. And I can't see at first glance which had conflicts while being picked. 
rvagg		I've been pondering similar thoughts but where I come to is that we're too early to be heavy on process here, at the moment our v3.x and v4.x branches are really immature and it's a simple process. As we get further along on the the 6+ months of a stable branch it's going to get increasingly complex and frustrating and we'll need to figure out process by then, but until we have a feel for the kinds of difficulty we're optimising around I'd rather not assume too much right now. Currently the `cherry-pick-stable` tag idea isn't very useful because it applies to almost anything. In fact, I made a `land-on-v3.x` for this purpose but only used it once. I think it'll be more useful when a branch goes LTS because we'll need to be a lot more choosy wrt what lands. So I'm thinking that, at least initially, those labels won't be helpful for stable branches, they may be later in a branch's life, but they will certainly be helpful for LTS branches.  Also, I've been pondering how we can build better tooling for this, using GitHub APIs. Right now I've started with https://github.com/rvagg/iojs-tools/tree/master/branch-diff which works like `git log` but only uses summary + `PR-URL` to decide what's changed, so it's very accurate for our specific use. I was thinking that coupling that data with a web dashboard that shows us what _could_ be done and helps us to tag and identify what _should_ be done and then we can check against when we release that what should have been done was done.  But again, it's early, and we shouldn't kid ourselves that we have a full grasp on the future, so I'm happy to let this one play out.  In the meantime I'd be happy letting people cherry-pick their own stuff to stable as long as there's nothing breaking and the cherry-pick resolution isn't too complex. I'm also happy having only a few of us doing it and then deferring the hard ones to PRs for reviews, like I did here: https://github.com/nodejs/node/pull/2608 
trevnorris		> Currently the cherry-pick-stable tag idea isn't very useful because it applies to almost anything.  Should have clarified, that was just a placeholder label for the label I just realized we had: `land-on-4x`. Now that I know about it, I'm using it to label PRs that should make it to v4 at some point, but they aren't necessary for the 4.0.0 release. I assume that's what it's there for? Since ATM there's only a 4.0.0 milestone, and I don't want to forget about PRs I know should eventually land on 4.x.  > In the meantime I'd be happy letting people cherry-pick their own stuff to stable as long as there's nothing breaking and the cherry-pick resolution isn't too complex.  Awesome. If a PR from master lands cleanly on 4x I'll be happy to flip through and get those in. Though that is conditional on the CI allowing me to push them over without it adding PR-URL info. And for those that don't apply cleanly, and after 4.0.0 is released, it'll be easy to open PRs for those that do conflict. If this process is good then it shouldn't be hard to flip through everything a couple times a week. 
orangemocha		If you use node-merge-commit you don't need to set the PR-URL. node-accept-pull-request only works on PRs, it uses node-merge-commit internally and it passes the PR-URL to it. So if you port over the cherry-pick _with a PR_, then there will that PR URL added. Seems reasonable. If you do it with a temporary branch and by using node-merge-commit, then there will be nothing added. 
orangemocha		You can also land the cherry picks in a PR without adding the PR-URL, by using node-merge-commit and specifying refs/pull/PR_ID/head as the commit. 
trevnorris		@orangemocha Thanks. 
Fishrock123		cc @nodejs/collaborators  
mscdex		Upgrade all the things! 
jbergstroem		Heads up @nodejs/build -- some machines might have a dead jenkins/java process (it sometimes dies without internet connectivity). When restarting, make sure you're either running the right init script or calling `start.sh` as the `iojs` user. 
orangemocha		Ouch, a 7-hour window :frowning:  
jbergstroem		@orangemocha 2-5 minutes per host though according to above. We should expect jenkins to crash... 
orangemocha		I wish we had a way to mitigate this. Daydreaming now... but imagine if we had redundancy of slaves across clouds, meaning that for no one platform all slaves are hosted on a single cloud provider. Then we could simply turn off the digital ocean machines for one day. 
ChALkeR		That's 2-5 minutes of network connectivity loss for each vm. Will that really crash jenkins? 
joaocgreis		Not really crash jenkins, but if a build is executing on the slave when the connection is lost, the build will fail. And have to be manually restarted.  We might be lucky and have all the servers loosing connectivity at the same time, but it may also be each one in turn, which may cause a lot of builds to fail in that time window. 
rvagg		Let's just make the list more complete shall we?  #   Hello!  We'd like to inform you that we're performing a planned software upgrade on the hypervisor that your droplet resides on.  Expected impact:  In the course of this upgrade, please be advised that your droplet will lose network connectivity for 2-5 minutes as part of the maintenance.   Your droplet will remain powered on during this period -- only network connectivity will be lost.  As always, we will do our absolute best to minimize this downtime.  Below is the scheduled window for this maintenance:  Maintenance start: 2015-09-10 08:30 UTC Maintenance end:  2015-09-10 10:30 UTC  Thank you, DigitalOcean  Affected Droplets: deb iojs-digitalocean-freebsd-linter-2 
rvagg		 We'd like to inform you that we're performing a planned software upgrade on the hypervisor that your droplet resides on.  Expected impact:  In the course of this upgrade, please be advised that your droplet will lose network connectivity for 2-5 minutes as part of the maintenance.   Your droplet will remain powered on during this period -- only network connectivity will be lost.  This will not cause a reboot of your droplet.  As always, we will do our absolute best to minimize this downtime.  Below is the scheduled window for this maintenance:  Maintenance start: 2015-09-11 09:30 UTC Maintenance end:  2015-09-11 11:30 UTC  Thank you, DigitalOcean  Affected Droplets: node-jenkins iojs-build-containers-1 iojs-build-containers-2 iojs-www 
Fishrock123		> node-jenkins  well crap. 
jbergstroem		Finally done! 
jbergstroem		@BethGriggs hey, thanks for this! Seeing how we're moving away from the "old" ansible scripts (that lives in setup) it would be great if you could adapt this to the new structure that lives in "ansible".
gibfahn		@jbergstroem yeah, I asked @BethGriggs to put this up as is so that people could look at it. The plan is to put it into `ansible/`.
jbergstroem		seems to be an opportunity for reusing what we've done in other setups too, like ccache. I suspect bootstrap will be the most different path for AIX. 
gibfahn		>seems to be an opportunity for reusing what we've done in other setups too, like ccache. I suspect bootstrap will be the most different path for AIX.  Yeah definitely. TBH it was easiest for testing to just put a single file together, reusing what we already have is the next step üòÅ .
gdams		We need to add the ramdisk setup to the ansible playbook too, shouldn't be too hard to add
maclover7		ping @BethGriggs @gibfahn 
BethGriggs		I have not had time to progress this for quite a while, I think I'll close this PR.   //cc @gibfahn or @nodejs/platform-aix who might want to pick up from where I got to
mhdawson		I believe that @gibfahn was going to be looking at AIX ansible this week, but if so reasonable to open new PR.
jbergstroem		Is `nodejs1 === release-osuosl-aix61-ppc64_be-1`? 
mhdawson		@jbergstroem yes, David updated the test machines. 
jbergstroem		Ok, done. Updated. 
mhdawson		Covered by https://github.com/gibfahn/build/commit/f196e433eec7a2408ac08c750d60f15d1f39cbb3 closing
jbergstroem		Do we possibly want to add a routine for removing older directories? I see no point in storing nightlies forever. 
rvagg		open for discussion, I don't like deleting stuff in general so it's difficult for me to embrace a full cleanup strategy, throwing them in S3 (or similar) would be great though 
jbergstroem		I'm meh but obviously prefer storage since it's the easy way out. The nightlies are easy to reproduce and have no purpose for anything older than $latest_release. People using them as a tool should use git anyway. 
jbergstroem		I'm a bit scared about `rm -rf`:ing with slashes, but since you guard against some empty variables I think its good enough.  Nothing to remark about. LGTM; thanks for adding these. 
mhdawson		nightly builds seem reasonable to me. 
jbergstroem		I'd like to see this end up in the nightly job at some point, but having it separate to speed up development makes sense. 
joaocgreis		Windows nightly releases are already in place. However, Linux releases cannot be built out of our current release servers, because the `xplat` branch [requires a more recent OS and some dependencies](https://github.com/nodejs/node-chakracore/tree/dd5a011356bac0b0ceb67fbdda32af5f8958c2db#linux).  In order to disturb or current infra as little as possible, I plan to create a new release server with Ubuntu 16.04 only for ChakraCore. I accept full responsibility for maintaining it (@orangemocha can be a backup if absolutely necessary). @nodejs/build any objections? 
joaocgreis		Ubuntu release server done, deployed using https://github.com/janeasystems/build/commit/8c8d28c1d9a1c7caecf50c70b558b052a5c27dad . Will follow with a OS X server if there are no objections.
joaocgreis		OS X server deployed using https://github.com/nodejs/build/tree/master/setup/osx and `brew install cmake icu4c`. Nightlies available for Windows, Linux and OS X: https://nodejs.org/download/chakracore-nightly/v7.0.0-nightly20161129efeaf88f0c/
joaocgreis		Node-ChakraCore has reduced external dependencies, so OSX nightlies are now being released from the OSX release server.
joaocgreis		Node-ChakraCore now supports Ubuntu 14.04. Deployed a Ubuntu 14.04 release server to replace 16.04 with https://github.com/janeasystems/build/commit/622ea305cccf4c06a9a7648e1dd0f4205ef8cb9b .
rvagg		this sounds promising, we've the times where we've had to revoke and replace the global keys we use have been pretty painful (Ansible has helped somewhat) and giving temporary access to collaborators usually involves forgetting that they were given access and leaving their keys in place
gdams		@rvagg, this is the exact issue that I was experiencing with AdoptOpenJDK before I started using KeyBox
mhdawson		Lets plan to have on the agenda for the next meeting which is Aug 22nd at 4 EST.  Next week !
gibfahn		@gdams agreement on this was that waiting till next meeting (3 weeks, 12th September) is fine, but if we don't hit that target it might be worth arranging a one off meeting to demo this. If you're going to be busy by the 12th, we should maybe look at organising something (probably on a Tuesday evening).
mhdawson		Demo happened, closing
gibfahn		@mhdawson I think at the meeting we said we should keep this open for other feedback, and for progress on getting it running.  Obviously it can just be a new issue, either way.
misterdjules		That sounds good to me.
jbergstroem		I've added a new worker but we still need the logic to branch based on version. If someone from the jenkins-admins team could help out that'd be great; otherwise I will attempt it later this week.  Note: I've also corrected the labels -- the smartos14 worker is removed from the smartos15 label, meaning we can now (correctly) rely on them.
mhdawson		I'll try to take a look at adding the logic probably tomorrow.
mhdawson		Testing out first cut in this job https://ci-release.nodejs.org/job/iojs+release-mdawson/nodes=smartos15-release/.  Still some tweaking/testing needed but first run for nightly master is building only on smartos15
jbergstroem		@mhdawson while refactoring; if you could remove relying on `post-1` and `pre-1` that'd be awesome. They are/were used for the 0.10, 0.12 -> smartos13 branching logic.  Also, instead of `cat ./foo | grep bar` you can just use `grep bar ./foo`.
mhdawson		@jbergstroem ok have tested it out and seems like it is doing the right thing across the different versions. Also fixed up the comments you made.    Can you take a final look and then I'll move over to the release job
jbergstroem		```console if [[ "${SMARTOS_VERSION:0:4}" -eq "2013" && ${MAJOR_VERSION} -lt "4" ]]; then ```  I don't think iojs 1-3 builds on smartos13. I would stick with MAJOR being 0 for smartos13 and use smartos14 for >=1 >> 4.  
jbergstroem		Also ‚Äì slightly nitpick ‚Äì but seeing how we call `getnodeversion.py` just below this, we could perhaps just invoke it prior and avoid grepping for MAJOR_VERSION. The checks would use something similar to `$NODE_VERSION =~ ^[0]` instead.
mhdawson		Ok, used   ``` MAJOR_VERSION=$(echo $NODE_VERSION | cut -d '.' -f 1) ``` to re-use output from getnodeversion.py  Adjustted ranges to that io.js 1-3 would use smartos14  Updated main release job with result so we should be good, now will validate that the next nightly runs ok.  @misterdjules  there were no changes to the build part but you might want to validate the binaries just in case. 
jbergstroem		@misterdjules: possibly stupid question but is there a way of getting smartos major version from the host os? I see it as part of `/etc/motd` but can't see how to regenerate it. Version seems to be consistent with pkg info, but yeah..
misterdjules		> possibly stupid question but is there a way of getting smartos major version from the host os? I see it as part of /etc/motd but can't see how to regenerate it. Version seems to be consistent with pkg info, but yeah..  It's a great question, and the answer is unfortunately not obvious.   By "from the host os", do you mean from the virtual machine/zone? That's what I'll assume in my answer.  You can get access to the information about the image that was used to provision a given VM/zone by running the following from within that VM:  ``` $ mdata-get sdc:image_uuid cc27973c-d64c-11e6-a58d-67e135aa72a8 ```  Then, you can query JPC's CloudAPI (e.g by installing [the triton CLI](https://www.npmjs.com/package/triton) to get some information about that image, including its version name:  ``` ‚ûú  ~ triton image get cc27973c-d64c-11e6-a58d-67e135aa72a8 | json version 16.4.0 ‚ûú  ~ ```  Note that the version here is the _image_ version, not the underlying hypervisor OS version which can be found by running `uname a`. I think only the image version is relevant to what you're trying to do here.
jbergstroem		@misterdjules yes and yes. Thanks for writing that up; exactly what I was looking for.
mhdawson		Ok so job ran ok today, but I missed adding smartos15-release to the list of platforms we should be build on so although smartos was green we did not get a nightly.  Have added so next nightly should include. 
jbergstroem		I think this could just as well belong with the docker-node repo seeing how it doesn't really involve ci or our infrastructure. My only remark to such a PR would be making a distinction wrt testing that **building** works versus **running the test suite** within that docker container. A badge wouldn't necessarily explain this. 
chorrell		So apparently I was a bit confused about how to go about setting this up -- I assumed the the build group had some kind of access that I didn't but it turns out third-party app integrations were disabled for the org:  https://github.com/nodejs/nodejs.org/issues/355#issuecomment-159252846  I think I've managed to figure out what I need to do to get this working:  https://travis-ci.org/nodejs/docker-node/builds/120784170  RE: the badge  Yeah, good point. It's not really necessary so I'll just not bother adding it since it may cause some confusion. 
gibfahn		Good point about upgrading, I notice that my Ubuntu 16.04 machine was pulling git from the Ubuntu default repo, and thus had `2.7.4`. Worth adding the [git stable ppa](https://launchpad.net/~git-core/+archive/ubuntu/ppa) and updating if you haven't already:  ```bash sudo add-apt-repository ppa:git-core/ppa sudo apt update && sudo apt upgrade ```  or for macOS:  ```bash brew update && brew upgrade ```
TimothyGu		Ubuntu [fixed](https://people.canonical.com/~ubuntu-security/cve/2017/CVE-2017-1000117.html) this in `1:2.7.4-0ubuntu1.2` for 16.04 (and `1:2.11.0-2ubuntu0.2` for 17.04), so you don't actually need 2.14.1 to get the fix.  For Debian, this was fixed a while back in `1:2.1.4-2.1+deb8u4` in jessie, and `1:2.11.0-3+deb9u1` for stretch. See [Debian Security Tracker](https://security-tracker.debian.org/tracker/CVE-2017-1000117).
addaleax		@TimothyGu Thanks, that was helpful info!  This might be a case where it‚Äôs good to ping at least @nodejs/website too.
refack		Remember CitGM. I'm still not versed in all the infra configs, but CitGM jobs should be more isolated than regular jobs, since they do pull a plethora of npm packages.
joaocgreis		Updated all Windows machines to 2.14.1.
maclover7		ping -- does this need to remain open?
rvagg		we never verified that we've upgrade enough of our machines, maybe someone could copy check-java-version in jenkins and make a git one to do this?
gibfahn		cc/ @piccoloaiutante @nodejs/streams 
piccoloaiutante		excellent, go ahead üëç 
mhdawson		+1
piccoloaiutante		@gibfahn let me know if you need any help on this
gibfahn		Created the [streams-admins](https://github.com/orgs/nodejs/teams/streams-admins/members) team and added @mcollina and @calvinmetcalf (and made both maintainers).   I don't know enough about the WG to know who else will be active in maintaining Jenkins jobs. If anyone else should be added comment in this thread.
gibfahn		Gave the two teams the appropriate access to [readable-stream-continuous-integration](https://ci.nodejs.org/view/All/job/readable-stream-continuous-integration/), @mcollina please confirm you still have configure access.
mcollina		It is all working. I removed some unneeded machines and the ones that suffer from https://github.com/nodejs/build/pull/785.  Some things that I would like fixed:  * some sort of aggregated test results, so that we know what failed across the environments, currently it reports `no result` * we need to be able to trigger this for all the Node.js versions at the same time, ideally from 0.8 to 8, and maybe even the latest nightly, can you help set that up as well? * can we post the status of the build to github as well?  Overall it works very well, I had to fix a configuration on our side. If you want to test, use the `core-ci` branch (https://github.com/nodejs/readable-stream/pull/308).  
gibfahn		Looks like @nodejs/streams didn't have build access (@nodejs/streams-admins did have edit access) because it's actually `@nodejs/Streams` with a capital S. Github lowercases everything, but the plugin doesn't. I've given both permutations (`streams` and `Streams`) access.  <br/>  >we need to be able to trigger this for all the Node.js versions at the same time, ideally from 0.8 to 8, and maybe even the latest nightly, can you help set that up as well?  What I've been doing for other projects is adding a pipeline that runs the job with a list of versions, current job is here: https://ci.nodejs.org/view/All/job/readable-stream-pipeline/. It doesn't do anything magical, it just splits the `NODE_VERSIONs` string by space, and spawns a job with each `NODE_VERSION`.
gibfahn		BTW, if you want to add or remove platforms you modify the `MACHINE` label expression. If you want to run on a subset of platforms as a one off you modify the `MACHINES` parameter.
mcollina		How can we have some form of aggregated test results for each platform and some reporting for the pipeline?  My jenkins fu is _extremely_ bad and out-of-date :(.
gibfahn		Sorry, I should have mentioned that I'm still working through your other requests!
mcollina		thanks @gibfahn!
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
jbergstroem		LGTM 
mhdawson		Landed as c6f9a7ce43847e29dab50122fe90746fcac0c9af 
Starefossen		Great work @jbergstroem üëèüèº Should we try to find vector versions of the remaining logos as well? 
williamkapke		Maybe the donation list should be brought current too 
jbergstroem		@williamkapke I believe Rod is doing that in another PR; didn't want too put too much stuff in here. 
jbergstroem		(ping @rvagg re licensing/image) 
phillipj		Without a doubt very nice üëç  
jbergstroem		> @Starefossen said: > Great work @jbergstroem üëèüèº Should we try to find vector versions of the remaining logos as well?  Feel free! 
jasnell		Please make sure you add a small attribution note to the picture along with the licensing. (@rvagg, if I may suggest, [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)?) 
jbergstroem		@jasnell yeah, waiting for feedback. 
Starefossen		**ARM (SVG)**  ![ARM](https://upload.wikimedia.org/wikipedia/commons/6/60/ARM_logo.svg)  **Joyent (SVG black)**  ![Joyent](https://docs.joyent.com/assets/images/logo-black.svg)  **Microsoft (SVG)**  ![Microsoft](https://worldvectorlogo.com/logos/microsoft.svg)  **Scaleway (SVG logo only)**  [https://cloud.scaleway.com/images/c16347e9.logo.svg](https://cloud.scaleway.com/images/c16347e9.logo.svg)   **Voxer (EPS)**  [http://voxer.com/images/voxer_walkie_variants.eps](http://voxer.com/images/voxer_walkie_variants.eps) 
jbergstroem		@Starefossen perhaps open a PR once you have them that replaces the current? 
Starefossen		Yeah, sure thing Johan üòÑ I'll probably hold off until this is landed to account for the changed paths. 
rvagg		sure, let's go with attribution using by-sa/au https://creativecommons.org/licenses/by-sa/3.0/au/deed.en  lgtm but hopefully people won't get the impression that _is_ our cluster! 
mhdawson		Seems only to be failing on  test-linuxonecc-rhel72-s390x-1 and passing on  test-linuxonecc-rhel72-s390x-2. 
mhdawson		Removed directory for project on test-linuxonecc-rhel72-s390x-1 and kicking off 2 builds to make sure we pass on both machines. 
mhdawson		https://ci.nodejs.org/job/node-test-commit-linuxone/nodes=rhel72-s390x/1442/  (test-linuxonecc-rhel72-s390x-2) https://ci.nodejs.org/job/node-test-commit-linuxone/nodes=rhel72-s390x/1443/ (test-linuxonecc-rhel72-s390x-1)  Now recreating run for last PR to see if that passes ok as well. 
mhdawson		That passed as well: https://ci.nodejs.org/job/node-test-commit-linuxone/1444/ so looks like all should be good now. 
gibfahn		Isn't this what https://github.com/nodejs/build/pull/785 (and previously https://github.com/nodejs/build/pull/658) aim to do?  Seems like a dup.
rvagg		oh, you know ... on occasion I've deleted that directory in general cleanup of machines that need cleaning up, this could very well be my fault on particular machines, will try and remember not to be overzealous with cleaning in future
Trott		> First reported by @Trott on IRC  By the way, I usually report in IRC because it often gets a faster response than opening an issue in the Build repo. And the Build repo has a terrifying ~200 open issues. But if I really ought to be opening issues instead or in addition, I'm happy to do that and save you a step. Just say the word.  (But if just copy/pasting into IRC like I usually do is perfectly fine, I'm gonna keep doing that!  :-D )
refack		> But if I really ought to be opening issues instead or in addition, I'm happy to do that and save you a step. Just say the word.  The current workflow work for _me_. First IRC notification is much lower volume for me so they get my attention quickers. Second some incidents are trivial to solve, and maybe not even worth a logged issue, so I for now I log only if I did something new/interesting, or I had to resort to a temporary solution and the incidents needs followup.  I'll initiate a discussion within the group about this.
rvagg		opening an issue here pings IRC and I read the queue there, we can also add it to the project here so it's properly in the queue so I think opening incident issues is probably the most optimal thing to do
rvagg		Took a bit to figure this one out, nfs can be really hard to debug and get right. It's caused by the "improvements" I'd made in #974, I'm pretty sure it's timing problems for mounting nfs. I've got it sorted now and it's happy over multiple reboots. See https://ci.nodejs.org/job/node-test-binary-arm/11516/  <img width="542" alt="screenshot 2017-11-05 19 41 26" src="https://user-images.githubusercontent.com/495647/32413234-56d545b8-c261-11e7-9ebb-9121d6d02425.png"> 
misterdjules		I won't be able to attend this time.  I onboarded @sam-github and @cjihrig in the releases management team. Because the release process depends on the work from the build WG, there might be a need to keep the communication open between release managers and the build WG. 
mhdawson		@jbergstroem ready for your review 
mhdawson		@jbergstroem  ping 
jbergstroem		@mhdawson sorry, just busy with work. Others -- feel free to review as well; otherwise I'll do it as quick as possible. 
mhdawson		@jbergstroem added setup of NODE_TEST_DIR 
jbergstroem		not sure how `/etc/security/login.cfg` looks, but could perhaps [lineinfile](http://docs.ansible.com/ansible/lineinfile_module.html) simplify this?  
mhdawson		@jbergstroem in respect to lineinfile I search on bullfreeware and I don't see it ported there so I don't think its available for AIX.  The line in question looks like this:   shells = /bin/bash,/bin/sh,/bin/bsh,/bin/csh,/bin/ksh,/bin/tsh,/bin/ksh93,/usr/bin/sh,/usr/bin/bsh,/usr/bin/csh,/usr/bin/ksh,/usr/bin/tsh,/usr/bin/ksh93,/usr/bin/rksh,/usr/bin/rksh93,/usr/sbin/uucp/uucico,/usr/sbin/sliplogin,/usr/sbin/snappd  were I need to add /bin/bash since it is not there by default. 
mhdawson		@jbergstroem wondering if you have more comments ?  
jbergstroem		Not sure what init system aix is running but is output from java being picked up? That'd be my only remaining feedback as of now. 
mhdawson		When you say "is output from java being picked up" can you clarify ?  Do you mean do we capture it and send it to a log etc ?  If so I'll look at what we do for other platforms.   
jbergstroem		@mhdawson on other platforms we usually pipe to a logfile; check other examples.  Btw, I just compiled ccache manually; if you could add /usr/lib/ccache preceding $PATH in your init script and possibly add the ccache compilation to that manual step document it'd be great. 
jbergstroem		Come to think of it; I recall the slave having a `-slaveLog` option. On init systems that doesn't redirect stdout/err we should probably implement that instead. 
mhdawson		I had looked at some other platforms and added the redirect to a log file like they had.  Hope that is good for now.  @jbergstroem if thats ok then I'm hoping this is ready to go in 
jbergstroem		Is there a global cert store in aix? It's a bit unfortunate that we can't validate certs in the playbook. 
jbergstroem		Other than that, LGTM. 
mhdawson		I'll look into the cert side, but as discussed unless somebody else objects I'll squash land this  tomorrow or Friday. 
mhdawson		Landed as f5a5dd95887ab0827a3adf03c9462e80fe619cc0 
joaocgreis		LGTM  Thanks!
joaocgreis		Landed in https://github.com/nodejs/build/commit/00faec62f6eea2ae28488eae9a4886ae0e7e1870  I changed the commit message a little, hope it's ok. Thanks @piccoloaiutante ! 
piccoloaiutante		Excellent! Thanks @joaocgreis 
MylesBorins		doodle... I just compiled and promoted the v4 and v6 releases. I figured that the build process would be deterministic and didn't check the sha's when I promoted the assets. The result is that a bunch of the promoted releases now have different sha's then the original release.  @jbergstroem do we have a way to roll back the release server? @rvagg / @nodejs/ctc is this an issue? Should we just keep the new shas? I'm a bit concerned about there now being different releases with the same sha's in the wild.  Sorry about making this mess   edit: we can see the original sha's in the nodejs.org blog releases  6.9.3 before ``` -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256  c55e35ccf71f868d6b7075f20c14b9d0c2c8a3ca98db823b0c5a83ba09e5a984  node-v6.9.3-aix-ppc64.tar.gz ae79277f15b8b2f173b97e44e2d4c65a8de4254c2c7da0dcd754b4e39658a779  node-v6.9.3-darwin-x64.tar.gz 239c196ab56ee875ad300159cfc3f5bd0a87ce457961046ce9518868a983d618  node-v6.9.3-darwin-x64.tar.xz 43764ddd3829cd3ad22b1e6870fd7d058e2f9a2cf3fd3ea21a25772a18fe0a88  node-v6.9.3-headers.tar.gz d0cd948b6d585f64e6ef9ba61c1ee6b3c703670f0bad04e613ad6914b011151f  node-v6.9.3-headers.tar.xz f9eaf8dbd926770795ec5a5670a824bab25ec5b19c9803584c342777daef272c  node-v6.9.3-linux-arm64.tar.gz fc461a64ef0d2f6267436e95f966df8673276a6344c9389d41cfa06da07ab878  node-v6.9.3-linux-arm64.tar.xz 5247665cac023be266cdff06abff0f784f0b5d737edff7dcabb12ceb115cdb36  node-v6.9.3-linux-armv6l.tar.gz 27941dcafa8d9cb0529f1b88831fc40837118a6471410cfca77fa42c5d57415e  node-v6.9.3-linux-armv6l.tar.xz 01793465bd7ddd6cdb798799c5e4ee107fd6dc77e013bfb602d9f677395d9465  node-v6.9.3-linux-armv7l.tar.gz 30fa10c799db76732998913a2195f45041f5c2417800740c43cee9b7dcaf7b33  node-v6.9.3-linux-armv7l.tar.xz 15fceb4cac03ea4cfa54e202d39ac260aec21575057029a0b5b21463ee88c683  node-v6.9.3-linux-ppc64le.tar.gz 0dc6cf753cc25f14f2f310d22d40dbc8c273dce38a9776b430bac319cb27ac6b  node-v6.9.3-linux-ppc64le.tar.xz 3ccf0fa6543714e1a745648fea9ab86e0566599e3618d578e9009835bfcc79a3  node-v6.9.3-linux-ppc64.tar.gz ce25234a057f1c0a744bfcfafd2f6d0fc78bc554451c422ec3220c7f1e755d7d  node-v6.9.3-linux-ppc64.tar.xz 8d387365cd3a7c56c5f603561458cb303351803cea3c409b7ff14a2f88bebf40  node-v6.9.3-linux-s390x.tar.gz c3ea05fd0a7e2216d699da8fda2223538f3b5ecc88fbdb5cdcefe10e4439bf19  node-v6.9.3-linux-s390x.tar.xz 89fbe01f6ccba0295a121ca32e3f0da772319406a8dba5f63e4797a4df6cb5ad  node-v6.9.3-linux-x64.tar.gz 7e60f6f54a836ab8346d0dc60f8c35522a839872084e76acba892920502392d2  node-v6.9.3-linux-x64.tar.xz 5e5e95f47a71eb3316ff4aa520f5174f622196eb591d11a0948314dc211d0e0e  node-v6.9.3-linux-x86.tar.gz 514ff425fd85179c8c065eb7c44c37416d5a80b2e6b87d3a1dcb496616cbc42e  node-v6.9.3-linux-x86.tar.xz b2898e8261a28df40d640672ee4fb61b4e46b4b87d601c863d2003ff97b5230b  node-v6.9.3.pkg a29cab343c4695c6609a80503b9a1fcab12952c1632f821f6d7a5851dadc6549  node-v6.9.3-sunos-x86.tar.gz 47e38e6ade9c300f003b28873163e193f76b5137dc9ef6fbf31b6ff7fd72fcee  node-v6.9.3-sunos-x86.tar.xz 5abdc3b77e011d664e13d74cab130680a8652b5cd23a63d2a17496d91399d5b5  node-v6.9.3.tar.gz 98ea92695e9df27c8a2719406e0be51967f06c5ed4e0f6ee5f1e8460814d2723  node-v6.9.3.tar.xz a58c42b95d5359de9c72c01e1c7abf772a294cafb2ac7428011f8b4c99efc868  node-v6.9.3-win-x64.7z bda76db75bed655b5b8f01022f33ca7e61b8be175b871dcc218bdf84f6403a1f  node-v6.9.3-win-x64.zip 76c13d2814c5bc2dd51cb6e5e49f6da8986a01d6f606e5aae2b024313a01b62b  node-v6.9.3-win-x86.7z b971e08b0dbe4285f743a2b612065ef187273731f59e10a012143c80f0c7861c  node-v6.9.3-win-x86.zip 791185c3a771350cd1ea62dac0b6ad0958eb7cf1f1f5e67e5c2ae68ca6db1e32  node-v6.9.3-x64.msi 5ead92cfd27d501ca60889cee1fd3c5bfe3b7ddf2e07a7f927cfeae52cfcf94c  node-v6.9.3-x86.msi 47fef39ededc0dcb348689676de82973ea42715367b84f841167c01cb6398725  win-x64/node.exe 4b1586514603e3d4f78374e05b0617681ae7729a8ecef230b57617714cfafab6  win-x64/node.lib 66968b714afe766c4ff7cbee9bcc77b330649a1e6861c9b6b61d35db81d41e5a  win-x64/node_pdb.7z 4363c4f40461fc08b53076984c6910147ebcbbf93ba43f243eb0d51311fa7d0f  win-x64/node_pdb.zip 9ffd3eec57eebb87cb5b5377dd2213d6c00e40bc8ae4cd5a74582a3cfc037556  win-x86/node.exe 1b044b08e8e2a0f78781c85cc5facdabd04c0a8e73fd991ae3e5a3053784c14f  win-x86/node.lib 921953d2ffb57fa6aa53c001e4bcfec2494f7a68400cd3985c8eaa23786247eb  win-x86/node_pdb.7z ff1e5f746eb731f81f80b75e41c27b104888361e0187d05195a242eaf9358d52  win-x86/node_pdb.zip -----BEGIN PGP SIGNATURE-----  iQEcBAEBCAAGBQJYbDoeAAoJEJM7AfQLXKlG2QoH/jN2p9Np5CGXlmaJn7hZJEf+ 3Z4MIE5uLeoADWsb+QKYQiS4OBQZDoKyivXzctv4EK0q8VlLt4QrSDUdziF3Uh4A hsyqoM4SjRHBsEJVFTcwuBbZUKOtGSvjkJJ97pFki70kd+R9xXCqHzyPmTVU3hLH awlPLIQENI1UxbjMXiOz1TOquP0J6PVUONd3Qe3qJ3oLG8e4aPDha75SFE84vvqa VnsFKMi45ngzt0nBrxW+ASpahPdEGgeJ1IIXYXG9sdCZBR+J0MYYL/r7HGmx6HlS jwYtJHsz/zd76JKpRnekk1m8VgEA9o7wRyUSI7bepRw2Q67wv3rFGG7IQHlccLU= =XtXG -----END PGP SIGNATURE----- ```  6.9.3 after ``` ced91ebd70173714117e3d4787036b7047c5000ab28896ac128fbd3607dd99f1  node-v6.9.3-aix-ppc64.tar.gz ae79277f15b8b2f173b97e44e2d4c65a8de4254c2c7da0dcd754b4e39658a779  node-v6.9.3-darwin-x64.tar.gz 239c196ab56ee875ad300159cfc3f5bd0a87ce457961046ce9518868a983d618  node-v6.9.3-darwin-x64.tar.xz 43764ddd3829cd3ad22b1e6870fd7d058e2f9a2cf3fd3ea21a25772a18fe0a88  node-v6.9.3-headers.tar.gz d0cd948b6d585f64e6ef9ba61c1ee6b3c703670f0bad04e613ad6914b011151f  node-v6.9.3-headers.tar.xz 2b0aec9caf1afb5b4cb417dafb2701a2a104e669a0dcb2005497c7f636211ed8  node-v6.9.3-linux-arm64.tar.gz a45a79d03b48704e140a640a45130aa7b84be9e57f070f9aade8448ce07d18dc  node-v6.9.3-linux-arm64.tar.xz 5247665cac023be266cdff06abff0f784f0b5d737edff7dcabb12ceb115cdb36  node-v6.9.3-linux-armv6l.tar.gz 27941dcafa8d9cb0529f1b88831fc40837118a6471410cfca77fa42c5d57415e  node-v6.9.3-linux-armv6l.tar.xz ba7cec96cf4893ac5eaf2aaf8768cf8e5d7b69ecf25a48e18e8832183fc39e9a  node-v6.9.3-linux-armv7l.tar.gz d86187fbf2ee4875438cc8fc53506db998398f165227ec162160a6fedef957cf  node-v6.9.3-linux-armv7l.tar.xz df59c6bd0b2de004fcec8101f2f671107f63a3e29d436eb02c922348a423fcc2  node-v6.9.3-linux-ppc64le.tar.gz a4d708f07a44a534c85ddc3a35d0557ce9412af3ad2a16373a871cea104942fd  node-v6.9.3-linux-ppc64le.tar.xz 979f1bec0deac73fdaf45c07f121f9ab7fad0f28c1543710851c878c4a14ac12  node-v6.9.3-linux-ppc64.tar.gz 2227f81699effb0ef9582582e5ad40172a1378aa1b1a0ece0a7a67083f94491d  node-v6.9.3-linux-ppc64.tar.xz 3acc43559272d5e2d17cc2eab797cc8646e16daa1655dbbceecccc18bc6cbf5f  node-v6.9.3-linux-s390x.tar.gz 09f428e4598be8d289a4c49e4dad1491335a15527fb6277cd542034b8772fa4d  node-v6.9.3-linux-s390x.tar.xz 5957fd9b65c346f0d0afb1adc8bde98fa04bf613ee51ef9570d287bda73314a2  node-v6.9.3-linux-x64.tar.gz f072719f5810a0fd8ba1d882eb19e546d54bf675b393b8478ff89b304669876b  node-v6.9.3-linux-x64.tar.xz fd25af7ac2728d2321417dfaa408c2f29a8fc1a230f9dbda1a43a2345e6be338  node-v6.9.3-linux-x86.tar.gz 5fd85f7e99bff5e3fa23293bb379ff8f0063c6dbf9bc6caa7bcb160edd38d76b  node-v6.9.3-linux-x86.tar.xz b2898e8261a28df40d640672ee4fb61b4e46b4b87d601c863d2003ff97b5230b  node-v6.9.3.pkg 229b336d9be8ce86a0a3d96ef11333df2e3fd2b57cfc4e54bb2355e0b6215a97  node-v6.9.3-sunos-x64.tar.gz edfb132cd42a51524a7a5b9970db65ba188667f74cefbadb51a3a44f1263c46f  node-v6.9.3-sunos-x64.tar.xz 36cbeeed50d4539cc650a7373c79f553d5ef44038e6ffe7d296553d1a12b0443  node-v6.9.3-sunos-x86.tar.gz a97a83f05234cf7a27bbaa6c680795a163496e07f7f801b8773c6f660fe1f51d  node-v6.9.3-sunos-x86.tar.xz 5abdc3b77e011d664e13d74cab130680a8652b5cd23a63d2a17496d91399d5b5  node-v6.9.3.tar.gz 98ea92695e9df27c8a2719406e0be51967f06c5ed4e0f6ee5f1e8460814d2723  node-v6.9.3.tar.xz abc083584830e01042655c18c9fd178177835da72ace54489163a65613091068  node-v6.9.3-win-x64.7z a6670c5790052c626e72bc5a0ef983236a54a71de784076784e8565688fcf7fc  node-v6.9.3-win-x64.zip 3613a8d0aa30450b8803d06badd42eaecffb37978ff691e557657a93c87cccf0  node-v6.9.3-win-x86.7z 338c19fbb7d7655fea1ccb65b85299269420ffa1f1b10cc0362d10ce6b7e818e  node-v6.9.3-win-x86.zip 7c4829e708ae9491b8af76c45ac5ab0eeeb04f084527d049b673a054a084bf11  node-v6.9.3-x64.msi b94eb09987990fc4b5dfa6df16106d23ad8089b26a9b87c0576de98aacbcfb69  node-v6.9.3-x86.msi 2551c19ee4e6c03ce55dd916ee4dfcbcfec3cd71300164abf4be8a06b77c2ea4  win-x64/node.exe c8ec2fae6968ff9ebf056fa95a42f7edc25146a46c1f70429d76d6acb4474a0f  win-x64/node.lib 18d84d29c7472bdc3974490e7dfcc7d6a4fd50a9d7a126351a13fa644d453d8f  win-x64/node_pdb.7z 83503d437d7869575bb69c02203b29871a4a51301bf8118aa391ed732a56b114  win-x64/node_pdb.zip 4d803cde8e701c67c434b05232f2686fa4dd1317a19dacc1e2f76d6219b85feb  win-x86/node.exe 54b43a3f880a175d75faead2a20a4a56103ba29ab6d325092d835cc480dc5b63  win-x86/node.lib a672dcf5206896ae1925273173c5ef7d4b7aec39dcb98929170b0e68a03aeafa  win-x86/node_pdb.7z 2db6c76614c56b4b43ad4ffc47b9bdab5e6fc548358eeff877e193e54c751ee9  win-x86/node_pdb.zip  ```
MylesBorins		discussed with @rvagg, he is going to attempt to recover the promoted releases from backup.
rvagg		OK, I could only find valid backups for v4.6.2 v4.7.0 and v6.9.2 in /backup/static/dist/nodejs/release on the backup server. @jbergstroem is this because the backup script hasn't run since the newest two were out, today/yesterday? Is there an incremental for dist or only this? Is it just because I caught it before the backup ran again that I happened to catch the old versions of these files (mtimes on them put them earlier than today and SHASUMS indicates they are the original‚Äîcompared to blog‚ÄîI haven't shasumed them myself). Is there anywhere else that we might find original copies of v4.7.1 and v6.9.3 other than in ci-release.nodejs.org jobs as artifacts?  What I've done with v4.6.2 v4.7.0 and v6.9.2 is the following: * Copied the entire dist/release directories for these versions into `/home/staging/nodejs/release/` on the staging/web server * Copied the sunos-x64 binaries from the new builds into the same staging directories * Removed the SHASUMS files * Ran `for i in $(ls); do touch ${i}.done; done` to make the `.done` flag files that tell the promote script that they are ready to be promoted. * Changed ownership back to staging.staging  So, @MylesBorins, they are in staging now for you to re-promote. If you run `tools/release.sh` and promote v4.6.2 v4.7.0 and v6.9.2, all of the files that are in staging will be moved over to dist and you'll get to make and sign a new SHASUMS file which will have the original shasums plus the new sunos-x64 shasums and be signed by you. You should probably then update the blog posts with the new SHASUMS content which has the sunos-x64 files.  For the other builds, we have two ways to _add_ just the sunos-x64 binaries: 1. Run the builds on ci-release and _cancel_ everything but the sunos-x64 builders. Then you should get just the files you want in staging and when you promote those it'll just be additive. 2. Run the builds as normal on ci-release but then on the server manually remove everything _but_ sunos-x64 in staging. I don't believe anyone but me has ever fiddled around in staging before but technically everyone on the release crew can do it as you have ssh access as staging@direct.nodejs.org and the files are located in `/home/staging/nodejs/release/vX.Y.Z/`. It's probably too intimidating though so I'm happy to step in and do this bit for you when the time comes.  And fwiw, the original releasers don't need to trigger the builds on ci-release, they only need to promote so they can sign. Any one of us who has ci-release access can trigger them at any time and they'll just wait in staging for the appropriate person, so no need for complex people-coordination to make this happen.
rvagg		If we can't recover the original v4.7.1 and v6.9.3 then my vote would be to simply update the blog posts with the new SHASUMS rather than mess around with pulling them out individually from Jenkins. The problem's we're having are simply about discrepancies which we can fix with brute force on the blog rather than attempting to roll back.
jbergstroem		> @rvagg said: > is this because the backup script hasn't run since the newest two were out, today/yesterday? Is there an incremental for dist or only this?  Unfortunately (in this case), we don't increment these: https://github.com/nodejs/build/blob/master/backup/backup_scripts/dist.sh  > @rvagg said: > If we can't recover the original v4.7.1 and v6.9.3 then my vote would be to simply update the blog posts with the new SHASUMS rather than mess around with pulling them out individually from Jenkins. The problem's we're having are simply about discrepancies which we can fix with brute force on the blog rather than attempting to roll back.  This is a big no-no for me. The blog posts are not the only place where these checksums are stored. Every package manager that's been updated have probably stored their own sum. The only way forward are version bumps. Each artifact at release needs to be considered pristine.  I think this also should sink into the workflow we do with releases. I'm not quite sure why these changed between the runs but one way of handling this could be making files read only on the dist server?
jbergstroem		Case in point: ```irc [08:25:23]  <damongant>	we've got a little security/release issue over in #node.js if a dev could take a look [08:25:48]  <damongant>	checksums for 6.9.2 changed [08:26:03]  <damongant>	and broke some builds [08:26:16]  <evanlucas>	damongant changed for what binaries? [08:26:31]  <damongant>	evanlucas, source package, contents are the same tho [08:26:46]  <damongant>	timestamp on the tar.xz is different [08:27:22]  <damongant>	https://aur.archlinux.org/packages/nodejs-lts-boron/ see the first comment, the sha256 I packaged against is still in the 6.9.2 announcement though ```  Doing another minor is probably the simplest way for most parties?
gibfahn		>Doing another minor is probably the simplest way for most parties?  @jbergstroem do you mean a  `v4.7.2` and `v6.9.4` (patch)?
jbergstroem		@gibfahn yeah.
gibfahn		Makes sense to me (but I don't have to do the release üòâ ). If we're going to do it the sooner the better though right?
MylesBorins		So I just attempted to promote the release and found that the permissions are set incorrectly and I cannot promote the builds. They are currently set to 755 rather than 775, and as such the build cannot be promoted
MylesBorins		Fixed the ones we had backups of. v4.7.1 and v6.9.3 still have the wrong shas
MylesBorins		I've proposed new releases v4.7.2 and v6.9.4 that are simply semver patch bumps to remove ambiguits with the release  https://github.com/nodejs/node/pull/10639 https://github.com/nodejs/node/pull/10640  Please chime in those issues on thoughts about the special releases
rvagg		> This is a big no-no for me. The blog posts are not the only place where these checksums are stored. Every package manager that's been updated have probably stored their own sum. The only way forward are version bumps. Each artifact at release needs to be considered pristine.  point taken, good call on doing brand new releases to clear things up @MylesBorins
MylesBorins		v4.7.2 and v6.9.4 have been released to fix this issue. I am going to go through and update the blog posts with the missing shas and information. Should we make an amendment to the v4.7.1 and v6.9.3 posts to explain what happened? Should we include the sha's that are now on the server?
gibfahn		+1 to updating the v4.7.1 and v6.9.3 blog posts to point people to the later version and including both SHAs they might have.
MylesBorins		Closing, reopen if we should revisit
Starefossen		Sticking to the [semver specification](http://semver.org); RC versions should be labeled `v3.0.0-rc.1`. 
rvagg		Thanks for pointing that out @starefossen, I hadn't noticed the dot separation of tags in the spec before tbh. Will adjust for rc.3.   
rvagg		Actually, getting feedback from @ljharb here would be useful before jumping straight in.  
ljharb		I'm fine with `vX.Y.Z-rc.N` as a format, as long as it stays consistent - if so, can that be updated on the existing iojs.org rc site? 
rvagg		the existing builds have `rcN` baked in so can't be changed, I could remove them from index.tab and index.json by changing the regexes in this PR, would that help? the next RC can be the first one supported by nvm then 
ljharb		hmm - if both formats are there it's going to be a _huge_ pain. As long as either the archive files themselves are all the same convention, across both existing and new releases, or the old releases don't exist in `index.tab`, I think I should be alright.  Basically I want to avoid having to code in support for more than one convention/format. 
rvagg		I've gone ahead and adopted the `vx.y.z-rc.n` format in Jenkins, I've updated this PR and this is live on the server now. I also deleted the older RCs so now there's only rc.3 @ https://iojs.org/download/rc/  Unfortunately node-gyp also needed the same treatment in `next`, I've proposed that here and there will be an rc.4 for this: https://github.com/nodejs/io.js/pull/2171 
mgol		Just a quick update - it will be https://nodejs.org/download/rc/ now. 
gibfahn		SGTM
mhdawson		SGTM
targos		The issue has been fixed thanks to @addaleax. I do not need access anymore. Thanks!
rvagg		ok .. I can't explain this off the top of my head but I've fixed them and they _should_ be good to go again now, give them another try 
rvagg		oh, and yes, this is a good way to report errors, hopefully there'll be more than me that can fix them soon 
joaocgreis		LGTM  The machine is using ccache, but it's good to keep this in sync. @jbergstroem Hope it's not too late! 
MylesBorins		/cc @jbergstroem @rvagg @joaocgreis 
rvagg		on it
gibfahn		Seems back (presumably thanks to @rvagg ).
rvagg		Fixed and it's up and running  @nodejs/build Disk space problem yet again, _but_ this time I dug a bit deeper and found that it's the workspaces, not the job data, that's causing us most grief with disk. Every time a job is run the master does the initial clone to manage the process but then we end up with a _lot_ of clones of some big repos and Jenkins is pretty messy about it, making multiple workspaces, even ones with `tmp` in their name.  Unfortunately it's not obvious to me how we could clean these up automatically. We could schedule a cron and delete but we don't want to be deleting workspaces that are in use. A "last modified" check might do the trick I suppose. I _believe_ that Jenkins doesn't keep internal state about the workspaces, they are just files on disk to be touched whenever.  Another option is to shunt this work off onto another host, a secondary, that the master uses for all of these workspaces. We have a rule in there that forces this work to be done on the master rather than some random node that's connected (that's the default). I think we could connect a secondary server with a really big disk as a slave node and have it do all of this workspace stuff, leaving the master to manage job coordination. That may have an additional side benefit of making the master more efficient and possibly faster (just a guess).
rvagg		100% down to 15% just by deleting workspaces FYI. `find /var/lib/jenkins/jobs/ -name workspace\* -exec '{}' \;` should do the trick for anyone who might need to do this in future.
gibfahn		Moving git clones off master (second suggestion) sounds like a good idea, running Jenkins is more than enough for one machine in my experience.
rvagg		Also, @nodejs/build, it may take a bit of coaxing to get all of these nodes reconnected. Some of them may not be retrying so any help in getting them back online will be appreciated.
gibfahn		@rvagg a list of the ones you expect to be online would be useful, trying to run `test-softlayer-centos6-x64-2` and [test-softlayer-centos6-x64-1](https://ci.nodejs.org/computer/test-softlayer-centos6-x64-1/) and getting this:   ``` Aug 9, 2017 4:55:20 PM hudson.remoting.jnlp.Main createEngine INFO: Setting up slave: test-softlayer-centos6-x64-2 Aug 9, 2017 4:55:20 PM hudson.remoting.jnlp.Main$CuiListener <init> INFO: Jenkins agent is running in headless mode. Aug 9, 2017 4:55:20 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Locating server among [https://ci.nodejs.org/] Aug 9, 2017 4:55:20 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Handshaking Aug 9, 2017 4:55:20 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Connecting to ci.nodejs.org:41913 Aug 9, 2017 4:55:20 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Trying protocol: JNLP2-connect Aug 9, 2017 4:55:21 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Connected Aug 9, 2017 4:55:21 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Terminated ```  I tried downloading a new slave.jar from https://ci.nodejs.org/computer/test-softlayer-centos6-x64-2/, but it gives this error:  ``` Exception in thread "main" java.lang.UnsupportedClassVersionError: hudson/remoting/Launcher : Unsupported major.minor version 51.0 	at java.lang.ClassLoader.defineClass1(Native Method) 	at java.lang.ClassLoader.defineClass(ClassLoader.java:648) 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:277) 	at java.net.URLClassLoader.access$000(URLClassLoader.java:73) 	at java.net.URLClassLoader$1.run(URLClassLoader.java:212) 	at java.net.URLClassLoader$1.run(URLClassLoader.java:206) 	at java.security.AccessController.doPrivileged(Native Method) 	at java.net.URLClassLoader.findClass(URLClassLoader.java:205) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:325) 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:296) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:270) 	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:406) ```  Do we expect this machine to connect?
gibfahn		Okay, everything is back except for the Pis and these machines:  - [test-mininodes-ubuntu1604-arm64_odroid_c2-1](https://ci.nodejs.org/computer/test-mininodes-ubuntu1604-arm64_odroid_c2-1/) - can't ssh in - [test-digitalocean-freebsd10-x64-1](https://ci.nodejs.org/computer/test-digitalocean-freebsd10-x64-1/) - can't ssh in - [test-softlayer-centos6-x64-1](https://ci.nodejs.org/computer/test-softlayer-centos6-x64-1/) - error mentioned above - [test-softlayer-centos6-x64-2](https://ci.nodejs.org/computer/test-softlayer-centos6-x64-2/) - error mentioned above  ~Also one of the macs is running out of space:~ Fixed (for now)  - [test-requireio-osx1010-x64-1](https://ci.nodejs.org/computer/test-requireio-osx1010-x64-1/) 
rvagg		Yes, I'm pretty sure that machine was working last week, I was working in the other centos6-x64 which was offline but this one was still fine.   What JVM is it using? That error sucks because there's no clear way to fix it. Just make sure you have the slave.jar from ci.nodejs.org and an updated JVM. I'll get on soon and see what I can do if you can't make headway.   Regarding which machines _should_ be online - all of them, unless you can't SSH in, most of the ones not in the ARM cluster are good, I have a bunch of pi's offline though. 
rvagg		test-mininodes-ubuntu1604-arm64_odroid_c2-1 needs a restart, it developed problems yesterday. I'm getting David @ miniNodes to deal with it.  test-digitalocean-freebsd10-x64-1 is an interesting one, I was trying to get it online on the weekend but failed - I've tried hard-rebooting it to no avail, I can get the web console open via digitalocean and it even responds (I can't login there of course). So it looks like a network problem. @jbergstroem should we just reprovision this machine? Is ansible OK with these in its current form? I've never done a freebsd provision before.  Working on the centos6 machines now.
joaocgreis		Both centos6-64 back, `service jenkins restart` did the trick. I don't see anything in place to restart when Jenkins crashes (no monit or systemd), so if nothing is really there we should add it.
rvagg		cleaned up a few more hosts too, looks like we're back on track now except for the freebsd10
refack		I think the aix failures in CitGM are related to the restart: https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/947/nodes=aix61-ppc64/console (__all__ packages fail to either download or install) ping @mhdawson @gibfahn 
mhdawson		I cleaned up some old processes and restarted the jenkins agent.  A lot of the test ran but still a bunch of failures.  What I can't tell is if this is different from before as citgm has lots of red overall.  https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/nodes=aix61-ppc64/952/consoleFull  
rvagg		@nodejs/build - I've just made some major changes to the way CI executes   * There are two new hosts, test-packetnet-ubuntu1606-x64-1 and test-packetnet-ubuntu1606-x64-1 and they have the label "jenkins-workspace" and can handle 20 parallel executors at a time. They are 4 core Atom servers (i.e. not huge but seem to perform quite nicely) but have 1Tb of attached storage on /home/. They are setup on the server like normal test hosts but on CI they are set up as generic workers.  * I've turned master into a "run only when specifically selected" and turned it to a single parallel executor (I wouldn't mind doing a 0).  * I've reconfigured a bunch of jobs (I've tried to do as many active ones as possible) so that they have "Restrict the nodes this job can be executed on" to be "jenkins-workspace" which means they do their management on those hosts, that's git clone, main script execution and coordination, not actual test running. There are some jobs that run specifically on other hosts, like benchmark and lint, so they aren't changed. But for the most part, jobs that have child nodes that execute the tests/build should execute the main coordination on one of these new hosts.  * Cleaned out workspace*/ directories from the master host again  It's possible that we may have some job configuration problems from this, so if things seem to be failing with whacky reasons then this could be the cause. There may be more ironing out to do. But this should take a big load off master and should take the disk pressure off there too so we should even be able to extend the number of days we retain data up from the current 5 (or 7, I don't recall what it was when I last looked).
jbergstroem		Thing is, they don't. I've been logged into the slaves while the timeout occurred. There's also: - https://ci.nodejs.org/job/iojs+release/224/nodes=pi1-raspbian-wheezy/console - https://ci.nodejs.org/job/iojs+release/225/nodes=pi1-raspbian-wheezy/console  ..so I'm suspecting either jenkins (yay) or perhaps even the ci host. 
jbergstroem		and more https://ci.nodejs.org/job/node-test-binary-windows/RUN_SUBSET=0,VS_VERSION=vs2013,label=win2008r2/170/console 
jbergstroem		minor update: I've updated most of our slaves to 1.52 -- seems like it didn't help; I've seen windows slaves go offline since. 
rvagg		fwiw @joaocgreis mentioned that the machines on azure had some weird networking problem that was causing the on/off behaviour, they are unique in this respect, judging by the CI status emails we're getting anyway. 
jbergstroem		@rvagg lets assume that was the case then. i'll try and keep a close look on fails over the next few days. 
jbergstroem		Still around: https://ci.nodejs.org/job/node-test-commit-plinux/nodes=ppcbe-fedora20/198/console 
jbergstroem		https://ci.nodejs.org/job/node-test-commit-plinux/nodes=ppcle-ubuntu1404/200/console 
jbergstroem		https://ci.nodejs.org/job/node-test-commit-linux/nodes=ubuntu1504-64/1111/console 
jbergstroem		I'm starting to think the host is to blame. Should we try updating jenkins? Can't find anything relevant in the changelog. 
joaocgreis		I've had connection problems very frequently when I was setting up the cross compiler machine (running Linux). I changed the connection to ssh from Jenkins and haven't seen it fail since.  I've installed Cygwin on `node-msft-win10-5` to try ssh to Windows ([it should work](https://wiki.jenkins-ci.org/display/JENKINS/SSH+slaves+and+Cygwin)), but no luck connecting so far.  It's strange that the machines in Azure are constantly having this problem, but the ones on Rackspace are always fine. 
jbergstroem		Here's another one from arm slaves: https://ci.nodejs.org/job/OLD-node-test-binary-arm/414/RUN_SUBSET=3,nodes=pi1-raspbian-wheezy/console 
joaocgreis		I might have figured out the problem with Azure machines. Jenkins slave has a keep alive signal with a 5 minutes default interval. That seems to be too much for Azure, the the connections were broken because of that. I added a JVM option to Azure slaves to reduce it to 2 minutes (that's what Azure uses for SSH). Let's see if that's the correct fix for the correct problem, but I'm hopeful.  On the other hand, Jenkins has been completely broken since https://ci.nodejs.org/job/node-test-commit/1107/ . Apparently, sub jobs are being started only if it detects any change in git, even though that option is explicitly disabled everywhere.  Right now, my best guess is that some plugin update broke it. The multijob plugin was updated (does it have automatic updates?) to 1.19, that introduced the "Resume build" button, and that button appears for the first time in the first build with problems. This might be a coincidence, I'm still looking into it, this is just to share progress. 
rmg		@joaocgreis I've noticed a similar problem on my own multi-jobs. I had to enable the "Only build when VCS changes are detected" because it doesn't actually mean what it says.  See https://issues.jenkins-ci.org/browse/JENKINS-30952 
joaocgreis		I downgraded the multijob plugin to 1.18 and it's building, looks good so far. I'd rather leave it at 1.18 instead of flipping all the "build only if VCS" checkboxes because we have quite a few. That issue is 8 hours old, perhaps the fix won't take too long. 
joaocgreis		The test-binary jobs did not work after downgrading the multijob plugin, had to upgrade again and flip all the switches. We'll probably have to flip them again when this gets fixed.  ~~But they still don't work: https://ci.nodejs.org/job/node-test-binary-arm/482/console and https://ci.nodejs.org/job/node-test-binary-windows/284/console~~ EDIT: I cloned the jobs to clear the history, they seem to be working now. 
joaocgreis		I haven't seen Azure machines failing again, so I assume the keep alive interval change fixed it. As for jenkins, jobs seem to be running well now.  So, keeping this issue alive is the (much fewer) random failures not tied to a specific set of slaves. Are those still happening? 
jbergstroem		I think we're improving on all fronts :+1:  
jbergstroem		We haven't seen disconnects for a long while. Very good news! Lets close this and sleep better at night, hoping it won't be reopened. I guess the bad part is that we didn't really identify a few of the issues as to why they disconnected, but it's pretty much established that lowering the ping interval between master and slaves did a lot. 
maclover7		I tried this out on ci.nodejs.org, and it works -- check out nodejs/node#14998 and nodejs/node#16703.
rvagg		Where will this end up being run? I guess we need to add the worker IP addresses to the bot whitelist? We do all of the work on test-packetnet-ubuntu1604-x64-1 and test-packetnet-ubuntu1604-x64-2, if we're able to restrict this work to those then that'd be easy I suppose. 
maclover7		> We do all of the work on test-packetnet-ubuntu1604-x64-1 and test-packetnet-ubuntu1604-x64-2  Yep -- all (26) of the builds so far have taken place on either of these two machines. These are the only two `jenkins-workspace` tagged machines, so we would only need to adjust the bot whitelist if more are added with the label. It _seems_ like the bot whitelist doesn't need to be edited as of right now since the requests all went through ok.
rvagg		great @maclover7, what are the next steps, do we merge this, add a github-based pipeline for it, add that plugin and wire it up?
maclover7		@rvagg yep, here are the next steps:  - merge this pr - create jenkins pipeline job backed by post-build-status-update.jenkinsfile (there is already a post-build-status-update job, imho we should just delete that and start from scratch, don't want anything edited in web interface to carry over) - add the plugin - go to linter job, and add new build step to call the pipeline before/after linting occurs, wrapped in a POST_STATUS_TO_PR parameter conditional step - let it run for a day or two, and if all goes well, add it to the remaining jobs
rvagg		ok, unless anyone @nodejs/build objects I'd be happy to push forward with @maclover7 on this plan today(ish)
refack		Do you know if there's a better way to report ‚ö†Ô∏è ? ![image](https://user-images.githubusercontent.com/96947/32405957-936f734e-c145-11e7-930b-3e6592f03562.png) these three sub-jobs are actually yellow in Jenkins. ![image](https://user-images.githubusercontent.com/96947/32405965-c10980c4-c145-11e7-897d-9f24a4740652.png) If there's no direct mapping IMHO they should be marked as green.
maclover7		updated @gibfahn   @refack Do you know what's making it be unstable? I think the issue is `parallel/test-async-wrap-uncaughtexception` or similar flaky tests failing? Do we normally consider those builds to be "passing"?
refack		> Do we normally consider those builds to be "passing"?  Yes, we map Jenkin's "unstable" status to runs that fail on only known flaky tests. Which means "failures are unrelated to current PR", so AFAICT they should be reported as passing for the PR (could have a "some tests were flaky" message)
maclover7		@refack Ok -- I've added a commit where if the status is unstable then a different message will be shown, but it will be shown as passing in the GitHub UI.
maclover7		Did a run of the jenkinsfile, and https://github.com/nodejs/node/pull/14998's `test/aix` has a green light but a message saying "flaky tests failed".
apapirovski		Just found this and wanted to give props for the work! Thanks @maclover7 üôè üíØ
refack		Suggestion: I'd put the file in a new `/jenkins/pipelines/` directory
maclover7		Going to merge this in -- it looks like the plugin is already installed on ci.nodejs.org (yay!), so I just need to have write access to node-test-linter to start rolling this out :)
maclover7		Landed in 58019a51345a4a647643725420e4b02f3da69556, going to keep this PR open until node-test-linter is setup :)
refack		> Landed in 58019a5, going to keep this PR open until node-test-linter is setup :)  FYI since we don't do auditing or backporting, you can just use the Green button to "rebase merge" if the commits are good as is, or "squash merge" is you want to edit the commit message.
phillipj		Great job finding a way for us to get up-n-running again!
maclover7		Looks like everything is going okay right now... assuming no failures, would someone be able to set me up with access to configure the remaining node-test-* jobs? I can rollout the rest tomorrow or Monday, would be great to wrap this up.
gibfahn		>would someone be able to set me up with access to configure the remaining node-test-* jobs?  Hopefully done, let me know if I missed any.
maclover7		@gibfahn thanks! Here's the current upgrade status:  - [x] node-test-linter - [ ] node-test-commit-arm -- unused? - [x] node-test-commit-freebsd - [x] node-test-commit-linux - [x] node-test-commit-osx -- no permissions - [x] node-test-commit-plinux -- no permissions - [x] node-test-commit-smartos -- no permissions - [x] node-test-commit-windows-fanned -- no permissions - [x] node-test-commit-linux-fips - [x] node-test-commit-linuxone - [x] node-test-commit-arm-fanned - [x] node-test-commit-aix  Could use permissions on a few more jobs, and then not sure what should be done with `node-test-commit-arm`
refack		I added `nodejs*build` access to * node-test-commit-osx * node-test-commit-plinux * node-test-commit-smartos * node-test-commit-windows-fanned
maclover7		thanks @refack, updating now
maclover7		Settings updated -- going to close and move discussion back to #790.
MylesBorins		Great work!!!
phillipj		@maclover7 just noticed this in the bot logs:  ```bash 07:39:51.639Z ERROR bot: Got error when retrieving GitHub commits for PR (req_id=ec2629b5-a83f-4c5b-baa3-9408674e316c, pr=canary, job=test/linux, status=success)     err: {       "code": "400",       "status": "Bad Request",       "message": "Invalid value for parameter 'number': canary"     }  ..  07:43:44.384Z ERROR bot: Got error when retrieving GitHub commits for PR (req_id=5b885da5-728a-4757-9d78-38bd0918c41e, pr=canary, job=test/linux-fips, status=success)     err: {       "code": "400",       "status": "Bad Request",       "message": "Invalid value for parameter 'number': canary"     } ```  Got any thoughts why some jobs seems to be posting `canary` as the pr-parameter, rather than a numeric value?
rvagg		That'd be from the scheduled daily jobs. They don't have an associated PR and aren't started from node-test-pull-request. In fact some collaborators (myself included) often just use node-test-commit or one of its children directly without giving a PR. So maybe just check if the PR value is numeric or not before posting to the bot?
refack		There's also `POST_STATUS_TO_PR` which the job now ignores (which IMHO is fine), so I'd say either check it or we should remove it.
maclover7		Yeah, I was worried this would happen. I tried locally to add a conditional step as a wrapper around triggering `post-build-status-update` jobs, but it seemed to execute after every shell block, so I didn't end up using that on ci.nodejs.org.
maclover7		Hmm, actually, what do you think about passing the `POST_STATUS_TO_PR` variable to `post-build-status-update`, and that will figure out if github-bot should be contacted or not?
gibfahn		+1 on checking `POST_STATUS_TO_PR == "true"`, and +1 on checking `PR.isInteger()`  @maclover7 correct me if I'm wrong, but I think https://github.com/nodejs/build/pull/981 will work.
mhdawson		@gibfahn can you go ahead and do that ?
gibfahn		Added to both test machines, PR is #728  CI run on AIX to make sure everything's still kosher: https://ci.nodejs.org/job/node-test-commit-aix/5980/  _**EDIT:**_ CI is green
rvagg		It turns out that I didn't press _record_ on this one, also, does someone have a link to the doc for this one? I can't find it! 
jbergstroem		The maintenance has been completed.  
jbergstroem		Update: we're having some routing issues on the machines. They will be up and running shortly. 
jbergstroem		Closed roughly two hours after the routing issues were posted. 
jbergstroem		After giving it some thought, lets stick test/release second and make it required, similar to what we have today: `$project-$flavour-$vm_provider-$os$os_version-$architecture-$unique_id` 
rvagg		Looks good to me, brackets are a problem so I like your last version better, I'm not too fussed as long as those donating the resources get named. We do have one awkward situation of hosting being provided by a different party to the donater‚Äîmost of the ARM machines are hosted by NodeSource but have been donated by third-parties, I've put names on the end of the string when doing this in jenkins in order to give them full credit. 
mhdawson		Last one looks good to me as well. 
orangemocha		Sounds great. Only knit, as @rvagg pointed out, is about $vm_provider: where does it live/who sponsors are not necessarily the same. I think it's worth keeping both pieces of information. 
jbergstroem		- Do we actually need project? - I'm a bit undecided regarding architecture -- some providers call 64-bit x64, others x86_64 or amd64. How about we use what's passed to node instead (x86, x64, armv7, etc)? This way we can map that to specific stuff in each ansible script. 
rvagg		> Do we actually need project?  maybe not, it's just more characters eh?  > architecture ... How about we use what's passed to node  +1 
joaocgreis		@jbergstroem +1 for dropping the project. About the architecture, `process.arch` is only `arm` in all the raspberries, so how would we distinguish pi1 from pi2? Is there some command somewhere that we can use for this? `tools/utils.py` distinguishes the two, but not `armv7` and `armv8`.   Regarding hosting/donator: should we just split `$vm_provider` into `$hosting-$donator`, omitting one if they are the same?  So:  ``` $flavour-$hosting-$donator-$os$os_version-$architecture-$unique_id test-nodesource-continuationlabs-raspbian-wheezy-arm-6 test-msft-win10-x64-1 ```  I assume we separate `$os-$os_version` if `$os_version` is not numeric. 
jbergstroem		@joaocgreis the problem with optionally omitting parts makes it harder to parse. 
jbergstroem		Ok, I'm going to proceed on using this convention for all bots forward. I'll open a PR for documenting this. The "final" form is currently as follows:  `$type-$provider{_$sponsor}-$os-$architecture-$uniqueidentifier` - $type: outlines the purpose of this machine. These types are the one's I found so far: release,test,lint,benchmark - $provider: hosting and/or hardware provider. Currently: nodesource, digitalocean, rackspace, joyent, softlayer, osuosl, voxer, ns (nodesource?) and scaleway - $sponsor: optional if hardware is provided by someone specific - $os: name of os including major (we're targeting toolchain abi here, no?) version, for instance freebsd10, centos5, fedora23 and so on - $architecture: what architecture the host os/hardware runs. This might differ from the actual job (think cross-compilation). The options here are available through `./configure --help` in node. - $unique_id: should always be 1 unless two or more similar machines are deployed on the same provider. If so, increment it.  Edit: Some names have some descriptions in there (xgene, rpi, etc) -- do we care about this? Should it live in a description instead? 
orangemocha		Not sure if like the purpose in there, as a machine can have multiple purposes and the purpose can easily change.  We might also want to capture few optional machine characteristics, like rpi, or the compiler version (eg VS2015).  We also have the option of using labels, which is probably a good idea since there are so many facets in this categorization.  So what if we used the machine name for immutable characteristics, and rely on labels for the rest?  Purpose would be a better fit for a label, I think. 
jbergstroem		I haven't seen too many examples of a machine switching (or having multiple) purposes. Release machines are the obvious off limits-reusage, lint machines are intentionally deployed on lesser hardware and some test machines are deployed with specific toolchains. All in all, I'd rather improve tooling for quicker deploys of hardware (which I am) instead of doing multi-purpose stuff.  I think labels could help for stuff like toolchain and convenience stuff (rpi, etc). I personally don't care if we're testing on a rpi or rpi2 but there should definitely be information somewhere. It's just irrelevant from a deployment perspective when setting the bot up. 
jbergstroem		Closing this. We've already more than halfway through the slaves in terms of redeploying/renaming. 
bnoordhuis		Any Intel CPU without AVX or just Skylake and above?  I have an Ivy Bridge system that I'm reasonably sure has no AVX I could use to investigate.  Is https://github.com/nodejs/node/issues/12691#issuecomment-298019749 the test case to try?
shigeki		The issue was only confirmed on Intel Celeron N3350.   I guess that it depends on the product name such as Celeron or Atom rather than the micro architecture name such as Skylake or Ivy Bridge. But it is worth while testing it other cpu if it does not support AVX.  https://en.wikipedia.org/wiki/List_of_Intel_Celeron_microprocessors#.22Apollo_Lake.22_.2814_nm.29_2 shows that they do support up to SSE4.2 not AVX.   Usually low-end note books or note pads have Celeron or Atom cpu wiht Windows. We have to look for someone who can build and test Node on such machines. 
santigimeno		@shigeki I have access to a `Intel(R) Celeron(R) M processor 1.00GHz` that apparently has no AVX support. From the output of `cpuid`: `AVX: advanced vector extensions         = false`. Would this work for you? If that's the case, what code should I run?
shigeki		@santigimeno That's great! Could you try to run a sample code of https://github.com/nodejs/node/issues/12691#issuecomment-298019749 at first?   Edit: Any version of Node is okay.
santigimeno		@shigeki tested with `v7.9.0` and `v4.7.0` and output looks fine:  ``` sha1:  2aae6c35c94fcfb415dbe95f408b9ce91ee846ed sha256:  b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9 sha384:  fdbd8e75a67f29f701a4e040385e2e23986303ea10239211af907fcbb83578b3e417cb71ce646efd0819dd8c088de1bd ```
shigeki		@santigimeno Thanks, but it is sad for me that we cannot reproduce the issue. Do you run it on Windows? 
santigimeno		No, its running a linux
shigeki		@santigimeno If possible, can you run the test it on Windows?  
santigimeno		@shigeki unfortunately that's not going to be possible. I think I have another old box with a different processor that doesn't support `AVX` and that I could probably run Windows on. I'll report back if I can work out something.
shigeki		@santigimeno Thanks. The result that the testing on Linux is good is great help for me. 
refack		I have an old (7 year old) machine with a Celeron. I'll give it a try...
santigimeno		I have tested on a `Core 2 Duo T7250` that doesn't support `AVX` with nodejs `v4.8.2` (I couldn't install anything newer as it's running `Windows Vista`) and the results are the same: ``` sha1:  2aae6c35c94fcfb415dbe95f408b9ce91ee846ed sha256:  b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9 sha384:  fdbd8e75a67f29f701a4e040385e2e23986303ea10239211af907fcbb83578b3e417cb71ce646efd0819dd8c088de1bd ```
jbergstroem		For build infra purposes; can we emulate it by skipping flags and running qemu?
shigeki		@santigimeno Thanks for testing.  For summarizing the current results, it shows that   | model | release date | features | OS |test result | |-------|--------------|----------|----|-----------| | Intel Celeron N3350 |  Aug 30, 2016 | SSE-SSE4, AES-NI, SHA extension | Win | NG | | Intel Celeron N3350  |  Aug 30, 2016 | SSE-SSE4, AES-NI, SHA extension | Win | OK (with disabling to use all AVXs in OpenSSL) | | Intel Core i7-6700K | Aug 5, 2015 | SSE-SSE4, AES-NI, AVX, AVX2 | Win10 | OK | | Intel Core 2 Duo T7250 | Sep 5, 2007 | SSE-SSESE3 | Win Vista | OK | |Intel(R) Celeron(R) M processor 1.00GHz| unknown | unknown | Linux | OK |  It has a possibility that SHA extension on Windows causes the issue.   
shigeki		It was confirmed that the issue is caused by building openssl with masm (Microsoft Assembler).  Thanks for your corporation for testing. 
rvagg		some input from @nodejs/platform-solaris might be helpful here, I've also noticed how slow it is and assumed we had slower machines there now but apparently not! The speed-up when enabling parallel tests was also unimpressive: https://github.com/nodejs/node/pull/4476 
jbergstroem		Seems to be better now. I'll close this but will keep monitoring. 
piccoloaiutante		other than what @gibfahn highlighted LGTM. 
mhdawson		@gibfahn updated
gibfahn		New format looks much better. One nit otherwise LGTM.
mhdawson		Fixed nit and landed as bc693e4b60de506af447620a3fed176718a473b3
rvagg		lgtm I think 
jbergstroem		Works properly now:  **python 2.4**  ``` TASK [Bootstrap | Check python version] **************************************** ok: [test-digitalocean-centos5-x64-3] => {"changed": false, "rc": 0, "stderr": "", "stdout": "Python 2.4.3\r\n", "stdout_lines": ["Python 2.4.3"]}  TASK [Bootstrap | Install ansible requirements] ******************************** ok: [test-digitalocean-centos5-x64-3] => {"changed": false, "rc": 0, "stderr": "", "stdout": "Loaded plugins: fastestmirror\r\nDetermining fastest mirrors\r\n * base: mirror.symnds.com\r\n * extras: mirrors.tripadvisor.com\r\n * updates: centos.mirror.constant.com\r\n\rbase <snip> ```  **python 2.6**  ``` TASK [Bootstrap | Check python version] **************************************** ok: [test-digitalocean-centos5-x64-3] => {"changed": false, "rc": 0, "stderr": "", "stdout": "Python 2.6.8\r\n", "stdout_lines": ["Python 2.6.8"]}  TASK [Bootstrap | Install ansible requirements] ******************************** skipping: [test-digitalocean-centos5-x64-3] => {"changed": false, "skip_reason": "Conditional check failed", "skipped": true} ``` 
jbergstroem		lets get it done: 1. python: lets use `raw` similar to what I did [here](https://github.com/nodejs/build/pull/228/files#diff-15eb5f14a10ef05c622b8abda9477763R15) 2. upstream ansible issue. we need to call yum 3. (solved) 4. just add libselinux-python to deps 
jbergstroem		FYI - I've pushed this to my ansible 2.0 branch (set to be out early january). It streamlines package management which makes things much simpler. I have a pre-check that bootstraps python if its missing for hosts that needs it. 
maclover7		Closing this in favor of #962 since it seems like we're at the stage of removing support for this, not adding it üòÑ 
rvagg		@refack did you mean to close this? could be to do with me messing with swapping ubuntu1610 for ubuntu1710, also changing the fedora config. Might need to restart Jenkins sometime to flush out all of these config changes that I've made on disk and requested a "reload" from the running Jenkins instance‚ÄîI don't really trust that Jenkins does the right thing with that.
jbergstroem		This will also "fix" a few of our flaky tests related to routing since there's no magic concerning `LOCALHOST` and `BROADCAST` any longer (side effect of freebsd jails). 
jbergstroem		We could explore setting up a jail in one of the freebsd slaves. Thinking this would hurt automating through ansible. 
rmg		Some _very_ minor nits...  Assuming the decision has already been made and this is just the code for it, LGTM. 
jbergstroem		> @rmg said: > Assuming the decision has already been made and this is just the code for it  We were told by Voxer to sunset the current resources. We've now got two 64-bit FreeBSD bots running at DO and I've asked [twitter](https://twitter.com/eatingfoodbrb/status/658457815346905089) for more. 
joaocgreis		I did not run this, but the changes LGTM. 
jbergstroem		Update: we're going to go with one vm at joyent and one at digital ocean. We're still short on 32-bit vm's which is unfortunate.  
Trott		Is there an expected/planned date for this? (I have no idea how these cutovers work.) 
jbergstroem		@Trott sorry, pretty much based on effort which varies. I'll see if I can squeeze this tomorrow -- just need to free resources on DO ahead. 
Trott		@jbergstroem Thanks. I don't mean to rush you or anything. There's no urgency. 
jbergstroem		@trott all good, I just agree that "as soon as possibly" is a shitty eta :) 
jbergstroem		I've now retired the remaining bots that are either run in jails or having different network setups. 
jbergstroem		sorry, didn't mean to close. 
jbergstroem		Rebasing since I already did a few things [here](3a65bc81b9228eb1de921fb078921dca2925aa32). 
jbergstroem		I've retired our Voxer freebsd bots as of today. This PR kind of took another direction but I just want to make sure we get all changes in there. 
jbergstroem		Updating topic since this issue floated a bit more south than expected.  
jbergstroem		I'm going to close this since I'm venturing further away from the original topic. I'll create a new PR which will illustrate a few of the simplified things we have in the pipe for our ansible stack. 
mhdawson		LGTM 
rvagg		LGTM but is the Python stuff necessary?  
jbergstroem		@rvagg python 2.x isn't installed by default:  ``` bash $ ssh -i .ssh/nodejs_build_test root@1.2.3.4 The authenticity of host '1.2.3.4 (1.2.3.4)' can't be established. ECDSA key fingerprint is SHA256:ANpSOekuS2L/ak1qzyRaqEMcnH48VEKk8KuciIyrqvM. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '1.2.3.4' (ECDSA) to the list of known hosts. Welcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-22-generic x86_64)   * Documentation:  https://help.ubuntu.com/  0 packages can be updated. 0 updates are security updates.   root@test-digitalocean_jbergstroemtest-ubuntu1604-x64-2:~# python -V The program 'python' can be found in the following packages:  * python-minimal  * python3 Try: apt install <selected package> ``` 
jbergstroem		Apologies for the silence. Seeing ho we are migrating over to the "new" [ansible setup](https://github.com/nodejs/build/tree/master/ansible) it would be great if you could focus your efforts there instead.
maclover7		Closing due to long inactivity -- thank you for your contributions!
joaocgreis		Thanks! Perhaps `git` can be removed in line 14 of `ansible-vars.yaml`. 
rvagg		done 
gibfahn		Hmm, works manually, might be a one off issue.
rvagg		I've done a cleanup and reboot just in case it's a state problem
jbergstroem		The benefit of monit is that it's already packaged in most distributions that needs it (centos5, debian7 et al). Lets just use that instead. 
kenperkins		@rvagg I believe we do at https://github.com/iojs/build/blob/master/setup/www/ansible-playbook.yaml#L73-73  Command comes from https://github.com/iojs/build/blob/master/setup/www/host_vars/iojs-www.tmpl#L3-3 
Fishrock123		Would we have access to the releases.json from inside the `gulp build` with this? (I assume not.) We might need to have it sync the json into the build container before build. 
rvagg		Yep, can do, how about we copy it in to the root of the project directory and name it `releases.json`, would that do? Of course you could just pull it from the website during build too and that way the build is portable. 
therebelrobot		+1 to that @rvagg. That would make it simplistic to pull into `gulp build`. @Fishrock123 so long as the `releases.json` is up to date, that would work for dynamic linking to the latest, shouldn't it? 
snostorm		+1 to just pulling it down via HTTP (with a local fallback, perhaps), although if we do "inject it" I'd prefer to overwrite it at `source/releases.json` before the build runs.  This way this publish script can also be triggered when new versions hit the web.  How long would this take to switch over? I'm all for the change happening any time. 
rvagg		What I might do is put the above script into a stand-alone file so it can be run by the webhook and also as a result of an update to index.json, so the site gets rebuilt when you push new stuff _and_ when there's new stuff available to it.  How about I set up a test version of this so you can play with it first and then we can go live later. Will try and do it this weekend. 
snostorm		I'll try to prep a proof-of-concept of using https://iojs.org/dist/index.json for version numbers, download links, etc. when I get a chance. I have an unrelated PR in queue which should make this pretty easy to switch over.  Tracking on https://github.com/iojs/website/issues/284 
rvagg		let's use a test branch on iojs/website, maybe actually call it "test" and we can webhook/pull from that instead of master for testing purposes 
snostorm		Sure. Should we wait for the "new stuff" or just test randomly as-is without the new .json hooks?  Edit: I guess we can do "both". Start with the current master, then push the new stuff when ready, and see if it all works as expected :) 
rvagg		yes, both! 
snostorm		Looks like I need to destroy an old "test" branch first, ha. Edit: done. Master === Test 
snostorm		https://github.com/iojs/website/pull/287 is now sitting on https://github.com/iojs/website/tree/test  It'll generate a `releases.html` from the latest https://iojs.org/dist/index.json as well as using it to power the version info seen throughout the site. 
snostorm		Is there going to be a domain where we can see the site running from the test branch?  https://github.com/iojs/website/pull/291 is now the latest PR for this work. 
rvagg		/cc @kenperkins   `/etc/github-webhook.json` now contains a line for the `test` branch of the website:  ``` json       "event": "push",       "match": "ref == \"refs/heads/test\" && repository.full_name == \"iojs/website\"",       "exec": "/home/iojs/build-site.sh" ```  and `/home/iojs/build-site.sh` looks like this:  ``` sh #!/bin/bash  set -e  pidof -s -o '%PPID' -x $(basename $0) > /dev/null 2>&1 && \   echo "$(basename $0) already running" && \   exit 1  cd /home/iojs/website.github.test git reset --hard git clean -fdx git fetch origin git checkout origin/test  docker pull iojs:latest docker run \   --rm \   -v /home/iojs/website.github.test/:/website/ \   -v /home/iojs/.npm:/npm/ \   iojs:latest \   bash -c " \     addgroup iojs --gid 1000 && \     adduser iojs --uid 1000 --gid 1000 --gecos iojs --disabled-password && \     su iojs -c ' \       npm config set loglevel http && \       npm config set cache /npm/ && \       cd /website/ && \       npm install && \       node_modules/.bin/gulp build \     ' \   "  rsync -avz --delete --exclude .git /home/iojs/website.github.test/public/ /home/iojs/www.test/ ```  There is also a new line in `/etc/crontab`:  ``` *  *    * * *   iojs    /home/iojs/check-build-site.sh ```  And `/home/iojs/check-build-site.sh` is:  ``` sh #!/bin/bash  indexjson=/home/dist/public/release/index.json indexhtml=/home/iojs/www.test/en/index.html buildsite=/home/iojs/build-site.sh  [ $indexjson -nt $indexhtml ] && $buildsite ```  _(Note the hardwiring of en/index.html, this is a little unfortunate and brittle.)_  There's also an nginx site setup for `test.iojs.org` on the same host that uses /home/iojs/www.test/ as the docroot so stick this in your /etc/hosts and you'll pick it up:  ``` 104.236.136.193 test.iojs.org ```  i.e. pushing to the `test` branch on the website repo _or_ creating a new release will now check out website/test and run the full `npm install` and `gulp build` process. You just need to be patient because the dep tree is nontrivial and takes a while to install, so give it a minute or so.  @snostorm you can give it a go now if you want to push any changes to `test` to see if they build properly. 
snostorm		Thanks @rvagg, everything sounds great. I have a push (hopefully) generating right now. Minor change where I lined up some out of date content (i18n mostly) between test..master.  I'll try to sneak in a build change which adds a copy of the en/index.html to /index.html. Awkward indeed. 
snostorm		>  so give it a minute or so  How long should we expect? Like 10+ minutes?. If so, things might be working as expected (it has been about that long.) If shorter, then it was a no-go on the auto push working.  Sanity checks: - http://test.iojs.org/en/index.html should show `Frequent[L]y Asked Questions` under `Nightly releases..` (spell fix, adds the **L**.) - http://test.iojs.org/es/index.html should show `FAQ` in the nav, not `Preguntas Frecuentes`.  I'll check back later myself to see if it finished. 
rvagg		@snostorm you pushed to the `test` branch? 
rvagg		should only be ~1 min, the time it takes to `npm install` and `grunt blahblah`, pretty fast but the dep list is massive (typical node app, sheesh!) 
snostorm		Hmm. https://github.com/iojs/website/blob/test/public/es/index.html#L29 is confirming the changes I'm looking for but not showing on the test subdomain. I can try a 2nd push. 
snostorm		"Test" is now at https://github.com/iojs/website/commit/87500531fffc18c2248fe02855e0e141dcedd069 ...  verifiable by view source and seeing if `<!DOCTYPE html>` is lowercase. Still waiting as before. 
rvagg		@snostorm it looks correct to me .. can you double-check for me and maybe test some more? It could also be that I promoted the armv6l build of 1.6.2 which _should_ have triggered a build too.  I'll leave it to you to make a call on whether this is solid enough to move to the next step and do `master` 
snostorm		No go still. I did a test ~14 hours ago, just remembered to check on it. Looks like the Github hook isn't quite working. 
snostorm		Any luck confirming the lack of build-on-push? FYI the build code of test is also in master now. Maybe we should try our luck targeting iojs/website#master vs #test? 
rvagg		figured it out -- github-webhook uses `exec()` and it was spewing way too much data to buffer and blowing the `maxBuffer`, so I've switched to `spawn()` and am streaming stdout and stderr to the log file now. the `test` branch appears to be working on commit now at least! 
snostorm		Great. I saw on the 1.6.3 release this is now live for master as well? Hurray for less "published public" type commits. 
Fishrock123		So where is iojs.org currently coming from? The `test` branch? 
rvagg		`master`, I turned off the `test` stuff completely, you can delete that branch in fact 
Fishrock123		hmmm, the website isn't updating after the 1.6.4 release.. 
rvagg		a previous build was blocking it, I had to manually kill it but it was paused somewhere in the build process -- perhaps in the gulp build? I'm manually running the build again now. 
jbergstroem		Unfortunately not. V8 are using C++11 features and have no intention of moving the other way. I suggest you install a newer gcc or clang using your package manager. See https://github.com/iojs/io.js/issues/1052 for more of the same. 
fengmk2		OK, thanks! 
jbergstroem		This is awesome. Will check soon! 
phillipj		Not sure how / where to mention the required vars from the secrets repo tho  
jbergstroem		That's fine, we can add that somewhere else. 
mhdawson		My first thought after reading was that we might want to split into 2 docs.  The first which has the links and general information which would go in the readme.md under setup and then a separate one with the specific example of how to do it with Vagrant.  This might avoid any confusion about Vagrant being used/required.   
jbergstroem		I'm more interested in the Vagrant guide seeing how that will make more people able to test the playbooks! I have a 'general' ansible guide (at least in terms of using it) at my branch: https://github.com/jbergstroem/build/tree/feature/refactor-the-world/ansible#ansible-scripts-for-the-nodejs-build-group-infrastructure 
phillipj		Thanks for reviewing, sorry about the late reply!  In an attempt to make this guide more Vagrant related, I removed what could be seen as an intro to ansible and the use of ansible in the project.  PTAL 
mhdawson		I think the content looks good now, but should probably be named something other than README.md.  I'm thinking the README.md for the setup directory should be an overview and point to docs like the "general" ansible guide and this one about testing locally. 
jbergstroem		I'm with @mhdawson -- let's reference it from README. 
phillipj		I won't push the merge button yet, as there hasn't been a clear LGTM from anyone yet, tho feedback has been positive and there's no outstanding issues afaik. 
refack		Cross-ref: https://github.com/nodejs/build/pull/841#issuecomment-325977049
gibfahn		Yeah that seems like an issue with the machine and the test. We should probably default to 2/4 (2 on the arm machines seems more reasonable).  Changed the test from `$JOBS` to `${JOBS:-2}`. I note there's also a comment from @jbergstroem saying that he changed it from `$JOBS` to `2`, so that seems like a reasonable default. 
gibfahn		Also @nodejs/build is `$JOBS` normally defined in the machine config? I was going to add it as an environment variable, but I couldn't see it in any of the others so wasn't sure that was correct.
joaocgreis		It's in the start.sh script, that way it can be defined independently for each host. - [`ansible/host_vars/README.md`](https://github.com/nodejs/build/blob/0707dc27d88eb3ee43cb761736d341b211d149e0/ansible/host_vars/README.md) - Example for systemd: [`ansible/roles/jenkins-worker/templates/systemd.service.j2#L17`](https://github.com/nodejs/build/blob/0707dc27d88eb3ee43cb761736d341b211d149e0/ansible/roles/jenkins-worker/templates/systemd.service.j2#L17)  The default shouldn't hurt.
gibfahn		I guess we need to work out why `JOBS` wasn't set on the machines.
gibfahn		So it turns out `$JOBS` wasn't undefined, it was actually set to `0`.  You can see by running this in the script console (e.g. [here](https://ci.nodejs.org/computer/test-mininodes-ubuntu1604-arm64_odroid_c2-2/script))  ```groovy println System.getenv("JOBS")  // Returns: 0 ```  I've added this to the job, but the real fix would be to make sure that JOBS is never set to 0 in the start.sh script.  ```bash [ "$JOBS" = 0 ] && export JOBS=2 ```
yoshuawuyts		This is mostly because of the mobile builds right? E.g. the tunnel to SauceLabs fails. If that's the case I'm not sure moving off Travis is as much of a benefit as investing in a more reliable tunnel.  On Mon, Mar 20, 2017, 10:39 Matteo Collina <notifications@github.com> wrote:  > Hey folks, do you think it might be possible to do this? What would it > require from our part? > At this point we are evaluating for options, as we can't get a full CI run > go green because of our current infrastructure. > > Our current Travis setup is quite articulated: > https://github.com/nodejs/readable-stream/blob/master/.travis.yml. And we > would still need #655 <https://github.com/nodejs/build/issues/655>. > > cc @nodejs/streams <https://github.com/orgs/nodejs/teams/streams> > @addaleax <https://github.com/addaleax> > > ‚Äî > You are receiving this because you are on a team that was mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/657>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/ACWlemRA73tBWWx2ktorzhm2hTnz2q7cks5rnklegaJpZM4MiMb5> > . > 
mcollina		@yoshuawuyts There is also the fact that we are unable to trigger/manage the Travis integration, because of the lacking permissions in GitHub OAuth. At this point, we can't even restart a single job that failed.  I would like to know how much effort is needed for this, and then we can make our call.
yoshuawuyts		Ah yeah, super fair.  On Mon, Mar 20, 2017 at 11:15 AM Matteo Collina <notifications@github.com> wrote:  > @yoshuawuyts <https://github.com/yoshuawuyts> There is also the fact that > we are unable to trigger/manage the Travis integration, because of the > lacking permissions in GitHub OAuth. At this point, we can't even restart a > single job that failed. > > I would like to know how much effort is needed for this, and then we can > make our call. > > ‚Äî > You are receiving this because you were mentioned. > > > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/657#issuecomment-287719607>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ACWlekHjQeiEe9Z1wAvVdtlu8x-CFGyqks5rnlHKgaJpZM4MiMb5> > . > 
gibfahn		Related to: https://github.com/nodejs/readable-stream/issues/265  There is now [documentation](https://github.com/nodejs/build/blob/master/doc/process/jenkins_job_configuation_access.md) for the process of adding new job types to the CI. @mhdawson has been handling it for other projects so far (currently citgm, node-report, and node-inspect).  Basic answer is that we have a reasonably good process in place now (covered in that document). We'd set up a team called `streams-admins` (or `readable-stream-admins`, whichever you'd like). Then we'd create jobs in [ci.nodejs.org](ci.nodejs.org) and give that team access.  There are currently two types of jobs, `streams-continuous-integration` would just pull a tar from nodejs.org (for when you're testing a change to readable-stream). Then there'd be a `streams-smoker` job (for when you're testing that a change to Node doesn't break readable-stream). If this doesn't work we can always do something different, but not rebuilding node every time means a lot less stress on the CI (and much quicker CI runs).  @phillipj is working on triggering CI runs with the GitHub bot through a comment by a collaborator for that repo in https://github.com/nodejs/github-bot/pull/128, and also on having the job status reporting (i.e. what we have in node core).  The only thing I don't think we'll get that Travis has is automatically running CI on PRs (for obvious reasons). However we can run on all the platforms that Node supports, which is a big plus.
gibfahn		@mcollina and to answer your question, it's not very much effort.
mcollina		@gibfahn do you think it would be hard to have a localtunnel instance running somewhere as well? #655   Or maybe the servers have a public reacheable IP address, so there is no issue?
mcollina		@gibfahn how would you setup those browsers runs that currently go to SauceLabs via localtunnel? As separate environments?  We are happy to pull Node.js, we just need to test this on all the Node.js versions we currently support, meaning 0.8, 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7 (and soon 8). We do not care too much about multiple-platform at this point, as the source code is all JS.  `readable-stream` is a module on NPM (probably the **most** downloaded), so we need to test on what's released, so no building core stuff.  This seems quite different from what you proposed.
gibfahn		>We are happy to pull Node.js, we just need to test this on all the Node.js versions we currently support, meaning 0.8, 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7 (and soon 8). We do not care too much about multiple-platform at this point, as the source code is all JS. > >readable-stream is a module on NPM (probably the most downloaded), so we need to test on what's released, so no building core stuff. > >This seems quite different from what you proposed.  Yeah, that's what I meant by `streams-continuous-integration`, pull a tarball from https://nodejs.org/dist/ and run your tests on it. See for example the equivalent for [node-report](https://ci.nodejs.org/view/All/job/nodereport-continuous-integration/). Different how?  >how would you setup those browsers runs that currently go to SauceLabs via localtunnel? As separate environments?  No idea, it's quite possible that your tests are doing something unusual that we'd need to make special arrangements for, could you give more details? cc/ @jbergstroem 
gibfahn		You'd also set up a pipeline that runs the same job with `0.8, 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7` as parameters, so you could just kick that off.
mcollina		> Yeah, that's what I meant by streams-continuous-integration, pull a tarball from https://nodejs.org/dist/ and run your tests on it. See for example the equivalent for node-report. Different how?  I didn't understood :D.  Part of our current CI test suite opens up a server, exposed that to the world via localtunnel, then it has saucelabs connect there with various browsers, and that is published. This is not really working well for us on the localtunnel dependency.
gibfahn		@mcollina Discussed this in the Build WG meeting (https://github.com/nodejs/build/issues/660). Answer was that the localtunnel webserver shouldn't be a problem.  >This is not really working well for us on the localtunnel dependency.  Whatever the better solution for this is, we should be able to accomodate on the build machines. Is there an issue somewhere to discuss alternatives for this? I'd like to understand in more detail.
mcollina		Let's start with setting up `localtunnel`, as it is causing massive issues even on Travis.yml #655. How can I help?
gibfahn		Let's first start with getting some Jenkins jobs set up and giving you (plural) access. I'll raise an issue to do that.
gibfahn		Working on this with @mcollina and @piccoloaiutante in the Collab Summit, will update with progress.  #### Subtasks:  1. Job to run on specific node version 2. Set up `streams-admins` and give them access 2. Pipeline to call the job with different versions of Node 3. Deal with the browser tests
maclover7		@mcollina @gibfahn Just did a run of https://ci.nodejs.org/view/All/job/readable-stream-pipeline/ and it passed -- can this issue be closed?
mcollina		Yes we can!
Trott		I totally get it if this has to wait until it's not a weekend somewhere, so feel free to ignore, but just in case this is the only thing that's keeping it from getting attention: @nodejs/build 
rvagg		on it
rvagg		problems on all 3 of them! I can't bring them back online and am going to have to physically remove them and resolve whatever is going on, we've been having trouble with at least one of them for a while now but having all 3 goes down is a bit suspicious although perhaps explained by operating system updates. I've taken them out of node-test-linux-arm for now and will hopefully have them back online in the next few days.
refack		And now... the two `ubuntu1604-arm64` workers are down https://ci.nodejs.org/label/ubuntu1604-arm64/ @rvagg anyway to know if it's a "real" issue? that is if something new in the code causes the builds/tests to crash the machines?  P.S. last active run already looks wonky https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/11209/console
refack		Not completely related, but there are two Windows jobs with > 2GB logs * https://ci.nodejs.org/job/node-test-binary-windows/10196/RUN_SUBSET=1,VS_VERSION=vs2015,label=win2012r2/ * https://ci.nodejs.org/job/node-test-binary-windows/10197/RUN_SUBSET=0,VS_VERSION=vs2015,label=win2012r2/  
Trott		node-test-commit-arm seems to be having real problems...  https://ci.nodejs.org/job/node-test-commit-arm/11219/console  ```console Started by upstream project "node-test-commit" build number 11472 originally caused by:  Started by upstream project "node-test-pull-request" build number 9425  originally caused by:   Started by user Rich Trott [EnvInject] - Loading node environment variables. Building on master in workspace /var/lib/jenkins/jobs/node-test-commit-arm/workspace  > git rev-parse --is-inside-work-tree # timeout=10 Fetching changes from the remote Git repository  > git config remote.origin.url https://github.com/nodejs/node.git # timeout=10 Fetching upstream changes from https://github.com/nodejs/node.git  > git --version # timeout=10 using GIT_SSH to set credentials   > git fetch --tags --progress https://github.com/nodejs/node.git +refs/heads/*:refs/remotes/origin/* +refs/pull/14536/head:refs/remotes/origin/_jenkins_local_branch # timeout=20  > git rev-parse refs/remotes/origin/_jenkins_local_branch^{commit} # timeout=10  > git rev-parse refs/remotes/origin/refs/heads/_jenkins_local_branch^{commit} # timeout=10 Checking out Revision 1766db0f07098a4fc19271b00dda2cebab2c6f16 (refs/remotes/origin/_jenkins_local_branch) Commit message: "test: refactor test-vm-new-script-new-context"  > git config core.sparsecheckout # timeout=10  > git checkout -f 1766db0f07098a4fc19271b00dda2cebab2c6f16 Using 'Changelog to branch' strategy. Cleaning workspace  > git rev-parse --verify HEAD # timeout=10 Resetting working tree  > git reset --hard # timeout=10  > git clean -fdx # timeout=10 Triggering node-test-commit-arm ¬ª ubuntu1604-arm64 FATAL: null java.lang.NullPointerException 	at java.util.HashSet.<init>(HashSet.java:118) 	at hudson.model.ParametersAction.shouldSchedule(ParametersAction.java:228) 	at hudson.model.Queue.scheduleInternal(Queue.java:615) 	at hudson.model.Queue.schedule2(Queue.java:585) 	at hudson.matrix.MatrixConfiguration.scheduleBuild(MatrixConfiguration.java:512) 	at hudson.matrix.DefaultMatrixExecutionStrategyImpl.scheduleConfigurationBuild(DefaultMatrixExecutionStrategyImpl.java:247) 	at hudson.matrix.DefaultMatrixExecutionStrategyImpl.run(DefaultMatrixExecutionStrategyImpl.java:159) 	at hudson.matrix.MatrixBuild$MatrixBuildExecution.doRun(MatrixBuild.java:364) 	at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:534) 	at hudson.model.Run.execute(Run.java:1728) 	at hudson.matrix.MatrixBuild.run(MatrixBuild.java:313) 	at hudson.model.ResourceController.execute(ResourceController.java:98) 	at hudson.model.Executor.run(Executor.java:405) Notifying upstream projects of job completion Finished: FAILURE ```
Trott		https://ci.nodejs.org/job/node-test-commit-arm/ Build History looks like this...  <img width="321" alt="screen shot 2017-07-31 at 7 58 30 pm" src="https://user-images.githubusercontent.com/718899/28807496-abfdb094-762a-11e7-94e2-d88c3380394b.png"> 
rvagg		on it
rvagg		3 out of the 4 of them had problems, I think it might be related to the datacenter those are in as I've had more success with one of their Asian datacenters.  They're restarted, updated and I've cleaned out the workspace of the centos7 boxes because that seemed to be causing problems on one of them. https://ci.nodejs.org/job/node-test-commit-arm/11222/
mikeal		+1 
bengl		+1! (but for http://nodejs.org/dist) 
rvagg		/dist is an alias for /download/release now fwiw 
fhemberger		Can we please add the rsync endpoint for /dist? https://github.com/nodejs/nodejs.org/issues/662#issuecomment-211986085 
jbergstroem		I can look at doing this if others in the build group agree. 
rvagg		So I had a thought (:boom:): one of the blockers we have here is hesitation about exposing an unencrypted endpoint while we're trying to move everything to TLS (http://nodejs.org will be fully redirected soon). So, how about we be upfront about this and stand up a host named `insecure.nodejs.org` and expose stuff via http, rsync and even maybe ftp (does anyone still use ftp?). The name points to our opinions re security, you opt in to the risk by putting "insecure" in your scripts. We can isolate it by having the primary server push updates to it so the new host doesn't have any special access to anything. When we go fully `https` for nodejs.org, having an `http` fallback host if people absolutely need it might be a nice touch. 
jbergstroem		@rvagg I like it. How about adding a http header about expiration as well; putting a final nail in the coffin to plaintext stuff? `X-Service-Will-Expire: 2020-01-01` (or the rsync equivalent in message) 
jbergstroem		(as well as serving the same message with /index.html) 
mhdawson		Would unencrypted.nodejs.org work ?  Not to bikeshed but insecure implies a lot more to me than just not protected by encryption which I think would be the main difference. 
jbergstroem		Just a quick update: We have an endpoint set up for testing; pm me on irc (jbergstroem) or shoot me an email (bugs at bergstroem dot nu) if you want to give it a whim. If everything works as intended we'll make this public shortly.  
rvagg		@jbergstroem do we have it syncing in cron yet? last I checked that wasn't enabled. 
jbergstroem		@rvagg nope, still todo. Also, adding it to the deploy script for 0day. 
jbergstroem		We're nearly there. I've gotten som feedback and based on that I'd like to suggest extending our EOL/reconsider to 2022 (+2y). I'll land the sync script we'll run from both deploy and cronjob this week. 
jsumners		I've been noticing an issue where the release files will sometimes not be available. For example, `rsync://unencrypted.nodejs.org/nodejs/release/v6.9.2/` may have the `docs` directory but nothing else. This definitely causes a problem when the `--delete` option is passed to rsync.
jbergstroem		@jsumners the rsync routine isn't fully up to par. Apologies about that. When we moved web servers a few things changed which made it break. It will obviously work pristine once deemed 'production' worthy, but that doesn't mean we shouldn't fit it as soon as possible. I'll have a look in within a day or so; just got mixed up amongst my other todo's.
jbergstroem		@jsumners hey; I've redeployed it now. It should be populating and be up to speed pretty soon; updating dns as we speak. If you could verify that things look better in the hour or so, that'd be cool!  Edit: update. Almost done with initial rsync. I moved from cron to systemd timers (yes, i know -- sigh) to get better visibility of running state. Should be done in 5 minutes and it should sync every 15.
jsumners		@jbergstroem permissions are broken:  ``` [jsumners@node] ~/ $ rsync rsync://unencrypted.nodejs.org/nodejs/ drwxrwxr-x           8 2016/11/17 13:02:06 . drwxr-xr-x          46 2017/01/03 05:10:02 chakracore-nightly drwxr-xr-x           2 2015/08/30 04:29:25 next-nightly drwxr-xr-x         492 2017/01/03 13:05:02 nightly drwx------           2 2017/01/03 16:05:29 rc drwx------           2 2017/01/03 16:05:29 release drwx------           2 2017/01/03 16:05:29 test ```
jbergstroem		@jsumners that's because rsync hasn't finished processing those folders just yet. Thanks for checking!
jbergstroem		Looks like it's done: ``` NEXT                         LEFT       LAST                         PASSED       UNIT                         ACTIVATES Tue 2017-01-03 22:45:00 UTC  10min left Tue 2017-01-03 22:30:01 UTC  4min 20s ago rsyncmirror.timer            rsyncmirror.service ```
jsumners		@jbergstroem looking good over here. I'll keep an eye on it.
ofrobots		Would it be possible to allow rsync over ssh?
jbergstroem		@ofrobots that means we'd have to figure out some kind of ssh jail, right?
ofrobots		Yup. Has this been considered? I haven't seen this discussed anywhere.
jbergstroem		@ofrobots http://nginx.org/en/docs/http/ngx_http_autoindex_module.html
gibfahn		Using https://nodejs.org/dist/index.json was suggested in the meeting (#737), @ofrobots let us know whether that works for you.
jbergstroem		[slight hijack here] According to documentation, autoindex is compiled in by default, so I'll see what we can do here.
jsumners		The latest release currently available on the rsync server is v8.6.0. It looks like it isn't being updated again.
joaocgreis		The timer that synchronizes the server was not enabled by default, and was disabled by a reboot. Should be fixed now.
jsumners		üëç
AndersTrier		I pulled a copy to mirrors.dotsrc.org which is hosted at Aalborg University, Denmark: http://mirrors.dotsrc.org/nodejs https://mirrors.dotsrc.org/nodejs ftp://mirrors.dotsrc.org/nodejs rsync://mirrors.dotsrc.org/nodejs  Fell free to send some traffic our way. We sync every 6 hours.  @ofrobots @jbergstroem  If you want to tunnel rsync over ssh, have a look at the rrsync restricted shell: https://www.samba.org/ftp/unpacked/rsync/support/rrsync https://www.guyrutenberg.com/2014/01/14/restricting-ssh-access-to-rsync/
ghinks		# More than a short description required. Firstly thank you Gibson for the talk at Node Interactive. I have taken the time to re-watch the talk you gave on the build wg a few times. Mostly to make sure that I am not missing things. I also watched the Working group on you tube on the 24th Oct.   Some of the things said during the during the talk need more attention if you want to be able to on board people and get them involved. I want to get involved and help but as mentioned in the talk, it is hard to know where to begin.  The mission is to  - build - test - benchmark - release - host   ( from the talk given)  But it is not self evident how these things are actually being achieved. What I think would really help is to take the most plain vanilla case and show how  - show how it is being built - describe what and how it is tested - how it is bench marked ...  I think a concrete set of steps for one example would work wonders.  I would be delighted to get involved and even write these descriptions but there would have to be a bit of hand holding to begin with as I really could not work out how these things are done from the Build WG github repo and the talk. But I'd be delighted to try.    
gibfahn		cc/ @abigsmall 
ghinks		I would still like to get involved in some way with the process. 
rvagg		mm, I'd like to see a fresh new intake of folks that can help keep our infra alive and happy (at a minimum), we seem to be getting a bit short. Do we think docs is what it's going to take? Are we too opaque right now?
ghinks		I'll be honest. Most folks kind of like the more **glamorous stuff**. But I actually quite enjoy seeing a good process ticking along. I think that docs alone will not do it. In most cases it is about attracting and keeping the correct people.  I heard Gibson talk in Vancouver and thought I could **get involved** here as it would not distract from my day time node duties. But after listening to the call **I could not fathom a way into what was going on**.  So I think it is about how you **on-board** people. They (we) are here to help but do not know how to start helping.  **I am still grateful. Node is putting my kids through school!**  Just reach out to me and I'll try to help  
rvagg		Cool, we need to sort this out because I know there are lots of people out there like yourself (and me!) that enjoy this kind of thing and it can be as casual as you need it to be.
maclover7		Largely solved by https://github.com/nodejs/build/blob/master/doc/services.md, closing for now
refack		I'd rather we keep this open. Seems to me that `services.md` is only part of the information that needs to be documented.
maclover7		@refack what else needs to be included?
refack		From the OP * ~permission groups~ * job scripts stuck in Jenkins (refack: improve visibility of job logic) * /ansible/ is future -> /setup/ is past * ~bot / email /~ website ~/ CI~ * ~CI test vs release~  Maybe some stuff should be added to the onboarding doc, but IMO this task is not completed, and closing the issue will "sweep that under the rug"
mhdawson		Cleaned up /tmp so should be ok now.  I know that we had prior issues with citgm filling up tmp will talk to @gibfahn to see if we think this was a left over of that or something else. 
gibfahn		If it's the same problem we had last time (#560) then the problem is caused by citgm. I thought that'd been fixed, but perhaps it hasn't yet. The way to tell is that most of the space is used by the folders listed in https://github.com/nodejs/build/issues/560#issuecomment-272867766.  We'll try to fix it in citgm anyway, because it's a good idea to have the temp directory in the workspace, but it might be worth increasing /tmp anyway.
joaocgreis		@seishun I'm not very familiar with the Linux hosts on CI, but I can do some guesswork from what I see. Let's hope I can help more than harm.  From your description, I'd say that you're right and either the Ansible script is broken because it was never even run, or it worked some time in the past (does wheezy-backports remove packages?).  @jbergstroem made a big effort to refactor and move the scripts from `setup/` to `ansible/`, but there are still platforms that were never tested, and it looks like you found one of them. This is probably because all the Raspberry Pis were deployed before, or the scripts have not been refactored yet.  Looking at the old `setup/` folder, I see 3 candidates: - `armv7-wheezy`: this is for some armv7 machines we had that were not Raspberry Pis. I believe @rvagg recently moved them to Scaleway, so the ssh config section is outdated. - `debian-wheezy-gcc`: looking at the files inside, this is only Docker. - `raspberry-pi`: now this folder has many `debian7-arm` so this is probably the one that works (or worked at some point).  The README inside `raspberry-pi` mentions that the `debian7` machines use `raspbian-2015-05-07`. I found this about a slightly older version on SO: https://raspberrypi.stackexchange.com/a/27968 . So, gcc-4.8 is already there and does not need the backports repo.  So, that makes sense for how the current `debian7` machines were deployed. I don't know why the `ansible/` folder has that backports repo, but I'd guess it was copied from somewhere else and never tested or meant just as a placeholder for something better later. I also don't know why we support the Raspberry Pi 1 if there is no maintained OS for it.  @seishun if you want to work on https://github.com/nodejs/build/issues/762, it looks like there is some urgency to it, just changing the old scripts in `setup/` should do, and we can run those. This would not be an option if you were adding a completely new platform or the `ansible/` scripts already worked well for it. If you want to get more involved, refactoring `setup/raspberry-pi/` to `ansible/` would be great and very welcome (but no small job).
seishun		Thanks, that makes sense.  It seems Ubuntu 14.04 is another platform where the `ansible/` scripts haven't been tested, since it tries to use `systemd` instead of `upstart` (and fails). Do you know how Ubuntu 14.04 machines are being deployed now? `setup/ubuntu14.04`?  It would be great to have a list of platforms along with their currently used method of deployment.  >If you want to get more involved, refactoring setup/raspberry-pi/ to ansible/ would be great and very welcome (but no small job).  I think this requires detailed knowledge about how it should work, which I don't have (yet). I'd rather entrust this to someone with more experience.
joaocgreis		I have already fixed Ubuntu 14.04: https://github.com/nodejs/build/pull/735 . The only thing left needed for that PR to land is one final test with the changes that were made during review. @seishun if you run it and it works well, let me know and I'll land it.  To answer your question: I used that PR to deploy the machine that is building node-chakracore nightlies. So, since it was not used before, all machines currently in CI have been deployed with `setup/ubuntu14.04`. 
seishun		@joaocgreis I see. I assume once that PR lands, all further deployments to Ubuntu 14.04 will be done using the scripts under `ansible`, so changes relevant to #762 should be done there rather than in `setup/ubuntu14.04`, correct?
joaocgreis		@seishun that is correct.
seishun		@joaocgreis I got it working on Ubuntu 14.04 (see https://github.com/nodejs/build/pull/797), now I'm looking into Centos 6. It looks like the `ansible/` scripts haven't been tested on Centos 6 because I get the following error:  ``` TASK [baselayout : disable sftp] ******************************************************************************************************* fatal: [test-softlayer-centos6-x64-1]: FAILED! => {"changed": false, "failed": true, "msg": "Aborting, target uses selinux but python bindings (libselinux-python) aren't installed!"} ```  If I install libselinux-python manually, then the following happens:  ``` TASK [baselayout : repo : add scl devtoolset] ****************************************************************************************** fatal: [test-softlayer-centos6-x64-1]: FAILED! => {"changed": false, "failed": true, "msg": "missing required arguments: name"} ```  Judging by the comments in the file, it seems `ansible/roles/baselayout/tasks/partials/repo/centos6.yml` is unfinished.  How would you suggest to proceed with the Centos 6 changes? I don't feel qualified to finish the refactoring.
jbergstroem		@seishun i think the centos 6 fail is a result of a recent change in ansible. Lets create another issue for that.
seishun		@jbergstroem Alright, created an issue: https://github.com/nodejs/build/issues/801.
rvagg		I think this is solved now via #1199 and #1204
mhdawson		Ok ran test job here: https://ci-release.nodejs.org/job/iojs+release-s390/2/nodes=rhel72-s390x-release/console  and it built ok, but it looks like it needs additional keys/setup as a release machine to be able to publish the binaries.  @jbergstroem  can you do the final configuration required for the release machine ?   Its https://ci-release.nodejs.org/computer/release-linuxonecc-rhel72-s390x-1/ and currently has the test key on it. 
jbergstroem		I can do the last bits this week. 
jbergstroem		@mhdawson I just tried logging into it with the test key but wasn't able to. Is it perhaps non-root? 
mhdawson		my bad I'd missed the step of adding the test key.  Should be ok now. 
mhdawson		Ok Nightly built ok here:  https://nodejs.org/download/nightly/v6.4.1-nightly20160825723fa9637c/ 
mhdawson		Ran CITGM on binary from nightly on one of our internal RHEL 7.1 linuxOne machines and things look ok. 
mhdawson		Ok need to guard so that only builds on 6.x and higher for linuxOne.  This seems to do the trick:  ``` if [[ $ARCH =~ s390x && ${NODE_VERSION%%.*} -lt "6" ]]; then   echo "Not building $disttype on linuxOne slave as only supported on v.6x and later"   exit 0 fi ```  @jbergstroem that look ok with you ?  If so you ok with me adding that to the release build script and then adding linuxOne to the set of machines we build for ?  
jbergstroem		@mhdawson LGTM; perhaps change message to (typo) `...as only supported on v6.x and later` 
jbergstroem		@mhdawson also, how about simplifying the expansion to `${NODE_VERSION:0:1}`? 
mhdawson		Ok incorporate comments and added to the regular release CI job.  Will check tomorrow to see how nightlies went. 
mhdawson		Did a check on the nightlies  using CITGM on our local linuxOne RHEL71 boxes and I don't see anything unexpected that is not marked as flaky.  Once we have a stable that goes out, I'll sniff check that and then close this issue. 
mhdawson		Ok linuxOne was included in the 6.7.0 release and quick check that I could uncompress and get to prompt indicates its ok.  Closing. 
jbergstroem		Meh. I've seen that before as well. My take is that we're going to have a hard time to find a universally installed compression algorithm that is as efficient. If it works for the linux kernel and most linux distributions, it works for me.   If data validation is a bigger concern to you, using git and/or .tar.gz is likely a better choice.
maclover7		Closing for now as something that would probably be nice to have, but seems to not currently be within our means. If someone wants to tackle this, please feel free to reopen.
mhdawson		Landed as a20315683b4dc093d43187f6d1510147a1493f07
mhdawson		I believe 1 and 2 should run on the full matrix as well.  I've been talking to @yhwang who is going to help get some testing in place for the shared library case, but a first step of a job doing compilation would be good.  I wish there was a way other than just cloning all of the existing jobs (as there are a lot of them) to get coverage across the full matrix.
refack		Maybe this could be parameterized?
mhdawson		@refack do you need anything more than us creating a job you can start with ? 
joaocgreis		@refack I believe this will be good to build upon: https://ci.nodejs.org/view/All/job/refack-node-test-library/ can be set to run periodically when done.  At some time it might be a good idea to include a job (or multiple) in test-commit to test some of this, so I also cloned the linuxone job so you can take a look and perhaps explore this. https://ci.nodejs.org/job/refack-test-test-commit/  Let me know if you have trouble accessing.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
refack		Superseded by https://github.com/nodejs/build/pull/992 and related Jenkins work
rvagg		fwiw I used https://github.com/redbo/cloudfuse a bit when transferring the nodejs.org server to its new location. It has copies of a lot of the important data from the old server. I haven't used it since but we could start using it for things like jenkins backups. There is a limitation about permissions, however, I don't think you can store that kind of metadata on the rackspace cloud files.  I'd really love to get backups for Jenkins and nodejs.org off DigitalOcean and into some kind of object store or other unmanaged storage space. 
joaocgreis		- [ ] Add `-Dhudson.slaves.ChannelPinger.pingInterval=2` to `<arguments>` in `jenkins.xml`, to support Azure slaves. 
rvagg		@joaocgreis is this why we haven't been seeing so many failures of late? can you explain why this worked? 
joaocgreis		@rvagg yes, the connection to Azure slaves should be fixed now. Azure has a load balancer that closes connections after 4 minutes of inactivity, and the default ping interval of Jenkins is 5 minutes. The Linux slave I created has `ClientAliveInterval 120` set explicitly in `sshd_config`, that's why ssh works so well. (Ref: https://github.com/nodejs/build/issues/232) 
Trott		@nodejs/build @jbergstroem Should this remain open?
gdams		@jbergstroem can this be closed? I think that the jenkins worker playbooks are now complete
jbergstroem		Closed.  
mhdawson		Do you have an example of the failures ?, PPC Be release machine seems to be running into what looks like memory issues as well.   Different runs fail at a different points.  One example:  Traceback (most recent call last):   File "../../tools/js2c.py", line 601, in <module>     main()   File "../../tools/js2c.py", line 597, in main     options.js)   File "../../tools/js2c.py", line 558, in JS2C     metadata = BuildMetadata(prepared_sources, sources_output, native_type)   File "../../tools/js2c.py", line 497, in BuildMetadata     "sources_declaration": SOURCES_DECLARATION % ToCArray(source_bytes),   File "../../tools/js2c.py", line 51, in ToCArray     return textwrap.fill(joined, 80)   File "/usr/lib/python2.7/textwrap.py", line 366, in fill     return w.fill(text)   File "/usr/lib/python2.7/textwrap.py", line 338, in fill     return "\n".join(self.wrap(text))   File "/usr/lib/python2.7/textwrap.py", line 326, in wrap     chunks = self._split(text)   File "/usr/lib/python2.7/textwrap.py", line 189, in _split     chunks = filter(None, chunks)  # remove empty chunks MemoryError 
jbergstroem		I just saw the kernel OOM going wild; not necessarily jenkins output. 
jbergstroem		Just adding a point made by @jasnell thats known but perhaps not considered if someone is thinking about this: the main difference between test and release jobs is that release jobs also builds against a full ICU. 
jbergstroem		While chatting with @mhdawson wrt the ppc be machine that saw similar issues we found out that the release job config in jenkins invokes `make -j $JOBS`. Since `$JOBS` is unset for the majority of the machines it will run in unlimited parallelism. This is likely the reason we get OOM's. We've set it to `-j 2` for now until we have a more robust `$JOBS` solution in place. 
jbergstroem		Closing, haven't seen the issue since we limited parallelism. 
gibfahn		Sounds like it just needs someone with access to do it. I think @mhdawson has access (not sure who else).
bnb		FWIW, wanted to say that admin access is needed for this. Unfortunately `nodejs` is taken, but it has very low activity and may be able to be gotten. I've discussed this in the past with @zibbykeaton of the Node.js Foundation, and we weren't able to land on anything concrete. That said, I'd very much like to see this get updated.  Going to tag @nodejs/community-committee on this, as I think it's an important thing for us to do as a community with a rather large amount of video content.
ZibbyKeaton		Yeah, this was something that I tried to solve awhile back, but never heard back from YouTube help - I will send them another note on this. Here's where I got stuck in case anyone knows how to solve this problem: I created a custom URL for the channel: http://www.youtube.com/c/nodejs-foundation, however when you search it uses the other URL and just automatically populates that URL vs our custom URL.  
refack		I just tested and https://www.youtube.com/nodejs-foundation works as well (and hence https://www.youtube.com/nodejs-foundation/live) but clicking on the channel name on any screen does bring up https://www.youtube.com/channel/UCQPYJluYC_sn_Qz_XE-YbTQ  From what I've seen on the youtube site that's the best we can do. I'm closing this issue for now, feel free to comment or reopen if I'm wrong
rvagg		argh, start on boot is so frustrating with systemd  lgtm 
jbergstroem		Merged in d5a44ca1a24efcc37fb2e876dedfde175dc2ab6a. 
piccoloaiutante		Just finished to listen the recorded version. I will come back for sure @jbergstroem and thanks for the right pronunciation of my name :-).  @joaocgreis in issue https://github.com/nodejs/build/issues/495 the other thing good for first contribution that you mentioned were:  - Generating  .rdp and/or .remmina files (I have a rough script for Remmina that can serve as a starting point). - Handle Windows secrets better - passwords are used only for Windows and currently the file we have is not parsable, so we can decide for something better that Ansible can pick from the secrets (currently I have a custom script to generate host_vars). - vcbuild.bat support for #387 , porting what is currently in the Makefile (introduced initially in nodejs/node#4704 ).  What do you think is the best to go on?
joaocgreis		@piccoloaiutante It's up to you, which one do you prefer? Since now you have some experience with ansible, there is a more complex task that you might also consider: adapt the current Windows ansible scripts to the new ansible scripts at https://github.com/jbergstroem/build/tree/feature/refactor-the-world/ansible . It would be great to find a way to include Windows there without using a `if(windows)` everywhere and while keeping it clean. We can discuss this more if you want to take it! Actually it would be better to do the first two points you mentioned (remmina/rdp and secrets) after this. However the last point (v8 tests) is independent of this and would also be nice to have.
piccoloaiutante		@joaocgreis perfect i'll go on with the first point "Generating .rdp and/or .remmina files (I have a rough script for Remmina that can serve as a starting point)."
piccoloaiutante		@joaocgreis can you share with me your script for generating .remmina file?
joaocgreis		@piccoloaiutante Sorry for the delay, I was out the last few weeks. Here it is: https://gist.github.com/joaocgreis/208e87942cba416467490947d7385efa This is just a prototype that I made to get me through, feel free to improve! Developing this in Johan's branch would be great, even if you do not port our current Windows setup scripts. There is already ssh config file generation there, this would be the equivalent for Windows. Thanks a lot!
piccoloaiutante		Excellent @joaocgreis, thanks 
piccoloaiutante		@joaocgreis I've started looking at your script and my ideal goal would be to arrive to write it as a plugin for Ansible like the one @jbergstroem has written for ssh (https://github.com/jbergstroem/build/blob/feature/refactor-the-world/ansible/plugins/library/ssh_config.py) and used here (https://github.com/jbergstroem/build/blob/feature/refactor-the-world/ansible/playbooks/write-ssh-config.yml). How does it sounds to you?
joaocgreis		@piccoloaiutante sounds great! Some ideas/issues come to my head:  - The authentication method for all servers currently in [inventory](https://github.com/jbergstroem/build/blob/feature/refactor-the-world/ansible/inventory.yml) is SSH keys. We have the key in secrets and who wants to use Johan's generator will only have to drop the key in secrets. But for Windows it'll have to be passwords, so you'll have to adapt this a little. In that gist above there is an example with the exact same format that we have in secrets now, but I don't think it's optimal, and we can change it. Perhaps use a yaml format instead (like the inventory) that Ansible can parse. Or save the full `host_vars` for Windows in secrets, but that will have a lot of redundant information. Ideally, usernames/passwords and ports (just to be extra safe) should be stored in secrets and everything else in the inventory. - Azure hosts are the only ones that use DNS, everything else uses IP addresses. The IP address will change if we have to turn off the server for some reason, so use DNS wherever possible (is it possible to replace IP by DNS in the inventory?). 
piccoloaiutante		@joaocgreis that's what i'm currently working on:  https://gist.github.com/piccoloaiutante/575d5e9d17729ebbca3a1d64f1956e8f it's still based on you old schema. Next step will be to move it to the yams format that Ansible can parse.
piccoloaiutante		@joaocgreis i've updated the gist with a working version of the plugin, with yaml file. It works and produces one remmina file per server. The only thing that I need before raising a PR is an example of encrypted server_list with certificate so I can be sure that the gpg decryption works smoothly. Here is an example of server list in yaml: ```yaml test-example-win10-x64-1:   ip: node-win10.example.org   port: 55443   username: Administrator   password: password1111  test-example-win10-x64-2:   ip: node-win10-2.example.org   port: 55443   username: Administrator   password: password1111 ```
joaocgreis		@piccoloaiutante that's fantastic! You can use dotgpg to encrypt the file I sent you. Here are some instructions:  * Install with `sudo gem install dotgpg` * Edit a file with `dotgpg edit <filename>` if you have access * Show the contents of a file without editing with `dotgpg cat <filename>` if you have access (same as `gpg --decrypt <filename>`) * New GPG keys can be added by users who already have access by using `dotgpg add` and pasting in a new key (exported using `gpg --armor --export <email>` or `dotgpg key`) * See the [dotgpg](https://github.com/ConradIrwin/dotgpg) page or `dotgpg --help` for more information  Let me know if this is not enough or if you have trouble setting it up. Feel free to take this to IRC or email if you prefer.
piccoloaiutante		@joaocgreis I've tested it and it worked with a dotgpg created file. I've opened a pr against @jbergstroem `refactor-the-world` build branch (https://github.com/jbergstroem/build/pull/2).  As next task I have: ``` vcbuild.bat support for https://github.com/nodejs/build/issues/387,  porting what is currently in the Makefile (introduced initially in nodejs/node#4704 ). ```  Is it still the next priority? We can discuss about in next WG meeting.
joaocgreis		@piccoloaiutante thanks for your PR, I'll take a look! The work to support v8 tests in Windows is still relevant, please go ahead if you'd like to take that one.
kunalspathak		I have started looking into `v8 tests in Windows `. 
kunalspathak		Update: On my local machine, i see 7 failures. Do we ignore it? Who is the right contact to check this about? I am on [5.8-lkgr](https://chromium.googlesource.com/v8/v8.git/+/2f06375912a79b55603cc75306738101921ce80a).  ```cmd [06:35|% 100|+ 27873|-   7]: Done ```
joaocgreis		cc @ofrobots , if it's better to open an issue let us know where. Thanks!
ofrobots		@kunalspathak can you paste which tests failed?  /cc @nodejs/v8 
fhinkel		@joaocgreis Do you mind opening an issue on the [V8 tracker](https://bugs.chromium.org/p/v8/issues/entry)? I don't have a Windows machine and it's easier for us to track progress if we have an issue. Thanks! üôá 
kunalspathak		@ofrobots - I did a reset to my environment. I will share that failures with you as soon as I run into it again.
kunalspathak		@ofrobots  - Here are the details of test failures.  Command ran ``` python tools\run-tests.py --arch=x64 --mode=release --quickcheck --no-presubmit --shell-dir=out/x64.release --junitout ./v8-tap.xml ``` Test failures:  <details><code><pre> D:\git\node-chakracore\deps\v8>python tools\run-tests.py --arch=x64 --mode=release --quickcheck --no-presubmit --shell-dir=out/x64.release --junitout ./v8-tap.xml >>> Running tests for x64.release Traceback (most recent call last):   File "tools\run-tests.py", line 932, in <module>     sys.exit(Main())   File "tools\run-tests.py", line 720, in Main     code = Execute(arch, mode, args, options, suites)   File "tools\run-tests.py", line 748, in Execute     raise Exception('Could not find shell_dir: "%s"' % shell_dir) Exception: Could not find shell_dir: "out/x64.release"  D:\git\node-chakracore\deps\v8>start.  D:\git\node-chakracore\deps\v8>python tools\run-tests.py --arch=x64 --mode=release --quickcheck --no-presubmit --shell-dir=out.gn/x64.release --junitout ./v8-tap.xml >>> Running tests for x64.release === debugger/debug/debug-sourceinfo === ============ Stress 1/5 ============ D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253: Failure: expected <1608> found <1635>  Stack: Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at D:\git\node-chakracore\deps\v8\test\debugger\debug/debug-sourceinfo.js:85:1     throw new MjsUnitAssertionError(message);     ^ Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at D:\git\node-chakracore\deps\v8\test\debugger\debug/debug-sourceinfo.js:85:1 Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\d8.exe --test --random-seed=-1567808436 --stress-opt --always-opt --enable-inspector --allow-natives-syntax --nohard-abort --nodead-code-elimination --nofold-constants D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js D:\git\node-chakracore\deps\v8\test\debugger\test-api.js D:\git\node-chakracore\deps\v8\test\debugger\debug/debug-sourceinfo.js === debugger/debug/debug-sourceinfo === D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253: Failure: expected <1608> found <1635>  Stack: Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at D:\git\node-chakracore\deps\v8\test\debugger\debug/debug-sourceinfo.js:85:1     throw new MjsUnitAssertionError(message);     ^ Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at D:\git\node-chakracore\deps\v8\test\debugger\debug/debug-sourceinfo.js:85:1 Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\d8.exe --test --random-seed=-1567808436 --enable-inspector --allow-natives-syntax --nohard-abort --nodead-code-elimination --nofold-constants D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js D:\git\node-chakracore\deps\v8\test\debugger\test-api.js D:\git\node-chakracore\deps\v8\test\debugger\debug/debug-sourceinfo.js === mjsunit/type-profile/collect-type-profile === {} {"519":["Object","number","string","number"],"526":["undefined","boolean","undefined","undefined"],"723":["Object","number","string","number"]} D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253: Failure (function testFunction(param, flag) {   // We want to test 2 different return positions in one function.   if (flag) {     var first_var = param;     return first_var;   }   var second_var = param;   return second_var; } failed): expected: "{\"503\":[\"Object\",\"number\",\"string\",\"number\"],\"510\":[\"undefined\",\"boolean\",\"undefined\",\"undefined\"],\"699\":[\"Object\",\"number\",\"string\",\"number\"]}" found: "{\"519\":[\"Object\",\"number\",\"string\",\"number\"],\"526\":[\"undefined\",\"boolean\",\"undefined\",\"undefined\"],\"723\":[\"Object\",\"number\",\"string\",\"number\"]}"  Stack: Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at check_collect_types (D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:12:5)     at D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:40:1     throw new MjsUnitAssertionError(message);     ^ Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at check_collect_types (D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:12:5)     at D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:40:1 Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\d8.exe --test --random-seed=-1567808436 --nohard-abort --nodead-code-elimination --nofold-constants --type-profile --turbo --allow-natives-syntax D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js === mjsunit/type-profile/collect-type-profile === ============ Stress 1/5 ============ {} {"519":["Object","number","string","number"],"526":["undefined","boolean","undefined","undefined"],"723":["Object","number","string","number"]} D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253: Failure (function testFunction(param, flag) {   // We want to test 2 different return positions in one function.   if (flag) {     var first_var = param;     return first_var;   }   var second_var = param;   return second_var; } failed): expected: "{\"503\":[\"Object\",\"number\",\"string\",\"number\"],\"510\":[\"undefined\",\"boolean\",\"undefined\",\"undefined\"],\"699\":[\"Object\",\"number\",\"string\",\"number\"]}" found: "{\"519\":[\"Object\",\"number\",\"string\",\"number\"],\"526\":[\"undefined\",\"boolean\",\"undefined\",\"undefined\"],\"723\":[\"Object\",\"number\",\"string\",\"number\"]}"  Stack: Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at check_collect_types (D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:12:5)     at D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:40:1     throw new MjsUnitAssertionError(message);     ^ Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at check_collect_types (D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:12:5)     at D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js:40:1 Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\d8.exe --test --random-seed=-1567808436 --stress-opt --always-opt --nohard-abort --nodead-code-elimination --nofold-constants --type-profile --turbo --allow-natives-syntax D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js D:\git\node-chakracore\deps\v8\test\mjsunit\type-profile/collect-type-profile.js === mjsunit/tools/tickprocessor === === testProcessing-Default === ===== actual output: ===== Statistical profiling result from v8.log, (13 ticks, 4 unaccounted, 0 excluded).   [Shared libraries]:    ticks  total  nonlib   name       3   23.1%          /lib32/libm-2.7.so       1    7.7%          ffffe000-fffff000   [JavaScript]:    ticks  total  nonlib   name   [C++]:    ticks  total  nonlib   name       2   15.4%   22.2%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)       1    7.7%   11.1%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)       1    7.7%   11.1%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)       1    7.7%   11.1%  exp   [Summary]:    ticks  total  nonlib   name       0    0.0%    0.0%  JavaScript       5   38.5%   55.6%  C++       0    0.0%    0.0%  GC       4   30.8%          Shared libraries       4   30.8%          Unaccounted   [C++ entry points]:    ticks    cpp   total   name       2   40.0%   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)       1   20.0%    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)       1   20.0%    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)       1   20.0%    7.7%  exp   [Bottom up (heavy) profile]:   Note: percentage shows a share of a particular caller in the total   amount of its parent calls.   Callers occupying less than 1.0% are not shown.     ticks parent  name       4   30.8%  UNKNOWN        3   23.1%  /lib32/libm-2.7.so        2   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)        1    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)        1    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)        1    7.7%  ffffe000-fffff000        1    7.7%  exp  ===== expected output: ===== ===== File: D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor-test.default ===== Statistical profiling result from v8.log, (13 ticks, 2 unaccounted, 0 excluded).   [Shared libraries]:    ticks  total  nonlib   name       3   23.1%          /lib32/libm-2.7.so       1    7.7%          ffffe000-fffff000   [JavaScript]:    ticks  total  nonlib   name   [C++]:    ticks  total  nonlib   name       2   15.4%   22.2%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)       1    7.7%   11.1%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)       1    7.7%   11.1%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)       1    7.7%   11.1%  exp   [Summary]:    ticks  total  nonlib   name       0    0.0%    0.0%  JavaScript       5   38.5%   55.6%  C++       0    0.0%    0.0%  GC       4   30.8%          Shared libraries       2   15.4%          Unaccounted   [C++ entry points]:    ticks    cpp   total   name       2   40.0%   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)       1   20.0%    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)       1   20.0%    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)       1   20.0%    7.7%  exp   [Bottom up (heavy) profile]:   Note: percentage shows a share of a particular caller in the total   amount of its parent calls.   Callers occupying less than 1.0% are not shown.     ticks parent  name       3   23.1%  /lib32/libm-2.7.so       3  100.0%    LazyCompile: exp native math.js:41       3  100.0%      Script: exp.js        2   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)       2  100.0%    LazyCompile: exp native math.js:41       2  100.0%      Script: exp.js        2   15.4%  UNKNOWN       1   50.0%    LazyCompile: exp native math.js:41       1  100.0%      Script: exp.js        1    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)       1  100.0%    Script: exp.js        1    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)       1  100.0%    Script: exp.js        1    7.7%  ffffe000-fffff000        1    7.7%  exp       1  100.0%    LazyCompile: exp native math.js:41       1  100.0%      Script: exp.js   D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253: Failure: expected: [] found: ["line 0: expected <Statistical profiling result from v8.log, (13 ticks, 2 unaccounted, 0 excluded).\r> found <Statistical profiling result from v8.log, (13 ticks, 4 unaccounted, 0 excluded).>\n","line 1: expected <\r> found <>\n","line 2: expected < [Shared libraries]:\r> found < [Shared libraries]:>\n","line 3: expected <   ticks  total  nonlib   name\r> found <   ticks  total  nonlib   name>\n","line 4: expected <      3   23.1%          /lib32/libm-2.7.so\r> found <      3   23.1%          /lib32/libm-2.7.so>\n","line 5: expected <      1    7.7%          ffffe000-fffff000\r> found <      1    7.7%          ffffe000-fffff000>\n","line 6: expected <\r> found <>\n","line 7: expected < [JavaScript]:\r> found < [JavaScript]:>\n","line 8: expected <   ticks  total  nonlib   name\r> found <   ticks  total  nonlib   name>\n","line 9: expected <\r> found <>\n","line 10: expected < [C++]:\r> found < [C++]:>\n","line 11: expected <   ticks  total  nonlib   name\r> found <   ticks  total  nonlib   name>\n","line 12: expected <      2   15.4%   22.2%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)\r> found <      2   15.4%   22.2%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)>\n","line 13: expected <      1    7.7%   11.1%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)\r> found <      1    7.7%   11.1%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)>\n","line 14: expected <      1    7.7%   11.1%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)\r> found <      1    7.7%   11.1%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)>\n","line 15: expected <      1    7.7%   11.1%  exp\r> found <      1    7.7%   11.1%  exp>\n","line 16: expected <\r> found <>\n","line 17: expected < [Summary]:\r> found < [Summary]:>\n","line 18: expected <   ticks  total  nonlib   name\r> found <   ticks  total  nonlib   name>\n","line 19: expected <      0    0.0%    0.0%  JavaScript\r> found <      0    0.0%    0.0%  JavaScript>\n","line 20: expected <      5   38.5%   55.6%  C++\r> found <      5   38.5%   55.6%  C++>\n","line 21: expected <      0    0.0%    0.0%  GC\r> found <      0    0.0%    0.0%  GC>\n","line 22: expected <      4   30.8%          Shared libraries\r> found <      4   30.8%          Shared libraries>\n","line 23: expected <      2   15.4%          Unaccounted\r> found <      4   30.8%          Unaccounted>\n","line 24: expected <\r> found <>\n","line 25: expected < [C++ entry points]:\r> found < [C++ entry points]:>\n","line 26: expected <   ticks    cpp   total   name\r> found <   ticks    cpp   total   name>\n","line 27: expected <      2   40.0%   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)\r> found <      2   40.0%   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)>\n","line 28: expected <      1   20.0%    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)\r> found <      1   20.0%    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)>\n","line 29: expected <      1   20.0%    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)\r> found <      1   20.0%    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)>\n","line 30: expected <      1   20.0%    7.7%  exp\r> found <      1   20.0%    7.7%  exp>\n","line 31: expected <\r> found <>\n","line 32: expected < [Bottom up (heavy) profile]:\r> found < [Bottom up (heavy) profile]:>\n","line 33: expected <  Note: percentage shows a share of a particular caller in the total\r> found <  Note: percentage shows a share of a particular caller in the total>\n","line 34: expected <  amount of its parent calls.\r> found <  amount of its parent calls.>\n","line 35: expected <  Callers occupying less than 1.0% are not shown.\r> found <  Callers occupying less than 1.0% are not shown.>\n","line 36: expected <\r> found <>\n","line 37: expected <   ticks parent  name\r> found <   ticks parent  name>\n","line 38: expected <      3   23.1%  /lib32/libm-2.7.so\r> found <      4   30.8%  UNKNOWN>\n","line 39: expected <      3  100.0%    LazyCompile: exp native math.js:41\r> found <>\n","line 40: expected <      3  100.0%      Script: exp.js\r> found <      3   23.1%  /lib32/libm-2.7.so>\n","line 41: expected <\r> found <>\n","line 42: expected <      2   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)\r> found <      2   15.4%  v8::internal::Runtime_Math_exp(v8::internal::Arguments)>\n","line 43: expected <      2  100.0%    LazyCompile: exp native math.js:41\r> found <>\n","line 44: expected <      2  100.0%      Script: exp.js\r> found <      1    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)>\n","line 45: expected <\r> found <>\n","line 46: expected <      2   15.4%  UNKNOWN\r> found <      1    7.7%  v8::internal::HashTable<v8::internal::StringDictionaryShape, v8::internal::String*>::FindEntry(v8::internal::String*)>\n","line 47: expected <      1   50.0%    LazyCompile: exp native math.js:41\r> found <>\n","line 48: expected <      1  100.0%      Script: exp.js\r> found <      1    7.7%  ffffe000-fffff000>\n","line 49: expected <\r> found <>\n","line 50: expected <      1    7.7%  v8::internal::JSObject::LookupOwnRealNamedProperty(v8::internal::String*, v8::internal::LookupResult*)\r> found <      1    7.7%  exp>\n","line 51: expected <      1  100.0%    Script: exp.js\r> found <>\n"]  Stack: Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at PrintMonitor.finish (D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:378:5)     at driveTickProcessorTest (D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:412:6)     at testProcessing (D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:440:28)     at D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:442:3     throw new MjsUnitAssertionError(message);     ^ Error     at new MjsUnitAssertionError (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:31:16)     at failWithMessage (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:253:11)     at fail (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:270:12)     at assertEquals (D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js:341:7)     at PrintMonitor.finish (D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:378:5)     at driveTickProcessorTest (D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:412:6)     at testProcessing (D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:440:28)     at D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js:442:3 line 5: unknown code state: undefined line 6: unknown code state: undefined line 7: unknown code state: undefined line 8: unknown code state: undefined line 10: unknown code state: undefined line 11: unknown code state: undefined Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\d8.exe --test --random-seed=-1567808436 --nohard-abort --nodead-code-elimination --nofold-constants -e TEST_FILE_NAME="D:\\git\\node-chakracore\\deps\\v8\\test\\mjsunit\\tools/tickprocessor.js" D:\git\node-chakracore\deps\v8\tools\splaytree.js D:\git\node-chakracore\deps\v8\tools\codemap.js D:\git\node-chakracore\deps\v8\tools\csvparser.js D:\git\node-chakracore\deps\v8\tools\consarray.js D:\git\node-chakracore\deps\v8\tools\profile.js D:\git\node-chakracore\deps\v8\tools\profile_view.js D:\git\node-chakracore\deps\v8\tools\logreader.js D:\git\node-chakracore\deps\v8\tools\tickprocessor.js D:\git\node-chakracore\deps\v8\test\mjsunit\mjsunit.js D:\git\node-chakracore\deps\v8\test\mjsunit\tools/tickprocessor.js === inspector/debugger/script-parsed-hash === Hash received: 1C6D2E82E4E4F1BA4CB5762843D429DC872EBA18 Hash received: EBF1ECD351E7A3294CB5762843D429DC872EBA18 [FAIL]: unknown hash 2E8017514061BC94478B46106FC9B5336DCD6908 Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\inspector-test.exe --random-seed=-1567808436 --nohard-abort --nodead-code-elimination --nofold-constants D:\git\node-chakracore\deps\v8\test\inspector\protocol-test.js D:\git\node-chakracore\deps\v8\test\inspector\debugger/script-parsed-hash.js === inspector/debugger/script-parsed-hash === Hash received: 1C6D2E82E4E4F1BA4CB5762843D429DC872EBA18 Hash received: EBF1ECD351E7A3294CB5762843D429DC872EBA18 [FAIL]: unknown hash 2E8017514061BC94478B46106FC9B5336DCD6908 Command: D:\git\node-chakracore\deps\v8\out.gn\x64.release\inspector-test.exe --random-seed=-1567808436 --stress-opt --always-opt --nohard-abort --nodead-code-elimination --nofold-constants D:\git\node-chakracore\deps\v8\test\inspector\protocol-test.js D:\git\node-chakracore\deps\v8\test\inspector\debugger/script-parsed-hash.js [06:11|% 100|+ 20443|-   7]: Done </pre></code></details>
piccoloaiutante		As 6 PM EST it's a little bit late for me I won't be joining today's call. I'll post my update later during the day.
joaocgreis		I can't make it today. I have no progress to report, I haven't been around for the past couple of weeks and won't be for some more time (but please mention me if there's anything urgent).
mhdawson		@nodejs/build anybody else able to make it ?   
kunalspathak		I might not be able to make today's meeting. Updates from me: * test-v8 PR merged - https://github.com/nodejs/benchmarking/issues/124 * fixed cctest failure inclusion - https://github.com/nodejs/build/issues/769 * opened windows Jenkins CI job improvement - https://github.com/nodejs/build/issues/805 * investigated the arm failure . PR is merged https://github.com/nodejs/node/pull/14533
rvagg		I can make it, but not a great time to be focused, helping get kids off to school. 
mhdawson		I'm distracted as well, so I'd be happy to skip until next time.  My update:  * working on adding z/OS machines to CI and getting libuv tests running * working on connecting arm 6 machine for use as backup release machine * working with benchmarking team to setup more CI jobs * working with N-API team on setting up more CI coverage for N-API modules. *working with gadams on job to validate dowloads 
rvagg		OK then, I'll try and post an update here shortly. It's been a busy few weeks for me on infra and it's not looking like lessening any time soon.
piccoloaiutante		My update:  - been working on https://github.com/nodejs/build/pull/785 and implemented feedback over it - started working again on a new module for generating remmina file https://github.com/jbergstroem/build/pull/2#issuecomment-298462706 .Will move this work against current master.   I'll be on vacation soon so I won't be here for next standup on 22 of August.
gibfahn		Async meeting, so no minutes. Closing.
MylesBorins		LGTM 
jbergstroem		Merged in 52b3437382bea2b32035bab73f34fe91b90b8fc7. Closing. 
refack		/cc @jasnell @Trott @rvagg @joaocgreis 
refack		on `test-requireio_securogroup-debian7-arm_pi1p-1` I did the following: * `git fsck` in `/home/iojs/.ccache/node.shared.reference` * run the full `git fetch --no-tags file:...` in `/home/iojs/build/workspace/node-test-binary-arm` * went to https://ci.nodejs.org/computer/test-requireio_securogroup-debian7-arm_pi1p-1/ to bring it back on-line, but it was already.
joaocgreis		@refack was this just to make a log or is something still not good?
refack		Just to log it, and document the conversation (that's why I closed it)
gibfahn		>Just to log it, and document the conversation (that's why I closed it)  We should do this more often if possible, Github logs are much easier to search than IRC IMO.
bnoordhuis		I wasn't planning on maintaining  branches for v0.10 and v0.12 once we bump major.  Was anyone else and if so, why?
joaocgreis		- Added Node version 8 - Node versions 0.x will only run if node-gyp < 4 (so this will simply work if we ever create a v3.x branch)  I believe that takes care of it, so I'll close. Please reopen if I missed something.
refack		> I wasn't planning on maintaining branches for v0.10 and v0.12 once we bump major. Was anyone else and if so, why?  https://github.com/nodejs/node-gyp/pull/1212#issuecomment-305539119 If someone finds a new bug (especially on Windows, ["Windows user are not happy"](https://github.com/nodejs/node-gyp/issues/629) and all that Jazz) ü§∑‚Äç‚ôÇÔ∏è personally I have no problem helping them. So maybe LTS is the wrong comparison, more like "lazy-copy-on-write-maintenance"?
refack		@joaocgreis üëç Looking good https://ci.nodejs.org/job/nodegyp-test-pull-request/27/
refack		Re: https://github.com/nodejs/node-gyp/commit/75cfae290fee1791a23fa68820ae5dd841e93e14 If I jumped the gun, shoot me.
gibfahn		>So maybe LTS is the wrong comparison, more like "lazy-copy-on-write-maintenance"?  I think the right statement is "we might consider doing another release on a case-by-case basis".
misterdjules		Thank you!
jbergstroem		@gibfahn sounds about right! PR welcome.
joaocgreis		The `cores` directory is filling up the drives now. We have several smartos machines offline in CI because of this. I just cleaned up the `cores` directory in `test-joyent-smartos15-x64-1` to unblock CI, there was no worker for `smartos15`.  What about adding something like `find /home/iojs/cores/ -type f -mtime +3 -delete` to crontab to keep only 3 days of cores? (I don't know if this is an appropriate value, but given that this does not depend on the number or size of files, it makes sense to be conservative.)
jbergstroem		> @joaocgreis said: > What about adding something like `find /home/iojs/cores/ -type f -mtime +3 -delete` to crontab to keep only 3 days of cores?   I like this; perhaps up it to a week or so though.
misterdjules		@joaocgreis   > What about adding something like find /home/iojs/cores/ -type f -mtime +3 -delete to crontab to keep only 3 days of cores?  Can we bump that to 7 days? I'm thinking 3 days might be a bit short between the time a test failure happens and someone is able to get the corresponding core from one of the hosts.
joaocgreis		7 days sounds good, I'll add it.  EDIT: done.
refack		Why can't we have this for all machines? Ref: https://github.com/nodejs/node/issues/14014
juggernaut451		@refack @joaocgreis @maclover7 @misterdjules @gibfahn   is this still open? 
orangemocha		Yes, we are working on it right now... 
joaocgreis		Up and running! First real run: https://ci.nodejs.org/job/node-test-commit/617/  Still need to move the test logic to the Makefile, but only after this proves stable for some time. 
mscdex		:tada: :+1:  
orangemocha		First run took 29 minutes.. nice improvement! :smile: :+1:  
mscdex		I noticed addon tests are still compiled on device. The addon tests themselves take ~40 seconds to run, so that means it spent ~5.5 mins compiling the addon tests on-device. So cross-compiling addon tests should save us a little more time yet.  @joaocgreis Is this what you meant by moving the test logic? 
orangemocha		They are compiled on device, but on a separate device so they run in parallel. We could cross-compile them but this was easier to implement and makes for shorter runs. 
joaocgreis		@mscdex The download size was also a concern, the addons don't add much but all the arms share a slow network. I also wanted to keep it as generic as possible. But we can certainly experiment more.  Right now the Jenkins jobs call configure and test.py directly, and have the list of files to archive hardcoded. I want to create rules like `ci-compile` and `ci-test-binary` in the Makefile that do all of that, so that other people can also experiment, and changes in the build process do not break the fanned job. 
mscdex		Ah ok. Keep up the good work @orangemocha @joaocgreis ! :sunglasses: 
jbergstroem		Thanks for taking the time to help us improve! I believe this is a dup of https://github.com/nodejs/build/pull/735.
refack		Yep... Closing as it's superseded by https://github.com/nodejs/build/pull/735
joaocgreis		Added, should be working now.  @nodejs/citgm I removed the `POST_REBASE_SHA1_CHECK` (it was introduced for accept-pr, was always broken and should be removed everywhere). Let me know if you used it for something.
refack		Only issue I have is `node-gyp@3.x.x`. We are committed to support node > 0.8.0 with that branch, and we also haven't yet cut a `node-gyp@4.0.0` (which drops node < 4).  I'm +1 on cleaning up hacks from Jenkins jobs.  BTW: when was the most recent pre 4 build done / what is the probability we'll have to cut another release?
refack		Just remembered the `node-gyp` jobs use prebuilt node binaries by definition, so not a real issue.
joaocgreis		Yes please. I would be very surprised if it all still works. Removing all that logic will make the jobs significantly cleaner and easier to edit. I haven't seen any pre-v4 jobs running in a long time.  This would allow us to remove `NODES_SUBSET` right away, I don't believe the target for docs only is used or usable now (full CI should run for every PR, I don't know why it was there in the first place). Ref: https://github.com/nodejs/build/issues/696#issuecomment-297087910
rvagg		Not sure if this is exactly the latest for pre-v4 but the last v0.10 was v0.10.28 on Oct 18 2016, the last v0.12 was v0.12.18 on Feb 22 of this year. io.js was last built on Sept 15 2015, that was v3.3.1.  So we're now 8 months past the last pre-v4 build and we've also been crystal clear that we don't support these any more, regardless of what security flaws there might be. Any builds of them by now is going to have to be unofficial and provided by a third-party. There was a time when some people (Mikeal most notable) were saying that "if you want to support old branches then you can put in the work and make it happen", but I've personally been advocating for a very clear line drawn on our release timelines so we don't cause uncertainty for users. And anyway, I've seen zero evidence of anyone wanting to use our infra to build old Node versions or even test them since we dropped the final v0.12‚Äîit was so decisive that I've not seen it questioned or challenged anywhere after it happened, it's a done deal.
gibfahn		I think this is a no-brainer , and we should just do it as standard policy. Maybe have a 6 month waiting period between EoL and removing from CI, so when 10.x goes LTS we remove 4.x from CI.  Of course this will be much less of a big deal once we get all our Jenkins config into git.
rvagg		OK, let's talk about this at our next meeting and then notify the TSC of our "recommendation" or "decision", however we want to frame it. They may have objections of course but we need to put the case forward with clarity.
MylesBorins		Huge +1  On Oct 24, 2017 7:27 AM, "Rod Vagg" <notifications@github.com> wrote:  > OK, let's talk about this at our next meeting and then notify the TSC of > our "recommendation" or "decision", however we want to frame it. They may > have objections of course but we need to put the case forward with clarity. > > ‚Äî > You are receiving this because you are on a team that was mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/926#issuecomment-338959332>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AAecV9tByuNNwrOkWnT2HaBrWR3KkSK3ks5svcmegaJpZM4P_9gc> > . > 
mhdawson		+1 from me.
rvagg		Oh, did this get discussed at the meeting yesterday after I left? Should we just move forward with a TSC recommendation, if they don't object in X days then we'll consider it a done deal?
mhdawson		I think we should just move forward with a recommendation.
gibfahn		>I think we should just move forward with a recommendation.  How would we do that? Issue in TSC repo? I'm happy to propose https://github.com/nodejs/build/issues/926#issuecomment-338954999 as policy.
rvagg		@gibfahn yeah, do that. How about you open a PR against our README with a note about such a policy and also add a tsc-review label to it and it'll be picked up in the next meeting and I, or Michael can speak about it.
jbergstroem		Did you try compiling it manually? We have an [ansible script for this](https://github.com/nodejs/build/blob/4a1107f9419163ffd1d5af60093c3f0ab7bc5412/setup/ansible-tasks/ccache.yaml) you could use as part of a playbook. 
mhdawson		Thanks for the pointer I've not looked at how we'd do it yet but opened this to make sure we did not forget.  Will try to find some time to check this out. 
mhdawson		@jbergstroem compiled/installed it and after apply one fix which should come in the next release it seems to be working.  I added the build/install instructions to https://github.com/nodejs/build/pull/323 so closing 
mhdawson		@jbergstroem can you take a look  
jbergstroem		LGTM  
mhdawson		Landed as fb2078e6cda3ab49406be4f1abb150c89b5a3528 
jbergstroem		:shipit:  
mhdawson		Landed as https://github.com/nodejs/build/commit/0b648b62b32618f7c326a2f3d8f8937c477ce0b9 
jbergstroem		..also, now that `NODE_COMMON_PIPE` is superseded by `NODE_TEST_DIR`, we should set these in the init scripts as well. The path should point to a temporary folder which is on the same filesystem as where nodejs is built (it for instance needs to be `/home/iojs/build` on arm since its mounted through nfs). 
jbergstroem		Question: now that these commits has been merged to 0.10 and 0.12 branches, do we want to continue supporting `NODE_COMMON_PIPE` moving forward in CI? Not setting it will potentially generate a fail for really old stuff; but I don't really see where or why we'd test against older branches. 
gibfahn		>Question [...] do we want to continue supporting NODE_COMMON_PIPE moving forward in CI?  No! We should definitely remove this going forward.
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
maclover7		Landed in 888030957977834494ea4f7bf8cf7081b7cd9c09
gibfahn		@nodejs/build any objections/thoughts? It'd be really useful to get this approved/done so that @gdams and @bzoz can run citgm jobs.
gdams		This would be really useful
jbergstroem		sounds good to me.
mhdawson		+1 from me
gdams		@mhdawson do you have the permissions to give us access to the CI or do we need to CC someone else?
mhdawson		I'm looking at it right now :). If you are around I'll ping you in a few minutes to see if you can try it out for me.
gdams		@mhdawson I'm on slack for the next hour or so :)
mhdawson		Ok as per doc, access to start jobs has been grated to all members of the citgm team.  Will now setup citgm-admin team, add Myles as initial member and let citgm team use process to nominate others that will be able to create/modify jobs.
mhdawson		Ok, create citgm-admins and the 4 CITGM jobs are now configured so that they can be created/modified by the members of that group.  Initially I've made Myles the only member of the group.
mhdawson		Ok, CITGM team went through process and George is now in the citgm-admins group and seems to be able to edit jobs.  Going to close this and we can reopen if something more is required. 
mikeal		Also, just to be aware, we're going to need to migrate the nodejs.org website and hosting. There's a thread in the Website WG about it already https://github.com/iojs/website/issues/350 .  Just keep in mind that even if the convergence work finishes we need a place to publish releases as seamlessly as we have been for io.js so this will need to get integrated as well. 
jbergstroem		Meeting was had and we've now converged. :tada:  
mhdawson		It would be good to run tests on what we ship.   Is there a good way to break the changes down into manageable chunks ?    While not as good as the end goal it would be a step in the right direction.  Seems like you are already thinking along those lines.  #513 is a good first step and the next one might be to run full unit tests. 
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
mhdawson		Seems like we have failures again.  From history looks like maybe attempts are made to fix tests, which cause failures and then processes hang around causing more failures.
mhdawson		Latest hanging processes cleaned up.
gdams		I cleanup up a lot of processes that seemed to be blocking the debug port: ``` bash-4.3# ps -ef | grep debug      iojs  9044040        1   0 04:22:57      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug-brk=12346 -i     iojs  9830610        1   0 00:26:13      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --interactive --debug-port=12346     iojs  9961586        1   0 04:23:57      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug --debug-port=12346 --interactive     iojs 10944584        1   0 03:14:39      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --interactive --debug-port=12346     iojs 11796530        1   0 00:23:13      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug-brk=12346 -i     iojs 14745740        1   0 18:01:54      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug-brk=12346 -i     iojs 15073392        1   0 18:04:54      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --interactive --debug-port=12346     iojs 15138942        1   0 03:11:39      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug-brk=12346 -i     iojs 15204440        1   0 18:02:54      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug --debug-port=12346 --interactive     iojs 15859876        1   0 03:12:39      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug --debug-port=12346 --interactive     iojs 17432822        1   0 04:25:58      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --interactive --debug-port=12346     iojs 17891350        1   0 00:24:13      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node --debug --debug-port=12346 --interactive ```
mhdawson		@Trott has PRs to do some cleanup of processes.  If we don't see a re-occurrence in the next week or so I'll close this.
maclover7		@mhdawson Can this be closed?
mhdawson		yes, closing.
mhdawson		This would be good. I've added a number of the new jobs for WG's and it would be great if @gibfahn could help out too.
jbergstroem		`+1` from me.
joaocgreis		+1 from me as well.
mhdawson		Agreed in meeting today, I'll go ahead and add him
jbergstroem		@gibfahn with great power comes great responsibility :)
mhdawson		added
joaocgreis		I can confirm. @rvagg @jbergstroem can you access?
rvagg		old nodejs_build_test I think, managed to fix it thanks to Rackspace's cloud interface that lets you force a password change  also, yay for the github project tool, I'm found this issue from there, a month late, sorry!
rvagg		lgtm 
thefourtheye		LGTM 
jbergstroem		(as of writing, we're running 4.1.1) 
PeterDaveHello		:+1:  
mhdawson		+1 from me and there are a few people I'd want to add to the list from IBM.  
mikeal		What does the hand-off look like right now between the Build/Release team and our Docker WG? @nodejs/docker 
chorrell		+1   There are some Joyent people who package Node.js for pkgsrc that should be included. Also, SmartOS/pkgsrc is absent from here: https://nodejs.org/en/download/package-manager/ 
jbergstroem		@mikeal we have overlap in terms of people. The trust to push to the docker repo lies in the docker group. 
Starefossen		@mikeal as Johan said there is a strong overlap between the two working groups. However we do want to automate the process of getting new versions of the Docker Images as much as possible. There is also a desire of getting new Docker Images tested - preferably through the existing Jenkins CI setup. 
Starefossen		@chorrell here is the source of that page https://github.com/nodejs/new.nodejs.org/blob/master/locale/en/download/package-manager.md 
chvostek		@jbergstroem , awesome that you are spearheading this effort. As a FreeBSD sysadmin and port maintainer, I can tell you that the mere fact that you're interested in improving this process means a heck of a lot.  As I mentioned on [the other issue thread](https://github.com/nodejs/node/issues/3716) where this came up, the FreeBSD community recommends strongly against the use of externally-derived binary packages. In FreeBSD, binary packages are always built from "ports", and the delay in getting up-to-date packages is almost always in getting the port updated, rather than building packages from the updated port.  As I also mentioned, Node developers have done a GREAT job so far of writing code that compiles cleanly in FreeBSD. At this point, the [**www/node**](https://svnweb.freebsd.org/ports/head/www/node/) FreeBSD port requires no patches to build or install, and it has almost no dependencies. This is rare and wonderful. :)  Regarding your `$version_bump` mailing list suggestion, I imagine many people would be interested in joining that list beyond just package managers. I know I would, so that if a port update becomes available, I have an easier-to-read description of the changes upgrading will require on my systems. On a related note, many people use tools like [portscout](http://www.inerd.com/software/portscout/) to be notified when the ports they maintain need to be updated.  How might we update the actual FreeBSD _port_ when a version gets bumped, though?  As a first step, could we automate the process of producing a patch on the port's Makefile, and submit that through the normal update channels?  If dependencies and the package list aren't changing, the update could be as little as two lines in two files (the PORTVERSION in the Makefile and the sha256 in the distinfo). 
jbergstroem		@chvostek without turning this issue into the actual email, here's a few comments. In the coming weeks I reckon the maintainers can expect a similar email from me. - Coming from ~15 years of using different distros/os:es I understand that even if upstream wants tight control we can't assume that distributions should leave packaging to us. Upstream rarely sees the full picture. - All package managers have different ways of handling security (most common scenario for nodejs is building against a shared openssl). Our ambition should be to make it as easy as possible for managers to enforce their own routines. - We already have a few options in place to notify people about version bumps. If I garner enough interest (and time), the "package manager mailing list" would give you enough information about what you need to care about from a packaging point of view -- not so much about new features, bug fixes or improvements. There are distributions that strives to build against an environment that has as many shared dependencies as possible. We've released enough versions to create a legacy and need to support that. 
chvostek		@jbergstroem ... Every OS community has its conventions. I haven't been involved in package building on Linux in the last few years, but I am very involved with FreeBSD and happy to help find common ground in that OS.  > Our ambition should be to make it as easy as possible for managers to enforce their own routines.  Yes, that's exactly what I was suggesting, both in the other issue and here, above.  I have some ideas as to how this process might be semi-automated for FreeBSD. As a long time FreeBSD port maintainer, I can tell you that anything that reduces my workload is looked upon favourably. I also have feedback on the "sendout" sample you included when you opened this issue.  If your preference is to deal only with the individuals who are package maintainers for node, and you feel as if you'll have success following that route, then I'll withdraw from this issue and wish you the best of luck. 
jbergstroem		@chvostek for freebsd-specific improvements I think that discussion better lives somewhere else (we're still interested in having it). Perhaps ping me on irc or by email? (jbergstroem@freenode, #node-build or bugs at bergstroem dot nu).  As for improvements to the email, shoot! I will attempt to get the email out sometime next week. 
Trott		@nodejs/build @jbergstroem Should this remain open? Is it something that we might consider asking another group to help with or take on, such as @nodejs/user-feedback or @nodejs/community-committee?
Trott		Given that the last time there was meaningful activity in this issue was over 3 years ago, I'm going to close it. Feel free to re-open if you think that's the right thing, although consider opening one or more very (narrow/specific) new issue(s) instead.
jbergstroem		@mikeal are you referring to the build group creating a monthly report or do mean that the script should be hosted as part of the build repo? 
mikeal		We just need it to run in an automated way and post an issue each month, so we need some scripts somewhere in cron with a github token that can post the issues. 
jbergstroem		gotcha. we'll deploy something somewhere once the scripts are in place. 
jbergstroem		@mikeal is there a PR/issue somewhere to track progress of scripts? 
mikeal		huh? 
jbergstroem		@mikeal as I understood it, we were to deploy scripts that generated the contribution reports which is why I asked if these are stored/written somewhere else. 
mikeal		ahhh. no, i wrote some code as a one-off for a few reports. nothing has been written to the https://github.com/bengl/contribution-report module and nothing has been written yet that will actually log the issue. 
mikeal		awesome :) 
maclover7		ping -- is this still needed?
gibfahn		Seems like a job for @nodejs/automation üòÅ .
evenstensberg		I can help out with this if wanted
joyeecheung		@ev1stensberg The previous effort seems to be stalled so I'd say go ahead!
evenstensberg		Awesome, will pick it up during the weekend @joyeecheung üëç 
evenstensberg		Where do you want the script for this placed? Which repo? 
gibfahn		@ev1stensberg probably nodejs/automation would be the best place for it.
joyeecheung		@ev1stensberg If the automation repo is going to be the monorepo, mind that https://github.com/nodejs/automation/pull/9 is trying to use lerna to manage things...
evenstensberg		May be better to setup monorepo first, and then add respective folders @joyeecheung ?
joyeecheung		@ev1stensberg You don't have to wait for that really, I think we are going to migrate existing repos to the monorepo anyway with git subtrees (if we are going to have a monorepo at all)
maclover7		Going to close this for now, nothing really actionable here. As far as I can tell, there has not been a real need for this lately. Please reopen if I am incorrect, although there may be a better repo for this issue to live in.
mhdawson		@jbergstroem could I get your ok and then I'll go ahead and land this.  
jbergstroem		LGTM, sorry for the delay. 
mhdawson		Thanks, I'll land when I'm back in the office next week 
mhdawson		landed as https://github.com/nodejs/build/commit/15e8935e3f2b154084e5006ac0eecfa77a9b44fb 
jbergstroem		I have a PR that aims to simplify redirects and I couldn't find any traces of `new.nodejs.org`. Seems I was wrong after all. I'll update the PR https://github.com/nodejs/build/pull/389. 
MylesBorins		LGTM 
Starefossen		LGTM 
rvagg		I'm on it
rvagg		should be good to go now
misterdjules		Thanks! Some of the history/data for jobs that were running seems to have been lost, but I suspect this is expected?
rvagg		@misterdjules this is one of the mysteries of Jenkins .. it seems to not flush state very frequently so a crash or restart can lead to blocks of missing data. So my guess is that if it's missing for you then consider it lost.
misterdjules		Sounds good, thanks again!
jbergstroem		Merged in d3e72bca5. nodejs.org updated and reloaded.
rvagg		should be live now, please test and confirm 
fhemberger		@rvagg Thanks, works. 
orangemocha		@Fishrock123 sure. Since you can sort of already do that with node-test-commit, are you looking for a simpler entry-point to test any branch, or for a job to run on daily schedule on master, or both? 
Fishrock123		@orangemocha I mostly want an overview that shows **just master**. Then automating it would be nice. 
orangemocha		No problem @Fishrock123 . I think running it on a fixed schedule should be easy. I can take care of it.  Note that once/if we adopt node-accept-pull-request, it will test every commit that goes into the branch so it will in theory not let any failures develop. Anyway, the master daily test can still be useful. 
orangemocha		https://jenkins-iojs.nodesource.com/job/node-daily-master/ 
Fishrock123		\o/ Thanks @orangemocha :) 
orangemocha		You are welcome @Fishrock123 ! It was actually really easy to make. 
Fishrock123		@orangemocha I noticed this test runs somewhat towards the day NA time (our busiest), do you think we could schedule it for like, night-time NA? 
orangemocha		@Fishrock123 : I changed it to 2am PST. 
rvagg		- Rod make sure Julien has Jenkins access - Rod to document full release process for io.js - Julien to continue work on enabling releases by additional people - Michael: PPC machines - Build team to verify that https://jenkins-iojs.nodesource.com/job/node-test-pull-request/ is as good as iojs+any-pr-multi - Alexis to continue working on PRs for reconciled CI - Rod to work on enabling 32-bit OSX builds on current CI - Rod to continue working towards signing keys and SSL certs from the foundation (make an issue for this) - Rod to continue working towards DNS changes  Items from last meeting, which is here (still to be merged) https://docs.google.com/document/d/189CVyNIWnWSYHNR1ntDu55LRaZaY8273FpLljLnVcCA 
misterdjules		Unfortunately, I won't be able to attend that meeting. I made some progress on making it easier for other contributors to make releases of joyent/node. See https://github.com/joyent/node/pull/25638, and https://github.com/joyent/node/wiki/Node.js-release-guide. @jasnell has volunteered to help me doing releases for the short term, but cannot commit to do that long term. We'll discuss how to support the release process as a team during the next TSC meeting tomorrow. 
jbergstroem		@saghul yep, I can look at adding it.  
jbergstroem		@saghul fixed both of your issues now. 
saghul		Thanks a lot, works fine now :-) 
gibfahn		>host_vars setup for non-infra non-jenkins-admins people‚Äîyou need Jenkins secrets in host_vars in order to fully run the scripts across hosts, build/test people should probably have an easier way to get these & set it up (I think?)  Having a web frontend like Ansible Tower which auto-manages the secrets for you would be amazing. I'd like to click a button to run a script on a host.
rvagg		good news to report, in #964 I have them running repeatedly on Ubuntu, Debian8 and Fedora: `ansible-playbook playbooks/jenkins/worker/create.yml --limit test-*ubuntu*-x*,test-*fedora*,test-*debian8-x*`. I can run that multiple times without serious problems. Failures are due to some old Fedora machines not being able to fetch updates cause they are EOL.  Also, maybe as a hint. I set up some of my missing host_vars files by doing the following type of thing:  upstart  ``` parallel-ssh -H test-digitalocean-fedora22-x64-1 -H test-digitalocean-fedora23-x64-1 -H test-digitalocean-fedora24-x64-1 -H test-digitalocean-fedora25-x64-1 -H test-digitalocean-fedora25-x64-2 -H test-rackspace-fedora22-x64-1 -H test-rackspace-fedora23-x64-1 -H test-rackspace-fedora24-x64-1 -i 'cat /etc/init/jenkins.conf | grep secret' parallel-ssh -H test-digitalocean-ubuntu1404-x64-1 -H test-digitalocean-ubuntu1404-x86-1 -H test-softlayer-ubuntu1404-x64-1 -H test-softlayer-ubuntu1404-x86-1 -i 'cat /etc/init/jenkins.conf | grep secret' ```  systemd  ``` parallel-ssh -H test-digitalocean-ubuntu1604-x86-1 -H test-digitalocean-ubuntu1610-x64-1 -H test-digitalocean-ubuntu1610-x64-2 -H test-nearform_intel-ubuntu1604-x64-1 -H test-nearform_intel-ubuntu1604-x64-2 -H test-rackspace-ubuntu1604-x64-1 -H test-rackspace-ubuntu1604-x64-2 -i 'cat /lib/systemd/system/jenkins.service | grep secret' ```  then manually doing this type of thing with the results:  ``` echo 'secret: abcxyz1234567890....' > host_vars/test-rackspace-fedora24-x64-1 ```  This could be automated a bit more and we could even make a set of ansible scripts to do this. It'd be easier to let all of build/test get these secrets off existing hosts than trying to get them access via jenkins.
maclover7		Below is where we are (or at least what I have been able to do myself) with the different Ansible scripts -- we are slowly migrating away from `setup` and to `ansible`, with the main stragglers being Raspberry Pi machines and Windows machines, but we are getting there. I want to try and write some more docs at some point about getting from _no setup_ to running `ansible-playbook`.  <details>  `playbooks/jenkins/docker-host.yaml`  - `test-digitalocean-ubuntu1604_docker-x64-1` - `test-digitalocean-ubuntu1604_docker-x64-2` - `test-joyent-ubuntu1604_docker-x64-1` - `test-softlayer-ubuntu1604_docker-x64-1`  `playbooks/jenkins/linter.yml`  - `test-rackspace-freebsd10-x64-1` - `test-joyent-freebsd10-x64-2`  `jenkins/worker/create.yml`  - `test-digitalocean-debian8-x64-1` - `test-rackspace-debian8-x64-1` - `test-rackspace-debian8-x64-2` - `test-softlayer-debian8-x86-1` - `test-digitalocean-freebsd10-x64-1` - `test-digitalocean-ubuntu1604-x86-1` - `test-nearform_intel-ubuntu1604-x64-1` - `test-nearform_intel-ubuntu1604-x64-2` - `test-packetnet-ubuntu1604-arm64-1` - `test-packetnet-ubuntu1604-arm64-2` - `test-rackspace-ubuntu1604-x64-1` - `test-rackspace-ubuntu1604-x64-2` - `test-joyent-ubuntu1710-x64-1` - `test-joyent-freebsd10-x64-1` - `test-joyent-ubuntu1710-x64-2` - `test-digitalocean-freebsd11-x64-2` - `test-digitalocean-ubuntu1404-x64-1` - `test-digitalocean-ubuntu1404-x86-1` - `test-softlayer-ubuntu1404-x64-1` - `test-softlayer-ubuntu1404-x86-1` - `test-digitalocean-freebsd11-x64-1` - `test-digitalocean-fedora27-x64-1` - `test-rackspace-fedora27-x64-1` - `test-digitalocean-fedora26-x64-1` - `test-rackspace-fedora26-x64-1` - `test-digitalocean-fedora25-x64-1` - `test-digitalocean-fedora25-x64-2` - `test-digitalocean-fedora24-x64-1` - `test-rackspace-fedora24-x64-1` - `test-rackspace-fedora23-x64-1` - `test-digitalocean-fedora23-x64-1` - `test-packetnet-centos7-arm64-1` - `test-packetnet-centos7-arm64-2` - `test-rackspace-centos7-x64-1` - `test-softlayer-centos7-x64-1` </details>
juggernaut451		would love to contribute on this. @gibfahn @maclover7 @rvagg  can someone mentor me on this
maclover7		Moving this to #1277
gibfahn		More info on the state of play at the OS level would definitely be useful (i.e. when/if major distros stopped supporting 32-bit OSs, i.e. who we'd be breaking). - Linux: maybe @rvagg or @chrislea would know off the top of their heads - Windows: @joaocgreis and @refack  - SmartOS: @nodejs/platform-smartos would be good to know your thoughts, I assume we don't have to drop 32-bit on SmartOS, but if we're doing it elsewhere it might be a good time to do it here too.
refack		Windows 10 still supports [32bit](https://www.microsoft.com/en-us/windows/windows-10-specifications#system-specifications) and "cross compiling" from a 64bit host to a 32bit target is also fully supported. Windows Server 2008R2 was not released as 32bit host, but Windows has side-by-side technology (WoW64) that allows 32bit applications to run natively on 64bit hosts, so there should not be an ***external limitation*** for building, testing and running node32 for the foreseeable future.  BTW: AWS/Azure/GCP doesn't rent 32bit VMs of our lowest supported Windows (Server 2008R2).
chrislea		There is absolutely still support for 32bit CentOS 6 / RHEL 6 Linux out there. Also, all Debian and Ubuntu releases still have 32bit versions, and Fedora still makes 32bit releases as well.  Red Hat dropped 32bit support with RHEL 7.  I don't have much of a sense of how many people are using 32bit builds in production. If I had to **guess** (emphasis mine there), I doubt many people are running 32bit Node builds on production servers, and I suspect the biggest impact might be not so much on production servers, but instead on appliances. For example, here's output from a [Synology NAS](https://www.synology.com/en-us) that I have at home: ``` chl@DSPlay01:~$ uname -a Linux DSPlay01 3.2.40 #15152 SMP PREEMPT Fri Sep 1 11:13:20 CST 2017 i686 GNU/Linux synology_evansport_214play ``` And yes, Node is an installable package should I choose to put it on this device. A lot of these appliance gizmos use 32bit chips because they don't have much memory and want to keep costs low, so older 32bit Atom or Celeron processors make sense.  Now, having said all that, if we assume I'm right, then I don't think we need to be too concerned about anything other than making sure that Node _will in fact build_ on a 32bit system with a modern enough compiler, because from what I can tell almost all of these appliances have their own packaging formats and tend to build the things they need for those packages themselves.
seishun		@gibfahn I don't see any reason to drop 32-bit builds on Windows.  @chrislea  >then I don't think we need to be too concerned about anything other than making sure that Node will in fact build on a 32bit system with a modern enough compiler  That shouldn't be a problem. We could just continue running CI jobs on 32-bit platforms where it's easy enough to install a supported compiler, e.g. Ubuntu 14.04.
joaocgreis		We shouldn't drop Windows 32-bit support. While our deps have support for it, it'll be easy for us to maintain. Windows and Visual Studio have full support for 32-bit.
bnoordhuis		Apropos Fedora, i686 is in a state of disrepair and has been for years: https://lwn.net/Articles/728207/  There is a Special Interest Group since a few months that fixed some things but I think it's safe to say no one runs Fedora i686 in production and it's not worth spending time on.
chrislea		Unfortunately I _have_ seen people running 32bit Fedora out there in the wild (same for CentOS 6 and occasionally an older Debian / Ubuntu release). But generally I feel like all the major distros want to drop it and are sort of looking at each other waiting to see who does it first. I know Arch Linux (not one of the major ones, but important among the more hacker / bleeding edge types) is dropping it officially in November of this year.  So I still feel pretty comfortable with what I said above, which is that I think we'd be okay if we continue to "support" it in the sense that we make sure it will in fact build on a 32bit x86 machine, but we stop making official release tarballs for it.
gibfahn		So maybe the answer is to drop it to Tier 2 or Experimental rather than dropping completely.
jasnell		Some informal research shows that there are still a fair number of users using 32-bit builds on smaller IoT devices. I doubt this is a *large* group, but it's still worth bearing in mind.  I'm going to tag this with a tsc-review label.
chrislea		Yes @jasnell if you'd like my $0.02 on the "devices" thing (I'm sure you're just _dying_ to hear) it's [here](https://github.com/nodejs/build/issues/885#issuecomment-330619737).
gibfahn		@chrislea wrong link? That seems to point to my comment above.
chrislea		My bad, try [this](https://github.com/nodejs/build/issues/885#issuecomment-330027207).
seishun		It's been a week and I see no objections to dropping public 32-bit Linux builds. Perhaps we can proceed with that for now and leave the discussion about dropping 32-bit support (or downgrading it to Experimental) for later?
refack		@nodejs/release did discuss this at the last meeting. @gibfahn what was the decision (I remember something about IoT, and maybe stop releasing, but keep testing..)?
gibfahn		>@nodejs/release did discuss this at the last meeting.  Discussed [here](https://www.youtube.com/watch?v=Y01b-pMIp8E&t=1568s), there wasn't a decision, but @jasnell mentioned that he believed he knew of some embedded devices that were Intel 32-bit Linux, and that he'd had requests from people for us to not drop 32-bit builds. So more info on that would be good. James did say that he thought continuing to test but not doing releases sounded reasonable.  >It's been a week and I see no objections to dropping public 32-bit Linux builds.   This was raised for `tsc-review`, but it looks like it got missed at the last meeting, https://github.com/nodejs/TSC/issues/359 (or at least it's not in the minutes).
gibfahn		>Perhaps we can proceed with that for now and leave the discussion about dropping 32-bit support (or downgrading it to Experimental) for later?  That seems like a good idea.  So, does anyone have an objection to us ceasing to do 32-bit xLinux releases from Node 9.x onwards? We will continue to run CI. cc/ @nodejs/build @nodejs/lts @nodejs/tsc 
mscdex		Might want to update the original post if this is about Intel 32-bit only and not other platforms (e.g. ARM).
gibfahn		>Might want to update the original post if this is about Intel 32-bit only and not other platforms (e.g. ARM).  It was originally supposed to be a more general discussion (including ARM). The xLinux discussion is just the most pressing matter.
mscdex		What is "xLinux?" Is it an IBM thing?
gibfahn		>What is "xLinux?" Is it an IBM thing?  x86_64 Linux, as opposed to aLinux (ARM Linux), pLinux (Power Linux), and zLinux ([z Linux](https://en.wikipedia.org/wiki/Z/Architecture)).  Probably used more often in IBM, as we deal with more architectures than most, but I don't think it's exclusive.
chrislea		Ubuntu is going to [stop making 32bit desktop ISOs](https://fossbytes.com/ubuntu-17-10-killing-32-bit-desktop-iso/) with their next release, just FYI.
rvagg		That full context on the Ubuntu decision is actually pretty informative about their perspective on the future of i386 
rvagg		https://lists.ubuntu.com/archives/ubuntu-release/2017-September/004212.html
seishun		Still no objections. What next?
gibfahn		No objections raised by anyone in Build or TSC AFAICT, so I think this is agreed unless someone wants to put it on `tsc-agenda` and discuss there. @jasnell is that something you want to do?  Otherwise this is done, it's just a question of someone adding the correct `if` statements to the release build job.  @rvagg @joaocgreis  you've done this more than most I think, if I come up with a diff can you review?
seishun		Do https://github.com/nodejs/build/pull/797 and https://github.com/nodejs/build/pull/809 need to wait until the release build job change is implemented?
refack		AFAICT #797 can land (we can build & test) and it's not used for releases. https://github.com/nodejs/build/pull/809 is problematic since ATM we can't even build for testing on x32.
seishun		> #809 is problematic since ATM we can't even build for testing on x32.  There are other 32-bit testing machines besides CentOS 6.
refack		>> #809 is problematic since ATM we can't even build for testing on x32. > > There are other 32-bit testing machines besides CentOS 6.  Yeah but we still want to CI on CentOS6.i686 to make sure people are able to at least build their own. From experience if we don't CI test a configuration it will rot. BTW: I'm now trying to build gcc for i686 and also trying to use a native i686 VM 
seishun		I doubt anyone is going to build gcc to build the latest Node.js on 32-bit CentOS 6, so going to such lengths on our side seems unnecessary. Not my call though.
refack		I'm still playing with the command line parameter so it'll wait for the install to finish
gibfahn		cc/ @piccoloaiutante @joaocgreis 
joaocgreis		> I'm still playing with the command line parameter so it'll wait for the install to finish  @refack thanks for this! Let us know when it's time to review.
piccoloaiutante		@refack looking forward to test it locally on my machine :)
BridgeAR		This should likely be closed. There was no progress for one year now.
joaocgreis		Superseded by https://github.com/nodejs/build/pull/1383. @refack I hope you don't mind me closing this, thanks for your work!
mhdawson		+1 from me.
gibfahn		+1
jbergstroem		Me too; +1
kunalspathak		It seems people are in favor of giving me Jenkins access :) @mhdawson , can you please give me access?
mhdawson		Ok added. 
joaocgreis		Windows Server 2016 is now added to the CI matrix.  Thanks @kunalspathak !
jkrems		I'm really tempted to walk back on the declarative syntax here given the amount of repetition once all possible architectures are added. Unless we'd move that logic to an inline `script {}` section and not model it as parallel `stage`s. Would have to see how that would be rendered in the UI.
refack		This is what came out https://ci.nodejs.org/job/node-inspect/2/flowGraphTable/ (first steps are checking out the pipeline script itself from @jkrems's fork)
jkrems		Can someone give https://github.com/nodejs/node-inspect/pull/53 a quick look to get the latest `tap` into master?
jkrems		For reference, here's how an example run renders in blue ocean when I was testing this script locally:  <img width="1584" alt="screen shot 2017-10-31 at 1 44 44 pm" src="https://user-images.githubusercontent.com/567540/32248066-b163a9b2-be41-11e7-8640-cb30da3f4e2c.png">  That view makes the job output a lot easier to consume. Unfortunately the out-of-the-box experience of looking at parallel stages in Jenkins is pretty bad (by which I mean pretty much unusable). :(
refack		> That view makes the job output a lot easier to consume. Unfortunately the out-of-the-box experience of looking at parallel stages in Jenkins is pretty bad (by which I mean pretty much unusable). :(  It's not that bad, it's just not trivial to find: * each step is avalible from the [flowGraphTable](https://ci.nodejs.org/job/node-inspect/2/flowGraphTable/)   for example the first failing step:   https://ci.nodejs.org/job/node-inspect/2/execution/node/43/log/ * An overview of the history of the job can be found on the "Full Stage View"   ![image](https://user-images.githubusercontent.com/96947/32255747-fb8be98e-be80-11e7-8561-f5a5b8de830e.png)  --  ![image](https://user-images.githubusercontent.com/96947/32255752-07ad0b26-be81-11e7-996a-390d6f3f4ca8.png)  
jkrems		Yes, I know. But with non-trivial workflows (e.g. parallel + anything happening afterwards) the flattened stage view becomes super confusing [especially if any stage name isn't globally unique]. This isn't a problem yet here (since it has just two fake "architectures" right now and nothing happening afterwards) but it won't get any better the more complex the workflow gets. Though maybe it'll be fine for node's builds. I might be scared by our internal deployment pipeline builds (with multiple validation stages consisting of parallelized things).
refack		@jkrems the log visibility issue was brought up by @joaocgreis and AFAIK is the last hindrance for full adoption. But as I see it we can continue to use MultiJob. With the current setup the fan-out is the last step of each sub-job and any other steps after that are in a new sub-job. So the flattened log is not that bad, just requires learning where to look for the failures.  On the other hand the step tree: ![image](https://user-images.githubusercontent.com/96947/32275384-706bf8d8-bee1-11e7-8f5f-896e92726981.png) Allows for a not log-reading-dependant way to quickly figure out what failed.  ---  tl;dr IMHO we should move away from log-based-groking to using parsing plugins (like tap or tap2junit), while also look-for/develop a better log parser. 
rvagg		ref https://github.com/iojs/io.js/pull/1389#issuecomment-92173028 
jbergstroem		I say re-purpose. They're not arm-slow and the queue hasn't shown signs of piling up more than two-three jobs.  If/when we run into a resource issue we should find a way of throwing more hardware at it. The real issue here is that we don't test an architecture we treat as primary and release binaries for. 
jbergstroem		Is this still a thing? 
orangemocha		Yes, but I would rather test 32-bit builds on the 64-bit slaves. 
maclover7		ping... should this remain open?
maclover7		Closing due to inactivity. Please reopen if this is still needed :)
ljharb		To sum up from the other issue, something easily parseable in a shell, with version number, shasum, and binary URL, would be wonderfully helpful. 
ghostbar		Something like http://ftp.debian.org/debian/dists/testing/Release ?  I think we could change the hashes with the list of packages to something like a hash with the list of releases tarballs ??  It comes to my mind something like:  ``` hash sizeInBytes version relative/path/to/the/index/file.tar.gz hash sizeInBytes version relative/path/to/the/index/file.xz ```  This allows us to give out several extensions of the same file and it's very easily parseable by a simple sed + awk script and from that create a JSON file. 
rvagg		I've created an initial format for a catalogue, currently on nightlies but will also be on dist: [index.json](https://iojs.org/download/nightly/index.json) and [index.tab](https://iojs.org/download/nightly/index.tab)  I'd really like feedback from version-manager authors on this, discussion happening in https://github.com/iojs/io.js/issues/301#issuecomment-69699195 for now  /cc @aaronpowell 
rvagg		derp, fixed 
ljharb		Referencing https://github.com/creationix/nvm/pull/616 
kenperkins		@rvagg it seems this can be closed now? 
jbergstroem		@joaocgreis had done a great job in adding all of this through azure. Closing. 
mhdawson		I think this is long since done.
phillipj		Dumping out some observations while digging into this:  - Last bot deploy 3rd July - All the latest statuses updates from Jenkins sets `status: pending`, resulting in the yellow status in GitHub PRs - Has to go two days back, to 10th July, to find Jenkins updates which set a status else than `pending`, meaning `failure|success`.  As the bot hasn't gotten any recent changes within the last two days, I'm suspecting something Jenkins related at the moment.
phillipj		Seems like the post build step configured to report the build result to the bot via `curl` doesn't exist anymore. The same is done to report the `pending` status when a build starts, easily spotted when looking at a build configuration, an "Execute shell" block sending a JSON payload via `curl` with a static status value of `pending`.  Pretty certain the same thing was done to report `failure|success`, configured as two post build steps per build. @jbergstroem can confirm that sounds familiar?  @nodejs/jenkins-admins has something particular been done to Jenkins or the nodejs/node related builds the last couple of days?
joaocgreis		@phillipj I can confirm that is correct. The build step to mark as `failure|success` disappeared. This is most likely because the Jenkins plugins were updated this week (while Jenkins was locked down). It's far from the first time we've had issues after updating Jenkins/plugins.  I took a look at the OSX job (picked randomly - any would have worked). The last modification was in 2016. The script to mark as `failure|success` in not there, there is only an empty Flexible Publish step. So I saved, without modifying anything, and [the log confirms that Jenkins removed those steps on its own](https://ci.nodejs.org/job/node-test-commit-osx/jobConfigHistory/showDiffFiles?timestamp1=2016-10-17_15-49-59&timestamp2=2017-07-12_19-48-14).  Thus, something in some plugin changed and what we were doing is no longer supported. We must find a new way or better understand what happened.
Fishrock123		ping @nodejs/build 
phillipj		Looks like we used the [PostBuildScript plugin](https://wiki.jenkins.io/display/JENKINS/PostBuildScript+Plugin) which was suspended in June and I cannot seem to find any replacement. I have no idea how to execute shell commands post build without that plugin in place.
joaocgreis		I can't explore this now, but someone else might be able to pick this up. If we create a new job with the script to post a success/failure to GH (based on a parameter), we may be able to use `Trigger parameterized build on other projects` to start it and post the status.
maclover7		ping @nodejs/build... is there anyway I can help with getting this working again?
refack		@maclover7 if you have an idea how to add a post-build-command to all the jobs, or if you can investigate the status of [PostBuildScript plugin](https://wiki.jenkins.io/display/JENKINS/PostBuildScript+Plugin) (maybe it's vulnerability does not affect our setup).
phillipj		The third alternative, if we can't get Jenkins to do as before, is to implement build polling in the bot. That's a lot more hassle and complexity in the bot, but an alternative nonetheless.  On Sat, 14 Oct 2017 at 17:20, Refael Ackermann <notifications@github.com> wrote:  > @maclover7 <https://github.com/maclover7> if you have an idea how to add > a post-build-command to all the jobs, or if you can investigate the status > of PostBuildScript plugin > <https://wiki.jenkins.io/display/JENKINS/PostBuildScript+Plugin> (maybe > it's vulnerability does not affect our setup). > > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/790#issuecomment-336641728>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ABLLE_xE3o_uawdDMWsq-WsR8XKfW_09ks5ssNE3gaJpZM4OU_LA> > . > 
maclover7		The [Post build task plugin](https://wiki.jenkins.io/display/JENKINS/Post+build+task) might also be an alternative. Maybe we can try out one solution, see what happens, and then go from there? Unfortunately I'm not a member of the Build WG, but maybe a sandbox job could be created to test this out.
gibfahn		>The Post build task plugin might also be an alternative. Maybe we can try out one solution, see what happens, and then go from there? Unfortunately I'm not a member of the Build WG, but maybe a sandbox job could be created to test this out.  @maclover7 this really needs someone to test it with their own repo and get something that works. Once we have that we can just PR changes to the github-bot repo (and someone can change whatever secret is needed in the Jenkins config).
maclover7		#973 has been rolled out to all node-test-commit sub jobs, so hopefully this should be resolved!  @nodejs/collaborators green/red CI status lights should be appearing now in PRs... if you encounter any errors please post here on this issue to try and keep the discussion all in one place. Unless any huge errors come up, will close out this issue in a few days ‚ù§Ô∏è 
maclover7		This largely seems fixed and to be working, going to close for now :)
gibfahn		Seems like a good idea in general, but I don't see anything in Jenkins to make this work with Github auth.   If we're going to do this I think it would be better to organise it with the Github teams, that way anyone who's a member can see who is in the inactive list (otherwise you could lose access and not be able to tell why).  This seems related to the CTC emeritus labels, maybe we should have an inactive collaborator group.  Added some labels at random.
bnoordhuis		> This seems related to the CTC emeritus labels, maybe we should have an inactive collaborator group.  That's currently under discussion in the CTC but the rules around that will probably be much looser than what is appropriate for CI access, hence this issue.
mhdawson		When you say CI access, you just mean the ability to start jobs right ?   I think that is all regular collaborators can do in the CI over what unauthorized users can do.
bnoordhuis		Yes.  > I think that is all regular collaborators can do in the CI over what unauthorized users can do.  That and use authenticated exploits as a jumping board. :-)
jbergstroem		I've been a fan of enforcing this for the build group seeing how we have access to a lot of machines. The tricky part is that we have to re-roll the keys (which we have an ansible script for) which means we should probably just have a purge every 6-or-so months. Sorry for potentially hijacking the thread.
piccoloaiutante		@jbergstroem a 6 month window seems a more appropriate time window to understand if someone become inactive. +1 for the `build-alumni` group @gibfahn so we can move them back quickly if they become active again. 
bnoordhuis		@gibfahn Is CI access currently predicated on <span>@</span>nodejs/collaborators membership?  <sup>(Note how I cleverly avoided cc'ing 500+ people by wrapping the at sign in `<span>` tags.)</sup>
Trott		> @gibfahn Is CI access currently predicated on <span>@</span>nodejs/collaborators membership? >  > <sup>(Note how I cleverly avoided cc'ing 500+ people by wrapping the at sign in `<span>` tags.)</sup>  @bnoordhuis Yes, CI access is predicated on collaborator GitHub team membership.  FWIW, that's 100+ people. 500 would be the members team. The collaborators team is more like 100.  
gibfahn		So @Trott removed some folks in https://github.com/nodejs/node/pull/16173, if we want to be more strict going forward, I think the answer is to move people to `emeritus` and remove them from nodejs/collaborators more quickly.  >Strawman proposal: $x == 3  Why not start with 12 and see how it goes? I think the usual "Raise PR, cc people, if no-one objects then go ahead" rules apply here.  I don't think it's worth doing something fancy in build to basically create a subsection of collaborators that don't have CI access, so I'll close this. If there's anything build-specific that should be done please reopen.
jbergstroem		The problem is that some vm's are running on 2G; meaning the more memory we have available for compilation and testing, the better. An idea I've been playing around with in my refactor is passing memory as a variable to the template and overriding it - if needed - for specific providers, operating systems or architectures (all of those are available to test against in Ansible).
mhdawson		Should I just close this given that we are moving to the refactor ? 
gibfahn		>Should I just close this given that we are moving to the refactor ?  Maybe we should change the default `server_ram` from 128M to 256M in the refactor?  ```bash ‚ûú  ansible git:(master) ‚ùØ rg server_ram                                                                                                                                             ~/wrk/com/build/ansible host_vars/README.md 20:- `server_ram`: how much memory the slave should assign to java-base  roles/jenkins-worker/templates/centos.initd.j2 20:JENKINS_SLAVE_ARGS="-Xmx{{ server_ram|default('128m') }} -jnlpUrl {{ jenkins_url }}/computer/{{ inventory_hostname }}/slave-agent.jnlp -secret {{ secret }}"  roles/jenkins-worker/templates/freebsd.initd.j2 36:jenkins_args="-Xmx{{ server_ram|default('128m') }} -jar ${jenkins_jar} \  roles/jenkins-worker/templates/jenkins_manifest.xml.j2 31:                   exec='{{ java_path[os] }} -Dsun.security.pkcs11.enable-solaris=false -Xmx{{ server_ram|default('128m') }} -jar /home/{{ server_user }}/slave.jar -jnlpUrl {{ jenkins_url }}/computer/{{ inventory_hostname }}/slave-agent.jnlp -secret {{ secret }}'  roles/jenkins-worker/templates/openrc.initd.j2 7:command_args="-Xmx{{ server_ram|default('128m') }} \  roles/jenkins-worker/templates/systemd.service.j2 25:ExecStart=/usr/bin/java -Xmx{{ server_ram|default('128m') }} \  roles/jenkins-worker/templates/upstart.j2 26:  exec java -Xmx{{ server_ram|default('128m') }} \ ```
jbergstroem		server_ram can be overridden from the host vars. The idea was to the specific hosts that needs it allow override.
gibfahn		Sure, my point was that if we think they'll mostly need 256M rather than 128M then we should consider updating the default. If it's just the odd one then I agree we don't want to.
jbergstroem		My experience is that most - at least so far - doesn't need more than 128m.
gibfahn		Might as well close then.
jbergstroem		I think the issue mutated after the branch landed -- it's much easier to override per host or even per group via config (https://github.com/nodejs/build/tree/master/ansible#adding-extra-options-to-a-host)
mhdawson		@rvagg  
jbergstroem		Should this perhaps be filed to the website repo? 
rvagg		filed fix @ https://github.com/nodejs/nodejs.org/issues/516 
rvagg		:boom: fixed 
rvagg		lgtm 
jbergstroem		Merged in 7d52f7b6a51166812ac8afca3445ac404a2a7e69. Thanks for reviewing. 
geek		LGTM.  Thank you, @joyent  
bnoordhuis		LGTM 
retrohacker		:+1: thank you @joyent :smile: 
jbergstroem		Merged in c2968712842151a32caeccf4ff693f26d458f6dc and removed the node from jenkins. Thanks for the Review.  
gibfahn		Looks like that list has to be manually updated (in two different places in the job). If you think of others let me know and I'll add them.  _**EDIT:**_ Added the job.
gibfahn		Reopen if there are others that need adding.
orangemocha		Rubber-stamp LGTM  
mhdawson		lgtm 
rvagg		lgtm 
matthewloring		@Trott Thanks for opening this. 
Trott		Surely there is an advocate or two for this on the Build WG who can give a LGTM? @MylesBorins perhaps?
gibfahn		SGTM
rvagg		Yeah, sorry, I saw this on IRC but got distracted. Have emailed some details to @matthewloring.
gibfahn		@rvagg we normallly leave the issue open so we remember to revoke access once it's no longer needed. Assuming we're going to do that in this case, I'll reopen.
matthewloring		I'm all done with the credentials. Thanks again!
gibfahn		@rvagg could you revoke access and close this?  **_EDIT:_** Maybe that's not how it works on Windows, IDK
rvagg		revoked by changing passwords and committing back to secrets
joaocgreis		That build and the 5 after it (up to including 1074) were waiting for executors that are not yet online. I was able to cancel them without any problem. Might be a problem with permissions, would be good to have a look from someone familiar with it. 
cjihrig		Can someone please post here once PRs are good to be tested through the CI again? 
Fishrock123		@joaocgreis are those still waiting to spin up? 
jbergstroem		@Fishrock123 yeah, they are still to be spun up. We will post an update to https://github.com/nodejs/node/issues/3709 in a few hours.  The reason you can't cancel is because we switched to the github authenticator plugin and an issue with permissions mapping. 
jbergstroem		Fixed; was an issue with the github authentication plugin. 
andrewlow		+1 on this - looks good.   It was nice of you to include Power8, but not necessary - when V8 supports it and the Node core team approves the pull request(s) to enable the platform, the capability will be there. We're also doing z/Linux (yup, Node on the mainframe!).  I've personally setup OSX (internally) for builds and it wasn't really too big a deal. You've got to deal with the Mac App Store to install XCode and the command line utilities, but after that it's just a Linux box. There is some UI elements (which you can use VNC to deal with) when you are configuring it, but after that it's fine to use remotely via SSH.   I haven't tried out a docker solution to provide containers on OSX. It should work in theory.  If we figure out some OSX hardware, I'd be willing to put in the time to setup (and document the process) to enable it for builds. 
trevnorris		@andrewlow IIRC latest libuv supports Power8. So it would be useful having machines that can properly test libuv builds. 
rmg		@andrewlow Docker on OSX is really Docker on Linux in a VM on OSX + some port forwarding to let the `docker` command run from OSX connect to the Docker instance in the VM. In other words, it doesn't give you anything different than Docker on Ubuntu, it just costs more to run (performance tax from running a VM and $ tax for doing it on Apple hardware).  I've used macstadium.com for Mac hardware to add OS X capacity to my Jenkins cluster, but it costs money. 
andrewlow		@trevnorris - true, I forgot we're also targeting libuv builds.  @rmg - doh. I knew that OSX docker was Linux in a container, again I forgot.  FWIW - my OSX build machine is a low end mac mini. Looking around there are some (pay) options for co-location of your own mini, or renting mac mini time like macstadium. So we may be able to get away with a cheaper solution, but I don't think any of the 3 corporate sponsors have OSX hardware. 
dshaw		@andrewlow Voxer has offered to host esoteric hardware for builds at their datacenter. We're provisioning a couple machines for that now. I've also reached out to several individuals at Apple to see if there is interest in contributing there. 
jbergstroem		LGTM 
mhdawson		Landed as https://github.com/nodejs/build/commit/30ed21140f9680fdecba7728171646e0fbe1a2ee 
gibfahn		No reason not to remove IMO. On the other hand if they're not having issues is there a reason to remove them? I'd be tempted to leave them, and just remove them from any jobs that have issues with them. We can retire them when they become an annoyance to maintain.
rvagg		Yeah, I'm pretty OK with this approach. As long as (a) we can continue to keep them maintained and (b) we don't lose the infra cause we can't start a new Ubuntu 12.04 on any of our providers if we needed it.
seishun		Two reasons to remove:  1. Adds noise in Jenkins and the nodejs/build repo. 2. Uses an outdated version of gcc.
gibfahn		>Adds noise in Jenkins and the nodejs/build repo. >Uses an outdated version of gcc.  Only if it fails. And if it fails then yes, time to remove.
rvagg		https://www.ubuntu.com/support/esm  > In April 2017, Ubuntu 12.04 LTS reached its end-of-life. For Ubuntu Advantage customers who can not upgrade to a newer version of Ubuntu Server, Canonical is offering Ubuntu 12.04 ESM (Extended Security Maintenance). ESM provides ongoing security fixes for the kernel and essential packages through April 2019.  Apparently this is working out really well for Ubuntu and they are expanding ESM to all LTS lines and expanding the range of packages they are supporting beyond just `main`. So perhaps we don't retire 12.04 just yet. I'm going to reach out to Canonical about this to see if we can get some rough idea of the audience size, or something that will help us.
mhdawson		Waiting on update from @rvagg, removing agenda tag until there is one.
refack		> not sure if this is intentional but the softlayer logo in tier-2-providers.png seems to be a bit chopped up?  IMHO it's a artifact of the GitHub image viewer: ![image](https://user-images.githubusercontent.com/96947/30505958-fce10d5a-9a45-11e7-8198-5264c9fcedac.png) vs https://github.com/refack/nodejs.build/tree/a6e26d7c3e498a80189e805ee68fd28cc5dd495d ![image](https://user-images.githubusercontent.com/96947/30505970-1ef90dfc-9a46-11e7-9fb6-12fa0d552e8f.png) 
gibfahn		@refack the `F` in your screenshot looks pretty weird compared to the old image.  ![image](https://user-images.githubusercontent.com/15943089/30516601-8c2b4fd4-9b3b-11e7-878a-1543fd14f5ca.png)  ![image](https://user-images.githubusercontent.com/15943089/30516612-b109e31a-9b3b-11e7-8676-a5ed1c5b86f9.png) 
refack		ahh, might have dragged a vertex inadvertently. Updated - https://github.com/nodejs/build/tree/73235ea03e5dce79dd67c7d81d0ff85741d0747d#tier-2-providers 
joaocgreis		We cycle passwords manually from time to time, but not with the frequency that Windows would like us to. I just ignore that warning. PR to disable in Ansible would be welcome!  Also, when git crashes, to be safe the best is to remove the whole workspace. Fist make sure the machine is not running anything and mark it temporarily offline. Then login and delete the whole `node-compile-windows`. Then bring the machine back online.
mhdawson		+1 from me.
jbergstroem		I've provided him access to `test-joyent-smartos16-x64-2` as well as basic instructions.
gibfahn		+1
matthewloring		I've finished diagnosing the issue and posted an update here: https://github.com/nodejs/node/pull/9304. You're welcome to revoke access now. Thanks!
jbergstroem		@matthewloring how about we keep the access until the pr is merged? If upstream files a patch I reckon you'd want to test anyway?
matthewloring		As long as that's ok on your end it would definitely be more convenient for me. I was going to re-request once there was a patch to try.
jbergstroem		Lets keep it open then. 
matthewloring		This can be closed now.
rvagg		revoked  thanks @matthewloring!
jbergstroem		LGTM 
fhemberger		@nodejs/build So can we merge this? 
jbergstroem		Happy to merge, just wanted to give it some time seeing how I was the only one ack:ing. 
rvagg		it probably should be `&&` not `;`, perhaps it should just be one npm script so you control it all in node/nodejs.org? `npm run deploy` 
fhemberger		Updated to `npm run deploy`. 
rvagg		lgtm 
rvagg		``` > find -E build -type f -regex '.*(html|css|js|xml|json)$' -exec gzip -kf9 {} \;  find: unknown predicate `-E' ```  tested on osx and not linux? reverting on the server until you give the :+1: here that it's ready 
jbergstroem		Urgh. Sloppy reviewing on my end. I'll revisit too. 
jbergstroem		Shouldn't that really just be `find build/ -type f ...`? 
fhemberger		Sorry for that!  Replaced the command with `find build -type f -name '*.html' -o -name '*.css' -o -name '*.js' -o -name '*.xml' -o -name '*.json' -exec gzip -kf9 {} \;` ‚Äì a bit longer, but no incompatible regex switches. Also tested this on Linux as well.  https://github.com/nodejs/nodejs.org/commit/ff6c50f8841032d072b567a0dc3fc8e38fac47b4 
rvagg		@fhemberger OK, I've switched it over to `deploy` now but I don't think it's quite working right yet, my guess is that it's only `-name '*.json'` that's getting picke up cause I see site.json.gz but the others are plain. 
rvagg		Indeed ...  ``` find . -type f -name '*.html' -o -name '*.css' -o -name '*.js' -o -name '*.xml' -o -name '*.json' ```  prints them all but  ``` find . -type f -name '*.html' -o -name '*.css' -o -name '*.js' -o -name '*.xml' -o -name '*.json' -exec echo '{}' \; ```  prints only the .json files, so `-exec` must apply only to the last `-name`  this works tho:  ``` find . -type f \( -name '*.html' -o -name '*.css' -o -name '*.js' -o -name '*.xml' -o -name '*.json' \) -exec echo '{}' \; ```  sort of like a grouped `||` 
fhemberger		@rvagg Okay, tested `find build -type f \( -name '*.html' -o -name '*.css' -o -name '*.js' -o -name '*.xml' -o -name '*.json' \) -exec gzip -kf9 {} \;` again, seems to work fine.  https://github.com/nodejs/nodejs.org/commit/3716408c02e5a59cd99730333ee82df644a0302c 
rvagg		neat, appears to be working: https://nodejs.org/en/index.html.gz  @jbergstroem can we confirm somehow that nginx is actually using these files? or do we just take it for granted? 
jbergstroem		@rvagg I've used it previously -- more info here: http://nginx.org/en/docs/http/ngx_http_gzip_static_module.html. I guess we could set up a test environment to prove it works? 
rvagg		Nah, let's not go overboard, it's just a shame we can't easily run a command and confirm that all this work actually achieved something. 
jbergstroem		We consume less cpu than before at least. 
rvagg		well that's certainly something, good work! 
refack		maybe calling `java -Dhudson.util.ProcessTree.disable=true -jar jenkins.war` as suggested in http://stackoverflow.com/questions/25655867/how-can-i-abort-a-jenkins-build
gibfahn		>To reliably kill processes spawned by a job during a build, Jenkins contains a bit of native code to list up such processes and kill them. This is tested on several platforms and architectures, but if you find a show-stopper problem because of this, you can disable this feature by setting a Java property named "hudson.util.ProcessTree.disable" to the value "true".  How does this mark out-of-matrix tests as grey?  I'm not entirely sure what issue you're asking for a fix for, but my guess is that you mean that when you go to https://ci.nodejs.org/job/node-test-commit-linux/ it doesn't show you the tests that haven't run yet for the latest build (it just shows green if the last time it ran it was green).  ![image](https://cloud.githubusercontent.com/assets/15943089/25535170/0fdd83f6-2c2e-11e7-84c5-c7221c6c9c7d.png)  If you go to a specific build though (e.g. https://ci.nodejs.org/job/node-test-commit-linux/9475/) it does show builds that haven't run yet (or that weren't run with this configuration) as grey:  ![image](https://cloud.githubusercontent.com/assets/15943089/25535188/1b872dc4-2c2e-11e7-98bf-72368d8ab124.png)  
gibfahn		Of course that doesn't work for things where we're doing an `exit 0` within the tests. I think the answer is to skip things in the combination filter where possible.
refack		I mean cases like the screenshot in the first comment ``` Not building v8 on Windows 2008R2 with vs2013 as it is only supported up to v5.x ... Finished: SUCCESS ```
joaocgreis		The screenshot is from `node-stress-single-test`, that uses a shallow structure of just one Jenkins job to do everything, for simplicity. It's debatable if this is an advantage going forward.  For the `node-test-*` jobs, this is skipped in combination filters (which in turn rises issues like https://github.com/nodejs/build/issues/647). This is only possible if there is a parent job that gets the version of node and passes it as a parameter to the subjobs, so it is not possible in the current structure of `node-stress-single-test`. 
refack		Thanks @joaocgreis 
gibfahn		>For the node-test-* jobs, this is skipped in combination filters (which in turn rises issues like #647). This is only possible if there is a parent job that gets the version of node and passes it as a parameter to the subjobs, so it is not possible in the current structure of node-stress-single-test.  As a workaround we could just assume any branch will have `v4` in the branch name if it's a `v4.x` branch (or equivalent for other major versions). That won't cover every case, but it allows us to skip things in the combination filter if the branch has the right name, and it'll work for all the official branches.
refack		I've digged just a little bit, but is there really no way to tell Jenkins to ignore the build from the build script?
kenperkins		+1 
therebelrobot		+1 
retrohacker		:+1:  
kenperkins		I wonder if we can use https://developer.github.com/v3/repos/releases/#upload-a-release-asset to upload our actual `.dmg`, `.dpkg`, `.msi`, etc binaries to GH. 
jbergstroem		@kenperkins I think that's what we want to do. I'll have a look at integrating this with jenkins build steps. 
rmg		@jbergstroem +1 on integrating it into the build steps 
fixe		+1 
jbergstroem		I built a prototype ages ago but I'm starting to feel that this is better hosted off nodejs.org regardless. I wish github gave us slightly more control in that regard. I'd +1 a WONTFIX here. 
rvagg		/cc @ralphtheninja I think you did some work on this for level\* stuff right? perhaps you can lend an opinion here for us. I'm not qualified, I don't use the feature (either as a publisher and I manage to mostly avoid it as a user). 
jbergstroem		So, if we're going to proceed with something here (I'm still `-1`, we lose download info) we probably want to separate the job from Jenkins seeing how indirect access through PR in jobs can bleed credentials. 
jbergstroem		Closing this. Don't see this happening or even why at this point (read: why replace nodejs.org/dist/). Feel free to reopen though. 
Trott		Maybe add a commit message linting/validation step that makes sure the final message: - has a first line that is not empty and is not longer than 50 chars - has a first line that matches a regular expression along the lines of this (untested): `/^[a-z,]+: \w.*/` - has a blank second line - has a non-blank third line - has a blank line before the metadata - has metadata consistent with what it knows about the PR (e.g., PR-URL exists and is correct) - has at least one Reviewed-by metadata line - individuals in Reviewed-by metadata are in fact collaborators (basically compare against a whitelist of people who can give a `LGTM` that counts) - all non-metadata lines are shorter than 72 chars 
jbergstroem		Excellent list by @trott. I'd like to add: - metadata ordering (suggesting: PR-URL -> Fixes -> Refs -> Reviewed-By)  It'd be really nice to have a LGTM validation/"check" (ok/not ok to land); but I think it's too hard to make realistic LGTM counts seeing how new commits could introduce new code. 
refack		Cross ref: #705   There's an issue of racing with humans/concurrent jobs. That's where the concept of a queue comes into play. (https://github.com/nodejs/build/issues/324#issuecomment-185791127)
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
rvagg		ssh keypair is in secrets/build/infra/benchmark_id_rsa & secrets/build/infra/benchmark_id_rsa.pub 
rvagg		As per #348, build ignores the charts/ directory and then it does an rsync from the benchmarking server for the real charts/ directory, i.e. there's a 6-hourly update but it'll also update whenever you push to master on the benchmarking repo. Sound good @mhdawson? 
mhdawson		Sounds good to me. 
mhdawson		Can this be closed now ?  
jbergstroem		Added some nitpick! LGTM regardless. 
mhdawson		I assume this should still be landed. @rvagg can you address @jbergstroem comments and then I'll land ?  
rvagg		done and merged, need to fix up my other www PR now that I've discovered this one .. 
gibfahn		Agreement in the WG meeting was to document this in the Readme, and have the access request process be the same as it is for general Build WG access, a PR to the Readme adding one's name.
joaocgreis		I approve, thanks for helping @gdams !
jbergstroem		+1, I approve.
Trott		I believe there's already enough approvals for this to move forward per our policy, but just in case I'm mistaken: +1 from me too.
mhdawson		K thanks, will talk to George and get his keys onto one of the machines tomorrow.
gibfahn		+1
mhdawson		I've added his key to test-osuosl-aix61-ppc64_be-2
mhdawson		Also added his key to test-osuosl-aix61-ppc64_be-1
thefourtheye		Great! Glad to get more help :-) Thanks @gdams. Have my +1 as well :-)
maclover7		ping -- is this still needed?
mhdawson		@gdams have we resolved the issues with Citgm on AIX ? I think the answer is yes via the ram disk but I thought I'd check.
gdams		We have definitely resolved the issue of speed. I'm happy for this to be closed now
mhdawson		Validated that users/keys don't seem to be there anymore.
joaocgreis		The `tapTestReport` is also a problem for me, but `systemInfo` opens without any problem.  This one was failing yesterday, now seems good: https://ci.nodejs.org/job/node-test-commit-other/423/nodes=smartos14-64/tapTestReport/ 
joaocgreis		The link to specific test outputs seems to always work (just by appending `/test.tap-###/`). E. g. for the link above: https://ci.nodejs.org/job/node-test-commit-linux/lastFailedBuild/nodes=centos5-64/tapTestReport/test.tap-123/ 
jbergstroem		This was tied to a starved nursery gc size. Reckon its fixed. 
rvagg		I think we should hand out badges for finding new errors. This one's a doozy so well done @Trott.  This machine is in their Sunnyvale datacenter, I think that's their original one and we've had a bunch of problems in there. I'm going to shut this machine down and reprovision elsewhere.
rvagg		all done
Trott		Unfortunately, that doesn't look like it fixed it. It's still happening.   https://ci.nodejs.org/job/node-test-commit-arm/11659/nodes=ubuntu1604-arm64/console  ```console gyp FATAL: command execution failed javax.crypto.BadPaddingException: bad record MAC 	at sun.security.ssl.EngineInputRecord.decrypt(EngineInputRecord.java:238) 	at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:974) Caused: javax.net.ssl.SSLException: bad record MAC 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208) 	at sun.security.ssl.SSLEngineImpl.fatal(SSLEngineImpl.java:1728) 	at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:981) 	at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:907) 	at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781) 	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624) 	at org.jenkinsci.remoting.protocol.impl.SSLEngineFilterLayer.processRead(SSLEngineFilterLayer.java:347) 	at org.jenkinsci.remoting.protocol.impl.SSLEngineFilterLayer.onRecv(SSLEngineFilterLayer.java:117) 	at org.jenkinsci.remoting.protocol.ProtocolStack$Ptr.onRecv(ProtocolStack.java:669) 	at org.jenkinsci.remoting.protocol.NetworkLayer.onRead(NetworkLayer.java:136) 	at org.jenkinsci.remoting.protocol.impl.NIONetworkLayer.ready(NIONetworkLayer.java:160) 	at org.jenkinsci.remoting.protocol.IOHub$OnReady.run(IOHub.java:721) 	at jenkins.util.ContextResettingExecutorService$1.run(ContextResettingExecutorService.java:28) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused: java.io.IOException: Backing channel 'JNLP4-connect connection from 147.75.105.54/147.75.105.54:48564' is disconnected. 	at hudson.remoting.RemoteInvocationHandler.channelOrFail(RemoteInvocationHandler.java:192) 	at hudson.remoting.RemoteInvocationHandler.invoke(RemoteInvocationHandler.java:257) 	at com.sun.proxy.$Proxy91.isAlive(Unknown Source) 	at hudson.Launcher$RemoteLauncher$ProcImpl.isAlive(Launcher.java:1043) 	at hudson.Launcher$RemoteLauncher$ProcImpl.join(Launcher.java:1035) 	at hudson.tasks.CommandInterpreter.join(CommandInterpreter.java:155) 	at hudson.tasks.CommandInterpreter.perform(CommandInterpreter.java:109) 	at hudson.tasks.CommandInterpreter.perform(CommandInterpreter.java:66) 	at org.jenkinsci.plugins.conditionalbuildstep.BuilderChain.perform(BuilderChain.java:71) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner$2.run(BuildStepRunner.java:110) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner$Fail.conditionalRun(BuildStepRunner.java:154) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner.perform(BuildStepRunner.java:105) 	at org.jenkinsci.plugins.conditionalbuildstep.ConditionalBuilder.perform(ConditionalBuilder.java:134) 	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:779) 	at hudson.model.Build$BuildExecution.build(Build.java:206) 	at hudson.model.Build$BuildExecution.doRun(Build.java:163) 	at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:534) 	at hudson.model.Run.execute(Run.java:1728) 	at hudson.matrix.MatrixRun.run(MatrixRun.java:146) 	at hudson.model.ResourceController.execute(ResourceController.java:98) 	at hudson.model.Executor.run(Executor.java:405) ```    https://ci.nodejs.org/job/node-test-commit-arm/11654/nodes=ubuntu1604-arm64/console  ```console FATAL: command execution failed javax.crypto.BadPaddingException: bad record MAC 	at sun.security.ssl.EngineInputRecord.decrypt(EngineInputRecord.java:238) 	at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:974) Caused: javax.net.ssl.SSLException: bad record MAC 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208) 	at sun.security.ssl.SSLEngineImpl.fatal(SSLEngineImpl.java:1728) 	at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:981) 	at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:907) 	at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781) 	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624) 	at org.jenkinsci.remoting.protocol.impl.SSLEngineFilterLayer.processRead(SSLEngineFilterLayer.java:347) 	at org.jenkinsci.remoting.protocol.impl.SSLEngineFilterLayer.onRecv(SSLEngineFilterLayer.java:117) 	at org.jenkinsci.remoting.protocol.ProtocolStack$Ptr.onRecv(ProtocolStack.java:669) 	at org.jenkinsci.remoting.protocol.NetworkLayer.onRead(NetworkLayer.java:136) 	at org.jenkinsci.remoting.protocol.impl.NIONetworkLayer.ready(NIONetworkLayer.java:160) 	at org.jenkinsci.remoting.protocol.IOHub$OnReady.run(IOHub.java:721) 	at jenkins.util.ContextResettingExecutorService$1.run(ContextResettingExecutorService.java:28) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused: java.io.IOException: Backing channel 'JNLP4-connect connection from 147.75.111.186/147.75.111.186:56790' is disconnected. 	at hudson.remoting.RemoteInvocationHandler.channelOrFail(RemoteInvocationHandler.java:192) 	at hudson.remoting.RemoteInvocationHandler.invoke(RemoteInvocationHandler.java:257) 	at com.sun.proxy.$Proxy91.isAlive(Unknown Source) 	at hudson.Launcher$RemoteLauncher$ProcImpl.isAlive(Launcher.java:1043) 	at hudson.Launcher$RemoteLauncher$ProcImpl.join(Launcher.java:1035) 	at hudson.tasks.CommandInterpreter.join(CommandInterpreter.java:155) 	at hudson.tasks.CommandInterpreter.perform(CommandInterpreter.java:109) 	at hudson.tasks.CommandInterpreter.perform(CommandInterpreter.java:66) 	at org.jenkinsci.plugins.conditionalbuildstep.BuilderChain.perform(BuilderChain.java:71) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner$2.run(BuildStepRunner.java:110) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner$Fail.conditionalRun(BuildStepRunner.java:154) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner.perform(BuildStepRunner.java:105) 	at org.jenkinsci.plugins.conditionalbuildstep.ConditionalBuilder.perform(ConditionalBuilder.java:134) 	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:779) 	at hudson.model.Build$BuildExecution.build(Build.java:206) 	at hudson.model.Build$BuildExecution.doRun(Build.java:163) 	at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:534) 	at hudson.model.Run.execute(Run.java:1728) 	at hudson.matrix.MatrixRun.run(MatrixRun.java:146) 	at hudson.model.ResourceController.execute(ResourceController.java:98) 	at hudson.model.Executor.run(Executor.java:405) ```
rvagg		OK, that's pretty strange, entirely new host in a different DC so must be the software stack. What I've done on this host (only) is install the Oracle JDK 8 and uninstalled the OpenJDK 8. Cross fingers I guess. Best leave this open and monitor, if this works then we'll need to do the same on the matching machine. If it doesn't work then ... I dunno! Will find some knob to twiddle I guess.
Trott		Here's the most recent one. Not sure if this is on a machine we hope is fixed or a machine that we will apply the fix to?  https://ci.nodejs.org/job/node-test-commit-arm/11730/nodes=ubuntu1604-arm64/console  ```console FATAL: command execution failed javax.crypto.BadPaddingException: bad record MAC 	at sun.security.ssl.EngineInputRecord.decrypt(EngineInputRecord.java:238) 	at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:974) Caused: javax.net.ssl.SSLException: bad record MAC 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208) 	at sun.security.ssl.SSLEngineImpl.fatal(SSLEngineImpl.java:1728) 	at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:981) 	at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:907) 	at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781) 	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624) 	at org.jenkinsci.remoting.protocol.impl.SSLEngineFilterLayer.processRead(SSLEngineFilterLayer.java:347) 	at org.jenkinsci.remoting.protocol.impl.SSLEngineFilterLayer.onRecv(SSLEngineFilterLayer.java:117) 	at org.jenkinsci.remoting.protocol.ProtocolStack$Ptr.onRecv(ProtocolStack.java:669) 	at org.jenkinsci.remoting.protocol.NetworkLayer.onRead(NetworkLayer.java:136) 	at org.jenkinsci.remoting.protocol.impl.NIONetworkLayer.ready(NIONetworkLayer.java:160) 	at org.jenkinsci.remoting.protocol.IOHub$OnReady.run(IOHub.java:721) 	at jenkins.util.ContextResettingExecutorService$1.run(ContextResettingExecutorService.java:28) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) Caused: java.io.IOException: Backing channel 'JNLP4-connect connection from 147.75.111.186/147.75.111.186:50334' is disconnected. 	at hudson.remoting.RemoteInvocationHandler.channelOrFail(RemoteInvocationHandler.java:192) 	at hudson.remoting.RemoteInvocationHandler.invoke(RemoteInvocationHandler.java:257) 	at com.sun.proxy.$Proxy91.isAlive(Unknown Source) 	at hudson.Launcher$RemoteLauncher$ProcImpl.isAlive(Launcher.java:1043) 	at hudson.Launcher$RemoteLauncher$ProcImpl.join(Launcher.java:1035) 	at hudson.tasks.CommandInterpreter.join(CommandInterpreter.java:155) 	at hudson.tasks.CommandInterpreter.perform(CommandInterpreter.java:109) 	at hudson.tasks.CommandInterpreter.perform(CommandInterpreter.java:66) 	at org.jenkinsci.plugins.conditionalbuildstep.BuilderChain.perform(BuilderChain.java:71) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner$2.run(BuildStepRunner.java:110) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner$Fail.conditionalRun(BuildStepRunner.java:154) 	at org.jenkins_ci.plugins.run_condition.BuildStepRunner.perform(BuildStepRunner.java:105) 	at org.jenkinsci.plugins.conditionalbuildstep.ConditionalBuilder.perform(ConditionalBuilder.java:134) 	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:779) 	at hudson.model.Build$BuildExecution.build(Build.java:206) 	at hudson.model.Build$BuildExecution.doRun(Build.java:163) 	at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:534) 	at hudson.model.Run.execute(Run.java:1728) 	at hudson.matrix.MatrixRun.run(MatrixRun.java:146) 	at hudson.model.ResourceController.execute(ResourceController.java:98) 	at hudson.model.Executor.run(Executor.java:405) ```
refack		@Trott has this cropped up recently? 
Trott		@refack Not that I've noticed.
joyeecheung		Similar issues showed up on ubuntu1604-arm64 again, this time it's in the test phase:  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14299/console ``` not ok 1270 parallel/test-regress-GH-1531   ---   duration_ms: 1.60   severity: fail   stack: |-     listening     Error: 281472750350336:error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac:../deps/openssl/openssl/ssl/s3_pkt.c:535:        ... ```
joyeecheung		cc @rvagg 
rvagg		Hah, that's pretty weird! afaik Java doesn't use OpenSSL so we're looking at something deeper here.  @nodejs/crypto if you're looking for an interesting challenge, this might be for you.  For this machine, on top of build 14299 these ones have failed with the same error:  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14307/ https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14317/ https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14268/  These ones have failed with different crypto errors:  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14300/  ``` not ok 260 parallel/test-crypto-binary-default   ---   duration_ms: 1.122   severity: fail   stack: |-     (node:65147) [DEP0091] DeprecationWarning: crypto.DEFAULT_ENCODING is deprecated.     assert.js:74       throw new AssertionError(obj);       ^          AssertionError [ERR_ASSERTION]: '¬ö√¶¬û_]\u0013G\u000e;F¬ß¬ê√ô\u0013e+¬∏\u000fc¬ë' strictEqual '√Ö√§x√ï¬í¬à√àA¬™S\r¬∂¬Ñ\\L¬ç¬ñ(¬ì '         at common.mustCall (/home/iojs/build/workspace/node-test-commit-arm/nodes/ubuntu1604-arm64/test/parallel/test-crypto-binary-default.js:685:12)         at /home/iojs/build/workspace/node-test-commit-arm/nodes/ubuntu1604-arm64/test/common/index.js:467:15         at PBKDF2.next [as ondone] (internal/crypto/pbkdf2.js:83:7) ```  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14284/console  ``` not ok 968 parallel/test-https-agent-session-reuse   ---   duration_ms: 1.4   severity: fail   stack: |-     events.js:116           throw er; // Unhandled 'error' event           ^          Error: 281473003442176:error:140940F6:SSL routines:ssl3_read_bytes:unknown alert type:../deps/openssl/openssl/ssl/s3_pkt.c:1508: ```  The other Ubuntu 16.04 ARM64 machine in CI is much more green but isn't free of crypto failures, there's this one:  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14194/  ``` not ok 1043 parallel/test-https-host-headers   ---   duration_ms: 1.101   severity: fail   stack: |-     test https server listening on port 45571     Got request: localhost:45571 /0     /home/iojs/build/workspace/node-test-commit-arm/nodes/ubuntu1604-arm64/test/parallel/test-https-host-headers.js:32       throw er;       ^          Error: 281472848347136:error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac:../deps/openssl/openssl/ssl/s3_pkt.c:535: ```  And these two that are different again:  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14316/  ``` not ok 1034 parallel/test-https-agent   ---   duration_ms: 1.339   severity: fail   stack: |-     /home/iojs/build/workspace/node-test-commit-arm/nodes/ubuntu1604-arm64/test/parallel/test-https-agent.js:62               throw e;               ^          Error: write EPROTO 281473430319104:error:1409441B:SSL routines:ssl3_read_bytes:tlsv1 alert decrypt error:../deps/openssl/openssl/ssl/s3_pkt.c:1500:SSL alert number 51     281473430319104:error:1409E0E5:SSL routines:ssl3_write_bytes:ssl handshake failure:../deps/openssl/openssl/ssl/s3_pkt.c:659:              at WriteWrap.afterWrite [as oncomplete] (net.js:866:14) ```  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64/14164/  ``` not ok 330 parallel/test-crypto-dh-leak   ---   duration_ms: 1.289   severity: fail   stack: |-     assert.js:243         throw err;         ^          AssertionError [ERR_ASSERTION]: The expression evaluated to a falsy value:            assert(after - before < 5 << 20)              at Object.<anonymous> (/home/iojs/build/workspace/node-test-commit-arm/nodes/ubuntu1604-arm64/test/parallel/test-crypto-dh-leak.js:26:1)         at Module._compile (module.js:666:30)         at Object.Module._extensions..js (module.js:677:10)         at Module.load (module.js:577:32)         at tryModuleLoad (module.js:517:12)         at Function.Module._load (module.js:509:3)         at Function.Module.runMain (module.js:707:10)         at startup (bootstrap_node.js:196:16)         at bootstrap_node.js:706:3 ```  And I can't find anything like this in the CentOS 7 ARM64 builds, so it's limited to Ubuntu 16.04. I don't know to interpret that cause it's the same OpenSSL being compiled on both. Compiler difference perhaps? We're on a gcc 4.8.5 for CentOS 7 and 5.4.0 for Ubuntu 16.04.  Not sure where to take this next tbh.
shigeki		Can I get login the machine? I do not have the arm64 machine.  OpenSSL assember files are platform dependent and they are generated on Linux in upgrading process of OpenSSL. I've never tested on them in a real ARM64 server so it might have some issues.
rvagg		yep @shigeki, root@147.75.74.174 has your github keys in it. That's the machine that throws these errors up most frequently. I was just in there running parallel/test-regress-GH-1531 in a loop and was getting failures roughly every 200 times. I even got an SSH authentication failure when I tried to log in to it the first time today .. I'm not sure if that's the same but it's certainly fishy.
shigeki		Thanks. I can login now. I will investigate the issue.
shigeki		I built openssl-1.0.2n and node and make tests.  OpenSSL tests worked fine and node test-ci had some flaky errors but not related to crypto and TLS.   I also made thousands tls connections between node tls server and client but no errors were found so that I could not reproduce the errors.  As far as checked the size of openssl assembler files between node and openssl-1.0.2n, they are the same size as below. ``` root@test-packetnet-ubuntu1604-arm64-2:/var/tmp/shigeki/openssl-1.0.2n# find crypto -mtime 0 -name  '*.S' |xargs ls -l -rw-r--r-- 1 root root 14899 Mar  6 05:14 crypto/aes/aesv8-armx.S -rw-r--r-- 1 root root  6496 Mar  6 05:14 crypto/modes/ghashv8-armx.S -rw-r--r-- 1 root root 27982 Mar  6 05:13 crypto/sha/sha1-armv8.S -rw-r--r-- 1 root root 31754 Mar  6 05:13 crypto/sha/sha256-armv8.S -rw-r--r-- 1 root root 28126 Mar  6 05:13 crypto/sha/sha512-armv8.S ``` ``` root@test-packetnet-ubuntu1604-arm64-2:/var/tmp/shigeki/node/deps/openssl/asm# find arm64-linux64-gas -name '*.S' |xargs ls -l -rw-r--r-- 1 root root 14899 Mar  6 04:56 arm64-linux64-gas/aes/aesv8-armx.S -rw-r--r-- 1 root root  6496 Mar  6 04:56 arm64-linux64-gas/modes/ghashv8-armx.S -rw-r--r-- 1 root root 27982 Mar  6 04:56 arm64-linux64-gas/sha/sha1-armv8.S -rw-r--r-- 1 root root 31754 Mar  6 04:56 arm64-linux64-gas/sha/sha256-armv8.S -rw-r--r-- 1 root root 28126 Mar  6 04:56 arm64-linux64-gas/sha/sha512-armv8.S ``` There may be other reasons to cause this issues. 
vielmetti		@rvagg can you confirm the IDs of these systems under test that are throwing errors? I will double check the firmware on those systems.  The output of `uname -a` on the affected machines would be useful too.
vielmetti		Also @rvagg feel free to spin up an additional machine just to loop `parallel/test-regress-GH-1531`, the one that fails 1 in 200 times. The systems you have are 32GB machines, so grab a 128GB machine as well and see if an otherwise unloaded different system will exhibit the same failures. Would love to have a reproducer that's not on your production environment.
neemah		Hi, we have SSL3_GET_RECORD error, Node 9.5.0, 9.8.0 on Heroku Cedar-14 stack (Ubuntu 14.04)  It's not frequent but couple of times happened during previous week.  <img width="1030" alt="screen shot 2018-03-12 at 15 27 31" src="https://user-images.githubusercontent.com/212695/37284271-f3a63542-260b-11e8-84ff-0481f57f41e0.png"> 
vielmetti		I'll note this post which reports a similar set of issues to what @neemah reported re Heroku.  https://serverfault.com/questions/859987/im-getting-error-ssl3-get-recorddecryption-failed-or-bad-record-mac
maclover7		These machines look to be doing better these days -- going to close out this issue. Please reopen if this requires further action
Trott		(11 consecutive failures and counting at time of this writing, just in case that's useful.)
refack		I think there is something wrong with the machine config: ![image](https://user-images.githubusercontent.com/96947/29489009-96d66814-84e4-11e7-9ac4-97cc054eb904.png) It seems like the job is trying to run on a Pi 
Trott		Same thing going on with the fips job. @refack reminded me that @rvagg had a power outage ("Pi-tastrophe" as @refack said) and it seems maybe an errant click or something in the cleanup after that maybe caused this or something like that?
gibfahn		>It seems like the job is trying to run on a Pi  So the job running on a PI is fine, that's the parent job, which just clones the repo and then triggers the actual build.  This looks like an issue with that machine.
joaocgreis		As @gibfahn mentioned, the `test-osx` job, like many others, is a multi-configuration job: there is something like a main/parent job that spawns sub-jobs that run on the actual platforms that we want to test (when there is only one, like in the OSX job, Jenkins calls it `default` instead of the platform label). The problem was that Jenkins was changed a few days ago to run all of the parent jobs in dedicated machines instead of the Jenkins master. Jenkins offers two mechanisms to bind jobs to a specific machine: "Preference of node" and "Restrict where this project can be run" (under advanced). We have always been using "Preference of node" without issue. However, now, probably because it's no longer `master`, Jenkins is not respecting the preference as absolute, or for some reason it's not working as before.  I'll change all jobs to "Restrict where this project can be run". This apparently solved the issue for OSX and FIPS, the OSX machine itself seems fine.
refack		BTW: perfect opportunity to use the misunderstood "Resume Build" button Original job - failed on osx and FIPS - https://ci.nodejs.org/job/node-test-commit/11909/ Rebuild - only tries those two: - https://ci.nodejs.org/job/node-test-commit/11913/  ![image](https://user-images.githubusercontent.com/96947/29490973-36bf43c4-851b-11e7-9c84-137dc6fe47cb.png) 
rvagg		I'm certain I restricted this job and fips and most of the rest of the unrestricted jobs to run on jenkins-workspace when I made the change. Have we had some kind of config reversion perhaps? I even grepped through config files to find out which ones might be impacted to do this. 
joaocgreis		@rvagg that was using the "Preference of node" option, I confirm it was set to `jenkins-workspace`. You did change all of them from `master`. That was how it's done before for master in many jobs, and has always worked until now. (Or are you sure you changed them to "Restrict where this project can be run" under "Advanced options"?)  I'm changing all the jobs to "Restrict where this project can be run", 41 left now.
rvagg		Pretty sure it was restrict, but if it's set in preference then maybe I did some wrong by mistake. That would explain the discrepancy I suppose. I'll do some more grepping when I'm at my computer later today and see what else might be off. 
joaocgreis		Done. I changed 74 jobs to "Restrict where this project can be run", another 68 were already working this way. I've confirmed that `canRun` is false in all jobs that have it (in `config.xml`) - it is true when "Preference of node" is used.  @rvagg this is what I've changed: https://ci.nodejs.org/view/All/job/node-test-commit-linux-fips/jobConfigHistory/showDiffFiles?timestamp1=2017-08-19_19-45-23&timestamp2=2017-08-19_19-46-40 , you might have check some of the jobs that already had this. My guess is that you did change everything and changed everything right, but missed that there are two different options that accomplish the same (except that one only works some times).  As a last note, I found some more jobs that were running in other nodes not `jenkins-workspace`: https://ci.nodejs.org/view/All/job/node-test-commit-linux/11946/ , https://ci.nodejs.org/view/All/job/node-compile-windows/11254/ . My guess is there are plenty more, and we were bound to see this happen when they some ran in the Pis. Apparently, when jenkins decides to use some node to run a job, it'll keep using it for all runs: that's why OSX was failing every time. 
rvagg		Excellent work @joaocgreis, thanks for doing this!
rvagg		@joaocgreis the coverity nightly builds were failing cause they were moved to jenkins-workspace but they depend on the coverity tools in /var/cov-analysis-linux64-2017.07/ which is installed on test-digitalocean-debian8-x64-1 which is where it was running before. I've installed these tools onto the jenkins-workspace servers so it can run there now. Unfortunately there's a compile involved and these machines aren't ideal for compiling (Atom), but they are not terrible and this isn't a time-sensitive build so I think this is OK.  The coverity tools aren't in Ansible (afaik) but I guess we should document this somewhere.
gibfahn		@rvagg documenting them would be great. Maybe another file in `doc/` that covers odd bits that aren't covered in ansible or other places.
rvagg		Done in #843
jbergstroem		You need to call python from `python` since we override default path on systems that ships too old versions (which we override by installer newer ones) 
MylesBorins		what would be the best way to do this if we are calling citgm, which is then running npm in a child process?  Would simply including the env var when running the process work?  `PYTHON=python citgm-all` ? 
jbergstroem		Actually, that's why we pass python through the environment variable `PYTHON` to the build system on centos5. We essentially do `export PATH=/home/iojs/bin:$PATH` and call `python`.  ``` bash [root@node-centos5-32 ~]# python -V Python 2.4.3 [root@node-centos5-32 ~]# /usr/bin/python -V Python 2.4.3 [root@node-centos5-32 ~]# /usr/bin/python2 -V Python 2.4.3 [root@node-centos5-32 ~]# /usr/bin/python26 -V Python 2.6.8 [root@node-centos5-32 ~]# /home/iojs/bin/python -V Python 2.6.8 [root@node-centos5-32 ~]# ls -al /home/iojs/bin/python lrwxrwxrwx 1 iojs root 17 Nov 11 20:03 /home/iojs/bin/python -> /usr/bin/python26 ``` 
MylesBorins		maybe you can take some time with me tomorrow and try and get centos working. I think I have just about everything "workingish" aside from windows, which I have not yet attempted 
jbergstroem		@TheAlphaNerd give what you wrote a try (`PYTHON=python $foo`) and let me know how it works. Happy to help out. 
jbergstroem		@TheAlphaNerd did you get it going? 
MylesBorins		@jbergstroem I ended up chasing some other bugs on ppc last week. I'll revisit this after wrapping up the 5.4.1 release 
jbergstroem		Updated title to better reflect the issue. 
jbergstroem		ping! 
MylesBorins		@jbergstroem I'll take a look at this tomorrow :D 
jbergstroem		ping @thealphanerd. I think we can close this :)  
jbergstroem		@TheAlphaNerd anything to report? 
MylesBorins		I have not tried getting citgm working on centos after this. I'll close this and reopen if we hit this again 
kenperkins		When we merge this @wblankenship we need to add a section that addresses #39. 
jbergstroem		Do we have any outstanding issues that needs to be addressed? I think this is in good enough shape to merge regardless of potential open questions since it'd be better to have this living in our repo than not. We could always address them in a forthcoming PR. 
retrohacker		I think this is ready for prime time :+1:  Still need to work out the sub-WG clause, but I agree that it can wait. 
kenperkins		:shipit:  
rmg		LGTM 
kenperkins		Pretty sure @rvagg wanted to have a pass at this before we merged. 
retrohacker		Can we apply the tag `WG-agenda`? 
kenperkins		done 
retrohacker		For consensus, we need a :+1: to merge from: - @othiym23 - @rvagg  - @bnoordhuis  Barring any changes to the :+1:s we have already received. 
othiym23		:+1: 
bnoordhuis		LGTM 
retrohacker		This fell off my radar during finals week. I seriously dropped the ball on it. We do need to get a governance doc merged in. Should I update this with the latest version from https://github.com/nodejs/io.js/blob/master/WORKING_GROUPS.md#bootstrap-governance 
rvagg		Yes, maybe ping the group for a few +1's before merging the new one  
retrohacker		Noticed @othiym23 is in this PR under Current Project Team Members and @geek is in the Readme under People. Are you both in the working group or are one/both of these stale documents?  Updating PR now, will leave both names in until told otherwise :smile: 
retrohacker		@nodejs/build  Updated to latest. Ready to merge if everything looks good. 
othiym23		I'm still participating! Probably more so in the future than in the past! 
retrohacker		@othiym23 sweet! :smiley:   I'll merge tomorrow evening if nobody has any objections 
rvagg		@wblankenship ping, don't forget this one 
dshaw		@mikeal What's the goal of this non-voting TC seat? Secretarial? Leason? Community representation? Happy to help bootstrap that if the TC needs contributors. 
mikeal		We just want to foster tighter collaboration between the TC and everyone the group tackling the build system. Today a lot of stuff came up that build should probably know about, so it the idea was that we should have whoever is leading the build effort (current @rvagg) on the TC meetings regularly.  The TC is consensus seeking, nothing has actually gone up for a vote yet because it's been unnecessary, but if something did become contentious and required voting the concern is that those closest to core should be the ones voting. 
rmg		I think @rvagg is the natural choice if the TC is making decisions that affect his efforts. 
dshaw		Awesome. Thanks for clarifying. Sounds good. 
rvagg		Makes sense, I'm happy to join and can make time for this. I can probably even take secretarial duties if that's a role that needs to be filled. 
bnoordhuis		I think we can close this now. :-) 
rvagg		@saghul yep, we can do this, we've done it for the iojs ones and the upcoming converged iojs/node ones too and so far I'm liking this although @orangemocha objects to the complexity!  I have some other work I want to do to bring the libuv jobs up to scratch; how much of a hurry is this for you @saghul? 
saghul		No rush whatsoever Rodd! Thanks for the prompt response. On Jul 10, 2015 3:57 AM, "Rod Vagg" notifications@github.com wrote:  > @saghul https://github.com/saghul yep, we can do this, we've done it > for the iojs ones and the upcoming converged iojs/node ones too and so far > I'm liking this although @orangemocha https://github.com/orangemocha > objects to the complexity! >  > I have some other work I want to do to bring the libuv jobs up to scratch; > how much of a hurry is this for you @saghul https://github.com/saghul? >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/132#issuecomment-120199792. 
orangemocha		Please disregard my objections :wink: I just noted that managing one job is a bit easier than managing 5 when you make configuration changes. But other folks in the build team pointed out the better reporting available with separate jobs, so I am neutral by now. 
saghul		Since I'm here, I'll add another item to my initial request: 2 extra targets for Windows, which run the tests on the shared library build. (it should fail now, I have an open PR which will land today fixing that) 
jbergstroem		@saghul do you support auto\* on windows as well or is that gyp-only? 
saghul		@jbergstroem We do support MinGW-w64 toolchain, but I didn't want to put more weight into the build team's shoulders :-) 
jbergstroem		@saghul lets stick with gyp for now then. I can probably look at adding this tomorrow. 
saghul		Great, thanks! 
rvagg		derp 
Trott		Looking at https://ci.nodejs.org/view/libuv/, I _think_ this can be closed as I see stuff for Windows, AIX, etc. I'm going to Be Bold and close this, but feel free to re-open or comment if I'm mistaken. This issue hasn't been updated in over a year, so I strongly suspect the situation has changed since it was last updated. 
mhdawson		Well I could not ssh in either from the public or private ip so I tried to soft reboot which failed, then shutdown which worked but now it will not start up.  Sent osuosl an update on the ticket. 
mhdawson		Been an ongoing saga. Has been brought online and then failed again a few times.  Currently down. 
mhdawson		Back up, latest update sent to osuosl:  Following up further based on a suggestion from Johan I checked the key that was being reported during the outage and I believe it was :  ECDSA key fingerprint is e5:c1:e0:a0:a9:ba:e1:38:f1:96:f9:35:e9:b5:4c:c5  which does not look like it matches the one reported when the machine is working properly:  RSA key fingerprint is d9:62:25:3e:2f:32:15:6d:74:c2:02:93:7a:19:53:68. 
mhdawson		After lots of discussion back and forth the machine has been moved to a different node and it looks ok after having run a number of jobs through it.  Closing 
jbergstroem		The discussion came up as the result of doing changes to a job and wanting to revert. The suggested procedure here is to copy a job if you're uncomfortable with editing "live".  Fwiw, my position is `-0`; I'm not a big fan of how we are wordpress:ing our jenkins setup with plugins and would like to shrink what plugins we use, not grow them. 
MylesBorins		@jbergstroem is there a way to get this functionality (reverting changes) without adding a plugin? 
rvagg		We could achieve this with a smarter backup system that saves differentials or incrementals, they are really all just XML files on the server. 
jbergstroem		Yep. I'd like to get [rsnapshot](http://rsnapshot.org) rolling. Perhaps fiddle with rsync depth to avoid job history? The other option would be looking at how long we store build logs. 
jbergstroem		We've as of last Wednesday deployed a configuration history plugin. Closing. 
MylesBorins		üéâüéâüéâ` 
joaocgreis		All servers are now back online. 
MylesBorins		I'll likely be submitting a talk about the work being done on smoke-testing + citgm.  @mikeal is there a preference for different material at each event, or is there a want to have repeated material to give attendees at both locations an opportunity to see the content? 
mhdawson		My take was doing this in multiple venues would make sense as even in cases were its some of the same people there will be updates of progress since the last time.    Not directly related to this issue but as a FYI, We have submitted WG talks to node summit and plan to submit to the Node interactive events (just need the final ok) to cover the following:   Benchmarking    Post-Mortem  Does make me think I should see if @stefanmb and Ian Halliday  who volunteered to move module API work forward  can co-submit one for the API WG. 
mhdawson		@jbergstroem, @rvagg , @orangemocha , @joaocgreis are you interested in submitting a talk on this front.  I'm happy to do so but think you 4 are more active so if possible would make sense for you to do it. 
jbergstroem		@mhdawson bad timing for me; count me out. 
rvagg		@joaocgreis & @orangemocha are the other top candidates for this, I'd like to hear if either of them would like to do this first? 
jbergstroem		(I'm happy to review/prepare slides pre if needed though) 
joaocgreis		It's not possible for me. Thanks! 
orangemocha		I can't commit to it either, sorry. 
MylesBorins		I'm open to talk about the smoke testing and ci work I've been doing. Not exactly the same as some of the deeper build infra, but thought I'd throw that out there 
mhdawson		Call for paper closes May 13 as an FYI.  @jbergstroem  in terms of timing are both the EU and US times a no go ? It does not necessarily need to be the same person which presents at each of the conferences. 
jbergstroem		@mhdawson unfortunately, yes. I will be working from Latin America for a while and need to be on location. 
mhdawson		@rvagg just a reminder that you only have a few days left if you want to submit a proposal 
rvagg		over to you @mhdawson, looks like everyone else is out at this stage 
mhdawson		Ok. Do we as a team think its a good idea ?  If so I assume people are willing to collaborate to put together the deck.  I'm ok with delivering but want it to reflect this groups message. 
rvagg		Yes, it's a good idea, whenever I've presented information about what we do either in online form or spoken form, people love it. There's some fascinating nerdy goodness about the work we do here and many people find it exciting to be able to look inside. We should be able to collaborate. 
jbergstroem		Count me in, as long as we're doing it in comic sans or times new roman üåà  
mhdawson		Ok sounds good. How about this for an abstract (please suggest revisions/updates)   Keeping the Node.js community insfrastructure humming -  an update from the build Workgroup  The build workgroup (https://github.com/nodejs/build/) has the mission to provide Node.js Foundation projects with solid computing infrastructure covering a wide range of platforms and different software stacks.  In this talk we will introduce the general philosophy of how infrastructure is sourced, the overall picture of the infrastructure, some of the interesting interactions and,  just as important, how you might get involved. Come learn about the infrastructure that powers the delivery of Node.js !  Can I also include you two (Rodd, Johan) as secondary presenters to reflect your contribution to the deck and just in case you do make  it to the conference ?  Also I think I should likely add anybody else who might make it but cannot commit just in case. 
rvagg		@mhdawson sounds good, feel free to include me, I may be there if we do a collaborator summit thingo too. In the abstract you could highlight more how we have built this massive CI infrastructure entirely through donations from companies and individuals, that's a great story IMO. 
jbergstroem		Get me in there too. Again, unlikely I will make it.  I'm with @rvagg; please put a strong emphasis on sponsors/donators; ideally I'd like a slide or two showcasing the visibility amongst the community you can grow by chipping in. A solid and diversified infrastructure is a key part of making Node.js reliable on all platforms. 
mhdawson		Ok, tweaked as follows to emphasize those points:  The build workgroup (https://github.com/nodejs/build/) has the mission to provide Node.js Foundation projects with solid computing infrastructure covering a wide range of platforms and different software stacks. In this talk we will introduce the general philosophy of how infrastructure is sourced, which has a strong focus on community donations, the overall picture of the substantial infrastructure we've manged to build using this approach, some of the interesting interactions and, just as important, how you might get involved. Come learn about the infrastructure that powers the delivery of Node.js ! 
mhdawson		Will submit, today so we don't miss the deadline, I can easily update the abstract at least until the end of Friday when the CFP closes. 
mhdawson		@Rodd, @jbergstroem  You two should receive emails asking you to complete your speaker profiles 
mhdawson		Submissions were made to Node interactive EU and US so closing this issue. 
mhdawson		Submissions were made to node interactive EU and US so closing this issue 
gibfahn		@nodejs/platform-smartos , anyone else want to review?
misterdjules		@gibfahn May I merge these changes? I don't know what the process is for merging changes in this repository, so a bit of guidance would be appreciated in order for me to ensure that I follow any process currently established.
gibfahn		>@gibfahn May I merge these changes? I don't know what the process is for merging changes in this repository, so a bit of guidance would be appreciated in order for me to ensure that I follow any process currently established.  If it's been reviewed and has been open for a couple of days, anyone in nodejs/build can merge. We tend to follow a more relaxed version of the core rules (so `Squash and merge` is fine).
misterdjules		Thanks for merging and for the info, I'll keep that in mind next time!
gibfahn		+1
jbergstroem		+1 -- can you facilitate?
mhdawson		Will get/add his key on monday
mhdawson		Actually adding his key right now.
maclover7		It looks like nodejs/v8#6 has been closed and fixed -- is access still needed?
mhdawson		Validated that access has been removed, closing.
mhart		I'd keep striving to get Alpine 3.4 working tbh ‚Äì it adds a bunch of very useful stuff for ppl in environments like Kubernetes and it'd be easier to deal with just one version of Alpine. Node.js definitely builds on it, I guess it's just a matter of finding out what's going on in this env.  I know very little about the nodejs build environment, so it'll take some digging to figure out how things are setup 
mhart		I explored the tests a while ago and found some reasons they were failing:  https://github.com/nodejs/docker-iojs/issues/44#issuecomment-95969892 
cjihrig		Two of the failures (parallel/test-net-better-error-messages-port-hostname and parallel/test-net-connect-immediate-finish) are `AssertionError: 'EAI_AGAIN' === 'ENOTFOUND'`, which are being discussed in https://github.com/nodejs/node/pull/5099 and https://github.com/nodejs/node/pull/5196. 
mhart		I'd also be interested to see if, instead of having to build inside of Alpine, you can just build with musl instead of glibc ‚Äì technically I think that should work. (I believe this is what rust does, for example) 
mhart		(actually, that idea doesn't really address the testing-in-Alpine aspect, so it's probably of only limited value) 
rvagg		Updated to Alpine 3.4 thanks to @mhart's pointer.  Here's a run on `master`: https://ci.nodejs.org/job/node-test-commit-linux/3803/nodes=ubuntu1604_docker_alpine34-64  Only 3 failures:  parallel/test-net-better-error-messages-port-hostname parallel/test-net-connect-immediate-finish parallel/test-setproctitle 
mhart		@rvagg the `setproctitle` one could possibly be fixed by adding `procps` to the list of `apk` packages to install ‚Äì apparently that allows support for the `-p` flag. Whether or not that's a good enough solution, I dunno. I'm not sure why the test has to specifically rely on that flag being supported though ‚Äì¬†perhaps it could be changed to something else? 
rvagg		great, procps has done it, updated Dockerfile and we're down to just the `EAI_AGAIN' != 'ENOTFOUND'` errors https://ci.nodejs.org/job/node-test-commit-linux/3804/nodes=ubuntu1604_docker_alpine34-64 
Vanuan		> After some digging I discovered that the preferable way to run Node in Alpine is to build it in alpine rather than reusing binaries that we build for generic Linux  It's not possible to build Alpine binaries in CentOS unless you use a cross compiler. CentOS (and almost any other generic linux) uses `glibc`, Alpine uses `musl`. 
Vanuan		So this is a continuous integration part (point 3 in https://github.com/nodejs/build/issues/301#issuecomment-220574960), right? Would it be trivial to port this to other versions of node? 
jbergstroem		I think we need bash as requirement 
jbergstroem		I guess its worth mentioning that we now have full alpine passes through both docker instances and joyent's virtualization. 
jbergstroem		We should probably update to Alpine 3.5. There's also a few new Ansible modules for managing (building/running) docker containers we can replace the shell commands with. I see both as optional before merging though.
SimenB		>We should probably update to Alpine 3.5.  Alpine 3.6 is released, FWIW.
rvagg		alpine is in the new ansible/ stuff so I'm assuming it's good there, I haven't used it myself though
SimenB		Is there a generic issue for tracking a build of node for alpine? Or should I just watch #989?
rvagg		New version of this in #992, that's what's active in CI now with Alpine 3.4, 3.5 and 3.6 running in node-test-commit-linux. I've taken the old test-digitalocean-ubuntu1604_docker_alpine34-x64-1 host out of rotation, it was set up according to this PR and has been running our Alpine 3.4 tests till now.
mhart		@SimenB this might be what you're looking for? https://github.com/nodejs/build/issues/75
orangemocha		Meeting is confirmed for tomorrow. Please take the time to tag any issues that you would like to discuss. Thanks! 
rvagg		I'm not sure I actually _want_ to get up early tomorrow, I may do but just in case I don't, here's some thoughts on issues: - Jenkins access:   - I'd like to make it easier for additional people to use Jenkins, in general   - I am concerned about further encouraging a lax attitude towards Jenkins use even by Collaborators. I don't think we've done a good job at reinforcing how important it is to check code before throwing it at Jenkins, mostly because it _feels_ like an isolated system and _looks_ like Travis et. al. but that's not the case and we have to ensure that everyone pushing code at Jenkins takes responsibility for doing so, knowing that they could be exposing our CI system to malicious code that has greater impact beyond individual runs. Perhaps we need to put some thought into this, a checkbox maybe that they have to tick to acknowledge the risks?   - We need to switch to the per-project authentication mode where we can assign different permission levels to different GitHub groups, this will make this process much easier and I'd be much more comfortable handing out access.   - We do a lot of hardware sharing so even a per-group restriction doesn't go far enough. For example, the smoke testing reuses CI machines. Do we need to be more rigid in our separation of servers? - Running V8 test suite: does this need to be on the agenda, is something blocking that a meeting is required for? - Publish benchmark results: yes, I'm guessing I'm the blocker here, sorry, there's no reason I should be a blocker as others have access to nodejs.org it's just that I express the strongest opinions about things done on there I suppose. I'm happy for someone else to propose the technical steps to getting this done, allowing review and then making it happen. Otherwise it can wait for me to get to it (I may have to be reminded again!). I'm working my way through a big backlog at the moment, I've let things slip over the past ~week so am quite behind. - Backups for our resources: what's the status here, what can we do to move forward or is it done? 
mhdawson		At this point I don't think "Running V8 test suite"needs to be on the agenda, in past meetings it was left there simply so I could give build team members an update.   I'm slowly working through the issues/questions that need to be resolved in order to get regular runs. 
orangemocha		Ok, I removed "Option to run V8 test suite [#199]" from the agenda.  I think the conversation about Jenkins access should be had off the air, given the security sensitivity.  
jbergstroem		Regarding backups: The plan was to get it in last week but when the jenkins security notice came up I decided to push this forward until it's settled and we have a 'known state' again. 
orangemocha		The private discussion ended taking a large portion of the hour, so we decided not to start the broadcast.  Meeting notes are here: https://docs.google.com/document/d/1srGblqnYaC5k-lHCSy08ar4aiq02lV2zWPA4vtzp5KQ/edit?usp=sharing 
jbergstroem		- ansible playbooks - backup routines - backup machines
joaocgreis		- Release Jenkins - jobs - Release Jenkins - infrastructure  And split Website into: - Website - WWW - Website - Nightlies (promoting, ...) - Website - Releases 
refack		Put me down for everything Windows.
Trott		Inactive for almost two years. Closing. Feel free to re-open if that's wrong and someone plans on actively completing this. 
gibfahn		``` root@test-joyent-freebsd10-x64-1:~ # df -h Filesystem      Size    Used   Avail Capacity  Mounted on /dev/vtbd0p2    9.2G    7.1G    1.4G    84%    / devfs           1.0K    1.0K      0B   100%    /dev /dev/vtbd1p1     48G     32M     45G     0%    /data ```
gibfahn		It seems a bit excessive to require a 10GB machine to have 1.5GB free space.  ``` root@test-joyent-freebsd10-x64-1:/usr/home/iojs # du -sh * .* 4.3G	. 1.7G		build 2.5G	.ccache 4.0K	.cshrc 102M	.jenkins ```  ``` root@test-joyent-freebsd10-x64-1:/usr/home/iojs/build/workspace # du -sh *  30M	libuv-test-commit-freebsd 789M	node-stress-single-test 893M	node-test-commit-freebsd ```  I just ran `node-stress-single-test` on FreeBSD, so I suspect that's what pushed it over the edge.  cc/ @jbergstroem 
Trott		FreeBSD 10 jobs in CI seem to not be running now. As a "I have no idea what I'm doing, guess I'll restart something" move, I restarted Jenkins on test-joyent-freebsd10-x64-1. No change, though.
gibfahn		![image](https://cloud.githubusercontent.com/assets/15943089/23845633/2821f582-07c1-11e7-9440-bb63d18c91dd.png)  @Trott short term fix is to keep clicking the "Bring this node back online" button. Proper fix is to clean up some space on the machine.  If the jobs are getting queued up find which label they want to run on, do a label search for that label (e.g. https://ci.nodejs.org/label/freebsd10-64/) and then work out what's up with those machines.
jbergstroem		Looks like the disk is mounted at `/data`. Don't recall this previously. I will look at mounting `/usr/home` to `/data` and mount binding it. Probably need to review other joyent hosts too.
gibfahn		@jbergstroem do you know why [test-digitalocean-freebsd10-x64-1](https://ci.nodejs.org/computer/test-digitalocean-freebsd10-x64-1/) is offline? Having neither of them means that no PRs will actually run unless someone keeps hitting the `Bring This Node Back Online` button. Currently it's me doing it.
jbergstroem		@gibfahn yes. digitalocean took it down for maintenance and now its not coming back up. potentially some bug with network config as provided by DO. support suggests to log in with console but seeing how we don't rely on root password I'm stuck there. I will redeploy or log into single user mode this week.
jbergstroem		Still can't quite explain why the mount that used to be `/usr/home` is now `/data`; but I moved the contents to the new volume and mounted it through fstab. I'll revisit the other VMs shortly.
rvagg		lgtm
mhdawson		Landed as 9774dae2b523f3252ff506f4e0a3ede8a33c389b
jbergstroem		Needs proper restart; I've pinged @rvagg about it. 
rvagg		sorry, I'm a FreeBSD noob, fixed it _properly_ now I hope 
rvagg		thanks for the reminder, see #218  
rvagg		goodo! I wouldn't mind seeing monit in here too, but that's optional  in fact, I started putting monit on some of the pi's and it basically replaced an init script because it handled all of the lifecycle itself 
jbergstroem		Happy to use monit -- the reason I haven't really considered it is because I haven't seen jenkins die (except Windows). 
jbergstroem		I'm going to leave monit out of it until we run into issues. Cool? 
rvagg		yes, cool daddy-o, :thumbsup: :watermelon:  
jbergstroem		Ok, haven't touched anything for a while now. Considering this being stable. Will merge shortly. 
jbergstroem		Merged in 8df1a177215ec378a56696c3717b826c17ae88e3. Thanks for the feedback making this land. 
jeevandongre		Is this issue resolve? I am facing the same issue. @joaocgreis 
gibfahn		>Is this issue resolved?  You'd need to raise that with the Jenkins Git Plugin, in https://github.com/jenkinsci/git-plugin/.
joaocgreis		@jeevandongre Not on our end, and not upstream as far as I know. If you experiment with it, it would be great if you could share what works and what doesn't, might help us move it forward here. Thanks!
maclover7		ping -- is this still warning happening?
joaocgreis		Yes. For reference, the `build.xml` for https://ci.nodejs.org/view/All/job/node-test-pull-request/11314/ is 17M, but becomes only 316K if the BuildData is blindly removed. The main challenge here is testing some solution without risking breaking everything.
joaocgreis		Changed the `node-compile-windows` and `node-test-binary-windows` jobs to stop using the Jenkins Git plugin and use git commands directly.  <details> <summary>Looking quite good for now!</summary>  ``` root@infra-digitalocean-ubuntu14-x64-1:/var/lib/jenkins/jobs# find node-test-binary-windows -name build.xml -exec du -sh '{}' \; | grep 19955 | sort -k2 20M     node-test-binary-windows/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/0/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/1/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/2/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/3/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/0/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/1/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/2/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/3/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/0/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/1/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/2/builds/19955/build.xml 5.1M    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/3/builds/19955/build.xml root@infra-digitalocean-ubuntu14-x64-1:/var/lib/jenkins/jobs# find node-test-binary-windows -name build.xml -exec du -sh '{}' \; | grep 19956 | sort -k2 12K     node-test-binary-windows/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/0/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/1/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/2/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win10/axis-RUN_SUBSET/3/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/0/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/1/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/2/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017/axis-RUNNER/win2008r2-vs2017/axis-RUN_SUBSET/3/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/0/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/1/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/2/builds/19956/build.xml 4.0K    node-test-binary-windows/configurations/axis-COMPILED_BY/vs2017-x86/axis-RUNNER/win2012r2/axis-RUN_SUBSET/3/builds/19956/build.xml ```  </details><br>  For reference, the main things to look at when changing from plugin to script are the refspec and the branch to checkout. If we use this for jobs that don't fetch from the temporary repo, we'll have to look at keys as well.  <details> <summary>Windows batch script</summary>  ```bat TASKKILL /T /F /FI "IMAGENAME eq node.exe" TASKKILL /T /F /FI "IMAGENAME eq cctest.exe" TASKKILL /T /F /FI "IMAGENAME eq run-tests.exe" TASKKILL /T /F /FI "IMAGENAME eq msbuild.exe" TASKKILL /T /F /FI "IMAGENAME eq mspdbsrv.exe" TASKKILL /T /F /FI "IMAGENAME eq yes.exe"  IF EXIST .git/index.lock rm -rf .git  SET "ORIGIN_REFS=+refs/heads/master:refs/remotes/origin/master" SET "ORIGIN_REFS=%ORIGIN_REFS% +refs/heads/v6.x-staging:refs/remotes/origin/v6.x-staging" SET "ORIGIN_REFS=%ORIGIN_REFS% +refs/heads/v8.x-staging:refs/remotes/origin/v8.x-staging" SET "ORIGIN_REFS=%ORIGIN_REFS% +refs/heads/v10.x-staging:refs/remotes/origin/v10.x-staging" SET "ORIGIN_REFS=%ORIGIN_REFS% +refs/heads/v11.x-staging:refs/remotes/origin/v11.x-staging"  @REM Manual git git --version git init git clean -fdx  git fetch --no-tags https://github.com/nodejs/node.git %ORIGIN_REFS% if %errorlevel% neq 0 echo Problem fetching the main repo.  grep -q ^%TEMP_REPO_SERVER% %USERPROFILE%\.ssh\known_hosts || (ssh-keyscan -t rsa %TEMP_REPO_SERVER% >> %USERPROFILE%\.ssh\known_hosts) echo off FOR /F "delims=" %%F IN ('cygpath -u %JENKINS_TMP_KEY%') DO SET "GIT_SSH_COMMAND=ssh -i %%F" echo on git fetch --no-tags %TEMP_REPO% +refs/heads/%TEMP_BRANCH%*:refs/remotes/jenkins_tmp/%TEMP_BRANCH%* if %errorlevel% neq 0 exit /b %errorlevel% set "GIT_SSH_COMMAND="  git checkout -f jenkins_tmp/%TEMP_BRANCH% if %errorlevel% neq 0 exit /b %errorlevel% git reset --hard if %errorlevel% neq 0 exit /b %errorlevel% git clean -fdx if %errorlevel% neq 0 exit /b %errorlevel% ```  </details><br>  <details> <summary>Shell script (not tested)</summary>  ```sh #!bash -ex  (if [ -e .git/index.lock ]; then rm -rf .git; fi) || true (if [ $(du -s .git | cut -f1) -gt 5000000 ]; then rm -rf .git; fi) || true  ORIGIN_REFS="+refs/heads/master:refs/remotes/origin/master" ORIGIN_REFS="$ORIGIN_REFS +refs/heads/v6.x-staging:refs/remotes/origin/v6.x-staging" ORIGIN_REFS="$ORIGIN_REFS +refs/heads/v8.x-staging:refs/remotes/origin/v8.x-staging" ORIGIN_REFS="$ORIGIN_REFS +refs/heads/v10.x-staging:refs/remotes/origin/v10.x-staging" ORIGIN_REFS="$ORIGIN_REFS +refs/heads/v11.x-staging:refs/remotes/origin/v11.x-staging"  # Manual git git --version git init time git clean -fdx || true  time git fetch --no-tags https://github.com/nodejs/node.git $ORIGIN_REFS || echo "Problem fetching the main repo."  grep -q ^$TEMP_REPO_SERVER ~/.ssh/known_hosts || (mkdir -p ~/.ssh && ssh-keyscan -t rsa $TEMP_REPO_SERVER >> ~/.ssh/known_hosts) time ssh-agent sh -c "ssh-add $JENKINS_TMP_KEY && git fetch --no-tags $TEMP_REPO +refs/heads/${TEMP_BRANCH}*:refs/remotes/jenkins_tmp/${TEMP_BRANCH}*" rm -f $JENKINS_TMP_KEY  time git checkout -f refs/remotes/jenkins_tmp/$TEMP_BRANCH time git reset --hard time git clean -fdx ```  </details><br> 
joaocgreis		The solution above fixed the issue for us. Essentially, stop using the Git plugin in the affected jobs.
rvagg		heads-up that we're dealing with some glibc bug on centos6 for 8.x builds on the linux distributions: https://github.com/nodesource/distributions/issues/513, waiting for Chris to shave that yak so we can understand more.
chrislea		Here is what I know thus far:  If you compile 8.3.0 on CentOS 6 with `gcc = 4.8.2` from `devtoolset-2`, the binary works fine.  If you compile 8.3.0 on CentOS 6 with `gcc = 4.9.2` from `devtoolset-3`, the binary does not work and gives the error: ``` ./node: error while loading shared libraries: cannot allocate memory in static TLS block ``` You see the same bad behavior if you use the compilers from `devtoolset-4` or `devtoolset-6`. The issue is constrained specifically to CentOS 6. CentOS 7 does not have this problem, nor does any version of Fedora, Debian or Ubuntu that we tend to care about.  The error appears to be due to [this bug](https://sourceware.org/bugzilla/show_bug.cgi?id=14898) which wasn't actually fixed until a newer version of `glibc`, meaning that fix is not at this point going to make it back to CentOS 6.  This is problematic as the compiler requirement recently moved to "4.9.x" basically. So the options seem to be:  1. Back the compiler requirement back down. I'm not sure if this will actually be technically possible going forward if V8 starts using language features that `gcc 4.8.x` doesn't have. 2. Stop supporting CentOS 6. Rather undesirable considering the amount of enterprise deployments still using it. 3. Some technical (or other) fix for this that's past my understanding.  Happy to provide more info if I have it, just let me know.
seishun		>Back the compiler requirement back down.  In other words, continue working around compiler bugs due to a glibc bug that was fixed 4.5 years ago.  I'd say we should just raise the minimum supported glibc version to 2.17 (which came out on 25 Dec 2012). Enterprises still using CentOS 6 would need to upgrade to CentOS 7 or get a newer version of glibc.  cc @nodejs/build 
chrislea		> I'd say we should just raise the minimum supported glibc version to 2.17 (which came out on 25 Dec 2012). Enterprises still using CentOS 6 would need to upgrade or get a newer version of glibc.  Just speaking from experience, that won't happen. If we do that, we should understand that we are dropping support for CentOS 6 / RHEL 6.  I'm not saying that to imply it's the wrong course of action, but we shouldn't kid ourselves about the consequences.
gibfahn		>Enterprises still using CentOS 6 would need to upgrade or get a newer version of glibc.  AIUI successfully upgrading glibc is basically impossible (or at least not something you'd ever contemplate for a production system).
seishun		It's possible to install a newer version in parallel: https://unix.stackexchange.com/a/299665/164374
chrislea		@seishun It's possible yes, but that seems like a _really_ bad thing for the Node Foundation to suggest for end users for several reasons.  1. As soon as you do that, you're putting the Node project in the line of sight for support for something other than installing Node itself, which is definitely not desirable. 2. Even worse, when the inevitable case happens where somebody does install an out-of-band version of `glibc`, does something to their system to invoke using it for Node (adjusting `LD_PRELOAD_PATH` or somesuch), and then forgets they did that, and some other application doesn't work properly as a result, the Node project will be seen as a bad actor for having suggested doing it in the first place. 3. Probably worst of all, one of the reasons we're having the discussion is that we don't want to back the compiler requirement back down to 4.8.x. So if we suggest this, we're basically saying "We aren't willing to work around issues with old `glibc` implementations anymore because it's just too painful for us. So instead we are pushing that pain point onto our end users." This is not a good strategy for making end users happy. You'd _never_ see Oracle suggesting such a thing for a Java build, for example. If you're at the point where you're telling an end user to compile system level libraries from source just to use the application software you're shipping, you've gone down a bad road.  There's a bit of a pickle here already of course, because 8.3.0 is already out in the wild, and was built with a compiler that doesn't meet the current compiler requirements. But that's probably a small issue in the grand scheme of things.  I still think that if we're not willing to use the older compiler, and there's no way to code around this, then it's time we simply say CentOS 6 / RHEL 6 is too old and it's not supported anymore. That day is going to come eventually, and it's going to come before those distros reach EOL, so if we're at that point so be it.
seishun		>the Node project will be seen as a bad actor for having suggested doing it in the first place.  Node project shouldn't suggest anything. It should just state the glibc and compiler requirements and let the users (or repo maintainers) figure out how to satisfy them.  >So if we suggest this, we're basically saying "We aren't willing to work around issues with old glibc implementations anymore because it's just too painful for us. So instead we are pushing that pain point onto our end users." This is not a good strategy for making end users happy. You'd never see Oracle suggesting such a thing for a Java build, for example.  Java is maintained by Oracle, so they can follow whatever policy they like. But Node is mostly maintained by unpaid volunteers and having to work around old compiler bugs will turn people away. Requiring a glibc version that's less than 5 years old is not an unreasonable demand.
chrislea		Hey, like I said, I'm not _in any way_ trying to suggest that saying "EL6 is too old, sorry" is the wrong answer here. Nor is it my decision to make. All I was trying to point out was that if that's what's happening, I think the project should be clear about it.
refack		I'd like to restate the working assumption "feature frozen system is a feature frozen system", that is, if user decided to freeze their OS for stability's sake, it is fairly safe to assume they have also frozen their `node` version. So the combination of an old OS + new major node version is highly unlikely. Hence we should only support old OSs for old node versions. Ref: https://github.com/nodejs/node/pull/12672
chrislea		Oh man @refack my life would be _so much easier_ if ^^^ was true. ;-)
refack		> Oh man @refack my life would be so much easier if ^^^ was true. ;-)  Well then you are our gatekeeper, personally I haven't seen many "minor version upgrade broke my system" bugs in the core repo. So thank you, and sorry ü§ó  But then again I'm gonna sue you for attempting to penetrate my system with `curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -` üòâ  <sub>maybe I've been preoccupied with "windows installer broke my system" bugs</sub>
rvagg		It's hard to overstate how poorly this will go down: "Enterprises still using CentOS 6 would need to upgrade to CentOS 7". Folks are still using EL5 in production out there. In very large companies, these kinds of moves are very complicated and therefore move very slowly. We're a little too comfortable catering to nimble startups and early-adopter types that are more used to working on the cutting edge, or close to it. As you go out further than that we hit real world constraints where these things are really painful.  Does clang offer us a get-out-of-jail here perchance?
bnoordhuis		There might be a simple workaround.  Can someone try this patch? ```diff diff --git a/deps/v8/src/trap-handler/handler-shared.cc b/deps/v8/src/trap-handler/handler-shared.cc index 7b399f5eea..e4f0136be7 100644 --- a/deps/v8/src/trap-handler/handler-shared.cc +++ b/deps/v8/src/trap-handler/handler-shared.cc @@ -23,7 +23,9 @@ namespace v8 {  namespace internal {  namespace trap_handler {   -THREAD_LOCAL bool g_thread_in_wasm_code = false; +// Note: sizeof(g_thread_in_wasm_code) must be > 1 to work around +// https://sourceware.org/bugzilla/show_bug.cgi?id=14898 +THREAD_LOCAL int g_thread_in_wasm_code = false;    size_t gNumCodeObjects = 0;  CodeProtectionInfoListEntry* gCodeObjects = nullptr; diff --git a/deps/v8/src/trap-handler/trap-handler.h b/deps/v8/src/trap-handler/trap-handler.h index 5494c5fdb3..ed9459918b 100644 --- a/deps/v8/src/trap-handler/trap-handler.h +++ b/deps/v8/src/trap-handler/trap-handler.h @@ -65,7 +65,7 @@ inline bool UseTrapHandler() {    return FLAG_wasm_trap_handler && V8_TRAP_HANDLER_SUPPORTED;  }   -extern THREAD_LOCAL bool g_thread_in_wasm_code; +extern THREAD_LOCAL int g_thread_in_wasm_code;    inline bool IsThreadInWasm() { return g_thread_in_wasm_code; }   ```
bnoordhuis		> Does clang offer us a get-out-of-jail here perchance?  Unlikely.  It's a glibc bug, gcc is not at blame.
seishun		@bnoordhuis Builds and runs fine with the patch.  @rvagg Without undermining your point about large and slow companies, comparing "working on the cutting edge" with "using an EOL distro that was superseded 6 years ago" is a bit of a false dichotomy.
bnoordhuis		Thanks.  I'll check if upstream is amenable to this change.
bnoordhuis		https://chromium-review.googlesource.com/c/612351 - let's wait and see.
seishun		I'm curious why this isn't an issue with gcc 4.8.2 though.
FireBurn		It's an issue for me with GCC 4.8.2 on RHEL6
chrislea		Confirming (also) that 8.3.0 builds and runs on EL6 using that patch with the compilers from ``` devtoolset-3: gcc = 4.9.2 devtoolset-4: gcc = 5.3.1 devtoolset-6: gcc = 6.3.1 ``` As seems often the case, a great big thanks to @bnoordhuis for saving the day.  A question (again I suppose for you Ben) since I'm not very familiar with how quickly things move through V8: is it reasonable to assume that they'll include this patch and that it will make it to the mainstream V8 that gets shipped with Node Soon (tm)? I ask because on the RPM packages side, it's fairly easy for me to use this patch and put in logic like "if el6 use the patch else don't". But if this is going to land in mainline next week or something, it's probably not worth it to do that, rebuild everything, and then back that out when it's no longer needed.  Thanks again!
refack		> But if this is going to land in mainline next week or something, it's probably not worth it to do that, rebuild everything, and then back that out when it's no longer needed.  V8 CLs can land very fast, if they are "amenable" to the actual change. P.S. @bnoordhuis, didn't they give you Dry Run permissions?
bnoordhuis		@chrislea What Refael said.  If I get a LGTM, I can land the patch and cherry-pick it into node.  That would then go out in the next release.  @refack I have, but this is one of those it-either-compiles-or-it-doesn't things.  I didn't feel it was worth spending Watts on a dry run.
chrislea		Okay, thanks Ben. If that's the case I'll to a one-off rebuild just for EL6 RPM packages and this should be settled.
refack		https://chromium-review.googlesource.com/c/612351 just landed (in the `candidate` branch). @MylesBorins @targos question: what are the nexts?
seishun		Now that https://github.com/nodejs/node/pull/14913 has landed, is there anything else blocking this PR?
joaocgreis		Adding to WG agenda together with https://github.com/nodejs/build/pull/797 so that this can be discussed in the next meeting.  However, if someone with the knowledge has the time to look at this before, feel free to remove.
refack		I've been searching the net and I couldn't find a precompiled `libstdc++` that's compatible with `devtoolset-6` for 64bit host and 32bit target (a.k.a `x86` a.k.a `i386` a.k.a `i686`) ```   g++ -pthread -rdynamic -m32  -o /home/centos/node/out/Release/openssl-cli -Wl,--start-group ... /home/centos/node/out/Release/obj.target/deps/openssl/libopenssl.a -ldl -Wl,--end-group /opt/rh/devtoolset-6/root/usr/libexec/gcc/x86_64-redhat-linux/6.2.1/ld: skipping incompatible /opt/rh/devtoolset-6/root/usr/lib/gcc/x86_64-redhat-linux/6.2.1/libstdc++_nonshared.a when searching for -lstdc++_nonshared /opt/rh/devtoolset-6/root/usr/libexec/gcc/x86_64-redhat-linux/6.2.1/ld: cannot find -lstdc++_nonshared ``` But maybe we could build it from source?
chrislea		If we're going to entertain the "let's build something complex from source to support 32bit x86" notion, I'd first attempt to just build the updated `gcc` from the [source rpm](http://cbs.centos.org/kojifiles/packages/devtoolset-6-gcc/6.3.1/3.1.el6/src/).  I've spent a fair amount of my life trying to do stuff like this. Sometimes it works and sometimes it doesn't, but it's almost always more work than you'd expect. That said, if somebody wants me to, I can take a quick swing at this tomorrow and see if it "just works". That doesn't happen too often, but it does every once in a while.
seishun		I'd say building from source crosses the "too much work for us" line. Let's first see where the discussion about dropping 32-bit leads us. (where was the issue created for that by the way? cc @gibfahn)
thefourtheye		Whenever I run CI on that PR, the leftover processes will be there in almost all the machines. So I'll have to clean them in all of them. I am fixing them case by case basis, e.g https://github.com/nodejs/node/pull/10597 fixes SmartOS failures (hopefully).  Is it possible to get permissions for all of them?
jbergstroem		Since Linux looks mostly ok, how about we for instance start with smartos, plinux and windows? I can facilitate the former; not sure how we create temporary accounts for windows. @joaocgreis?
Trott		The only ones that I noticed stalled jobs on were SmartOS and FreeBSD, so I'd be sure to include access to those two at a minimum.
jbergstroem		@Trott: @thefourtheye mentioned [on IRC] that he was running FreeBSD locally and that the failures/stalls was similar across os'es. I think he's doing local testing at the moment.
thefourtheye		Setting up the FreeBSD vm in a slow internet connection. I'll update the status about that tomorrow.
mhdawson		Let me or @gibfahn know when you need access to plinux/aix
jbergstroem		I can arrange it as well; seems like we have me and @mhdawson (so far?) approving your access.
gibfahn		+1 to giving @thefourtheye access
joaocgreis		I also approve giving @thefourtheye access. According to our docs, giving temporary access to a collaborator can be done immediately, so I went ahead and created a temporary account on a Windows machine (emailed credentials). Given that stalling tests can interfere with normal CI runs, I disconnected the machine from CI.
jbergstroem		@joaocgreis true. I will help facilitate the rest on demand, as well as making a note here on when access is granted and finally revoked.
mhdawson		To confirm +1 from me.
thefourtheye		Thanks people :-) I finally managed to setup a FreeBSD instance locally and test this. https://github.com/nodejs/node/pull/10822 looks like fixes it. PTAL.
joaocgreis		@thefourtheye Still using the machine? If you are, no rush, just let me know when you're done!
gibfahn		ping @thefourtheye @joaocgreis , I think this can be revoked now.
thefourtheye		@gibfahn @joaocgreis Yes, this can be revoked now. Thanks :-)
rvagg		let's set up a separate job just for linting, add a few new small machines with `iojs` on them, then add that new job in to the multi-pr job(s):  ![screen shot 2015-06-11 at 8 44 02 pm](https://cloud.githubusercontent.com/assets/495647/8105245/c43cd008-107a-11e5-9c62-5919268a34e2.png)  I think we can add that to the top of the build steps and make it block until that one passes. 
rmg		:+1: for using linting as a gate for running full tests. 
jbergstroem		Sounds like a plan, then. If no one objects, I'll setup a machine on DO shortly. 
jbergstroem		We now have two linters. Each run seems to be around ~40 secs (on tree's that are relatively similar and lives in block cache). A cold tree run is about 1min 10sec. The vm's are run in New York and San Fransisco for redundancy. The final step is to have this project block the main project and merge https://github.com/nodejs/io.js/pull/1965. 
jbergstroem		Rolled into production a week ago. Closing. 
rvagg		lgtm I guess, I wouldn't have chosen FreeBSD but he who does the work gets to choose I guess! 
jbergstroem		@rvagg finally you have a reason to spend more time with the devil :smiling_imp: (<- red) 
rvagg		:rage: is probably closer for more than reasons of colour 
jbergstroem		This seems to be working well. I'll merge in a day if there are no objections. 
rvagg		lgtm 
jbergstroem		Merged in 6b3867b99f555f3f1bb0b84c81ea20db2ca343b6. Spread that FreeBSD religion :circus_tent:  
tkelman		I'll second what @saghul said in https://github.com/node-forward/build/issues/1#issuecomment-51710565 about MinGW. The requirements of Julia, a downstream consumer of libuv (currently on a fork but would like to fix that), cannot be met by MSVC (even 2013; 2015 should help some but not completely). Legacy 32-bit-only mingw.org is far less important to us than the MinGW-w64 toolchain which has both i686 and x86_64 variants, but issues do still occasionally get filed at libuv when something breaks with the legacy toolchain. 
rvagg		Moved "Mac OS X 10.10 (Yosemite) + XCode 6" up to class A, thanks @someoneweird for picking that up. Does it still make sense to include 10.8 in the list here?  A couple of additional items worth noting:  This list of full architectures will be reserved for use by the libuv-core team rather than for everyone submitting pull requests. The list of people who's pull requests will trigger a full-set build will be controlled by that core team and will be kept relatively small to a trusted set due to security concerns of running arbitrary code from pull requests on most of these platforms. At the moment we have a containierized build set working @ http://jenkins.node-forward.nodesource.com/ that will be used to test incoming pull requests from untrusted sources. Core members will then be able to request a full build of any pull request if it makes sense to do so (the exact mechanism of this request is yet to be decided). The containerized set simply includes the last 3 LTS releases of Ubuntu but we could expand that if necessary and possibly even include other architectures where this is easy (SmartOS/Solaris, FreeBSD). So that set is a matter for another discussion but will also come down to difficulty involved in coordinating this to make it workable.  The other item worth noting here is that we have hardware contributions from multiple companies to make this happen: DigitalOcean (Linux), Rackspace (Linux + Windows), IBM (via Softlayer) (Linux + Windows + POWER), and Voxer have also stepped up with a couple of Mac Minis in their datacenter. We also have people power provided by NodeSource (myself and @ryanstevens) and a large number of very interested parties who want to help out because they care deeply about the success of Node.js and libuv and believe this is the best place they can contribute. Over the coming months we'll be trying to make this project much more organised and give clear avenues for additional contributions by companies and community members. 
jbergstroem		Does these classes also reflect usage? Will all classes have buildbots? Are fails accepted in Class C? I think a better definition of these classes would improve understanding of what the greater goal is.  Since hardware/vm is pretty cheap, I think the Linux prevalence should be diversified with BSD's (Open, Net, Dragonfly comes to mind). Illumos/SmartOS also feels like a pretty relevant inclusion. Since the build isn't limited by V8 (which would be the showstopper for most of nodejs), libuv would only improve by testing on these os'es or even additional architectures.  Also; above targets seems to be "point in time" distributions (10 lts vs 12 lts) -- a possibly more relevant mindset would be testing functionality such as disabling ipv6 or similar (ref https://github.com/libuv/libuv/issues/10). Point being that the major difference between all those Linux'es being toolchains or kernel versions. 
tkelman		Oh, and I should note that I very frequently cross-compile libuv (and lots of other things) from Linux to Windows via MinGW-w64 and autotools. As far as libuv's build system goes I'd like to see that continue to work, though other communities might not do that kind of thing as often. Cross-compiling is highly relevant for ARM as well, but I haven't seen any mentions of it yet. 
bnoordhuis		> Does it still make sense to include 10.8 in the list here?  Data point: I still use 10.8.  I may very well be an outlier, though.  > I think the Linux prevalence should be diversified with BSD's (Open, Net, Dragonfly comes to mind). Illumos/SmartOS also feels like a pretty relevant inclusion.  I don't really disagree but the number of BSD users that are not FreeBSD users is practically a rounding error, and I say that as the one who did the DragonFly BSD port and spent the most time maintaining the OpenBSD port.  It's okay to add such platforms to the CI matrix but test failures (and arguably build failures too) should not block tier 1 platforms.  Likewise with Illumos/SmartOS.  Realistically, the only users are Joyent customers and as they pay Joyent good money, I think it's fair that responsibility for maintaining the Illumos port falls to Joyent (or their customers, if they are willing to step up.) 
rmg		**My opinion on platform support, without any real background in the libuv project**  I think it would make sense to treat all of the listed platforms as class A and only move them to class B if/when they start failing in a way that puts a drag on productivity. There is no class C in this scheme because it's a wish list and support for those platforms belongs in an issue tracker instead. 
jbergstroem		@bnoordhuis Without going into the guesttimation game for user/os adaption, my suggestions didn't assume a successful build or test suite - rather the fact that if hardware is available the effort to increase visibility is pretty low. Portability is generally a good thing and exposing shortcomings doesn't mean that it has to get highest priority. This is also why my comment started with "what does different classes mean". 
rvagg		Regarding VMs being "cheap" leading to wanting greater diversity, the equation isn't as simple as throwing as many options as we can think of against this project. - The maintenance overhead of a large diverse cluster is quite large. It's manageable with enough input from the community and if people concerned with libuv want to get involved in helping maintain an active cluster then that's great. I'll have to monitor the situation over time and if the burden ends up falling on me (or someone else, or a small group) then we'll have to trim to make it manageable. - Thankfully libuv is a quick build and test but we still need VMs that are adequately performant so as not to have slow feedback cycles so we can't just throw super-cheap VMs at this. We have very generous support by some hardware providers at the moment but we do need to be careful to manage the burden that we're placing on them.   I suggest we target the _obvious_ choices now and expand over time as it becomes _obvious_ that expansion is needed&mdash;just like you would add a test case when you encounter a regression, let's expand when it becomes clear that we have an important blind-spot.  **What the "obvious" choices are is actually a matter for the libuv core team and I'd like them to come up with that list after taking into account the discussion here**. That includes architectures, build modes etc.  Regarding cross-compiling: we can test that it's possible to do the compiling but testing the builds produced from cross-compiling is a non-trivial exercise and would involve coordination of multiple VMs. I'm personally skeptical of the value of including cross-compiling in the CI set but I'm happy to be told otherwise by the libuv team.  Regarding the classes&mdash; I grabbed them off the [README](https://github.com/node-forward/build#test-configurations) which includes details about what happens when failures occur on the lower classes. However, this is just a starting point and that list is mainly focused on Node. I expect to land somewhere different with libuv and would love to hear more discussion here on how to define the classes, how to handle failure cases in the classes, etc. 
jbergstroem		@rvagg After reading the README it makes more sense, thanks. If there's anything I can do to help wrt build environment or maintenance, ping me at IRC (freenode, same username). 
ghostbar		FYI, Debian wheezy is soon to be oldstable. Debian is currently on "freeze" to move current testing to stable. 
bnoordhuis		Closing, I think we've implemented most of what is discussed here.  Testing with older compilers is maybe the exception but that hasn't been a problem so far.  I'll file a separate issue for that when it becomes a problem. 
jbergstroem		LGTM 
mhdawson		landed as ab125b809ddba698dcefa60287bc3fbdf7ad62d7 
rmg		Could the flaky tests be parsed out and posted as individual statuses? Might make them visible enough. 
jbergstroem		@rmg you mean as fails? if they're flagged as pass gh will still fold into "success" 
rmg		As passes. The "folding" is only a UI thing, the status list could still be expanded to show them, which is a lot more visible than having to go through to the build logs. 
jbergstroem		Then it could just as well be part of the current, which would currently show: All tested passed! (3 flaky, 14 skips, 790 total) 
Trott		This may be unrealistic, but I would like to see a small and committed group of people aggressively work on the flaky tests and make them Not Flaky.   Speaking of which, https://github.com/nodejs/node/pull/3636 needs an LGTM. Just saying. 
Trott		(Gratuitous self-promotion, but I am [doing my part to try to rally people to fix flaky tests](https://www.youtube.com/watch?v=1F8F-EbdX40&feature=youtu.be&t=382).) 
jbergstroem		@Trott what are your thoughts on making it a realistic goal? How can we put more pressure on either flagging tests as flaky or making sure they are followed up? 
Trott		Here's what I've been thinking: - Form a WG. Once the flaky tests are all fixed, the WG can disband. One big benefit will be that it will be a small-ish group of people that talk to each other and are paying attention to what each other are working on. So things like "Hey, can someone go to https://github.com/nodejs/node/pull/3636 and look at not just the code changes, but the CI runs, and the history from the previous pull request that it came off of and the issue filed and..." ... yeah, all that will happen easier hopefully. "Oh, you finally solved that thing we've been having a back-and-forth about! Brilliant! Oooh, that is a clever fix. Bravo! LGTM!" - This is harder to solve, but for me, the biggest demotivator to fixing the flaky tests is that buildbots seem to fail with alarming frequency. ("Seem" is the operative word. I don't know that they actually do, but it sure seems like it.) So fixing that would indirectly be a huge help. Otherwise, why bother getting the tests to run reliably if they're still not going to run reliably because the buildbots are going to fail on CI? Maybe that can be the same WG. Maybe that's this WG? Maybe this WG is where the flaky tests stuff belongs rather than a separate WG?  (Wherever the flaky tests firefighters assemble, obviously, I want in.) 
thefourtheye		Maybe we can raise alerts if the buildbots go down 
orangemocha		The current documented policy for flaky tests (https://github.com/nodejs/node/wiki/Flaky-tests#what-to-do-when-you-encounter-a-new-flaky-test) calls for opening an issue to track them when you mark the test as flaky, and assigning the issue to the next release milestone. I am not sure if we have followed it in 100% of cases. During the node-accept-pull-request experiment we were aggressively marking tests as flaky, but there were so many different ones that were failing randomly that we had to give up. Reliability of the build bots is definitely something that we need to address, and I certainly haven't given up on that. Btw, @joaocgreis is working a CI job that helps stress a single test in CI to determine if it's flaky or not, which should be a useful tool in this context.  Assuming we are going to finally improve the reliability of build bots, the part that I think could use some improvement is clarifying who is going to take responsibility for fixing the flaky test / how to motivate people to do it. The person who marks the tests as flaky is usually the collaborator who is making the determination that the test is not failing because of the current pull request changes. They are not motivated to fix the test and not necessarily the most qualified in the particular test that is failing.  In a dev team working for one company, you could probably just assign the issue to the test author/owner. I am not sure that this would work in an open source project.  So how do we motivate collaborators to investigate and fix these failures? Here are some options we could consider: 1. We stick to the current policy of assigning them to the next release milestone. The main problem with this is that we might see the list too late in the release cycle, and decide to punt. To counter that, we could try to increase awareness of issues that are flagged for a milestone, throughout the milestone (for example by sending periodic reports). I think this might be useful even beyond flaky tests. 2. We somehow set a timeout for how long a test can be marked as flaky, or a limit to the number of tests marked as flaky, or both. When the limit is reached, we block merging all pull requests until the situation is brought back under control. This makes the whole team accountable for fixing flaky tests. I recall @domenic saying that this is what the v8 team does. 3. We try to motivate collaborators by some sort of reward mechanism, like keeping a scoreboard with how many issues they have resolved in a given period (and assign extra points for flaky tests). 4. We come up with an algorithm for assigning the flaky test issue to a specific collaborator, in addition to assigning a milestone. For example, the assignee could be the last active collaborator to have modified the test logic. If the search doesn't yield a suitable candidate, we could fall back to round-robin selection between a pool of collaborators, ideally choosing the pool based on the the area of the test. 
rvagg		Possibly something to completely hand off to the new testing group, @trott has basically been owning this whole area in the time between when this issue was posted and now. Shall we close? Is there anything in here that @nodejs/testing wants to capture first? 
jbergstroem		Sounds good to me. For an 'outsider' it might be good to define the scope of the build and testing workgroup somewhere (where do I post what problem). 
Trott		@jbergstroem wrote:  > For an 'outsider' it might be good to define the scope of the build and testing workgroup somewhere (where do I post what problem).  The Testing WG is putting together docs so we can be chartered by the TSC. Because some of our charter is likely to overlap with the existing Build WG charter, the Build WG will probably have to ratify our charter as well.  It would be great if as many build folks as were willing would read the very short draft docs for the Testing WG and comment. You can find them here: https://github.com/nodejs/testing/pull/2/files 
Fishrock123		cc @rvagg  
Fishrock123		@bnb Also I'm pretty sure the answer is yes, but not 100% sure lol 
jbergstroem		Yes, since its part of path:  ``` 1.1.1.1 - - [04/Jul/2016:18:34:42 -0400] "GET /download/release/v4.1.2/node-v4.1.2-headers.tar.gz HTTP/1.1" 200 468778 "-" "node-gyp v3.0.3 (node v4.1.2)" "1.1.1.1" 1.1.1.1 - - [04/Jul/2016:18:34:47 -0400] "GET /download/nightly/v5.7.1-nightly201602271ecbdec2cf/node-v5.7.1-nightly201602271ecbdec2cf-linux-armv7l.tar.gz HTTP/1.1" 200 11597789 "-" "github.com/cnpm/mirrors.robot@1.0.0" "1.1.1.1" 1.1.1.1 - - [04/Jul/2016:18:34:47 -0400] "GET /download/release/v6.2.2/node-v6.2.2-headers.tar.gz HTTP/1.1" 200 474770 "-" "node-gyp v3.3.1 (node v6.2.2)" "1.1.1.1" 1.1.1.1 - - [04/Jul/2016:18:34:49 -0400] "GET /download/release/v5.11.1/node-v5.11.1-headers.tar.gz HTTP/1.1" 200 472366 "-" "node-gyp v3.3.1 (node v5.11.1)" "1.1.1.1" ```  Anyone keen on writing log parsers? :) 
rvagg		Yes, this is possible. Read all about what's available @ https://nodejs.org/metrics/, there is a https://nodejs.org/metrics/logs/ directory that contains the logs in as raw form as we can provide without giving up too much user information (but please read the main page for details on how this all works).  So, if you just wanted to look at yesterday's downloads and see what specific versions got more downloads then you could do something like:  ``` sh curl -sL https://nodejs.org/metrics/logs/nodejs.org-access.log.20160703.1467564509.csv \   | awk -F, '(NR > 1 && $5 != ""){versions[$5]++} END{for (v in versions) { print versions[v], v }}' \   | sort -n ```  (note that "yesterday" is a fuzzy concept, even though the file has 20160703 in it it's not precisely the logs for just that day, see the main notes for details on this).  You'll need to download all of those individual log files for the period you're interested in and concatenate them when processing. For now you'll have to do some scraping to get them but we'll be introducing an easier mechanism to get resources list this .. stay tuned.  The log processing used to create these files as well as the summary aggregates is at https://github.com/nodejs/build/tree/master/setup/www/tools/metrics if you're interested in seeing how the sausage is formed and perhaps making suggestions for improvements. 
rvagg		failing on ppc cause flow-cli needs a binary that's not available  failing elsewhere because ... I have no idea. I'm on the debian8 machine trying to replicate it but I can't get a non-zero exit status. If someone wants to look, try test-digitalocean-debian8-x64-1, do `export PATH=/home/iojs/build/workspace/citgm-smoker/nodes/debian8-64/smoker/bin/:$PATH`, then `cd ava-0.23.0` and `npm test`. Redirect it to a file to get the tap output, e.g. `npm test >& /tmp/out`.  Of course this is not necessarily using the same version of the node & npm build that is failing in 1065. But I can't even see what the problem might be in https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/1065/nodes=debian8-64/testReport/junit/(root)/citgm/ava_v0_23_0/
MylesBorins		@rvagg I did a bit more review and it appears that the ubuntu machines that are passing are not exiting the test suite early... really hard to wrap my head around why / what is happening. Only can repro using the citgm job
rvagg		@MylesBorins one major difference is the lack of proper stdio. If you redirect stdout and stderr you get close to the way it's running `... >& /tmp/out`, even pushing it to the background with `&` might help by getting rid of stdin. That's the first thing to try and assess.
MylesBorins		With some more research it would appear this might be triggering a race condition bug in npm 3.x  I am marking ava flaky in node < 8 and will track this problem over on the citgm repo  https://github.com/nodejs/citgm/pull/509
rvagg		rly? is this necessary? do y'all have 640x480 monochrome monitors still? 
jbergstroem		The w80 kinda snuck in there -- I read somewhere that the recommended markdown width should be 80 [link needed] which is why I changed it while at it. 
jbergstroem		It's kind of useful should you open README.md in your favourite terminal emulator as well. 
bnoordhuis		Rubber-stamp LGTM. 
jbergstroem		@rvagg plz? 
rvagg		your call, manual line wrapping makes zero sense to me 
jbergstroem		Guessing a lot in the readmes will change once I start messing with abstractions. I'll close this and tell my better future self to revisit. 
rmg		I took a quick (as in 15 minutes of `git grep` and head scratching) look at the build scripts and couldn't see any places in iojs itself where the tarball name isn't derived from the directory name.  @rvagg could this is happening in an upload script? 
rvagg		yes, it's done at upload time to keep the names consistent in /dist/:  ``` farch=$ARCH [[ $farch == 'ia32' ]] && farch='x86' ssh iojs-www "mkdir -p staging/release/${version}" scp -p iojs-${version}-linux-${ARCH}.tar.gz iojs-www:staging/release/${version}/iojs-${version}-linux-${farch}.tar.gz scp -p iojs-${version}-linux-${ARCH}.tar.xz iojs-www:staging/release/${version}/iojs-${version}-linux-${farch}.tar.xz ssh iojs-www "touch staging/release/${version}/iojs-${version}-linux-${farch}.tar.gz.done staging/release/${version}/iojs-${version}-linux-${farch}.tar.xz.done" ```  prior to this the build is done with:  ``` make -j$JOBS binary DESTCPU=$DESTCPU ARCH=$ARCH ```  it's `ARCH` that needs changing on those machines 
jbergstroem		Not a bug. @rvagg? 
rvagg		was a bug, fixed now afaik 
MylesBorins		LGTM (not 100% I am able to, but üëçüèΩ for @ofrobots from me) 
mhdawson		LGTM 
jbergstroem		Cool, thanks. I've added his key.  @ofrobots: when you're done either let us know here or on irc! 
jbergstroem		I've expanded his access to: - test-joyent-alpine34-x64-1 - test-osuosl-aix61-ppc64_be-1  - test-joyent-smartos14-x64-2 
ofrobots		Thanks! I no longer need access to the machines. 
jbergstroem		Removed access. Closing. 
jbergstroem		Merged (and deployed) with 964af46ceb62850ac9138efc9d01a793422bad02.
maclover7		ping @phillipj, have you had a chance to look into this?
phillipj		@joyeecheung just found that the bot has to be added as collaborator to the repos we want it to push status updates to.
joyeecheung		Yes, the setup should be similar to https://github.com/nodejs/automation/blob/master/enable-travis-under-nodejs.md except it's a different CI to poll
joyeecheung		@maclover7 Which repo does the bot need to have access to? I can add it.
maclover7		@joyeecheung ah ok. If you could add nodejs/http-parser, that'd be amazing :)
joyeecheung		@maclover7 OK I've added the bots team to http-parser, can you check if it can update the status now?
joyeecheung		Also, http-parser does not seem to have setup the github webhook, doesn't it need one?
maclover7		Perfect, http-parser-test jobs seem to be updating properly now, thank you for your help!  > Also, http-parser does not seem to have setup the github webhook, doesn't it need one?  This was to allow ci.nodejs.org to sent status updates, not Travis CI :)
joyeecheung		@maclover7 I meant the PR & push events of http-parser would not be sent to the bot, since there are no hooks configured. But if the bot can get the events somehow (or it does not need those events) then it's good.
joyeecheung		Ah, I see, the events would be manually triggered using the Jenkins UI, instead of being sent from GitHub, like what we do to the node core.
joyeecheung		(Which reminds me: can we make the bot smarter? Like if the author is a collaborator and they put a keyword in the PR text then the bot will start the CI automatically and reply in the thread?)
phillipj		For sure.  I've had a go at triggering Jenkins builds by posting comments in PRs. It worked for a whole week or so in the CITGM repo, before we suddenly started seeing "permission denied" responses from Jenkins IIRC: https://github.com/nodejs/github-bot/issues/146  Mind mentioning that my impression is there's lots of discussions to be had before we can enable something like that on the nodejs/node repo because of the "certify safe" option: https://github.com/nodejs/github-bot/pull/128#issuecomment-287187944
rvagg		before my time, my guess is that it dates back to pre-installer days, perhaps even pre-bundled-npm days 
bnoordhuis		> perhaps even pre-bundled-npm days  ^ That. 
mhdawson		So I guess the question is whether it still makes sense to provide just the exe or whether a zip would make more sense ?  
rvagg		Seems to be in wide use: https://github.com/coreybutler/nvm-windows/blob/master/src/nvm.go#L333-L348  `node.lib` is used by node-gyp too  using the download stats, for Q4 2015:  ``` $ cat nodejs.org-access.log.20151* | awk '/\.msi,v/ { msi++ } /\.exe,v/ { exe++ } END { printf "msi=%s, exe=%s, %s%\n", msi, exe, (exe / (exe + msi)) * 100 }' msi=4765206, exe=890241, 15.7413% ```  That's surprisingly big. I don't think we can do anything here without more information on how and why it's being used. 
gibfahn		Are you talking about [citgm-smoker](https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker/) and [citgm-smoker-nobuild](https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker-nobuild/)?  If so then yeah, we should make them identical. In fact there's no reason we shouldn't just have the one job. I set up `nobuild` because I didn't want to break the main smoker run while testing the `nobuild` version. 
refack		<sub>Sorry clicked enter (read: submit form) ü§¶‚Äç‚ôÇÔ∏è when writing the title</sub>  ## `citgm-smoker`: * command: `citgm-all -J` 1. aix61-ppc64 2. debian8-64 3. fedora22 4. fedora23 5. osx1010 6. ppcbe-ubuntu1404 7. ppcle-ubuntu1404 8. rhel72-s390x 9. ubuntu1204-64 10. ubuntu1404-64 11. ubuntu1604-64 12. win-vs2015 __+__   ## `citgm-smoker-nobuild`:  * command: `citgm-all` 1. aix61-ppc64 2. centos5-64 __-__ 3. debian8-64 4. fedora22 5. fedora23 6. osx1010 7. ppcbe-ubuntu1404 8. ppcle-ubuntu1404 9. rhel72-s390x 10. ubuntu1204-64 11. ubuntu1404-64 12. ubuntu1604-64  Also some just dont work: e.g. `ubuntu1604-64`: <details> <pre> Started by upstream project "citgm-smoker-nobuild" build number 145 originally caused by:  Started by user Refael Ackermann [EnvInject] - Loading node environment variables. Building remotely on test-rackspace-ubuntu1604-x64-1 (ubuntu1604-64) in workspace /home/iojs/build/workspace/citgm-smoker-nobuild/MACHINE/ubuntu1604-64 [ubuntu1604-64] $ /bin/sh -xe /tmp/hudson6263762753138764280.sh + rm -rf /home/iojs/build/workspace/citgm-smoker-nobuild/MACHINE/ubuntu1604-64/* + [ Nightly == Nightly ] /tmp/hudson6263762753138764280.sh: 4: [: Nightly: unexpected operator + DOWNLOAD_DIR=https://nodejs.org/download/release/ + curl https://nodejs.org/download/release/ + grep v8 + sort -t. -k 1,1n -k 2,2n -k 3,3n + tail -1 + cut -d" -f 2 + tr -d /   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed    0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0   0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0 100 51472    0 51472    0     0  34337      0 --:--:--  0:00:01 --:--:-- 34360 + LINK= + OS=linux + ARCH=x64 + EXT=tar.gz + curl -O https://nodejs.org/download/release//node--linux-x64.tar.gz   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed    0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0   0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0 100  6227    0  6227    0     0   3170      0 --:--:--  0:00:01 --:--:--  3170 + + tar xf - gzip -cd node--linux-x64.tar.gz  gzip: node--linux-x64.tar.gz: not in gzip format tar: This does not look like a tar archive tar: Exiting with failure status due to previous errors Build step 'Execute shell' marked build as failure Recording test results ERROR: Step ‚ÄòPublish JUnit test result report‚Äô failed: No test report files were found. Configuration error? Notifying upstream projects of job completion Finished: FAILURE </pre> </details> 
refack		> Are you talking about citgm-smoker and citgm-smoker-nobuild?  I think it could be very useful job by using the nightly as a baseline, also for quick testing specific packages vs. platform.
maclover7		ping @refack -- is still needed?
seishun		``` ERROR! Syntax Error while loading YAML.   The error appears to have been in '/mnt/c/Users/Nikolai/build/ansible/roles/baselayout/vars/main.yml': line 110, column 3, but may be elsewhere in the file depending on the exact syntax problem.  The offending line appears to be:     ubuntu1404: [   ^ here ```  It seems there's a missing comma.
joaocgreis		@seishun should be fixed, thanks!
seishun		``` TASK [baselayout : install packages] *********************************************************************************** failed: [test-softlayer-ubuntu1404-x64-1] (item=ntp) => {"cache_update_time": 1499117895, "cache_updated": false, "failed": true, "msg": "'/usr/bin/apt-get -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"     install 'ntp'' failed: E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/n/ntp/ntp_4.2.6.p5+dfsg-3ubuntu2.14.04.10_amd64.deb  404  Not Found [IP: 91.189.91.26 80]\n\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n", "package": "ntp", "rc": 100, "stderr": "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/n/ntp/ntp_4.2.6.p5+dfsg-3ubuntu2.14.04.10_amd64.deb  404  Not Found [IP: 91.189.91.26 80]\n\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n", "stderr_lines": ["E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/n/ntp/ntp_4.2.6.p5+dfsg-3ubuntu2.14.04.10_amd64.deb  404  Not Found [IP: 91.189.91.26 80]", "", "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?"], "stdout": "Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following extra packages will be installed:\n  libopts25\nSuggested packages:\n  ntp-doc\nThe following NEW packages will be installed:\n  libopts25 ntp\n0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 477 kB of archives.\nAfter this operation, 1682 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu/ trusty/main libopts25 amd64 1:5.18-2ubuntu2 [55.3 kB]\nErr http://archive.ubuntu.com/ubuntu/ trusty-updates/main ntp amd64 1:4.2.6.p5+dfsg-3ubuntu2.14.04.10\n  404  Not Found [IP: 91.189.88.162 80]\nErr http://security.ubuntu.com/ubuntu/ trusty-security/main ntp amd64 1:4.2.6.p5+dfsg-3ubuntu2.14.04.10\n  404  Not Found [IP: 91.189.91.26 80]\nFetched 55.3 kB in 0s (57.2 kB/s)\n", "stdout_lines": ["Reading package lists...", "Building dependency tree...", "Reading state information...", "The following extra packages will be installed:", "  libopts25", "Suggested packages:", "  ntp-doc", "The following NEW packages will be installed:", "  libopts25 ntp", "0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.", "Need to get 477 kB of archives.", "After this operation, 1682 kB of additional disk space will be used.", "Get:1 http://archive.ubuntu.com/ubuntu/ trusty/main libopts25 amd64 1:5.18-2ubuntu2 [55.3 kB]", "Err http://archive.ubuntu.com/ubuntu/ trusty-updates/main ntp amd64 1:4.2.6.p5+dfsg-3ubuntu2.14.04.10", "  404  Not Found [IP: 91.189.88.162 80]", "Err http://security.ubuntu.com/ubuntu/ trusty-security/main ntp amd64 1:4.2.6.p5+dfsg-3ubuntu2.14.04.10", "  404  Not Found [IP: 91.189.91.26 80]", "Fetched 55.3 kB in 0s (57.2 kB/s)"]} changed: [test-softlayer-ubuntu1404-x64-1] => (item=ccache) changed: [test-softlayer-ubuntu1404-x64-1] => (item=g++) ok: [test-softlayer-ubuntu1404-x64-1] => (item=gcc) changed: [test-softlayer-ubuntu1404-x64-1] => (item=git) changed: [test-softlayer-ubuntu1404-x64-1] => (item=automake) ok: [test-softlayer-ubuntu1404-x64-1] => (item=bash) changed: [test-softlayer-ubuntu1404-x64-1] => (item=libtool) ok: [test-softlayer-ubuntu1404-x64-1] => (item=sudo) ```  It seems an `apt-get update` is required before installing packages.
seishun		If I manually run `apt-get update` before running the script, I get the following:  ``` RUNNING HANDLER [baselayout : restart sshd] **************************************************************************** fatal: [test-softlayer-ubuntu1404-x64-1]: FAILED! => {"changed": false, "failed": true, "msg": "Could not find the requested service sshd: host"} ```  It seems you have to use `ssh` on Ubuntu 14.04. Later versions have `sshd` as an alias for `ssh`.
joaocgreis		Updated. Added 2 commits: - Run `package-upgrade` before `baselayout`, to ensure `apt-get update` is run before installing packages. This fixes the error reported above. I only tested the jenkins worker playbook, but others should have the same issue. - Increase the timeout for downloading `slave.jar`: because it takes about 40 seconds and the default timeout is 10.  @seishun I did not run into the issue with `ssh`/`sshd`. I used a freshly installed `ubuntu-14.04-server-amd64.iso` (14.04.0, not one of the updates, to make sure the package installation would fail). At this point it would be better to land this and follow up in a new issue if you want, but please let me know if you would like this to be resolved here. 
seishun		I still get the `"Could not find the requested service sshd: host"` error. I'm using the latest "ubuntu/trusty64" Vagrant box.  How are you testing this? Did you install the iso manually in a VM? If so, could you provide the steps you followed to configure ssh etc?
seishun		I just installed `ubuntu-14.04-server-amd64.iso` in a VM and `service --status-all` doesn't list `sshd`, only `ssh`.
joaocgreis		The handler only runs when `lineinfile` finds the line, so I added it to sshd_config and was able to reproduce. Added a commit to address it. @seishun let me know if this works!  For reference, I installed the iso directly in a VM, selected the OpenSSH server when installing, and added my key to root. Then on my base machine edited the inventory adding a new machine that pointed to it and added a dummy `host_vars` file.
seishun		Runs successfully now.
joaocgreis		@nodejs/build can you PTAL at the last 3 commits? Both @seishun and myself tested this, so I feel confident landing with just a rubber stamp.
joaocgreis		Landed in https://github.com/nodejs/build/compare/9614094...0707dc2
chorrell		Out of curiosity, is this build instance an Alpine 3.4 docker container running under a VM on Joyent or is it a the Joyent alpine-3 container-native instance? 
jbergstroem		@chorrell this is native (triton). I'll try out the docker stuff too. 
chorrell		Ah, ok. So something to be aware of with the (triton) Alpine Linux instances:  ```    __        .                   .  _|  |_      | .-. .  . .-. :--. |- |_    _|     ;|   ||  |(.-' |  | |   |__|   `--'  `-' `;-| `-' '  ' `-'                    /  ;  Instance (Alpine Linux 3.4 20160620)                    `-'   https://docs.joyent.com/images/container-native-linux  09eec1ff-a221-4743-887f-65ce288a04af:~# uname -a Linux 09eec1ff-a221-4743-887f-65ce288a04af 4.4 BrandZ virtual linux x86_64 Linux ```  You're not actually getting a Linux kernel (it's SmartOS telling an elaborate lie). This will probably have some implications in terms of testing. 
jbergstroem		@chorrell oh, ok. We pass tests though: https://ci.nodejs.org/job/node-test-commit-jbergstroem-alpine34/6/  `libuv` tests (probably more relevant from a kernel functionality post of view) here: https://ci.nodejs.org/view/libuv/job/libuv-test-commit-linux/nodes=alpine34-x64/  I just want to start building and testing against the musl/libc++ toolchain. 
chorrell		In terms of testing musl/libc++ I think it might be OK, but not so much for any test of kernel functionality.  Alternatively, I can look into creating a KVM Alpine 3.4 image that you can use. 
jbergstroem		@chorrell kvm would be amazing. You on IRC? If so we should perhaps continue chatting there instead of pinging @Starefossen :)  
chorrell		Yep: chorrell. I should already be in  #node-build :) 
jbergstroem		Before I forget: The monit script needs to be expanded; sometimes the slave might die without exiting properly, forcing us to `zap` (resetting pid/init state). I don't think there's any harm calling zap in between unless it's actually running. 
jbergstroem		Fully working playbook now in my refactor: https://github.com/jbergstroem/build/tree/feature/refactor-the-world/ansible  install prereqs (sorry, requires ansible 2.2 üò¢) and run something in style with:  ``` bash $ ansible-playbook playbooks/create-jenkins-worker.yml --limit "test-joyent-alpine34-x64-2" ``` 
BridgeAR		This should likely be closed. There was no progress for more than 1.5 years now and we use Alpine 3.6 at the moment.
maclover7		Closing since we have moved from Alpine 34 to 36/37.
joaocgreis		SGTM 
jbergstroem		LGTM 
jbergstroem		Merged in 1373c304585a8e9cd34d1aa053870749d2b8dbf5. 
mhdawson		Its been that way as far back as history goes, Feb 4
mhdawson		Looks like it is ok now closing.
joaocgreis		First experimental release: https://nodejs.org/download/chakracore-release/v8.1.2/
kunalspathak		@joaocgreis  - I believe this can be closed now?
joaocgreis		Closing, if there's any issue with the releases we can reopen.
jbergstroem		I can be part of this too.
rvagg		üëç 
gibfahn		And also take copious notes in the meeting which can then go into a doc somewhere.
mhdawson		Ok, lets plan to include this in the agenda for the next meeting. 
refack		Done https://youtu.be/c2X8R50SR0w?t=2281 (Thanks @rvagg üíê)
rvagg		The simple fix I have in mind is to put the .zip in a specific place on the build machines (/home/iojs/icu4c-55_1-src.zip would work for everything non-Windows as it stands now) and do a `cp` as part of the Jenkins scripting to drop it in to place. These failures are becoming too frequent‚Äîanother variation is a failed checksum once its downloaded. Also, it adds precious minutes to the build process which we could do without.  /cc @nodejs/intl  
jbergstroem		I have a patch adding a download-path to configure. Good enough? 
rvagg		can you use that to point to a directory and it'll check if the .zip is in that directory? that'd be perfect because it'd solve for the case where we don't have the .zip in place or it's not the right version. 
jbergstroem		Yep. I'd also like to switch to curl/wget. On windows curl is aliased to `Invoke-RestMethod` which accepts `-Outfile /path/to/$foo`. This way we can at least have resume support on !Windows 
rvagg		this is getting out of hand, the OSX release slaves have been dying part way through ICU download, latest incarnation was  ```  Fetch: : 26.4MB total, 10.4MB downloaded  Slave went offline during the build ERROR: Connection was broken: java.io.EOFException     at org.jenkinsci.remoting.nio.NioChannelHub$3.run(NioChannelHub.java:613)     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ... ``` 
orangemocha		FYI, @srl295 and I have been pondering whether it would be feasible to just host it in git. Not much progress on that investigation, but it's on our plate. 
jbergstroem		@orangemocha as in, bundle in `deps/`? I'm -1 to growing our git repo for a currently optional build-time dependency. 
srl295		Ahh, let me see if I can get the sourceforge mirror to work again. There's an issue somewhere to have a backup URL. 
srl295		@jbergstroem ICU 54 and 55 are now redirects to sourceforge CDN, please let me know if this helps. 
srl295		@rvagg:  > can you use that to point to a directory and it'll check if the .zip is in that directory?  You can already do that! `--with-icu-source=<somepath>/icu.zip`  _edit_ also it will check if its `icu4c-xx-src.zip` file  already exists in `deps/` and use that.  
srl295		The "use backup URLs" issue is now https://github.com/nodejs/node/issues/2746 
rvagg		> edit also it will check if its icu4c-xx-src.zip file already exists in deps/ and use that.   That's what I'm after! I'm going to hook this up to some of the Ci machines and get them to do a `cp` just before `configure` 
srl295		Perfect.  El martes, 8 de septiembre de 2015, Rod Vagg notifications@github.com escribi√≥:  > edit also it will check if its icu4c-xx-src.zip file already exists in > deps/ and use that. >  > That's what I'm after! I'm going to hook this up to some of the Ci > machines and get them to do a cp just before configure >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/190#issuecomment-138725139. 
rvagg		We really need to do something about this folks, it holds up _every_ release now, it's been a huge frustration to both @Fishrock123 and I for each release we've pushed through, many many start/stop cycles and the problems with the Windows linker hanging for minutes and causing the _next_ build to fail if you start it too soon only exacerbates the problem. If I need to take some action please let me know, I'm unsure what the state of this is.  See https://ci.nodejs.org/job/iojs+release/nodes=win2008r2-release-ia32/192/console for the latest drama, happens most frequently on Windows. 
rvagg		![screen shot 2015-10-05 at 11 12 52 pm](https://cloud.githubusercontent.com/assets/495647/10280146/ac85e886-6bb6-11e5-8731-a8433dc603c1.png)  ^ that was 4.1.1 btw, a wasted hour trying to get a successful build even started 
rvagg		```  Fetch: ' 26.4MB total, 26.4MB downloaded     ** Error occurred while downloading  <http://download.icu-project.org/files/icu4c/55.1/icu4c-55_1-src.zip> Traceback (most recent call last):   File "configure", line 1075, in <module>     configure_intl(output)   File "configure", line 974, in configure_intl     localzip = icu_download(icu_full_path)   File "configure", line 843, in icu_download     nodedownload.retrievefile(url, targetfile)   File "tools\configure.d\nodedownload.py", line 38, in retrievefile     msg = ConfigOpener().retrieve(url, targetfile, reporthook=reporthook)   File "C:\Python27\lib\urllib.py", line 289, in retrieve     "of %i bytes" % (read, size), result) urllib.ContentTooShortError: retrieval incomplete: got only 27021421 out of 27025781 bytes ```  Same error, persistent on both Windows boxes, I need to get v4.1.2 out today as announced for fixing the DoS bug and I can't get past this. Halp! @nodejs/intl @nodejs/build  
rvagg		Cleaned out workspaces, emptied trash and rebooted those machines and they both suddenly work again, perhaps this is a temporary workaround, a pretty terrible one but it's something. 
srl295		Rod, thought this was solved. Let's chat about this as soon as possible. _edit_ did anything come of the plan in https://github.com/nodejs/build/issues/190#issuecomment-138725139 ? 
srl295		This will improve matters once it lands:  https://github.com/nodejs/node/pull/3200 - if we land this one before [ICU 56](https://github.com/nodejs/node/issues/2917) is landed,  then manually caching ICU-54 and ICU-55 on the buildslaves will be sufficient. 
jbergstroem		I believe we're now hosting these on all release-machines. We should improve our ansible-stack by downloading these as part of the setup, though. 
joaocgreis		All windows test machines have this now.  ``` C:\>dir node-icu  Volume in drive C has no label.  Volume Serial Number is 8AD6-8BDC   Directory of C:\node-icu  12/09/2015  01:44 PM    <DIR>          . 12/09/2015  01:44 PM    <DIR>          .. 12/09/2015  01:42 PM        26,945,277 icu4c-54_1-src.zip 12/09/2015  01:43 PM        27,025,781 icu4c-55_1-src.zip 12/09/2015  01:44 PM        27,128,416 icu4c-56_1-src.zip                3 File(s)     81,099,474 bytes                2 Dir(s)  82,721,615,872 bytes free ``` 
jbergstroem		@joaocgreis these are only relevant for release machines (for now) 
mhdawson		Spent a few hours with @gdams today.  We have the networking setup so that the mac machines can connect out to the internet and can be reached using ssh using the jump box server.  We have images for each of 10.8 through 10.12 and the next step is to build an initial ansible script to configure those machines.
mhdawson		FYI @rvagg @jbergstroem 
mhdawson		We have a number of the machines up and connected to the CI.  Now testing them with this temporary job: https://ci.nodejs.org/view/All/job/node-test-commit-osx-macstadium/
mhdawson		We are seeing some failures when running on the new machines.  Tracking issue here: https://github.com/nodejs/node/issues/17164 
mhdawson		Also as context.  Here is the PR for the ansiable scripts used to configure the machines:  https://github.com/nodejs/build/pull/971
mhdawson		Things to close out on this  - [ ] validate CC is working  (@gdams) - [ ] run updated ansible scripts acrosss all of the machines (@gdams) - [x] land the ansible PR https://github.com/nodejs/build/pull/971  - [ ] add 10.11 and 10.12 to regular CI runs (@mhdawson) - [ ] add 10.13 machine to setup - [ ] complete documentation of setup - [x] presentation to the rest of the build WG on setup. - [x] check for firewall popups after a few builds (@gdams).
refack		@gdams can you report the status of this? @maclover7 you had a bug to report RE the playbook?
gdams		@refack the only issue right now is that ccache is not on the path on jenkins. It looks like I need to add it to the java start command
jbergstroem		Do it! 
mhdawson		Done 
joaocgreis		> Length and consistency of involvement with Node.js working groups and/or community. > This one is a little weaker  Not at all, @gibm has been helpful all around! Giving him access sounds good to me. 
gibfahn		@joaocgreis Thanks! Just FYI, I merged my work/other Github accounts under @gibfahn, so I'm using that rather than @gibm. 
jbergstroem		Just a heads up: me/@joaocgreis figured out why machines sometimes 'fell out' of iptables. Fixed! 
mhdawson		@joaocgreis can I take your response as the second required approver ? If so my default path would be to add his key to the AIX/PPC machines as that limits access to just the required set.  @nodejs/build FYI, if @joaocgreis acts as the second app rover I'll go ahead in the next few days unless I hear objections before then.   
joaocgreis		@mhdawson sure, I approve. (Sorry if I was not clear before) 
mhdawson		Ok, @gibfahn keys are on the AIX machines and I'll look at adding him to the PPC ones.  Sounds like he is going to be onboarded as a collaborator.  At that point I'll probably ask that we just get him access in the manner that we would give ongoing access to a collaborator instead. 
mhdawson		Since Gibson is now a collaborator I'd like to propose we just give him access to the test keys so that he has access to the PPC, AIX and 390 machines. Will discuss in WG meting today. 
Trott		> Since Gibson is now a collaborator I'd like to propose we just give him access to the test keys so that he has access to the PPC, AIX and 390 machines. Will discuss in WG meting today.  SGTM 
jbergstroem		Suggesting we close this in favor of #514 (once landed) 
mhdawson		Agreed 
rvagg		testing comment 
Starefossen		I'll be there. 
mhdawson		I'll be there It would also be a good time to get feedback on the presentation for Node Interactive 
mhdawson		@nodejs/build
rvagg		Unlikely to make it this week sorry. Status:  * Restored a couple of Pi's, pi2-2 and pi2-10 (don't have their real names at hand). Wiped and reinstalled but I'm worried about their storage, they may be faulty so we'd best keep an eye on them.  * Pinged miniNodes and got the 3rd Odroid C2 back online * Did the CF log download thing, PR in this repo. Still haven't done a full validation that the data is exactly how we want it but that's 1/2 done - first pass is to turn it into nginx log format and pass it through our metrics scripts to see if they line up. * Pending TODO from Johan to test the LB work he's done, on the surface it looks OK but I haven't probed deeply enough to give a :+1: but if others are available to help validate then please let him know! * Reached out to Voxer to see if we can at least get status info on our relationship with them. * Still needing to make proper progress with the other macOS line of enquiry we have open, will report back soon on that.  * Got hooked up with packet.net thanks to ARM introducing me to them - they are a SoftBank investment like ARM so have a strategic interest in server-side ARM so part of their offering is bare metal ARM64 cloud resources based on the latest server chips and it's very impressive. Myself, Johan and Michael have access to start tinkering and figuring out how best to use those resources because they're also offering general cloud resources to help us diversify which is great!
jbergstroem		While at it; perhaps establish a standard where we use a certain emoji to indicate availability? +1/-1 or something. It would not just show if we can be there; but also if you abstain or just don't reply (general build wg activity). 
piccoloaiutante		@jbergstroem lets go üëç and üëé for showing who is going to attend or not..
jbergstroem		@piccoloaiutante yeah; lets make a note about it in the meeting and add it to the 'template' for next meeting. 
piccoloaiutante		has it already started?
joaocgreis		@piccoloaiutante no, we're waiting for the link from @mhdawson 
mhdawson		just about to start the hangout, 2 mins
mhdawson		link: https://hangouts.google.com/hangouts/_/ytl/vPmERoYEfy59XZzr6SjnYXIlmOVcWq1pDnbXXjz2714=?eid=100598160817214911030&hl=en_US 
kfarnung		I'm getting 403 forbidden for the hangouts link.
gibfahn		@kfarnung odd, it's working for me/us: https://hangouts.google.com/hangouts/_/ytl/vPmERoYEfy59XZzr6SjnYXIlmOVcWq1pDnbXXjz2714=?eid=100598160817214911030&hl=en_US
joaocgreis		@kfarnung try removing everything after `?` or the `=` before, worked for me once
kfarnung		Thanks @joaocgreis, not sure what was going on, but opening incognito worked.
orangemocha		@misterdjules : would it make sense to re-use https://github.com/nodejs-jenkins ? Would you be able to share the credentials? 
Starefossen		A third option might be to look into [GitHub's Status API](https://developer.github.com/v3/repos/statuses/). 
Trott		If I understand correctly, node-accept-pull-request and node-merge-commit aren't coming back any time soon (which makes me sad, but what can you do?) so I think this can probably be closed. Feel free to re-open or comment if I'm mistaken. 
ChALkeR		Does node.green explain the ¬´shipping¬ª/¬´staged¬ª/¬´in progress¬ª status of the features and the `--es_staging`/`--harmony`/`--harmony-*` flags? Afaik it doesn't, and that information is valuable there.  It should be moved somehwere, perhaps to a new page. 
fhemberger		@ChALkeR Let's discuss this in the main thread please: https://github.com/nodejs/nodejs.org/issues/671 
fhemberger		Closing this PR, we're keeping this page for now and just link to node.green 
jbergstroem		Seems to be back now. 
Trott		Logged into the host and did `cd /etc/init.d/jenkins restart`. That seems to have fixed it. Didn't investigate *why* it had the problem in the first place.  So, uh, feel free! But I'm going to close. Thanks!
cjihrig		Thanks @mhdawson  
saghul		Thanks @mhdawson this will be really helpful! 
jbergstroem		..now we only have to get tests passing :-) 
mhdawson		Agreed on the tests.  We are working on the AIX ones.  Down to 6 from a larger number (would have been 4 if new functionality was not being added in parallel) 
mhdawson		Ok given no objections/complains about what I have @saghul should we tell contributors to start using https://ci.nodejs.org/job/libuv-test-commit/ instead of the pre-existing job ?  
saghul		I'd say so! On Apr 1, 2016 16:25, "Michael Dawson" notifications@github.com wrote:  > Ok given no objections/complains about what I have @saghul > https://github.com/saghul should we tell contributors to start using > https://ci.nodejs.org/job/libuv-test-commit/ instead of the pre-existing > job ? >  > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/issues/372#issuecomment-204533773 
mhdawson		@nodejs/collaborators FYI, there are new libuv CI jobs as outlined in this issue that should now be used instead of the previous https://ci.nodejs.org/job/libuv+any-pr+multi/ job when running the libuv tests. 
jbergstroem		FreeBSD was missing from the job matrix, so I added it: https://ci.nodejs.org/job/libuv-test-commit-freebsd/ 
jbergstroem		@michaeldawson: Btw, no need for the conditional steps in the subjobs (that checks ostype). 
mhdawson		@jbergstroem FreeBSD was covered under linux.  I was on the edge as to whether to break it out or not.  Now that its separate it should be removed from the linux job 
jbergstroem		Oh, didn't know. Sorry. 
mhdawson		Removed freeBSD from linux group, closing as complete 
rvagg		/cc @nodejs/build  also @joaocgreis would you mind sanity checking the setup since you're our resident multi job expert? 
joaocgreis		@mhdawson I cannot edit the osx job, can you check the permissions for jenkins-admins?  All others look fine. 
jbergstroem		@joaocgreis My bad. Fixed. 
joaocgreis		@jbergstroem Thanks!  All LGTM. 
gibfahn		What's the current status of the `ansible/` folder for Windows?
joaocgreis		No support at all yet for Windows in `ansible/`. I have it in my queue and would really like to see it done, but it's no small task and will take me a while to get there.  So, for now, Windows is only in `setup/`.
joaocgreis		Landed in https://github.com/nodejs/build/commit/2c2a3e20a95b6ed3061b48e404da93440f44b42f
mhdawson		+1.  It would be really nice to get these kinds of changes out into pipelines or scripts to make them easier to track/review
gibfahn		>+1. It would be really nice to get these kinds of changes out into pipelines or scripts to make them easier to track/review  A good first step would be to put this in a shell script in this repo, then have the Jenkins jobs clone the repo and run the shell script.  Should be a low-impact change.
maclover7		Going to close for now, we are slowly moving to pipelines / scripts living in `jenkins/`.
refack		I'll take this on (unless someone with access wants to). In this case I'll need the samples.
refack		> I'm happy to work with someone on this but it's going to be tedious work on my end to replicate it off the server.  Send me the code and a sample of input and output. I'll manage.
refack		Is this the API you're using? https://support.cloudflare.com/hc/en-us/articles/216672448-Enterprise-Log-Share-REST-API
jbergstroem		@refack yes. 
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
rvagg		-> https://github.com/nodejs/build/pull/987
rvagg		Oh, you know, I did this recently and ran into a ton of trouble because they've changed a bunch of stuff over at scientificlinux where we pull this from. Unfortunately I missed the memo that we'd finally made the switch to the new ansible stuff and did it in the old ansible tooling, I'd be grateful if you were able to do the conversion for me, otherwise just use the old playbook: https://github.com/nodejs/build/pull/740
seishun		Is the old playbook non-functional without #740?  How would you suggest to proceed with the compiler upgrade? Base it on your PR? I don't feel qualified to do the conversion due to insufficient knowledge about the build infrastructure.
rvagg		#740 is good to go, you could just use that to provision a centos6 machine, it's just not merged because ideally it should be against the new ansible infra instead. 
seishun		Is it planned to be merged soon? Otherwise my PR will be blocked.  Or do you mean that #740 is waiting for the conversion?
rvagg		@seishun I just merged it, better than nothing at this stage so if you want to PR against CeotOS6 setup then you'd better do it against the old Ansible playbook unless you're going to do the conversion but there's no requirement that you do.
rvagg		pretty sure this is fixed and has moved on to discussions about which devtoolset, https://github.com/nodejs/build/pull/1203 for one
rmg		LGTM 
refack		Sampled several machines, could find any dump files. Closing.
cjihrig		As annoying as it might be, I'm OK with the VPN requirement.
mhdawson		I'm not on the release team, but have done a fair amount in terms of configuring the jobs which I assume would required VPN access as well.  I'm ok with the VPN requirement as well.  I just hope the client can co-exist with other VPN clients as I have to use our corporate one as well.
maclover7		Closing for now as something that would be nice to have, but seems to not currently be within our means. If someone wants to tackle this, please feel free to reopen.
jbergstroem		Just killed these. 
rvagg		also need to add nightly builder to crontab, that's not there yet 
maclover7		ping @rvagg -- does this need to stay open?
rvagg		addressed in #976
rvagg		We could add an optional email field in the build request page, since we have email infra working now and the IBM boys seem to be successfully using it post-build, perhaps it wouldn't be hard to add.
refack		I would be very appreciative üòç  
gibfahn		I'd much rather not have emails on Job completion, I guess it'd be okay if it was opt-in (maybe another checkbox in node-test-pull-request).  I just go through the [PRs that I raised](https://github.com/pulls) and [the ones I've self-assigned](https://github.com/pulls/assigned) and check for any updates.  We're using the email notifications to signal that something's wrong with the IBM machines.  If we get to the point where the github bot is running PRs for us then the bot could edit its comment with the result of the build once it finishes. Do edited comments trigger another email notification?
mcollina		I'm üëç  on receiving an email field, and if you want to be notified you can just past your email there. I'm üëé  on automatic emails.  @gibfahn editing comments do not trigger an email notification. I'd prefer a direct email than a comment in a gh pr. 
mhdawson		An optional email field seems like a good idea to me.  If we get consensus I can take a look at doing that.  As long as we can use dynamic elements in the list of emails notifications get sent to it might be pretty easy.
cjihrig		I'm +1 on being able to opt in to getting an email on job completion.
refack		Opt in is great üíØ 
refack		> Do edited comments trigger another email notification?  Only if the edit has a new mention.
mmarchini		I like the optional e-mail idea. We could also have an optional webhook (if someone wants to get notified through Slack, for example).  Does Jenkins have the option to create custom fields for users? If so, we could let each user set up their preferred default behavior.
jbergstroem		VM's were restarted. 
gibfahn		On it
gibfahn		Okay, looks like the ramdisk is only 500MB for some reason:  ```console # df -m . Filesystem    MB blocks      Free %Used    Iused %Iused Mounted on /dev/ramdisk0   5120.00      0.00  100%   693603    70% /ramdisk0 ```
gibfahn		Actually it looks like I can't read!  It was completely full, as it's just a folder it never gets cleaned up. We probably need a cron job that cleans them up every week or so.
refack		https://github.com/nodejs/build/issues/895 same thing machine 1 `/dev/ramdisk0   10485760         0  100%   676528    72% /ramdisk0` machine 2 `/dev/ramdisk0   10485760       136  100%   674529    74% /ramdisk0`
mhdawson		@rvagg 
mhdawson		Ping @rvagg 
rvagg		yep, I was doing a whole lot of work on these scripts yesterday and running them against the pi1 test machines so I'll merge what I have with yours. I have a _slightly_ more elegant way to deal with nfs that I've deployed to the pi1's with success.
mhdawson		From my perspective there are advantages to sharing our infra in that its going to be hard for smaller projects to build the breadth of platform support that we now cover and for things that are important for Node.js having the testing across the platforms is important.  As rod points out it comes down to how to draw the line.  On idea I've not fully thought through is if another jenkins instance to run tests for "non-core" but important projects might help address the concerns over security and donor intentions.  Donors could indicate if they want their contributions limited to the "core" jenkins or not.  The downside is of course that we likely end up with less efficient use of resources since we can't share machines across the two. 
MylesBorins		I had a conversation with @ljharb about using our infra for nvm... specifically to be able to test on all supported architectures. I think this is a very valuable service to offer to key projects in the ecosystem 
jbergstroem		From a budget perspective we have a bit of wiggle room for disposable vm's. I reckon we could fit 5 2G 2cpu 40G machines without interfering with our own agenda. 
williamkapke		Any hardware I've donated can be used in any way the build WG sees fit. I have no 'hopes' for it used in any particular way ;) 
reconbot		I maintain `serialport`, it's founded by Chris Williams. Bocoup (my company) sponsors me, and a bunch embedded hardware we use for testing as part of my Web Connected Devices department. Our involvement in open source foundations is pretty common.  The test matrix we have is pretty large and I'm looking to [grow it even further](https://github.com/EmergingTechnologyAdvisors/node-serialport#platform-support) using docker images to emulate the hardware. I'd like to leverage our large collection of linux based single board computers we have that run other architectures (arm v5-9, mipsel, etc). We also have a weird requirement to have a serial device hooked up for integration tests (currently using an Arduino Uno). And recently (a few days ago) I was informed we now have a windows machine part of this infrastructure with an uno running nightly tests.  This summary is to say we have a good cross section of alternative hardware we can add to the Jenkins fleet and take operational responsibility for. I take testing very seriously for the `serialport` project and would happily explore joining the foundation to keep things maintainable and above board. 
rvagg		As per discussion just now in Build WG meeting, we need to add words _somewhere_ in a PR to outline a policy and get agreement from the TSC. Proposal that was just workshopped is:  Projects qualify for use of the build infrastructure if: - They fall within the scope of responsibility under the TSC‚Äîwhich includes the CTC and all of the activity in the nodejs org and some related projects like libuv - _Or_ the TSC, or CTC and the Build WG agree that an externally managed project should have access in some form‚Äîan example of this right now is V8, which isn't managed by the org in any way but the CTC wanted to be able to run in our infra. Another example might be nvm, which may or may not end up under TSC in some form but the TSC and Build WG may agree that the trust relationships are adequate to allow for it to access our infra in a certain way (details of how that would technically work would have to be figured out).  And of course, any project would need to be able to run on our infra, there's no implicit assumption here that it's even possible or that personnel can be allocated to do the work required. 
jbergstroem		We suggested removing this from the wg-agenda at last meeting; doing so. 
shigeki		Does anyone take care of this?  It's no problem to move all files in `out/Release`.
joaocgreis		Sorry @shigeki , I set up those jobs but was away for some weeks so I only noticed this now. I included those files in the binary package. Here is a test run: https://ci.nodejs.org/view/All/job/node-test-commit-windows-fanned/6870/ . For the `arm-fanned` job I added the code but did not test because the job is disabled (https://github.com/nodejs/build/issues/611). Please reopen if there's any problem!
shigeki		@joaocgreis No, problems. Many thanks for doing it. I'm going to submit a new ci job from now.  I appreciate your work. 
rvagg		I've gone ahead and swapped 16.10 for 17.10.  fda8381730202606
gibfahn		+1 on the strategy of maintaining coverage of the latest non-LTS build, and all their supported LTS builds.
mhdawson		Make sense to me, latest non LTS + LTS supported version.
bnoordhuis		Has 17.10 been deployed yet?  I can reliably trigger a kernel lockup from cctest with the x86_64 4.13 kernel (both -generic and -lowlatency) that has been fixed in the mainline 4.13.11 kernel.
rvagg		@bnoordhuis yes, we've had 17.10 in the [node-test-commit-linux](https://ci.nodejs.org/job/node-test-commit-linux/) mix for a week now, green and looking OK.  ``` $ ssh test-joyent-ubuntu1710-x64-1 uname -a Linux test-joyent-ubuntu1710-x64-1 4.13.0-16-generic #19-Ubuntu SMP Wed Oct 11 18:35:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux $ ssh test-joyent-ubuntu1710-x64-2 uname -a Linux test-joyent-ubuntu1710-x64-2 4.13.0-16-generic #19-Ubuntu SMP Wed Oct 11 18:35:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux ``` 
mhdawson		@nodejs/build discussed in last meeting.  No objections.
refack		Renewed issue for 2019 - https://github.com/nodejs/build/issues/1668
rvagg		@silverwind this is now active on  the server, can you confirm for me that it's working before I merge to master?  thanks so much for this 
jbergstroem		@rvagg Whatever you did in the server configuration, I recommend reverting. All tarballs now 404, both for 1.0.1 and 1.0.0.  Edit: you forgot to add root inside the location {} scope. 
rvagg		reverted 
silverwind		Sorry, haven't been around to test. It worked on my local nginx, but that one has a slightly different `gzip` setup. Will test further. 
rvagg		one guess is that I actually manually put this at the top of the `location` list but the order probably matters 
silverwind		Yeah, it shouldn't matter. What nginx version are you running, by the way? 
rvagg		nginx/1.4.6 (Ubuntu) 
rvagg		tried putting it last and get 404s still 
silverwind		Anything in `/var/log/nginx/iojs.org-error.log` ? 
rvagg		hard to tell with all the noise but this could have been during a _turned on_ period:  ``` 2015/01/15 22:23:35 [error] 22024#0: *772283 open() "/usr/share/nginx/html/dist/v1.0.1/iojs-v1.0.1.tar.gz" failed (2: No such file or directory), client: www.xxx.yyy.zzz, server: iojs.org, request: "GET /dist/v1.0.1/iojs-v1.0.1.tar.gz HTTP/1.1", host: "iojs.org:443" ```  so it's looking in /usr/share/nginx/html/ for dist, so it's overriding the proper `location` for /dist/ (although why is it showing an index?) 
silverwind		You're right, I see the problem. The `/` location is pointing to the correct path, but archives don't have that `root`. One possible fix could be something like this, but I'll check the docs if there's a more elegant way, so `root` is only defined once.  ``` location ~* (\.xz|\.gz|\.pkg|\.msi)$ {     root /home/iojs/www;     index index.html;     default_type text/plain;     gzip off; } ``` 
jbergstroem		As mentioned on IRC and slightly above, either add root within the location scope or move root outside of the other location scope to inherit it.   I started doing a new branch with some other improvements (namely removing the www-check since it's executed on every request) - but that's for another day. 
silverwind		Another possibilty would be to nest the regex location, but moving root outside and into the `server` scope would get my vote for being cleanest. 
rvagg		I'm open to changes here, my background is deeply with Apache and it's only the last 6 months or so that I've been really digging more deeply into nginx but I still consider myself a n00b with it 
silverwind		Updated the PR. I moved the `root` into server scope and removed the `/` location because the remaining directives were set to their default values, so not needed any more.  I'm available for some tests in the next hour, but almost certain that it'll work as expected now. 
jbergstroem		LGTM. Lets add other improvements in other PR's -- but it's really heading into premature optimisation-ville :-) 
silverwind		@ljharb could you please retest download resumption? While this change isn't live, I think this may actually have never been an issue in first place. 
Fishrock123		> @silverwind  >  > ``` > location ~* (\.xz|\.gz|\.pkg|\.msi)$ { >     root /home/iojs/www; >     index index.html; >     default_type text/plain; >     gzip off; > } > ```  If you do that default type I don't think you'll be able to download the files normally. 
silverwind		@jbergstroem No, I meant the original issue of non-working resumption.  Here's the correct `Accept-Ranges`, which means the download is resumable:  ``` curl -I --compressed https://iojs.org/download/release/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz HTTP/1.1 200 OK Server: nginx/1.4.6 (Ubuntu) Date: Fri, 16 Jan 2015 05:09:51 GMT Content-Type: application/x-gzip Content-Length: 6463647 Last-Modified: Wed, 14 Jan 2015 04:38:07 GMT Connection: keep-alive ETag: "54b5f2af-62a09f" Strict-Transport-Security: max-age=63072000 X-Frame-Options: DENY X-Content-Type-Options: nosniff Accept-Ranges: bytes ```  And here's a gzip response, supressing `Accept-Ranges`:  ``` curl -I --compressed https://iojs.org/download/release/v1.0.1/SHASUMS256.txt HTTP/1.1 200 OK Server: nginx/1.4.6 (Ubuntu) Date: Fri, 16 Jan 2015 05:10:00 GMT Content-Type: text/plain Last-Modified: Wed, 14 Jan 2015 05:04:19 GMT Connection: keep-alive Strict-Transport-Security: max-age=63072000 X-Frame-Options: DENY X-Content-Type-Options: nosniff Content-Encoding: gzip ```  I believe that https://github.com/iojs/io.js/issues/424 actually never was an issue. 
silverwind		@Fishrock123 Already refactored that. It's just an fallback mime, and [it's at its default](http://nginx.org/en/docs/http/ngx_http_core_module.html#default_type) value, so shouldn't have been an issue. 
ljharb		@silverwind I still get `curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.` from `curl --progress-bar -L -C - https://iojs.org/dist/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz -o /tmp/iojs.tar.gz` - I'm not sure what you mean by "may actually have never been an issue in the first place". It's clearly an issue; it doesn't affect nodejs.org (`curl --progress-bar -L -C - http://nodejs.org/dist/v0.10.35/node-v0.10.35-darwin-x64.tar.gz -o /tmp/node.tar.gz` multiple times in a row works just fine); and it does affect iojs.org. 
silverwind		I don't see an issue, the second curl finishes instantly. What's your `curl -V`?  ``` $ curl --progress-bar -L -C - https://iojs.org/dist/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz -o /tmp/iojs.tar.gz ######################################################################## 100.0% $ curl --progress-bar -L -C - https://iojs.org/dist/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz -o /tmp/iojs.tar.gz ######################################################################## 100.0% $ curl -V curl 7.39.0 (armv6l-unknown-linux-gnueabihf) libcurl/7.39.0 OpenSSL/1.0.1k zlib/1.2.8 libidn/1.29 libssh2/1.4.3 Protocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp scp sftp smtp smtps telnet tftp Features: AsynchDNS IDN IPv6 Largefile GSS-API SPNEGO NTLM NTLM_WB SSL libz TLS-SRP ```  Edit: sorry, ran against nodejs first, but same result on iojs. 
ljharb		`curl 7.37.1 (x86_64-apple-darwin14.0) libcurl/7.37.1 SecureTransport zlib/1.2.5` I presume whatever comes with OS X Yosemite. 
silverwind		And what do you get on this one?  ``` $ curl -I https://iojs.org/dist/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz HTTP/1.1 200 OK Server: nginx/1.4.6 (Ubuntu) Date: Fri, 16 Jan 2015 05:37:40 GMT Content-Type: application/x-gzip Content-Length: 6463647 Last-Modified: Wed, 14 Jan 2015 04:38:07 GMT Connection: keep-alive ETag: "54b5f2af-62a09f" Strict-Transport-Security: max-age=63072000 X-Frame-Options: DENY X-Content-Type-Options: nosniff Accept-Ranges: bytes ``` 
ljharb		Looks like exactly the same:  ``` HTTP/1.1 200 OK Server: nginx/1.4.6 (Ubuntu) Date: Fri, 16 Jan 2015 05:42:04 GMT Content-Type: application/x-gzip Content-Length: 6463647 Last-Modified: Wed, 14 Jan 2015 04:38:07 GMT Connection: keep-alive ETag: "54b5f2af-62a09f" Strict-Transport-Security: max-age=63072000 X-Frame-Options: DENY X-Content-Type-Options: nosniff Accept-Ranges: bytes ``` 
silverwind		I'm using the exact same curl version here on OS X and don't get your error, so I'm really baffled. Could there be an proxy interfering on your side? Maybe your curl is aliased?  Still wouldn't explain why nodejs.org works. 
silverwind		Added some -vvv and I finally saw your 'error':  On OS X (curl 7.37.1):  ``` > GET /dist/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz HTTP/1.1 > Range: bytes=6463868- > User-Agent: curl/7.37.1 > Host: iojs.org > Accept: */* > < HTTP/1.1 416 Requested Range Not Satisfiable * Server nginx/1.4.6 (Ubuntu) is not blacklisted < Server: nginx/1.4.6 (Ubuntu) < Date: Fri, 16 Jan 2015 05:50:35 GMT < Content-Type: text/html < Content-Length: 221 < Connection: keep-alive < Strict-Transport-Security: max-age=63072000 < X-Frame-Options: DENY < X-Content-Type-Options: nosniff < Content-Range: bytes */6463647 < * HTTP server doesn't seem to support byte ranges. Cannot resume. * Closing connection 0 ```  On Linux (curl 7.39.0):   ``` > GET /dist/v1.0.1/iojs-v1.0.1-darwin-x64.tar.gz HTTP/1.1 > Range: bytes=6463647- > User-Agent: curl/7.39.0 > Host: iojs.org > Accept: */* > < HTTP/1.1 416 Requested Range Not Satisfiable < Server: nginx/1.4.6 (Ubuntu) < Date: Fri, 16 Jan 2015 05:51:18 GMT < Content-Type: text/html < Content-Length: 221 < Connection: keep-alive < Strict-Transport-Security: max-age=63072000 < X-Frame-Options: DENY < X-Content-Type-Options: nosniff < Content-Range: bytes */6463647 * Connection #0 to host iojs.org left intact ```  As you see, it requests bytes beyond the `Content-Length` to which the server responds correctly. OS X's version displays an 'error', which really isn't one.  Finally, I CTRL-C mid download, restarted, and it resumed fine on both Linux and OS X, so if there's an issue here, then it's with OS X's outdated curl. 
silverwind		@ljharb [Here's](http://sourceforge.net/p/curl/bugs/1443/) the relevant bug which was fixed in curl 7.39.0. 
ljharb		Thanks - so you're saying the `nodejs.org` server is responding incorrectly, which is why the error doesn't appear when installing `node`? 
silverwind		@ljharb both servers respond correctly, it's just curl getting confused by the missing `Accept-Ranges`. By the way, I still see it on nodejs.org (second run there):  ``` curl -vvv --progress-bar -L -C - http://nodejs.org/dist/v0.10.35/node-v0.10.35-darwin-x64.tar.gz -o /tmp/node.tar.gz * Hostname was NOT found in DNS cache *   Trying 165.225.133.150... * Connected to nodejs.org (165.225.133.150) port 80 (#0) > GET /dist/v0.10.35/node-v0.10.35-darwin-x64.tar.gz HTTP/1.1 > Range: bytes=5119341- > User-Agent: curl/7.37.1 > Host: nodejs.org > Accept: */* > < HTTP/1.1 416 Requested Range Not Satisfiable * Server nginx is not blacklisted < Server: nginx < Date: Fri, 16 Jan 2015 18:46:06 GMT < Content-Type: text/html < Content-Length: 206 < Connection: keep-alive < Expires: Thu, 31 Dec 2037 23:55:55 GMT < Cache-Control: max-age=315360000 < Content-Range: bytes */5119135 < * HTTP server doesn't seem to support byte ranges. Cannot resume. * Closing connection 0 ``` 
targos		There is also ubuntu1204-clang341-64 to skip
targos		I am willing to help here. Is there anything I can do?
targos		Perhaps we can use https://wiki.jenkins-ci.org/display/JENKINS/Conditional+BuildStep+Plugin and read https://github.com/nodejs/node/blob/master/src/node_version.h#L4 ? 
jbergstroem		In release builds to distinguish workers based on node version we currently do something like: ```console NODE_VERSION=$(python tools/getnodeversion.py)  if [[ $ARCH =~ s390x && ${NODE_VERSION:0:1} -lt "6" ]]; then   echo "Not building $disttype on linuxOne slave as only supported on v6.x and later"   exit 0 fi ```  The tricky part is exiting a job early. Perhaps its not tricky at all -- I just don't know how to do it without failing the job.
gibfahn		It's a bit of a hack, but if the problem with exiting early is that the TAP plugin won't find a tap file, you could always generate a file with a single skipped test, something like:  ```bash cat >"test.tap" <<-EOF 1..1 ok 1 # SKIP Not building $disttype on linuxOne slave EOF ```  That way it'd show up yellow (if you wanted it green you could just not use the skip). Not sure if that'd be enough to work.
ofrobots		Is there anything that can be done to move this forward? V8 5.5 was released more than a month ago but we haven't been able to land it in master because of this issue. Being able to release Node.js nightly builds with the latest stable V8 helps find bugs early.
mhdawson		I have this in the v8 nightly test jobs: ``` if [ "X$DESTCPU" = "Xs390x" ]; then  # ICU tests are diabled for BE platforms because the ICU included by V8 is le only  # This is not an issue when we build in Node as we include the proper data files for BE but  # it does mean that we need to build/test without ICU when running the v8 tests.  DISABLE_V8_I18N_OPTION="DISABLE_V8_I18N=1"    # 390 is only supported on v6 and later for now  MAJOR_VERSION=`cat src/node_version.h |grep "#define NODE_MAJOR_VERSION" | awk '{ print $3}'`  if [ "$MAJOR_VERSION" -lt "6" ]; then    RUN_TESTS="DONT_RUN"  fi fi  if [ "$RUN_TESTS" = "RUN" ]; then   #make -j $(getconf _NPROCESSORS_ONLN) v8 $DISABLE_V8_I18N_OPTION DESTCPU=$DESTCPU ARCH=$DESTCPU.release $ADDITIONAL_CLANG_OPTIONS    # when run under jenkins the d8-os.js test hangs because signals don't set to the process when a os.process times out   # it works just fine when run from the command line so its not an issue with v8 itself.  I'd like to figure out why   # it fails under jenkins but so far have not managed to so remove this test before running   rm deps/v8/test/mjsunit/d8-os.js || true   make -j $(getconf _NPROCESSORS_ONLN) test-v8 $DISABLE_V8_I18N_OPTION DESTCPU=$DESTCPU ARCH=$DESTCPU.release $ADDITIONAL_CLANG_OPTIONS ENABLE_V8_TAP=True V8_EXTRA_TEST_OPTIONS="--progress=dots --timeout=120"  else   echo $SKIP_MESSAGE   # fake out so we don't get failures   echo '<testsuite tests="1"><testcase name="none/none.none test were skipped" time="0.001"/></testsuite>' >v8-tap.xml fi ```
mhdawson		Which fakes out the tap file as @gibfahn suggested
jbergstroem		@mhdawson perhaps you can edit the node-test-smartos job and add it?
jbergstroem		Unless anyone else from the build group steps up I'll do my best to get this accomplished in the coming day.
mhdawson		Starting up update now.   First step is cloned job I can test it out on.
mhdawson		Just running some tests now, should be ready soon.
mhdawson		Main job updated, running CI on original PR that was blocked: https://ci.nodejs.org/job/node-test-pull-request/6052/
mhdawson		the 2 smartos platforms ran/green reporting that tests were skipped as not supported, other platforms running so this should be done closing and we can reopen if there is some later problem.
jbergstroem		@mhdawson great work; thanks for helping out.
mhdawson		ok I see there is still work for ubuntu1204-clang341-64, I only did the smartos part
mhdawson		for ubuntu1204-clang341-64 is it that we don't support this for Node vergion > 8
mhdawson		Testing skip of build/test for linux flavors with clang in the name for Node version >8.  Once I get confirmation that is what we need I'll swap into the regular job.
mhdawson		Ok, tests look good, ready to swap into regular job, just want to confirm with @jbergstroem that simply skipping ubuntu1204-clang341-64 for Node version >8 is the right thing to do.
jbergstroem		@mhdawson yeah, that's what I read out of it as well.
targos		Thanks for doing this @mhdawson ! I think skipping is the correct thing to do. There is no good reason to keep supporting this clang version.
mhdawson		Ok CI run on PR where issue was raised was all green, closing.
retrohacker		@rvagg: It would appear I don't have the ability to assign issues, mind assigning me to this? 
rvagg		sorry, misconfigured since the move from node-forward, @wblankenship, @ghostbar, @ryanstevens you now have proper access to this repo 
retrohacker		https://github.com/wblankenship/build-container-sync  Proposed solution. Going to get  http callback working in `docker-webhook` and then it should be production ready. 
retrohacker		@rvagg: https://github.com/wblankenship/build-container-sync now works. Can move it into the io-js repo if you would like. Instructions on how to use it are listed in it's README.md 
jbergstroem		Is this still relevant? We currently have no docker instances running tests. 
retrohacker		I would like it to be. We still have it blocking a few issues in the docker project (mainly an alpine variant of our images).  I could look into a travis-ci build process to simplify the maintance if that is acceptable. 
jbergstroem		Is this for running tests inside a docker host for alpine? That'd be awesome to have and we've definitely got room for it. Are there scripts somewhere that helps us set it up via ansible? 
retrohacker		Ah, sorry, should have given more detail.  The nodejs/Docker group is interested in exploring an alpine image. Since alpine isn't explicitly tested for by the Node project, we want to have automated testing in place for that docker image to ensure the runtime works properly in that environment.  This was the original motivation for building [Dante](github.com/retrohacker/dante). We don't have an ansible script for it, but the workflow is this:  (On PR) 1. Checkout repo 2. Run `dante test` in base directory of repo  3. If Dante returns 0, pass. If 1, fail.  All logs from dante go to stdout. 
maclover7		ping -- is this still needed?
addaleax		> Any idea how this happened? Did the nightly build from a branch other than master, or did master branch rebase/force push at some point?  Yes ‚Äì this was me. And yes, it was force-pushed a bit after that commit landed.  The issue was that some of the commits from https://github.com/nodejs/node/pull/14001 were landed in `master` without full metadata (e.g. the parent of said commit nodejs/node@60f2fa9a8b050563d2b7f3642a84ff192bd362a6 had no `PR-URL` or `Reviewed-By` fields), which screws with our tooling for managing releases.  The contents of said commit are identical to the ones of nodejs/node@9e08695f85d4273f01e010cf384f42030d66b453:  ``` $ git diff 60f2fa9a8b050563d2b7f3642a84ff192bd362a6 9e08695f85d4273f01e010cf384f42030d66b453|wc       0       0       0 ```
joaocgreis		Also, that nightly is quite old because there were no nightlies for a few days. The problem is now fixed and tonight there should be a new nightly build.
MSLaguana		Great, thanks for that. I'll use the new commit you point out for our pump. Regarding the nightlies being behind: We are operating at a few day delay anyway, so that's not so much of an issue for us.
joaocgreis		I assume this can be closed now, please reopen or let me know if not.
jbergstroem		(ping @rvagg) 
jbergstroem		We now do this:  ``` bash if [[ \     ( $NODE_LABELS =~ pre-1-release && $NODE_VERSION =~ ^[0] ) \  || ( $NODE_VERSION =~ ^[^0] ) \ ]]; then ``` 
jbergstroem		It would perhaps be simpler just to exit instead of scoping the rest? 
rvagg		yes, probably, that'd be fine I think 
jbergstroem		Just a heads up; tried landing this in the jobs but it's flawed. Will rewrite, post here and redeploy. 
gibfahn		Seems the tag is still there: https://ci-release.nodejs.org/label/post-1-release/  üòû 
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
refack		> Seems the tag is still there: [ci-release.nodejs.org/label/post-1-release](https://ci-release.nodejs.org/label/post-1-release/) >  > üòû  FTR it's a phantasm. Jenkins show that page for host/label/* i.e. https://ci-release.nodejs.org/label/refael-like-to-walk-in-the-woods/
rvagg		/cc @nodejs/docker   Infrastructure is easy, do you just need a server? Do you need to be able to hook something to Jenkins to be able to run it easily? 
Starefossen		Yes, a server with Docker >= v1.5 installed. It would be nice to hook this up to Jenkins for easy testing, not sure how well Jenkins integrates with building Docker images though. 
retrohacker		@Starefossen I have played around with it a bit. Jenkins is actually surprisingly good at testing docker containers. We pretty much just need a Jenkins server with Docker installed. I was running Jenkins inside of a docker container so it needed a custom DinD + Jenkins build, but I don't think we want Jenkins in a container?  I just got done with college, writing a plugin for Jenkins to fit our needs is on the top of my todo list. I'll get a real proof of concept ready here soon and we can base our requirements off of that? 
rvagg		I'm pretty sure you don't need a plugin, just script what you need. We're already doing stuff with Docker for https://github.com/iojs/build-containers. 
retrohacker		I had a bash script that did it, will revisit. 
jbergstroem		We retired the machine offered a while ago because of inactivity (and being out of space for other machines). Happy to revisit once we have something actionable (thinking jobs) in place. Closing for now. 
mhdawson		@nodejs/build please take a look. Supports: https://github.com/nodejs/build/issues/526
mhdawson		@jbergstroem, @gibfahn @joaocgreis updated based on comments so far.  Let me know what you think. 
mhdawson		@gibfahn @jbergstroem  second set of comments addressed.
mhdawson		Addressed next set comments.
mhdawson		Addressed new comments.
mhdawson		@jbergstroem can you review now that @gibfahn is happy with it.
gibfahn		@mhdawson After https://github.com/nodejs/security-wg/pull/9#discussion_r92052847, maybe we should `s/workgroup/working group` everywhere...
mhdawson		Ok addressed most of the remaining comments.  @jbergstroem in respect to formatting to W80 is there an easy way to do that ?  I've tried to stick under the 72 chars but evening out by hand would be a bit of work and I don't think it matters since when its viewed in markdown, markdown will handle making it look good.
jbergstroem		@mhdawson: 72 or 80 doesn't really matter. For me, the point of markdown is that its very readable rendered (github) and in your editor (vim, et al). A consistent line length is just generally easier to consume. Anyway, since we're down to bikeshedding I'm `+1` regardless.
gibfahn		@nodejs/build would be really good to get this landed so people in teams can get access to jobs.  @mhdawson With two reviews I think if there are no objections this should be good to land, people can always PR improvements.
phillipj		Other than a microscopic nit, this LGTM
mhdawson		Since there are 2 LGTMs now I'll plan to land tomorrow.
gibfahn		Landed in https://github.com/nodejs/build/commit/1fcbba2ae6242588a1f7cb779a859d8734213097
rvagg		this is the current state of the new web server so I'm going to merge this, I expect there to be changes this week as we fix for node.js builds but that can be a separate PR 
jbergstroem		See: https://github.com/nodejs/node/pull/8325 
rvagg		systemd ... change lgtm, systemd doesn't lgtm 
jbergstroem		@Fishrock123 i'm not sure if it autodeploys. lets see what the norwegian says. 
rvagg		![screen shot 2016-11-17 at 11 32 18 am](https://cloud.githubusercontent.com/assets/495647/20371531/98f57aee-acb9-11e6-9ff0-8227e2946e1b.png)  The webhook service on the machine is running and I've asked GitHub to redeliver it and it's worked and the logs on that server says it's updated now.  A blip with that machine I guess .. `*shrug*` 
phillipj		Have you experienced things like this before? Can't remember noticing similar blips with nodejs.org which more or less has the exact same webhook setup...  One thought for later might be to expose the webhook logs on the bot server side, for easier debugging for github-bot team members who doesn't have SSH access to the bot server. 
rvagg		Nope, can't say I have! Perhaps it's due to this particular machine, it's size, resource constraints, network, something like that. Pretty odd but looking in the webhook record on GitHub it's certainly a one-off. 
mhdawson		hmm, looks like the rsync to the web server is destructive in that it removes data which is not in the source.  I'm going to restore a bit more history.
maclover7		ping @mhdawson is still needed?
BridgeAR		Ping @mhdawson 
mhdawson		We still need to add some automated cleanup but for now I'll close this.
rmg		How does this model stand up to attacks over time? Because it gets stored in git you basically lose the ability to change the encryption key as a means of extending the shelf life of the secrecy. In other words, once you go down this path, the first password/key you use has to be strong enough to last "forever" (as long as secrecy is needed).  I think this model might only be suitable for private repos, where you have at least some control over who gets to try to break your keys. 
kenperkins		These are good questions. We already have an invalidation problem for sensitive data should someone leave the project.   We also know that some of the values here are time-boxed; for example we'll probably have new certs every year or two.  That said you may be right; I'm doing some further digging. 
kenperkins		https://groups.google.com/forum/#!msg/ansible-project/iu4ISgHStTY/sDA4o6YimEQJ  And then a [commit](https://github.com/ansible/ansible/commit/2d478b16279fa2d6eda1c8ebd5a1881b4172f69c#diff-df4c9760154e1be8d4a92830ff61b2bf) to Ansible a week after that group discussion that appears to add a`AES256` and `PBKDF2`.  Still trying to dig more.  
kenperkins		I feel like absent a more open security review from a known expert, I'm in agreement that this isn't worth the risk right now. 
rmg		I agree with the goal of a consolidating these things, though. Thanks for taking a stab at it! 
rvagg		lgtm 
jbergstroem		Thanks for the feedback. Merged in d8c0f1d4df143dac2e3cea05fe2e936eb3e75d00. Closing. 
jbergstroem		Closing, seems to be more active here: #233.
rvagg		Or #812
MylesBorins		Worth mentioning this is happening for any + all attempts to run CI right now
gibfahn		To be clear, this is happening for all runs of [node-test-commit-v8-linux](https://ci.nodejs.org/job/node-test-commit-v8-linux/), not `node-test-commit` right?  @joransiu @jbarz do I recall you guys talking about something like this on Slack?
jbajwa		This started failing after https://chromium-review.googlesource.com/c/chromium/tools/depot_tools/+/656565. @john-yan is working with chromium's infra team on this https://bugs.chromium.org/p/chromium/issues/detail?id=764087 . In the meantime could add the following to the jenkins job ``` cd <path>/depot_tools git revert -n 509776e cd - export DEPOT_TOOLS_UPDATE=0 ```
mhdawson		Have put the work around into the job and kicked off a run to see if it resolves the issue.
gibfahn		Assuming it's https://ci.nodejs.org/job/node-test-commit-v8-linux/912/ then it looks like it worked.  We should leave this open to track the proper fix though, sounds like John is making progress.
mhdawson		Looks like work around did the trick, lets leave this open until the proper fix is in place.
mhdawson		@jbarz @joransiu this looks different than this issue https://bugs.chromium.org/p/chromium/issues/detail?id=764087&can=2&start=0&num=100&q=&colspec=ID%20Pri%20M%20Stars%20ReleaseBlock%20Component%20Status%20Owner%20Summary%20OS%20Modified&groupby=&sort=  But it did remind me to ask where we are on this one.  At some point I should remove the work around.
john-yan		It should be fixed now. All we need to do is adding an extra env variable. export VPYTHON_BYPASS="manually managed python not supported by chrome operations"
MylesBorins		Looks like the temporary patch is now failing üò≠  I'm hacking on the CI job right now to make a permanent fix
MylesBorins		Was not able to fix it. I attempted to revert all of the hacks we had currently put in and added `export VPYTHON_BYPASS="manually managed python not supported by chrome operations"`  Same error reported in OP 
refack		~The error I'm seeing is:~ env issue  Using https://ci.nodejs.org/job/node-test-commit-v8-linux-fedora24/nodes=fedora24,v8test=v8test/398/console as reference. Manual on `test-rackspace-fedora24-x64-1`
refack		Ok found some stuff: 1. The fedora machines (https://ci.nodejs.org/job/node-test-commit-v8-linux-fedora24/) are failing because they don't have `bzip2` (and their yum repos have been archived to http://archives.fedoraproject.org/pub/archive/fedora/linux), but I see this is only run by timer... 2. ppc getting:    ```    iojs@test-osuosl-ubuntu14-ppc64-be-3:~/build/workspace/node-test-commit-v8-linux/nodes/ppcbe-    ubuntu1404/v8test/v8test/depot_tools$ ./update_depot_tools    UNKNOWN Machine architecture: ppc64    ```    which lead me to https://bugs.chromium.org/p/chromium/issues/detail?id=764087.    I think it's a `depot_tools` regression of that issue.  ---  The best I got is to revive the test on the `fedora24` - https://ci.nodejs.org/job/node-test-commit-v8-linux-fedora24/399/nodes=fedora24,v8test=v8test/
MylesBorins		@refack were you testing with `VPYTHON_BYPASS="manually managed python not supported by chrome operations"`
refack		> @refack were you testing with VPYTHON_BYPASS="manually managed python not supported by chrome operations"  In multiple variations with no success (with `"` without, in the same line, with `export`, before `make` after `make`, with just calling `gclient sync --with_branch_heads`). Also it wasn't there yesterday when the last green job run https://ci.nodejs.org/view/All/job/node-test-commit-v8-linux/953/nodes=ppcbe-ubuntu1404,v8test=v8test/consoleFull  But now I see that we used to roll back `depot_tools`, and it broke sometime in the last 24h: ``` + git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git Cloning into 'depot_tools'... + cd depot_tools + git revert -n 509776e error: could not revert 509776e... [gsutil] run through "vpython" (2) hint: after resolving the conflicts, mark the corrected paths ``` 
refack		@MylesBorins need someone with job edit capas. add this `git checkout 633a9830a9fce875d277b248c6e6fb14d72e8183` just after `cd depot_tools` and before `git revert -n 509776e`  That should do it. 
gibfahn		>@MylesBorins need someone with job edit capas.  My copy and paste skills are pretty sharp, will have a go
gibfahn		https://ci.nodejs.org/view/All/job/node-test-commit-v8-linux/962/  New log:  ```bash # temporary work around for https://github.com/nodejs/build/issues/886 cd depot_tools git checkout 633a9830a9fce875d277b248c6e6fb14d72e8183 git revert -n 509776e ```  Fails with:  ```bash + cd depot_tools + git checkout 633a9830a9fce875d277b248c6e6fb14d72e8183 Note: checking out '633a9830a9fce875d277b248c6e6fb14d72e8183'.  You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout.  If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example:    git checkout -b new_branch_name  HEAD is now at 633a983... [gsutil.vpython] Update packages. + git revert -n 509776e error: could not revert 509776e... [gsutil] run through "vpython" (2) hint: after resolving the conflicts, mark the corrected paths ```
refack		sorry new SHA - 77b7687e4837223f820e1e98c369d36696f3f2b3
gibfahn		```bash git checkout 633a9830a9fce875d277b248c6e6fb14d72e8183 git revert -n 77b7687e4837223f820e1e98c369d36696f3f2b3 || true ```  Seems healthier. Actually the previous SHA was fine, there was a conflict, but we're not committing so not an issue.  https://ci.nodejs.org/view/All/job/node-test-commit-v8-linux/967/ üôè 
refack		I think it should be: ``` git checkout 77b7687e4837223f820e1e98c369d36696f3f2b3 git revert -n 509776e ``` And don't add the ` || true` cause if it fails, it won't build.
refack		P.S. the conflict is the crux of the issue
gibfahn		Okay, my flight is delayed so lots of time to fix this...  Seems to be working better.
gibfahn		FWIW this, and other things like the FreeBSD bugs, are really regressions in V8 and tools on platforms that Node supports but V8 doesn't (so for V8 I guess not technically regressions, but whatever). I'm not really sure how to solve this, but if we're running Node on V8 to make sure V8 commits don't break Node, we should probably think about platform support. Most regressions arent going to be on xLinux or macOS, they're going to be on less-used platforms.
refack		I think we can with not too much effort remove the dependency on `depot_tools`, it's a very fragile monstrosity (this current issue is because of `vpython` which is a frankenstein of `python` and `virtualenv` compiled together in `go` ü§¶‚Äç‚ôÇÔ∏è . For that, depot_tools brings it's own `python`, `clang`, and `go` compiler).  I have a partial recipe for syncing V8 without `depot_tools` (partial as in works-on-my-computer), I can try to mature it a little.
refack		Holly molly! It's green https://ci.nodejs.org/view/All/job/node-test-commit-v8-linux/968/
MylesBorins		Looks like the git changes broke stuff... failures across all environments rn
refack		@MylesBorins that's a `v4.x` PR right? (https://ci.nodejs.org/view/All/job/node-test-commit-v8-linux/970/) Do we know when was the last known good revision for testing `v4.x`?
MylesBorins		nvm... I think I broke stuff (in my branch)... Will check in again in am and confirm
mhdawson		@john-yan I tried putting in the final fix by adding  ``` # temporary work around for https://github.com/nodejs/build/issues/886 #export DEPOT_TOOLS_UPDATE=0  export VPYTHON_BYPASS="manually managed python not supported by chrome operations" ```  But it is still failing on the PPC platforms.  See this job which is a clone of the regular CI job with the change in place:  https://ci.nodejs.org/view/All/job/node-test-commit-v8-linux-test-mdawson/configure  
mhdawson		Is it just me or does it look like the job is no longer running all of the tests ?
nebrius		I think this sounds really great, I hope we can get some forward momentum on it.  Are you envisioning the query described in step 1 as a part of the NPM install process, i.e. 1) user types `npm install` 2) the NPM client queries this service running on Azure 3) the service returns the binary if it has the binary cached 4) if not, the service builds it on the fly, if it has the correct build configuration and caches that build 5) otherwise it returns an error to the NPM client   Did I summarize that correctly? 
othiym23		This sounds fantastic and aligns with a bunch of stuff on npm's [road map](https://github.com/npm/npm/wiki/Roadmap). How can npm help? 
jbergstroem		This also makes the assumption that node/io.js has been built a specific way (i.e. no shared libraries).   Edit: as well as linked dependencies being met (mysql, postgres, nan, etc) 
orangemocha		Really looking forward to making this happen :+1: The native module compile issue has been a thorn on my side for quite some time.  I will start by looking at the Azure side of things. That is, unless someone knows of a cloud provider that can virtualize OSX/Darwin. 
orangemocha		@othiym23 when a module is published to npm, npm makes no guarantees that it doesn't contain any malicious code, right? I suppose the same guarantees (or lack thereof) will have to apply to the prebuilt version. 
rmg		+1 on using foundation money to pay for an OS X pseudo IaaS since it also solves the trust problem with access to private networks. 
rmg		Three groups come to mind that would benefit from a service like this, but one of them might not even be able to use it. 1. Slow environments like ARM where any reduction in work during module install would be appreciated. 2. Users who don't have a build environment because it is too difficult to install/configure. 3. Users who don't have a build environment because they are prohibited from installing one.  The 3rd group, unfortunately, also seems the least likely to be allowed to use such a service. Is there anything that can be done to make these binaries trustworthy to those users? Some combination of signed npm packages and signed binaries maybe? @othiym23 I didn't see anything like that (signing) on the npm roadmap (I was only skimming), should it be? 
indexzero		Many moons ago @bmeck and I wrote a few modules around this with this very use-case in mind. In particular it deals with [cross-platform, multiple architecture, and multiple node versions](https://github.com/nodejitsu/module-smith/blob/master/lib/builder.js#L278-L320) from a single source. Might be useful for these current efforts. 
cjbj		As a [native add-on owner](https://github.com/oracle/node-oracledb), these are issues/requirements I see: - Improvements to/over npm for download of non-npm'able required   components, e.g. platform specific third party database client   libraries.  Scripting this with https://docs.npmjs.com/misc/scripts   gets messy. - ability to query/match versions of dependent 3rd party libraries.   I.e. extend the queries @rvagg listed in   https://github.com/nodejs/build/issues/151#issue-99730733 to include   at least one more field, for example which database client library   version the add-on should be compiled with. - The node-gyp requirement for Python 2.7 is going to be an ongoing   issue for native extensions, unless you replace this part of the   process too. - Making extension licenses visible before install (or better, support   click-through licenses now or in future) would make the big   corporate ecosystem happy.  Users and their bosses like clear   expectations about liability. - Potential integration with external compile/build services that host   extensions you decide not to, or services that are hosted inside   company firewalls (e.g. replicas to cut down external bandwidth   usage, or with private modules).  These would presumably all be   running the same software. 
rvagg		Thanks @cjbj, great input on all counts, particularly re integration with alt build services. Can you elaborate on your point about licenses though and how it related specifically to this effort? 
cjbj		@rvagg Pretty much any software you will serve has a license.  Lawyers like click-throughs for lawyerly reasons and will quote precedences ad infinitum.  Allowing add-ons to choose to have a click-through before their binary is downloaded will let owners satisfy any legal restrictions, either for the add-on code itself, or for its third-party dependencies.  If click-through support isn't feasible to implement, then the second best option is to make sure the add-on license(s) are visible or accessible prior to serving any software.  Either the full licence text or SPDX identifier should be accessible.  Of all the points in my [previous post](https://github.com/nodejs/build/issues/151#issuecomment-129662584), having click-through support is the most important to me. 
nebrius		@cjbj just to clarify, is click-through a feature you would like to see in the NPM client? 
cjbj		@nebrius absolutely yes.   
othiym23		@orangemocha / @rmg I'm overdue to add a section on trust and provenance to the npm CLI road map, but this is something npm, Inc has been thinking about for a long time. It's a big, cross-team project that's hard to get right and easy to mess up, which I think is part of why we keep letting it get pushed down the priority queue.  I think the most relevant thing here is that npm is planning on adding some notion of signed packages and identity management (at some undefined point in the future), but that this kind of delegated build system can and should manage trust independently of that, i.e. npm-the-service should probably not be delegating authority to an external service.  That's not set in stone, though, and we're still open to input (that doesn't require npm users to use GnuPG or generate X.509 certificates üòé). I'd be interested to hear if @evilpacket has any thoughts on this. 
rmg		@othiym23 agreed, and I hear you on how things like that seem to get pushed down the priority queue.  I didn't mean to suggest that there be any delegation, just that we need the building blocks (signed packages) in place to have any hope of at least considering a chain of custody proof. Glad to hear it's at least in the queue _somewhere_ (would have honestly been surprised if it wasn't). 
springmeyer		Thanks for the /cc @rvagg - I'm interested in supporting this effort and being a resource. I'm primarily coming from the perspective: - wanting very fast, predictable deploys based on minimal network and resources that can go down (for aws autoscaling) - this is why I chose Amazon S3 as the preferred host for node-pre-gyp and why I'm very hesitant to depend on github releases: I trust more in S3 over github. - wanting very fast, predicable automated testing of modules in pure js that depend on native modules (so that travis runs are speedy). - needing to ease the maintenance of lot of native modules, particularly since for nearly all the modules I maintain I ship binaries for both windows, osx, and linux. - making native modules just as easy to install as pure js modules for users even if those native modules depend on dozens of libraries and weigh in a many MB (like `node-canvas` and `node-mapnik`) - So, being able to install something like `node-canvas` without any `apt-get` or external step is my key usecase.  I'm mostly offline currently and over the next couple weeks due to some family issues. But I'm available by email (dane@mapbox.com) or phone if anyone here wants to pick my brain or hear about the pain points with node-pre-gyp and building binaries on appveyor and travis. Overall I'm committed to maintaining node-pre-gyp as https://www.mapbox.com/ depends on it heavily. But I would also be keen for an official solution to completely replace node-pre-gyp and allow it to go away eventually. 
tohagan		Excellent news!  I started the thread above to request a solution to this issue for NodeJS and am delighted to see a well thought out approach that not only avoids local builds but also speeds up and CI's the whole deployment process so we get consistent _uniform_ builds. Please consider some of my suggestions in this thread for lobbying for code signing and signature verification. At a minimum we should at least use SSL for all binary downloads. We might also be able to use certificate pinning if a limited set of download servers is employed (CDN?). Without this we simply can't establish the trust required for many deployment scenarios.  Thanks again! 
tohagan		Just had a thought ... Since its really a CI service ...  it would make sense, particularly if Azure did not come to the party ... to approach Travis CI. I think it would fit their existing business model very well and align with their generous open source commitment. Integration would probably also be least effort for NodeJS developers since most would be using Travis already. 
rvagg		Travis just started offering a beta of OSX support, so suddenly they are a more interesting party in this discussion. 
tohagan		If they are historically a Linux and now OSX shop they may feel some reluctance to consider CI on Windows but really it's just a matter of hiring someone with the right ops/security expertise and seeing that there is a clear business case. In approaching them, I'd point out that binary builds are a pain point for almost _all_ dynamic languages and that their business opportunity is much wider than just NodeJS. I think it would be a _win-win_ for them and us. We'd get the all the public NodeJS binary open source code built and downloadable and it would be paid for by their customers who need their private projects and products CI'd on the same system. Travis has already proven that it works. 
bjouhier		Cool idea! Very useful for people like us who have to support several target platforms. 
orangemocha		I have been investigating this further, with lots of help from @joaocgreis, to try and clarify some of the design aspects and come up with a plan for delivering a working solution. We iterated over a few approaches, some of which turned out to be dead ends but now we think the story is good enough, so we would like to share our findings and get as much feedback as possible before we start implementing a prototype. @nodejs/npm, a review from someone at npm would be especially helpful.  /cc @nodejs/build @nodejs/addon-api @nodejs/node-gyp  ## Goals  The main goal of this effort is, as already stated in this issue, to implement a service that would compile native modules on behalf of end users, to remove the need of compilation on the user machine during npm-install. This should sound pretty uncontroversial, but I explicitly want to call out that anything outside of npm-install is a non-goal. In particular, all issues related to configuring node-gyp by the module author at development time are beyond the scope of this proposal.  Another important goal, or opportunity, is to deliver a solution that would ease the pain of users very soon - at least in the vast majority of use cases. There are multiple ways to skin this cat. The "right" long-term solution from a design perspective would probably entail an ABI-stable abstraction layer in the native modules API, but that might take years to get implemented and adopted. If there is a way to relieve the pain quickly, that's the direction that I think we should focus on with this build service effort. This assumption has dictated the design approach that we have undertaken for the first incarnation of the service, as detailed below.  More goals: - The service code should be open source, and released under a permissive license.  - Even though we might enable certain platforms before others, the service code should be generic enough to support all primary Node.js platforms.  - It should be cost effective in terms of computing resources used, so that we can operate it with a reasonable budget. - The service should work with private npm registries and within enterprise wall.  It would be nice to include support for private modules, but at this point I am not sure if that's feasible. That's something that we still need to explore. I am assuming that we can live without it, at least in the beginning.  ## The node-pre-gyp approach (tabled for now)  One of the first solutions we considered was to leverage all the great work that has already been done in [node-pre-gyp](https://github.com/mapbox/node-pre-gyp). node-pre-gyp is designed to address the very issue that we are trying to solve, so it was a natural choice. It already defines a workflow for downloading modules as binaries, with possible fallback to compilation. The part where it falls short, is that it requires module developers to set up their own deployment sites (though it integrates easily with AWS) and also manually rebuild modules for new versions of Node, or set up their own automation to do it. The approach we considered was to extend node-pre-gyp to use the module build service, so that module authors wouldn't have to manage their own compilation and distribution.  While this seems like a reasonable solution, it does requires a modification of existing modules and an opt-in by module authors, at a minimum to make their module use a modified version node-pre-gyp. Since this is outside of our control, and it could take a long time, we set this approach aside and instead started investigating possible ways to make the module build service provide compilation for existing modules, without requiring any module modification or opt-in.  ## Supporting existing modules without modification  In order to support existing native modules without any modifications, we need to make some changes in the npm stack, either on the client or the server, or both.  The burden will then be on us (the build service maintainers) to test all the modules and whitelist the ones that are supported. We can have additional server-side configuration to define different modalities so that we can support as many modules as possible, and even special case a few. There are ~1750 native modules, with the top ~150 most downloaded accounting for 99% of all native downloads, so going down the list to validate whether they work with the service and configure the service appropriately for each module seems like a feasible task.  The module consumer will be able to choose whether they want to use the service, through a configuration setting or a flag. For illustration purposes: `npm install --use-module-build-service`.  For pure JS packages, this will behave like regular npm install. For native modules, instead of downloading the package and compiling the source to a native library, it will delegate compilation to the server and download a snapshot of the results, as if the compilation occurred on the client.  ### Current npm-install workflow  Disclaimer: we poked around npm but didn't go into too much detail, so it's possible that some elements in the description below might slightly differ from the actual implementation. The overall picture should be pretty close though. [@nodejs/npm please let me know if I missed anything important here]  When npm installs a module, it first fetches the registry entry for that module, which is a json file that pretty much includes the package.json for all the versions of the module (see [here](https://registry.npmjs.com/utf-8-validate) for an example). It will first look for it in a local cache, and if it's not already there download it from registry.npmjs.com.  The client then does some magic to decide which version to install, downloads the corresponding tarball (which also may be cached), extracts it and executes any preinstall/install/postinstall scripts as specified in the [scripts section](https://docs.npmjs.com/misc/scripts) of package.json.  Native compilation typically happens by means of npm invoking `node-gyp rebuild` in one of the install scripts, but this can be specified in multiple ways. The module author can invoke `node-gyp rebuild` from one of the scripts, or even from inside arbitrary shell scripts invoked by npm scripts. If the install and preinstall scripts are empty and there is a .gyp file in the root of the package, npm will automatically set the install script to `node-gyp rebuild`.  The package definitions (both in the registry entry and package.json in the tarball) contain a `gypfile` property, which is set by npm (before publishing?) if there is a .gyp file in the root. This seems like a reliable indication that the module is potentially native. I am saying 'potentially native', because a module can have the gypfile property set, without actually invoking any install scripts or having a gyp file. We found at least [one instance](https://github.com/websockets/ws/blob/master/package.json#L45) of a false positive.   ### Modified npm-install workflow (for the first prototype)  For the initial prototype, we are striving to minimize the changes in the npm client, so that it will be easier to maintain. Once the prototype proves its value and we have had the opportunity to learn from some real-world usage, we can look at how to extend the npm client syntax to support the changes in a more correct way, and/or perhaps go down the node-pre-gyp approach.   The following description is only meant to give an overview of the modified workflow. Details may vary in the actual implementation.  The user runs:   ``` npm install -g npm-precomp npm-precomp install native-module-xyz ```  npm-precomp is a modified version of the npm client, which defaults to using the module build service. npm-precomp behaves exactly like npm, except for when you use the `install` command and the module is native.  After having retrieved the registry entry for the module, the client knows that the module is potentially native (`gypfile` is true). Instead of fetching the official tarball, it sends a request to the build service, providing the module name, and all the parameters that uniquely identify the client configuration from the build service perspective (listed in https://github.com/nodejs/build/issues/151#issue-99730733, point 1).  To handle this request, the build service examines its configuration for the given module and platform. Some platforms or some individual modules might not be supported (for various reasons). In that case, the service returns a `not supported` response and the client falls back to the standard workflow, which might include compilation.  If the module is supported and it's already been compiled, then it's served from the service cache. Otherwise an environment is spun up in a slave VM/container that is suitable to perform the compilation on behalf of the client. The slave pretty much executes the same `npm install`, then stores a snapshot of the resulting folder into a tarball and returns it to the client, while also adding it to the cache for later re-use.  The build service also needs to modify the scripts section in the package.json for the precompiled package, so that any invocations of `node-gyp rebuild` are removed. Since `node-gyp rebuild` can appear in a few places, and since there can be additional commands in those scripts, how to rewrite the scripts will need to be configurable for each module. We'll need to be able to specify which portions of the scripts need to be run on the server or on the client.  Back on the client, it is also important that the precompiled tarball doesn't get stored in the regular npm cache, so that `npm-precomp install` doesn't interfere with the normal `npm install`.  [Implementation note: the npm client currently executes the scripts as specified in the registry entry, not the package.json in the tarball. The above workflow seems simpler, so we'll try to modify npm to read scripts from package.json. Otherwise, we'll need to implement a slightly different request/response pattern.]  #### Maintaining the modified npm client (npm-precomp)  One obvious way of accomplishing this would be to maintain a temporary fork of npm. However, this would require us and our users to keep updating it as new versions of npm are released. So we are pursuing a different approach instead. npm-precomp can be just a wrapper around whatever npm you have installed on your machine, and inject its custom behavior where needed, as seen here (warning: severe hack ahead!): https://github.com/janeasystems/npm-inject-test/blob/master/index.js.  ## Scaling the service up and down  We can expect the service workload to be bursty. Whenever a new ABI-changing version of Node is released all modules will need to be recompiled for that version, so there will be a peak in the service load. As precompiled packages get cached, the compute usage will slowly return to normal levels.  To handle this usage pattern, an elastic use of resources is a must. New environments will be spun in parallel when compilations are requested, up to certain limits. Since we want to control the total expenditure while still allowing for bursty usage, we will define the limits in terms of resources used within a given time period, e.g. maximum resources used within an hour, maximum resources used within a month. We might also want to specify limits per module and per platform. Once a limit is reached, the corresponding requests get put in a queue. If the service cannot sustain the load some requests will eventually time out.  Storage is much cheaper than computing resource, so we'll be caching precompiled modules extensively. Compilation environments will be spun up dynamically and shut down shortly thereafter. At times when no compilations are in progress, the service should use no compute resources other than for the entry point itself.  ### CDN for scalability, DDoS protection etc  In order to make the service scalable, we'll use a CDN for serving request as much as possible. The precompiled tarballs will be stored in a CDN.   Although, we haven't looked at any specific CDNs yet, the hope is that even the initial request for the tarball can be sent to the CDN first, and that we can leverage the CDN for caching by specifying the caching policy in the response from the service to the CDN.  A CDN might also provide mitigation to DDoS attacks.  ## Security considerations  ### Trust between npm and the module build service  Since the build service will act as a proxy for multiple npm clients towards npm, and hence the source of more traffic than the average client, we will want to make sure that our friends at npm have a scheme in place to not mistake the build service for an attacker.  The build service will inherently trust npm and rely on whatever mechanism it will provide to verity authenticity of modules (https://github.com/nodejs/build/issues/151#issuecomment-130740382).  ### Package integrity checks  We can compute fingerprints of precompiled packages, and perform additional integrity checks at various points (including on the client) to reduce the risk of precompiled packages being manipulated in storage or during transfer.  ### Slave integrity  Build slaves will be executing arbirtray code (as specified by the module author) when running `npm install`. Slaves will have to hosted in a sandboxed environment, and they will need to be reset to a known state before each build.  ## Support for private registries  The first version isn't likely to include support for private registries, but this is something that we should consider adding in the future. Like you can operate your private npm registry within corporate walls, you would be able to run your own build service. At a minimum this will require being able to configure where/how slaves are spawned, and where the tarballs are stored.  ## Other approaches considered and ruled out  ### A failed attempt to avoid changes in the client  We had hoped that we could leverage to the `--registry` npm flag to avoid having to make changes in the client.   The idea was to clone the npm registry and keep it up to date via CouchDB synchronization. Clients would hit the service by specifying the its URL with the `registry` flag/setting. On the server we would run a modified version of [npm-registry-couchapp](https://github.com/npm/npm-registry-couchapp) that could handle the precompilation for supported modules, or defer to the official npm service for everything else.  Not having to change the npm client sounded very enticing. But alas, the npm client doesn't send the parameters needed to identify the client configuration, and without those we don't know what to compile for. While we tought it would be acceptable to include the platform and arch parameters in the registry URL, it would be a poor solution for the Node ABI version number.  ## Paying for operational costs  In the beginning we'll make this service available for a subset of modules and platforms, to make sure we can fit the costs into a preset budget. One of the goals of this initial phase will be to measure the operational costs of the service and make a projection of what it will cost to open it to a larger set of modules/platforms. In terms of modules, we will enable building the top X most downloaded modules (with X to be determined). In terms of platforms we'll certainly want Windows in there, because it's the platform that causes the most grief to users of native modules. It would be nice to also have support for OSX, if we can overcome the technical challenge of virtualizing it and get access to enough hardware resources for it.   This initial phase will be sponsored by Microsoft. Once we have assessed the demand for the service, and have gathered more data about its cost, we can figure out the best way to sustain it in the long term.  ## Next steps - Prioritize platforms for prototype. Windows is for sure. Then OSX? Or ARM? - Investigate which cloud providers to use for the slaves - How to host compilation for OSX? - Investigate which CDNs we can use - Build a prototype 
othiym23		@orangemocha I'll try to have a response to you before the end of the week, but there's a lot in here (& I also have a lot going on), so it may take me a few days. Thanks for putting this together! 
orangemocha		Sounds good @othiym23 , thank you! 
mhdawson		In what you have outlined I assume there would be a way to specify to npm-precomp different back ends so for example a company could provide their own inside their firewall ?  
orangemocha		Yes. Even though I don't have any data on the usage of private registries, I assume that it would be valuable to support that scenario, or at least provide a path to it.   It's a non-trivial aspect though and additional input from people who uses private registries would be helpful.   Assuming that for the public service we'll host the build slaves on a cloud provider (e.g. Azure or AWS), setting up a similar mechanism for slaves to run within corporate walls would require a considerable effort and I am doubtful that many organizations would find such effort justified by the benefit of overcoming the npm-install hurdles. Perhaps compilation in the public cloud with the ability to configure a private account for it would be a reasonable compromise? The service entry point could run within corporate walls, fetch the sources internally, and push them privately to the cloud slaves for compilation. 
JCMais		What about packages that have dependencies on some libraries being present?   For example, node-oracledb requires the Oracle client libraries, how would that be possible with that alternative?  Edit: nvm, found it.  > We can't support all native add-ons, consider node-canvas as one case where there are system dependencies that are necessary to both compile and use it. Unless a native add-on can ship its dependencies via npm it's going to be difficult for us to support it.  But anyway, any ideas for a workaround here? 
tohagan		Thanks once again for the thought, planning and collaborative effort you've put into this. Deeply appreciated!  A couple ideas in case they are of use ...  I suspect that the private builds will be your best bet for paying for the service long term. It would be great to get a measure of market interest in this to help you assess long term viability based on [these principles](http://theleanstartup.com/principles). I dare say you already know this.   In case you find the need to sync part of the npm CouchDb database, you may be interested to know that CouchDb 2.x (which I suspect the npm team will be keen to use for it's clustering features) is planning to support a changes feed for indexed views that will support an efficient filtered replication (I'm thinking filtered by `gypfile === true`).  Further down the track [AvanceDB](https://github.com/RipcordSoftware/AvanceDB) may also be of interest. 
indexzero		@orangemocha I'll start off by parroting everyone else and say great job on an in-depth spec. As I mentioned previously in this thread, @bmeck and I have tackled similar problems in [module-foundry](https://github.com/nodejitsu/module-foundry) and [module-smith](https://github.com/nodejitsu/module-smith). Wanted to share learnings from that:  > The build service will inherently trust npm and rely on whatever mechanism it will provide to verity authenticity of modules.  One detail you might want to be aware of is the [`unsafe-perm` flag](https://docs.npmjs.com/misc/config#unsafe-perm), which is responsible for UID/GID flipping when running `npm` commands. We defaulted to `nobody` and `nogroup` (see: [relevant code in `moduleSmith.prepareRepository`](https://github.com/nodejitsu/module-smith/blob/master/lib/builder.js#L367-L444), which is probably the best approach for this service as well to strip unwanted privileges from the `npm` process.  There are also two deceptively complex features that this service that I wanted to highlight:  #### 1. Supporting multiple Node ABI versions?  > Otherwise an environment is spun up in a slave VM/container that is suitable to perform the compilation on behalf of the client. The [minion] pretty much executes the same npm install, then stores a snapshot of the resulting folder into a tarball and returns it to the client, while also adding it to the cache for later re-use.  Spinning up an individual container per build seems pretty sane. Do you intend on spinning up a container per build **and** per Node ABI version? In `module-smith` we were able to build a module for multiple versions of `node` and `arch` from the same `node` process by using specific `npm` environment variables:  ``` npm_config_nodedir npm_config_user npm_config_cache npm_config_node-version npm_config_registry npm_config_arch ```  There may be additional environment variables that are now important, but if you're going to build multiple versions side-by-side in the same container each one will need it's own `npm_config_nodedir` and `npm_config_cache` to prevent side-effects between node versions. There is [relevant code in `moduleSmith. getBuildDescription`](https://github.com/nodejitsu/module-smith/blob/master/lib/builder.js#L278-L329) which outlines these env vars.  #### 2. Windows support?  If this is taken on it would be a huge win for `node` users on Windows. Configuring all the necessary prerequisites on Windows is significantly more difficult than other platforms and one of the biggest (if not the biggest) pain point `node` users were reporting on Microsoft in 2013 _(may have changed since then)._  At the time they contracted us to add Windows support to `module-smith` and `module-foundry`. Hopefully some of that work is still useful / relevant here. The complexities here are pretty nuanced: - Consistency in [environment setup](https://github.com/nodejitsu/module-foundry/blob/master/docs/install/windows.md#sdks--tools) - `process.arch` changes (`ia32` vs `x64`)  - Additional changes to `Path` env var. - Case sensitivity in env vars (`Path` vs `PATH`) - Setting additional env vars `USERNAME`, `APPDATA`, `HOMEDRIVE`, `HOMEPATH` - Explicit spawning of `node.exe npm/bin/npm-cli.js` instead of `npm`  There is yet more [relevant code in `moduleSmith.spawnNpm`](https://github.com/nodejitsu/module-smith/blob/master/lib/builder.js#L466-L522)  Hope that you find this information useful and looking forward to seeing what you come up with! 
distracteddev		@indexzero Do you happen to know if module-foundry/module-smith can still be used today? Or is there another, userland module/system that accomplishes a similar task? Even something simple that only works on a pre-configured environment would work for my particular usecase (just trying to speed up some docker builds where the main bottleneck is building native modules) 
orangemocha		/cc @nodejs/jenkins-admins  
jbergstroem		Issue tracker with a few bugs open about this here: https://issues.jenkins-ci.org/browse/JENKINS-31527?jql=project%20%3D%20JENKINS%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20%3D%20'multijob-plugin' 
jbergstroem		So, in the spirit of forking, does anyone want to take a stab at fixing these so we can get the "build if scm changes" fix? 
rvagg		![screen shot 2016-02-02 at 4 09 58 pm](https://cloud.githubusercontent.com/assets/495647/12740461/748e1880-c9c7-11e5-9c25-59935a0027f7.png)  https://github.com/jenkinsci/tikal-multijob-plugin/commits/master  They appear to think they got that build condition thing sorted out in 1.20.   As for the NPE, I'd love to see the errors we had in https://github.com/nodejs/build/issues/263#issue-118334975 but the logs are now gone cause of auto-cleanup. If it's the same stack-trace with the same line# as in the JIRA report then it'd be from https://github.com/jenkinsci/tikal-multijob-plugin/blob/master/src/main/java/com/tikal/jenkins/plugins/multijob/MultiJobBuilder.java#L170 which hasn't changed since 1.20 but I'd like to confirm that. It'd be to do with `lastJob` not having a workspace for `getWorkspace()` and probably wouldn't be hard to fix but we'd need a failure case to be able to test it.  Unfortunately this all means upgrading the plugin, trying it out, understanding where our breakage is and fixing from there. 
jbergstroem		We have all old jobs at the backup server. I'll try and find something around when the dates when we complained. 
joaocgreis		Finally fixed by https://github.com/nodejs/build/issues/775 . I hope we don't end up with some different issue like in 1.20.
richardlau		https://github.com/nodejs/build/issues/674
Trott		Fixed with `rm -rf /tmp/*-*-*-*-* /tmp/npm-*` although a more permanent fix will have to be concocted. But that's a discussion for #674, so I'll close this. (Feel free to comment or re-open if you think it shouldn't be closed!)
jbergstroem		I don't think so but we could always set one up I guess?
gibfahn		Maybe we could just disable IPv6 on one of our Linux boxes?
maclover7		ping @gibfahn, is still an issue
phillipj		Yupp, and it's already there. The auto deployment hook we've got puts a log file there.  [github-bot/ansible-playbook.yaml#L45](https://github.com/nodejs/build/blob/master/setup/github-bot/ansible-playbook.yaml#L45) & [github-bot/ansible-vars.yaml#L10](https://github.com/nodejs/build/blob/master/setup/github-bot/ansible-vars.yaml#L10)
mhdawson		Looks like the p-linux machines were actually back on-line at 2:15 AM.  The AIX ones are back on-line now and building.  Will check that the builds are successful.
mhdawson		AIX builds have passed on both test machines.
mhdawson		Release build is running on AIX, waiting for that to complete before closing this issue.
mhdawson		Release build completed on AIX as well, and linux on P builds had completed earlier.    Looks like everything is back and working.
mhdawson		Seems to have run out again. Not sure why we are having problems today.  The only thing I can think of is maybe the CITGM runs mean we need more /tmp space  https://ci.nodejs.org/job/node-test-commit-aix/2445/nodes=aix61-ppc64/console
mhdawson		Bumped both aix machines /tmp size with   chfs -a size=+300000 /tmp  it were ok after that we'll have to add to the Ansible jobs
mhdawson		Has been ok for the last 5 days and free space on /tmp still looks good on both machines.  Closing.
gibfahn		be-1 has run out of space again, there was quite a lot of `npm-` and `watchify` directories in there, but most of the space was from these:  ```bash bash-4.3# df -m . Filesystem    MB blocks      Free %Used    Iused %Iused Mounted on /dev/hd3         320.00      0.00  100%    45608    62% /tmp bash-4.3# du -sm * | sort -nr 71.33   6276db7c-1da8-4420-92aa-862871c797c5 37.26   bab451f4-488c-4a61-9587-32ac224457c3 33.22   06d521bd-f86e-4b75-a5f8-4e674344ba6c 23.22   1ab1e7e7-5d0a-48b1-928c-32deb0c9321d 22.31   ced3faad-d7ed-4049-bfb5-1bd9cca4dd02 21.04   17969024-e3e0-40ad-9650-ff2e89cca9c4 16.08   77fa230b-6b7e-446f-a722-89abc740e8ff 12.70   09815a7c-3c6b-42f0-814a-25bd55bd4a53 9.16    cfc64c98-b52e-4fda-8084-a0f2f5cce5b7 6.97    2573b430-09d5-46b6-96a7-fb2a4811616a 6.64    5f91a8f0-93ac-4442-9484-8919374910fd 3.64    4e5af01d-33b8-40c0-998a-964c69b90718 2.13    da43158e-8bdb-4c42-b9ab-0a180f8e2b9d 1.93    baa44f63-92b2-48ca-8b0b-6095d74f756a 1.30    a6f64f82-b7b6-4536-a4c1-9d8d34f72dfd 1.15    074b3e8a-9498-421c-9eaa-2f20954d6744 1.07    npm-2753104-c178f83e 0.92    npm-2752980-c06b9028 ```  They seem to be citgm tmp folders. Hopefully this is something that will be fixed by https://github.com/nodejs/citgm/pull/320. @gdams @MylesBorins.  ```bash bash-4.3# ls -l *-*-*-* 06d521bd-f86e-4b75-a5f8-4e674344ba6c: total 2440 drwxrwxr-x   10 iojs     staff          4096 Jan 16 03:35 async -rw-r--r--    1 iojs     staff       1241752 Jan 16 03:12 async-2.1.4.tgz drwxr-xr-x    4 iojs     staff           256 Jan 16 03:28 npm_config_tmp  074b3e8a-9498-421c-9eaa-2f20954d6744: total 2352 -rw-r--r--    1 iojs     staff       1201098 Jan 16 03:32 libxmljs-0.18.2.tgz -rw-r--r--    1 iojs     staff             0 Jan 16 03:32 npm-debug.log.2534276428 drwxr-xr-x    3 iojs     staff           256 Jan 16 03:31 npm_config_tmp  09815a7c-3c6b-42f0-814a-25bd55bd4a53: total 8 drwxrwxr-x    3 iojs     staff           256 Jan 16 03:39 inherits -rw-r--r--    1 iojs     staff          2408 Jan 16 03:31 inherits-2.0.3.tgz drwxr-xr-x    4 iojs     staff           256 Jan 16 03:31 npm_config_tmp ```
gibfahn		I've removed the citgm stuff, so as long as we fix the `/tmp` issues before we add AIX to the citgm runs we should be okay.  ```bash bash-4.3# df -m . Filesystem    MB blocks      Free %Used    Iused %Iused Mounted on /dev/hd3         320.00    295.38    8%      680     1% /tmp ```
jbergstroem		I think its a great idea. The more machines we test against the better if you ask me. What we need to figure out longer term is rather which machines we deem as "must pass" versus "heads up". 
srl295		nodejs/io.js#2165 was a bugfix (according to my sources) ‚Äî it would give opportunity for node to feed bug reports upstream!  Possibly not so much OS bugs per se, but could give tooling feedback (headers and such).  Tooling fixes could prevent more headaches later. 
silverwind		If we have the opportunity to test unreleased platforms, I'd say go for it. I need to do more research on  https://github.com/nodejs/io.js/issues/2165 before I can do anything about it. Does the CI have an option to skip certain tests like because of known issues? Might be good for this case, as I don't really want to land a workaround for it. 
Trott		Closing in favor of #367. Feel free to re-open or comment if you think that's an error. 
mhdawson		@jbergstroem @rvagg  
jbergstroem		LGTM, overdue :+1:  
rvagg		aye, lgtm  we need to remove linaro and put something about ARM Holdings, also we're getting FreeBSD from somewhere new right @jbergstroem? 
jbergstroem		@rvagg correct. lets add the freebsd stuff once its in. 
mhdawson		Landed as daccbc476f3261e7f02a631611fab103f88d3d9c 
bnoordhuis		Sounds reasonable.  I did wonder why it was still around. 
ChALkeR		+1 for dropping support for anything that doesn't get security updates anymore. 
jbergstroem		Anyone from the build group wants to chip in? 
rvagg		I'm _sure_ I fired off a +1, but can't find it, this is the second issue where this has happened in nodejs/build recently so I'm either having mental or technical problems.  +1 from me, we probably need to make an official policy of killing off any Linux distro that has its upstream support dropped so we don't even need to ask these questions. 
jbergstroem		@rvagg fwiw I've had cases where I swear I posted a comment but later finding that it wasn't there. 
ChALkeR		@jbergstroem @rvagg Btw, I remember having similar issues about two weeks ago, and those turned out to be actually GitHub lags ‚Äî https://twitter.com/githubstatus/status/724933464273903622. 
jbergstroem		@rvagg I believe we need a policy for all platforms/versions. I think as soon as we land in a new OS X environment we should define what we will support (which hopefully equates to what we test and have tested for a while).  I will retire this today and open a new issue about formal support. 
jbergstroem		Retired. 
refack		Bug in agenda generator?  ![image](https://user-images.githubusercontent.com/96947/29466848-52faa138-840c-11e7-8bab-969f5ff3819b.png)  Actual list: * [Create subteams to document release/infra/github-bot access](https://github.com/nodejs/build/issues/826) * [Demo KeyBox at next WG Meeting](https://github.com/nodejs/build/issues/806) 
gibfahn		>Bug in agenda generator?  Refresh the page. There isn't actually a generator, I just copy it in from [the template](https://github.com/nodejs/build/blob/master/doc/templates/wg-meeting-issue-template.md) then add the items by following the links.
mhdawson		@jBarz FYI.
refack		so bug in ![image](https://user-images.githubusercontent.com/96947/29467306-f5c023ec-840d-11e7-9dff-ddc085144957.png) üòú  
gibfahn		I guess. The point is that you can add your own agenda items though, so leaving it unfilled in is fine too.
Trott		@gibfahn I doubt making these agendas is a problem, but if I'm wrong and it *is* tedious, we can get you set up with [`make-node-meeting`](https://github.com/rvagg/make-node-meeting).
gibfahn		@Trott It's not massively tedious, but if there is automation then that would be great. The bigger issue is remembering to create the issue.  Does that get run on a schedule or is it run manually? I'd ideally like a bot that worked out meeting times from the calendar and created the issue, and then updated it when people added the tag to new issues.  Ideally it'd parse minutes and raise the PR automatically too.
Trott		> Does that get run on a schedule or is it run manually?  @gibfahn Manually, but I don't see why it can't be automated or at least put in a cron job that mails you the results so you get a reminder to open the issue via copy/paste from the email.
gibfahn		@mhdawson @joaocgreis @rvagg could someone with streaming/org permissions host the call?
mhdawson		About to start the meeting
mhdawson		https://hangouts.google.com/hangouts/_/ytl/-3A12YAc4qSLIuSdimpjaIuJNcuVKuUOAqlwSXaUTho=?eid=100598160817214911030
gibfahn		Any questions please post here
gibfahn		Refs: https://github.com/nodejs/TSC/issues/211
gibfahn		@mhdawson I had a bunch of really minor nits/suggestions, so I just committed them directly to your branch. If you don't agree with any of them feel free to revert!
gibfahn		We should probably get feedback from @nodejs/TSC and module authors, e.g. @nodejs/citgm , @nodejs/post-mortem (`node-report`), and @nodejs/diagnostics (`node-inspect`).
mhdawson		@gibfahn pushed commit to address comments.
jkrems		+1 (from `node-inspect` side)
rnchamberlain		+1 from node-report side Can you add nodejs/llnode to the first bullet list?  When doing an npm publish, the proper thing to do*  is to update the package.json version and tag the master branch of the respective github project, and update its changes file. So I think the nodejs-foundation userid will need push access on the projects.  * according to Sam and Jeremiah, and they are wise
gibfahn		>When doing an npm publish, the proper thing to do* is to update the package.json version and tag the master branch of the respective github project, and update its changes file. So I think the nodejs-foundation userid will need push access on the projects.  I'm not sure this is necessary right now, as we don't expect `nodejs-foundation` to actually publish modules. If we have to (down the line) then I'm pretty sure any [`nodejs/` org owner](https://github.com/orgs/nodejs/people?utf8=%E2%9C%93&query=%20role%3Aowner) can give the bot access.  ---  >Can you add nodejs/llnode to the first bullet list?  Done
mhdawson		Ok, will plan to land this tomorrow.
joaocgreis		Sorry for asking this so late. Should the credentials be in the `test` folder? Given that this account is only to be used to add or remove users and that someone with the credentials could cause a lot of harm, I believe we should reduce the scope. We could create a new folder `tsc` and give access to people who are simultaneously Build WG and TSC members (and perhaps more on a case-by-case basis if it makes sense).  Actually, I'm inclined to say this is completely outside of the Build WG purpose, and the only reason we're doing this here is because of access to the secrets repo. The TSC team could be given access to secrets and the new folder `tsc` created on the root, instead of inside `build`. 
mhdawson		@joaocgreis, @jbergstroem did have the discussion around whether it would be ok in the test repo.  There will be other collaborators with access to each of the npm modules as well who would be able to do the same things.  I'm ok with creating a new one if we think its needed though. @jbergstroem what's your take ?  
joaocgreis		@mhdawson I have no intention of blocking this forever, feel free to land if that's your intention. This can always be moved later if somebody else agrees with my concerns above.
mhdawson		@joaocgreis Since the key is already there, it might make sense to land the doc, and then we can move/update the doc when we complete the discussion.
mhdawson		@joaocgreis is it ok if I land as is and we revise if we decide to so something later on.
joaocgreis		@mhdawson sure, we can revise later.
mhdawson		landed as 85b33e3f42988261afea99da2879482a927afea5
joaocgreis		Combination filter: Happy to welcome a simplification for this! There are at least 3 mechanisms for selecting what configurations run for what versions: - `NODES_SUBSET`: This is used to support testing versions 0.10 and 0.12 (and iojs at some point). Can we already delete that? It will no longer be possible to test 0.10 and 0.12 (at least without knowing what configurations can be ignored). I suspect testing doc only changes also uses this. - `NODE_MAJOR_VERSION`: I introduced this one to compile with VS2013 only for <=v6.x, I believe it is the cleanest way to achieve this. It is used by the Windows jobs, please do not delete it. Others could be converted to this mechanism. - An `if` in the beginning of the Jenkins script: (used at least by node-test-commit-linux to exclude clang) I don't like this one because it will show a configuration as run and green when it is actually not supported and fails.  Label expression: I'm ok if you want to change.
gibfahn		This should probably still be done, but I'm going to be optimistic and hope that https://github.com/nodejs/build/issues/838 will handle it.
joaocgreis		There are two issues here: 1. Jobs that have tests marked as flaky show as green instead of the expected yellow. 2. Posting results back to PRs shows failure when the job is a success. 
joaocgreis		About 1: Jobs need a post step to mark the job as unstable. Only the windows job had that, so I copied it to all others. Seems to have worked in all but osx and arm fanned: https://ci.nodejs.org/job/node-test-commit/4563/ . However, this seems to be a problem with `test.py`, not Jenkins: note that tests are not marked as flaky in the console output (cc @jbergstroem @Trott )  About 2: Could be a bug in the code to do this in the AIX job. Didn't check carefully, but seems to be different from others (e.g. test-linux). EDIT: Checked carefully and made the job similar to others, might be fixed, but there are no AIX servers to test now. 
Trott		Possibly/likely related: Most jobs respect tests that are marked as flaky, but the Raspberry Pi arm-fanned job seemingly does not. This is quite a problem for the recent test-crypto-timing-safe-equal. 
not-an-aardvark		~~I think the issue is that the Raspberry Pi build is the only one that's failing; it might not be the only job that's not respecting flaky tests.~~ (edit: never mind, see below) 
Trott		@not-an-aardvark That's what I mean. (Or maybe I'm misunderstanding you?) On the other jobs, when that test fails, it turns them yellow rather than red. That's the result of that test being marked flaky. But on the Raspberry Pi, that test failing turns the job red.  
not-an-aardvark		Never mind, I hadn't realized some of the other tests turned yellow. I had thought the Raspberry Pi tests were the only ones that failed. 
mhdawson		@joaocgreis  In terms of the jobs being green instead of yellow, that is now fixed so this issue may be able to be closed. 
joaocgreis		This issue still exists for `osx` and `arm-fanned`. I suspect everything is OK on the Jenkins side and the failure is in `test.py`, but was not yet able to investigate further. 
joaocgreis		The `FLAKY_TESTS_MODE` variable was not passed on those two jobs. Fixed now. Credit to @rvagg , thanks! 
rmg		@othiym23 the docker images being used for CI are in https://github.com/iojs/build-containers and _think_ you should be able to just pull them down and run them against any iojs checkout. 
rmg		As in `docker run -itv /local/path/to/iojs:/opt/iojs --rm iojs/build:iojs-ubuntu-trusty` where /local/path/to/iojs is the iojs checkout you're modifying to run npm's tests as part of the core tests. 
othiym23		Thanks, @rmg! Consider this a tracking issue for me to keep everyone up to date on my progress getting npm tests working properly with CI. 
rmg		Happy to help :-) 
rvagg		@othiym23 how about this instead: let me give you access to the io.js Jenkins and you can trigger builds across all the platforms by pointing it to your local (or npm) fork of io.js, then you can tinker all you want to get them right prior to a PR to io.js. I'm just about to add a bunch of new collaborators to Jenkins so I'll add you to the list while I'm at it. 
othiym23		@rvagg Sounds good. Right now I'm working on getting all of npm's tests passing on Travis under iojs and node 0.12; it turns out that there may be a bug in the new Keep-Alive support on `http.Agent`. Once I can get npm's tests all-green in that environment, I'll start working on getting the builds right in io.js's CI environment. Let me know if / where I need to send you an SSH key. 
jbergstroem		This nowadays lives in the [citgm repo](http://github.com/nodejs/citgm/). 
jbergstroem		This will make all 404's redirect to `/en/` for the nodejs.org scope (besides dist) - right? Not sure that is the desired effect. 
phillipj		Good question. Got to admit I didn't think about URLs other than the actual website. I'll dig more into that as soon as I'm back from one week vacation.  On Tuesday, 2 February 2016, Johan Bergstr√∂m notifications@github.com wrote:  > This will make all 404's redirect to /en/ for the nodejs.org scope > (besides dist) - right? Not sure that is the desired effect. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/nodejs/build/pull/316#issuecomment-178272016. 
phillipj		> This will make all 404's redirect to /en/ for the nodejs.org scope (besides dist) - right?  I assume you would rather see it check the /en/ equivalent exist before doing a redirect, correct? Haven't been able to get that working... Got a trick up your sleeve @jbergstroem? 
phillipj		To elaborate on what I've tried to accomplish without luck so far:  ``` location @english_fallback {     set $english_filename '';      if ($request_filename !~* /en/) {         set $english_filename ...; # need string replace??     }      if (-e $english_filename) {         rewrite ^(/\w+/)(.*)$ http://nodejs.org/en/$2;     }      return 404; } ```  The biggest challenge here is doing a string replace on the current `$request_filename` to get the english equivalent need to do the existence check. AFAIK the kind of manual string replace functionality needed, would require [nginx_substitutions_filter](https://www.nginx.com/resources/wiki/modules/substitutions/) to be installed.  Anyway the use `if` is strongly discouraged by the nginx team and community... 
jbergstroem		How about going at it the other way? redirecting all other 'known' languages that doesn't exist to en? 
phillipj		@jbergstroem something like this?  ``` location @english_fallback {     rewrite ^/(it|ko)/(.*)$ http://nodejs.org/en/$2;     return 404; } ```  That surely removes the complexity and no if's in sight :+1:  
phillipj		@jbergstroem just updated the PR with the simplified version posted above ^^ with explicit known languages as you suggested. Also added the english fallback to the `https` server directive. 
phillipj		@jbergstroem any thoughts on this after it got updated? 
Fishrock123		Didn't we have something like this for iojs.org?  Looks like it does work on iojs.org: https://iojs.org/fa/es6.html  Ideally, however, it would not re-write to /en/ so that any future links can still go to the native language? I think? I'm not sure. 
Fishrock123		Hmm, looks like we just made redirect static files for each page: https://github.com/nodejs/iojs.org/blob/master/source/static/es6.html 
Fishrock123		As a note, 404's don't even work correctly at the current time: https://nodejs.org/ko/404/ - yet we clearly have one for korean: https://github.com/nodejs/nodejs.org/blob/master/locale/ko/404.md 
Fishrock123		Ok I spent some time trying to get the localized 404 pages working today.  What I've come up with:  ``` nginx location @english_fallback {     if ($uri ~* ^/(it|ko)/) {         set $lang $1;     }     rewrite ^/(it|ko)/(.*)$ /en/$2; # I've used a relative url here for testing, not sure if it should be absolute or not     # No `return 404;` here, I think that makes our page actually return that status code ontop of serving.. } ```  Where we _currently_ declare `error_page 404 /en/404.html;`:  ``` nginx error_page @404; # use localized 404 pages if possible location @404 {     try_files /$lang/404.html /en/404.html; } ``` 
Fishrock123		@phillipj Hopefully you have time to look at this, I'd definitely like to get it live! 
jbergstroem		I will follow up here shortly too. 
phillipj		@Fishrock123 awesome, thanks for pitching in here! Just pushed an update which contains your localized 404 hackery üëç 
Fishrock123		@nodejs/build could some people look at this please? This is really important for website translation and I'd like to get it live ASAP. 
phillipj		Rebased to fix merge conflict.  Also decided to not go for relative rewrites to do things as before, and not raising more questions than necessary. 
Starefossen		Looks good to me üëç  
jbergstroem		Sorry for the delay -- I just need to do some performance testing (if's are evil yada yada) 
Fishrock123		`if`s might be evil but I don't know another way to do it. :( 
Fishrock123		@jbergstroem Can we merge this and if it really turns out to be a problem, undo it and have a single, multi-language 404 page? This is badly blocking translation groups as I understand... I know you're busy. :(  (I don't really know how to benchmark this... if you just want me to run `ab` or `wrk` against it I can do that though!) 
jbergstroem		@Fishrock123 I usually replay traffic with logs. Didn't know this was badly blocking translation -- I'll try and get this done shortly then. 
Fishrock123		Without this, unlocalized pages simply 404, meaning either you translate everything, or don't really bother. :/ 
jbergstroem		Performance wise this looks ok but I'm not sure it works as intended; I don't get served /en/about/ if I visit /it/about/ and it 404's. 
Fishrock123		Oops didn't see that. Looking... 
Fishrock123		@jbergstroem can you check if it works with `/it/about.html`?  My testing env (not this full ngnix) works with `.html` but not `about` or `about/`.. Maybe that's the problem? If not I can try using the node.js nginx with this patch. 
Fishrock123		See https://github.com/nodejs/nodejs.org/issues/764 for a bug report on this.  Will try to clone this config asap and try it... 
phillipj		I've tried setting up nodejs.org the ansible way wo/luck. Any pointers to a good ansible introduction which relates to this ansible setup would be awesome! 
Fishrock123		cc @rvagg  
jbergstroem		@phillipj have you read the instructions at https://github.com/nodejs/build/tree/master/setup/www ? 
gibfahn		Looks like the `linuxone` parameter was wrongly configured. `node-test-commit` passes down `$POST_STATUS_TO_PR` (and that's what the linuxone script checks), but `node-test-commit-linuxone` was setting `$POST_TO_PR_ID` instead.  Should be fixed now.
refack		I see the name change, but type is still "text" ![image](https://user-images.githubusercontent.com/96947/27502792-b0dfcc68-5844-11e7-96e9-9f2d54e88448.png) 
refack		Just has an idea to pass `true` as a string and that works. ![image](https://user-images.githubusercontent.com/96947/27502879-5df48196-5845-11e7-98bd-d787c6e75658.png) 
gibfahn		>I see the name change, but type is still "text"  Oh yeah, I didn't think about that. In Jenkins parameters are just environment variables, so boolean params are just the string `true` or `false`. I've fixed that now.
refack		Wonderful 
maclover7		Confirmed this is fixed, going to close out.
jbergstroem		I think as soon as we have an accepted deployment strategy we should be fine. I can spawn one or two vms (does it/should it support being run from multiple locations?) once that's done.  As for deployment, I'd be cool with auto-deploying on master if the repo started following a more strict merge strategy, similar to nodejs/node (signoffs, tests, commit messages, etc). 
phillipj		> I think as soon as we have an accepted deployment strategy we should be fine.  Do you have similar processes running, or have anything special in mind? ATM it's pretty much `npm install && npm start` as long as the required $ENVs with 3rd part tokens are set.  > I can spawn one or two vms (does it/should it support being run from multiple locations?) once that's done.  Looking at the minimal traffic it receives/generates atm, there's not a need for multiple instances. To keep it simple at first, I suggest we keep it as one instance.  > As for deployment, I'd be cool with auto-deploying on master if the repo started following a more strict merge strategy, similar to nodejs/node (signoffs, tests, commit messages, etc).  Cool! In regards to tests we absolutely have to improve when things get settled. In regards to deployment, I'll create a smoke test just to ensure we don't try to merge and deploy code which doesn't even start.  Not sure we should aim to be just as strict as in nodejs/node at this stage being 3-4 devs involved. Would n't a green Travis build and at least one LGTM from another dev be good enough?  /cc @Fishrock123 @williamkapke 
Fishrock123		> As for deployment, I'd be cool with auto-deploying on master if the repo started following a more strict merge strategy, similar to nodejs/node (signoffs, tests, commit messages, etc).  Not sure I really agree here. The website has a similar lax policy, autodeploys, and arguably can effect a wider ranger of people in worse ways.  The bot only needs basic repo access afaik (stauses), so I am not too worried. I think the org inviting thing should probably be a separate bot due to security reasons. 
Fishrock123		@nodejs/build should we take this to the next build WG meeting perhaps? 
Starefossen		Great suggestion @Fishrock123, I have tagged this with `wg-agenda`. 
jbergstroem		@Fishrock123 I'd like to counter-suggest reviewing for the nodejs.org repo too then. I don't feel it would slow down much to have one other person say "hey, this looks good -- I'm ok with you pushing this change that can affect a lot of people". If its about collaborators not being used to git I'm open to find a middle ground. 
jbergstroem		> @phillipj said: > Looking at the minimal traffic it receives/generates atm, there's not a need for multiple instances. To keep it simple at first, I suggest we keep it as one instance.  I prefer redundancy is all. Especially if we are to collect raw logs from github that will be the foundation for audits. If we don't support running two slaves (races et al) lets at least have it in mind and try to put it on a roadmap.  > @Fishrock123 said: > Not sure I really agree here. The website has a similar lax policy, autodeploys, and arguably can effect a wider ranger of people in worse ways. >  > The bot only needs basic repo access afaik (stauses), so I am not too worried. I think the org inviting thing should probably be a separate bot due to security reasons.  (as follow up to my above comment)  Regarding implementing code-reviews, I'm fine with having a lower bar for entry until collaborators become more acquainted; I guess similar in a way to how we'd have a quicker merge for documentation typos @ the node repo.  What I'd like to at least raise concerns about is the "laissez fair" approach for pushing changes. Every time we have jenkins security issues we're on lockdown for unknown amounts of time and worst case need to redeploy. I don't want to add other components to a build infrastructure that has an unknown/lax state of trust. I know "Reviewed-by" doesn't mean much if someone really wants to compromise a repo (especially seeing how we'd autodeploy on push), but it's a start and makes someone else than $committer partially responsible for what can happen.  Finishing off, It's not super clear to me what we would discuss at an agenda this point but I guess security/trust would be at least one topic. 
phillipj		@jbergstroem we definitly use LGTMs in nodejs.org, but there's no strict requirement to have tests at all times or format commit messages. So far this has worked perfectly for that project, still it's far from the process in core.  P.S. the release blog posts can be merged right away by the author to get them out asap, although they're still PRs which might get comments later on. 
Fishrock123		Another option would be: check for tags and only build ones that are signed by one of specific gpg keys.  In hindsight, I don't really see a problem using LGTMs on the bot. Not sure about adding reviewed-by metadata since we usually merge by the github button but those all link back to the PRs. 
williamkapke		I had a chance to chat with @mikeal & @Fishrock123 at NodeConf this weekend about this topic and got some positive responses so I would like to try to help move this forward.  I just listened to the Build WG 2016-06-07 #434 meeting to get the update. It sounds like the participants wanted a bit more background.   ### Background  There were originally 2 bots being created in parallel (on accident): [1 by myself](https://github.com/williamkapke/hookbot) with a focus on creating tasks to [manage the Github](https://github.com/nodejs/TSC/issues/51)/YoutTube Communities & 1 by @phillipj which focused on doing build status and tagging. I moved my work to @phillipj 's project to combine efforts.  I also opened an issue on the bot's repo to start a discussion about [deployment, access, & permission](https://github.com/nodejs-github-bot/github-bot/issues/23) since my scripts will need elevated permissions to the Github Org and YouTube/Google+.  ### What elevated permissions?  **Github:** This is only needed to add people to the org. Working demo video: https://github.com/nodejs-github-bot/github-bot/pull/29 **YouTube:** I have a few ideas for using the bot to administer/automate things on YouTube. e.g.: Scheduling meeting streams, Sync'ing/Automating Video Descriptions, open sourcing the video transcripts... **Google+:** For [calendar integration](https://github.com/nodejs/TSC/issues/63). **Jenkins:** I don't know the details on this, but @jbergstroem mentioned a few in the Build WG meeting video.  ### What are the next steps needed? - The bot is already doing real work on some of the Node.js Github Repos. I suggest we get the repo moved to the Node.js Github account. - Currently, @phillipj is manually deploying this to his own Digital Ocean account. Hopefully the Build WG can take this off his shoulders. - Deployment procedure/policy/requirements decided. - Load balance for redundancy.  Optional: - I created [an SSE Relay Server](https://github.com/williamkapke/hook-relay) to make development from  the community possible without needing to set up their own Github Org, deployment, and webhook. It is currently running on my Heroku Account and isn't a burden- but it should probably be within this umbrella. - @Fishrock123 created https://github.com/TestOrgPleaseIgnore and I created https://github.com/noduh which are used to test scrips against and has webhooks pointed to the SSE Relay mentioned above. Maybe the test org should go be under the umbrella too? (Consider that there isn't a CoC or moderation of any kind there)  I have paused work on development while I figure out of this can/will move forward since my tasks are depend on this. Perhaps a high-level "actionable item" for the WG is just to vote on whether the WG feels it is within their domain (or is even interested) in taking on this responsibility. Looking at the [WG's charter](https://nodejs.org/en/about/working-groups/#build), I'm not 100% sure it is- so I hope I'm not conveying pressure.  /cc @nodejs/tsc  I recognize this is additional load and scope placed on the Build WG which you MAY not have time (or interest?) to commit to it. I'm happy to dedicate time & help out however possible. 
phillipj		@jbergstroem could you elaborate on the need for elevated Jenkins access that you mentioned in WG meeting? In the testing we've done so far when updating PRs w/Jenkins build status, the bot hasn't had the need to even authenticate to Jenkins, as Jenkins pushes status updates to the bot via HTTP/curl. IIRC those status updates doesn't contain anything secret either.  Not trying to say the bot isn't subject to security issues, as it already has elevated GitHub access to several of our repos. But we might not need to give it elevated Jenkins access. 
jbergstroem		@phillipj sorry for the late reply  > @jbergstroem could you elaborate on the need for elevated Jenkins access that you mentioned in WG meeting? In the testing we've done so far when updating PRs w/Jenkins build status, the bot hasn't had the need to even authenticate to Jenkins, as Jenkins pushes status updates to the bot via HTTP/curl. IIRC those status updates doesn't contain anything secret either.  As-is, nope. Elevated access to jenkins is not required; anonymous would even work fine but if we want to query builds for release or perhaps "sync", as in figure out current state seeing how jenkins will likely bug and send us half of the curl requests we expect it too, we will have a problem. I guess it was unfair to speculate on future access, but I feel it might be unavoidable.  > Not trying to say the bot isn't subject to security issues, as it already has elevated GitHub access to several of our repos. But we might not need to give it elevated Jenkins access.  We could perhaps redeploy when elevated access is required? 
phillipj		> We could perhaps redeploy when elevated access is required?  Sounds good. It would be great if we could identify showstoppers for deployment as-is, and tackle challenges as they come. 
phillipj		@nodejs/build any thoughts/preferences on deploying this? E.g. ordinary scripts or docker? 
jbergstroem		We use ansible for most stuff, so that's preferable. 
jbergstroem		Removing wg-agenda -- nothing to discuss. 
phillipj		Made a separate issue for auto deployment https://github.com/nodejs/github-bot/issues/69, closing this for now.  Feel free to open more focused issues here or in the github-bot repo if there's something in this issue which still needs to be addressed. 
phillipj		Just noticed v6.8.1 also has `zlib: null` 
targos		It's also missing the `npm` property 
targos		And `uv` is an empty string 
phillipj		/cc @nodejs/release  
MylesBorins		/cc @nodejs/ctc  
jbergstroem		The likely cause is us switching www hosts (was running out of disk space). Not sure exactly what. 1. We moved the dist indexer to a new place: https://github.com/nodejs/nodejs-dist-indexer 2. The dist indexer at our old www host is pretty much identical to currently deployed at new place (just making sure that no un-committed code was running) 3. The scripts look like they are invoked the same way 
rvagg		I've run it again and it's updated. This can happen if the tag is not pushed to the public repo before being promoted. @evanlucas I think you did 6.8.1, do you recall if this happened? 
evanlucas		Looking back, I may not have pushed the tag when I promoted 
rvagg		pretty sure https://github.com/nodejs/node/issues/9142 will solve this 
gibfahn		cc/ @nodejs/github-bot 
phillipj		I'll skip the long story of why we've got a far from ideal Travis integration setup, the main point is that we don't allow any third GitHub applications private access to the nodejs organisation.  As far as I remember, it is in fact a github.com telling Travis to trigger a build, but Travis is not allowed to post updates back to github.com/nodejs/*. That's why we've made the github-bot poll Travis continuously for updates on the build started for a particular PR. That polling kicks off either when the PR has just been created or changed (the `synchronize` GitHub webhook event).  Refs the related bot code: https://github.com/nodejs/github-bot/blob/master/scripts/display-travis-status.js
LaurentGoderre		Why don't you create an artifacts repo that the bot was write access? We have done this for years now and it has been great!
maclover7		Unfortunately this is not something that is immediately actionable, and is related to our handling of third party apps in the nodejs GitHub org.  It looks like the org management policy has been updated to allow apps under certain circumstances -- the details are available at https://github.com/nodejs/admin/blob/master/GITHUB_ORG_MANGEMENT_POLICY.md#use-of-bots-and-services.  Closing for now, since not immediately actionable by the Build WG.
jbergstroem		Have we ever referenced this path? 
fhemberger		I had in my RSS reader, having subscribed to the old website's feed. Took a while to recognize it was gone, as I'm reading most postings as PR notifications. ;) 
jbergstroem		Ok, cool. It looks like the line below tries to attempt the same thing though. Perhaps look at fixing that instead (suspected missing trailing slash)? 
fhemberger		@jbergstroem Okay, updated the PR. Rewrite specific feeds first, everything else starting with 'atom', 'feed' or 'rss' should redirect to the current blog feed. Also much easier to read. ;) 
jbergstroem		LGTM. I can merge this in a few days. 
fhemberger		Can we please get this fixed? 
jbergstroem		Sorry; I'll get it done. 
jbergstroem		Merged in 04bb6ba5758f618a88216fde404d235a9ab4ce29. Btw, guessing you edited through online? Suggesting you change your email through git config (currently `fhemberger@users.noreply.github.com`) and use commandline to push instead. 
fhemberger		@jbergstroem Thanks, flag for "private email" was set on the web interface. 
snostorm		Note: switched this to `public/` to match the change in the sibling PR. 
rvagg		live 
gibfahn		#### Failure:  https://ci.nodejs.org/job/node-test-binary-arm/11211/RUN_SUBSET=3,label=pi1-raspbian-wheezy/console  Building remotely on test-requireio_chrislea-debian7-arm_pi1p-1 (pi1-raspbian-wheezy) in workspace /home/iojs/build/workspace/node-test-binary-arm  ```bash + git clean -fdx warning: failed to remove out/Release/.nfs00000000000f537d000002f9 ```
refack		I had an idea, by way of cataloging toward a HOWTO, to add a "code" to such failures - then we can count them and add steps for troubleshooting:  * `failed-to-remove-.nfs` has the following signature:   ```   + git clean -fdx   warning: failed to remove out/Release/.nfs00000000000f537d000002f9   ```   possible step for resolution:   *  (build/test): check that there are no `[node] <defunct>` zombies.       If there are they need to be `sudo kill`ed.  ```bash ps -ef | grep "[node] <defunct>" # To check ps -ef | grep "[node] <defunct>" | awk '{print $2}' | xargs sudo kill # To clean up ```  Seen several times recently: #### [2017-10-25](https://ci.nodejs.org/job/node-test-binary-arm/11223/RUN_SUBSET=0,label=pi1-raspbian-wheezy/console) * `test-requireio_chrislea-debian7-arm_pi1p-1` * probably the same incident as above * Trott took machine offline @ 16:30EDT  * resolved by refack @ 18:00EDT (zombie killed)  #### [2017-10-17](https://ci.nodejs.org/job/node-test-binary-arm/11024/RUN_SUBSET=0,label=pi1-raspbian-wheezy/console) * `test-requireio_securogroup-debian7-arm_pi1p-1` & `test-requireio_bengl-debian7-arm_pi1p-2` * similar signatures * resolved by rvagg @ 2017-10-18-08:00EDT (zombie killed)  #### [2017-10-29](https://ci.nodejs.org/job/node-test-binary-arm/11325/RUN_SUBSET=0,label=pi1-raspbian-wheezy/console) * `test-requireio_davglass-debian7-arm_pi1p-1` * signature a little different:   ```   + git clean -fdx   warning: failed to remove out/   Removing out/   Build step 'Execute shell' marked build as failure   ``` * `[node] <defunct>` confirmed and killed by refack @ 2017-10-29-11:00EDT 
gibfahn		@refack great idea, but I'd add something else to the `Resolution:` section: who has the access (node/build, node/infra, just Rod etc.)
rvagg		@refack you offering to start a HOWTO somewhere?
gibfahn		Thinking this might be a good use for the wiki, stuff that changes often, we don't really need to worry about source control for, basically a scratchpad for anyone to edit.  Looks like Johan had a similar idea a while ago.
rvagg		I'm starting to keep notes now on my maintenance of the cluster. I can drop them in here also if that's helpful. If there are repeating correlations with erroring machines even after replacing SD cards then we might be able to pinpoint ones that need to be retired.  Today I r/w tested the SD cards on `test-requireio_bengl-debian7-arm_pi1p-1`, `test-requireio_mhdawson-debian7-arm_pi1p-1` and `test-requireio_securogroup-debian7-arm_pi1p-1`. Notice they are listed above in @refack's comment, they show up regularly and I'm pretty sure I've reprovisioned these before, `test-requireio_ceejbot-debian7-arm_pi1p-1` is another one that shows up more often than I'd like. I don't have good enough records to make a solid assessment though so I'm just going with SD card testing for now.  I've thrown one of the cards out and inserted a new one and set these 3 up from scratch and they are back in the cluster.  Also, I'm hoping that by pulling back on the overclocking on these that we might have more stability. We'll see.
refack		I've added the "kill defunct" line to the job config (after manual testing and one mistake): https://ci.nodejs.org/job/node-test-binary-arm/jobConfigHistory/showDiffFiles?timestamp1=2017-11-05_03-13-42&timestamp2=2017-11-14_06-49-15
BridgeAR		There are multiple builds where pretty much all arm runs failed See e.g. https://ci.nodejs.org/job/node-test-binary-arm/12674/ https://ci.nodejs.org/job/node-test-binary-arm/12685/
Trott		I'm removing stale `.git/index.lock` on the Raspberry Pi devices as I find them, but I don't know the cause.
Trott		Also seeing multiple failures that look like this:  ```console 11:56:00 Started by upstream project "node-test-binary-arm" build number 12701 11:56:00 originally caused by: 11:56:00  Started by upstream project "node-test-commit-arm-fanned" build number 13496 11:56:00  originally caused by: 11:56:00   Started by upstream project "node-test-commit" build number 14936 11:56:00   originally caused by: 11:56:00    Started by upstream project "node-daily-master" build number 977 11:56:00    originally caused by: 11:56:00     Started by timer 11:56:00 [EnvInject] - Loading node environment variables. 11:56:01 Building remotely on test-requireio_rvagg-debian7-arm_pi2-1 (pi2-raspbian-wheezy) in workspace /home/iojs/build/workspace/node-test-binary-arm 11:56:03 [node-test-binary-arm] $ /bin/sh -xe /tmp/jenkins1159433904551122903.sh 11:56:03 + set +x 11:56:03 Tue Dec 19 16:56:03 UTC 2017 11:56:04 + pgrep node 11:56:04 7241 11:56:04 7247 11:56:04 7252 11:56:04 7253 11:56:04 7258 11:56:04 7260 11:56:04 7269 11:56:04 7274 11:56:04 7276 11:56:05 [node-test-binary-arm] $ /bin/bash -ex /tmp/jenkins199448051140569265.sh 11:56:05 + rm -rf RUN_SUBSET 11:56:05 + case $label in 11:56:05 + REF=cc-armv7 11:56:05 + REFERENCE_REFS=+refs/heads/master:refs/remotes/reference/master 11:56:05 + REFERENCE_REFS='+refs/heads/master:refs/remotes/reference/master +refs/heads/v4.x-staging:refs/remotes/reference/v4.x-staging' 11:56:05 + REFERENCE_REFS='+refs/heads/master:refs/remotes/reference/master +refs/heads/v4.x-staging:refs/remotes/reference/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/reference/v6.x-staging' 11:56:05 + REFERENCE_REFS='+refs/heads/master:refs/remotes/reference/master +refs/heads/v4.x-staging:refs/remotes/reference/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/reference/v6.x-staging +refs/heads/v7.x-staging:refs/remotes/reference/v7.x-staging' 11:56:05 + REFERENCE_REFS='+refs/heads/master:refs/remotes/reference/master +refs/heads/v4.x-staging:refs/remotes/reference/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/reference/v6.x-staging +refs/heads/v7.x-staging:refs/remotes/reference/v7.x-staging +refs/heads/v8.x-staging:refs/remotes/reference/v8.x-staging' 11:56:05 + ORIGIN_REFS=+refs/heads/master:refs/remotes/origin/master 11:56:05 + ORIGIN_REFS='+refs/heads/master:refs/remotes/origin/master +refs/heads/v4.x-staging:refs/remotes/origin/v4.x-staging' 11:56:05 + ORIGIN_REFS='+refs/heads/master:refs/remotes/origin/master +refs/heads/v4.x-staging:refs/remotes/origin/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/origin/v6.x-staging' 11:56:05 + ORIGIN_REFS='+refs/heads/master:refs/remotes/origin/master +refs/heads/v4.x-staging:refs/remotes/origin/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/origin/v6.x-staging +refs/heads/v7.x-staging:refs/remotes/origin/v7.x-staging' 11:56:05 + ORIGIN_REFS='+refs/heads/master:refs/remotes/origin/master +refs/heads/v4.x-staging:refs/remotes/origin/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/origin/v6.x-staging +refs/heads/v7.x-staging:refs/remotes/origin/v7.x-staging +refs/heads/v8.x-staging:refs/remotes/origin/v8.x-staging' 11:56:05 + git --version 11:56:06 git version 2.15.0 11:56:06 + git init 11:56:06 Reinitialized existing Git repository in /home/iojs/build/workspace/node-test-binary-arm/.git/ 11:56:06 + git fetch --no-tags file:///home/iojs/.ccache/node.shared.reference +refs/heads/master:refs/remotes/reference/master +refs/heads/v4.x-staging:refs/remotes/reference/v4.x-staging +refs/heads/v6.x-staging:refs/remotes/reference/v6.x-staging +refs/heads/v7.x-staging:refs/remotes/reference/v7.x-staging +refs/heads/v8.x-staging:refs/remotes/reference/v8.x-staging 11:56:09 fatal: Couldn't find remote ref refs/heads/v7.x-staging 11:56:09  11:56:09 real	0m3.441s 11:56:09 user	0m0.050s 11:56:09 sys	0m0.040s 11:56:09 + echo 'Problem fetching the shared reference repo.' 11:56:09 Problem fetching the shared reference repo. 11:56:09 + git fetch --no-tags file:///home/iojs/.ccache/node.shared.reference +refs/heads/jenkins-node-test-commit-arm-fanned-13496-binary-pi1p/cc-armv7:refs/remotes/jenkins_tmp 11:56:09 fatal: The remote end hung up unexpectedly 11:56:11  11:56:11 real	0m1.893s 11:56:11 user	0m0.180s 11:56:11 sys	0m0.400s 11:56:11 + ps -ef 11:56:11 + grep '\[node\] <defunct>' 11:56:11 + awk '{print $2}' 11:56:11 + xargs -rl kill 11:56:11 + rm -f **** 11:56:11 + git checkout -f refs/remotes/jenkins_tmp 11:56:22 HEAD is now at 6c29aa6896... added binaries 11:56:22  11:56:22 real	0m11.204s 11:56:22 user	0m1.480s 11:56:22 sys	0m9.850s 11:56:22 + git reset --hard 11:56:26 HEAD is now at 6c29aa6896 added binaries 11:56:26  11:56:26 real	0m3.466s 11:56:26 user	0m1.460s 11:56:26 sys	0m0.860s 11:56:26 + git clean -fdx 11:56:30 warning: failed to remove out/Release: Directory not empty 11:56:30 Removing config.gypi 11:56:30 Removing icu_config.gypi 11:56:30 Removing node 11:56:30 Removing out/Release/node 11:56:30 Removing out/Release/openssl-cli 11:56:30 Removing test.tap 11:56:30 Removing test/.tmp.0/ 11:56:30 Removing test/abort/testcfg.pyc 11:56:30 Removing test/addons-napi/testcfg.pyc 11:56:30 Removing test/addons/testcfg.pyc 11:56:30 Removing test/async-hooks/testcfg.pyc 11:56:30 Removing test/doctool/testcfg.pyc 11:56:30 Removing test/es-module/testcfg.pyc 11:56:30 Removing test/gc/testcfg.pyc 11:56:30 Removing test/internet/testcfg.pyc 11:56:30 Removing test/known_issues/testcfg.pyc 11:56:30 Removing test/message/testcfg.pyc 11:56:30 Removing test/parallel/testcfg.pyc 11:56:30 Removing test/pseudo-tty/testcfg.pyc 11:56:30 Removing test/pummel/testcfg.pyc 11:56:30 Removing test/sequential/testcfg.pyc 11:56:30 Removing test/testpy/__init__.pyc 11:56:30 Removing test/tick-processor/testcfg.pyc 11:56:30 Removing test/timers/testcfg.pyc 11:56:30 Removing tools/test.pyc 11:56:30 Removing tools/utils.pyc 11:56:31 Build step 'Execute shell' marked build as failure 11:56:31 TAP Reports Processing: START 11:56:31 Looking for TAP results report in workspace using pattern: *.tap 11:56:32 Did not find any matching files. Setting build result to FAILURE. 11:56:32 Checking ^not ok 11:56:32 Jenkins Text Finder: File set '*.tap' is empty 11:56:32 Notifying upstream projects of job completion 11:56:32 Finished: FAILURE ``` 
Trott		Note that the above is the aforementioned `warning: failed to remove out/Release: Directory not empty` but there is are other errors above it that may or may not be significant like:  ```console 11:56:09 + git fetch --no-tags file:///home/iojs/.ccache/node.shared.reference +refs/heads/jenkins-node-test-commit-arm-fanned-13496-binary-pi1p/cc-armv7:refs/remotes/jenkins_tmp 11:56:09 fatal: The remote end hung up unexpectedly 11:56:11  ```
Trott		Certainly some NFS issues showing up now:  ```console 2:11:32 + git clean -fdx 12:11:40 warning: failed to remove out/Release/.nfs00000000000b132d00000005: Device or resource busy ```
Trott		But it seems to be self-healing. https://ci.nodejs.org/job/node-test-binary-arm/12703/ is now 2/3 green and looking promising. All I've done is remove stale `.git/index.lock` files and re-run the one CI job a few times.
gibfahn		FWIW our (IBM) Jenkins farm used to have all the workspaces running on a shared NFS mount, but we moved to having everything local because it kept causing problems like this.
Trott		> FWIW our (IBM) Jenkins farm used to have all the workspaces running on a shared NFS mount, but we moved to having everything local because it kept causing problems like this.  I imagine that's probably not an option with Raspberry Pi devices. :-(  Still, good to know.
rvagg		This is caused by the nfs server having some internal problems, twice now in a few days. I'm a bit embarrassed to admit that it's likely to do with the hot weather we've been having down here (no I don't have a cooled datacenter in my garage unfortunately). I've done some restarting and cleaning up and have a couple of jobs working at the moment that seem to indicate that it's all good now.
rvagg		added your keys to the debian8 machine, it's currently @ 104.130.166.132 
mhdawson		Assume not comments is no objection since this is in line with the agreed policy, will start to setup today.
gibfahn		SGTM as long as it's documented
joaocgreis		SGTM. You'll have to know what type of job to create (Freestyle, Multiconfig, Multijob) when creating them because that can't be changed later without deleting and re-creating.
gibfahn		>You'll have to know what type of job to create (Freestyle, Multiconfig, Multijob)  Presumably for CI the jobs will probably all be multiconfig jobs
mhdawson		Ended up cloning existing node.js core jobs since they fit what was needed.  Leaving issue open until we get the docs in place across CITGM/node-report and post-mortem
geek		LGTM 
rvagg		lgtm sans comments 
jbergstroem		Added TJ. I'll hold off for a second until we've sorted out DESTCPU since it might drop out of the manifest. 
jbergstroem		Ok, we're sticking with DESTCPU for now -- passing it to configure in jenkins. I'll squash and merge shortly -- thanks for reviewing. 
ChALkeR		Note: I opened this issue primarily for the very top-level preliminary discussion now, to decide what to do with it (and whether we should do anything).  If this will seem interesting, I could look up what exact steps would we need to take to setup a repo. I don't expect that to be hard, though, as I suppose that we could use existing binaries for that, just with additional metadata and repackaged.  /cc @nodejs/ctc 
mscdex		This is the first time I've ever heard of 'flatpak'.
ChALkeR		@mscdex I might indeed be overreacting, in this case we should just freeze this (aka do nothing) until some point in the future and see what comes out of it.  *Upd:* I added some refs to the initial post.
gibfahn		I remember a big buzz about FlatPak (Redhat/Fedora backed) and [Snappy](http://snapcraft.io/) (Ubuntu backed) about six months ago, but I haven't heard much since.  If we're going to look at supporting one of these is there a reason to choose FlatPak over Snaps (or an existing package format like AppImage)? I have no strong opinion about which is better, but I don't think we want to be doing all of them. I was hoping one of these would win out in terms of adoption, and everyone could just standardise on that.
probonopd		[AppImage](http://appimage.org) has the advantage that if you put together its ingredients carefully (e.g., compile on a reasonably old build system), then the resulting AppImage will run on most desktop distributions out-of-the box, without the need for runtimes or anything else apart from what comes with the distributions by default. There is a [growing number](https://github.com/probonopd/AppImageKit/wiki/AppImages) of projects using AppImage as their native upstream-provided download format for Linux.  Edit: In other words, because it __does not require runtimes__, an AppImage can always run _standalone_ on most out-of-the-box desktop Linux distributions.
ChALkeR		@gibfahn @probonopd This issue is not about packaging Node.js as an Flatpak/Snappy/AppImage/whatever application ‚Äî that is not very important imo.  This is about providing Node.js as a flatpak _runtime_, so that other flatpak packages might depend on Node.js without bundling it ‚Äî so security updates will reach users faster, the archives would be smaller, etc.  Afaik, Snappy doesn't have the concept of runtimes, so that is out of scope. I just took a look at AppImage ‚Äî and it looks like it also doesn't have the concept of runtimes.  _Please, don't discuss packaging Node.js as an Flatpak/Snappy/AppImage/whatever application in this issue ‚Äî open a separate one for that, if needed._
gibfahn		Oh I see, my bad for misreading the issue. So this is something more along the lines of a shared library (I guess technically it's probably a package of shared libraries and binaries) that multiple applications can require?  Looking at the other options, it sounds like Snappy is going for file level deduplication rather than having a "runtime" equivalent so we wouldn't have to support multiple competing formats (yet).  So can flatpak runtimes depend on other runtimes, or do you just have a single level hierarchy? It looks like you only get OS->Runtime->App (not OS->Runtime->Runtime->App), so if Electron wanted their own runtime they wouldn't be able to build off a Node.js one.  It sounds like something we should investigate, there's no rush to support or commit to actually maintaining something if flatpak doesn't take off.   ---  Looking at the provided [Flatpak runtimes](http://flatpak.org/runtimes.html), and the [examples in the wiki](https://github.com/flatpak/flatpak/wiki/Examples) (which includes Slack and Electron apps) it looks like the runtimes that exist so far are really low-level things (like Qt or Gnome), and you can only have one. It'd be good to find out whether a Node.js runtime is the right thing for us to be doing.
tieguy		It is an [officially disfavored analogy](http://flatpak.org/faq.html#Is_Flatpak_a_container_technology_), but it might be helpful to think of providing a flatpak runtime as similar to providing a Docker/OCI image - a layer that app developers can build on. (I don't know if you can stack layers like you can with Docker, though.)  (I stumbled across this issue because I was poking around at what it would take to package [Wordpress Calypso](https://github.com/Automattic/wp-calypso) in Flatpak, and a nodejs runtime would be helpful for that.)  (Disclaimer: I'm not an expert by any stretch, just a Linux desktop user who is frustrated that I'm unzipping tarballs to get Calypso in 2017, and figured that might be an interesting way to dip my toe in the flatpak waters.)
ChALkeR		Basic information on flatpak runtimes: http://flatpak.org/runtimes.html  More info on creating a runtime: > [18:10] ebassi > So, the GNOME runtime is this one: https://git.gnome.org//browse/gnome-sdk-images/ > [18:10] ebassi > It's just a flatpak-builder manifest > [18:11] ebassi > https://git.gnome.org/browse/gnome-sdk-images/tree/org.gnome.Sdk.json.in built using: https://git.gnome.org/browse/gnome-sdk-images/tree/Makefile > [18:11] ebassi > Then you follow the same steps for hosting it that you'd follow for applications: https://blogs.gnome.org/alexl/2017/02/10/maintaining-a-flatpak-repository/  I will try to take a closer look at that, but I'm not sure when would that happen =).
egee-irl		Its worth pointing out that Snapcraft (the building engine behind Snaps) already has a [plugin](https://snapcraft.io/docs/reference/plugins/nodejs) for NodeJS. There's even one for [Gulp](https://snapcraft.io/docs/reference/plugins/gulp).  There's even a [tutorial](https://tutorials.ubuntu.com/tutorial/build-a-nodejs-service?utm_source=snapcraft.io&utm_medium=buildsnapsindex&utm_campaign=tutorials#0) on the Snapcraft website for setting up a NodeJS service as a Snap.  Not sure if it's worth building a NodeJS runtime for Flatpak.
ChALkeR		@egee-irl That is completely unrelated to the issue here. As already said above, snap (afaik) does not have the concept of runtimes, and the minor things like plugins that simplify builds are unimprotant as they (and should) be managed outside of Node.js (i.e., in/by snapcraft itself).  On the contrary, flatpak runtime is (ideally) supposed to be managed by the software/platform authors themselves, i.e. by Node.js. This is why I opened this issue.  Once again, this is not a thread for flatpak-vs-snap-vs-appimage-vs-whatever, this issue is solely about providing a flatpak runtime.
ChALkeR		Also relevant and deserves a separate mention: http://blog.manuq.com.ar/posts/building-electron-apps-offline-for-flathub/
maclover7		ping -- does this need to remain open?
ChALkeR		@maclover7 I believe so, yes. Just didn't have time to look at this closer and actually try doing things on this.
Yhozen		Maybe the GNOME runtime  is just enough
jbergstroem		There's a similar discussion in #448. 
gibfahn		@jbergstroem True, although I was thinking more about the details of Jenkins job configurations than the higher-level access stuff. 
mhdawson		I'd agree with @gibfahn .  I think #448 would be fore projects not under github/nodejs while this issue to is to address those that are.   I agree not having to build node would be nice.  The nightlies and latest supported versions sound like the right answer to me.  I don't think omitting coverage across platforms is a good idea, at least for all cases.  In cases like nodereport and llnode there are definitely components that are os specific so a change could break one but not another.   We already have all of the platforms in the nightlies and I'd be thinking we'd limit ourselves to the platforms that we produce binaries for on the other releases.  This would be all for 6.x and a subset on 4.x and 0.12.x. 
mhdawson		One other consideration is whether we'd want to run these tests as part of the release validation process.  In that case the nightly and last supported release might not be what we want.  In that case the option to build node would be useful.  Having said that it might be that we normally want to use the nightly/latest but make the job flexible enough so we can build a specific version of node when that is required.  
gibfahn		@mhdawson Sorry, when I said we wouldn't need 1 job per Arch, I meant you could put all the Archs into 1 multi-config job. Not that we wouldn't test all Archs.  For the release validation process we'd be talking about testing node, in which case (assuming everything can be added to CitGM) a CitGM run would be sufficient. 
Fishrock123		I think additional projects probably need to not be running jobs very frequently or else we'll end up needing a bunch more hardware? If getting hardware is a problem (seems to be for e.g. OS X)...
Trott		Should this be closed at this point? 
jbergstroem		Great to finally see our `README.md` getting some treatment :clap:  -- comments inline. Also, how about we ask the members of the suggested working group if they (still) want to be part of it? 
Starefossen		Looks very good, Rod! Nice work :thumbsup:  
orangemocha		Thanks for doing this, @rvagg !   One nit about Microsoft sponsorship: the sponsor is actually "Microsoft". "Microsoft Azure" is the platform where some (ok, all except for the serial port machine) of those hardware resources are hosted.  LGTM otherwise. 
rvagg		@orangemocha can you check with someone at Microsoft re what they'd like represented? I've got the impression from folks there that pushing the Azure brand is pretty important at this stage. Happy to comply with whatever you/they want to go with. 
orangemocha		Ok, @rvagg , I am double checking. Thanks! 
orangemocha		One more nit: all verbs where the subject is a company name appear conjugated in the 3rd plural person "provide" "donate". Shouldn't it be "provides" "donates"? (disclaimer: I am not a native English speaker) 
mhdawson		Could you change   **[IBM](https://www.ibm.com/)**, via their cloud company, [SoftLayer](https://www.softlayer.com/) and the [Oregon State University Open Source Lab](https://osuosl.org/services/powerdev) provide PPC-based test and build infrastructure and other key hardware for testing and benchmarking for the Node.js project's CI system.  to:   **[IBM](https://www.ibm.com/)**, via their cloud company, [SoftLayer](https://www.softlayer.com/) and the [Oregon State University Open Source Lab](https://osuosl.org/services/powerdev) provide PPC-based test and build infrastructure and other key x86 hardware for testing and benchmarking for the Node.js project's CI system.  I don't want it to sound like we only provide PPC resources.   
mhdawson		Just read @jbergstroem's  earlier comment. Splitting it up would also work  
Trott		> One more nit: all verbs where the subject is a company name appear conjugated in the 3rd plural person "provide" "donate". Shouldn't it be "provides" "donates"? (disclaimer: I am not a native English speaker)  In US English, it would be "IBM provides..." etc. In UK English, company names (and band names and all sorts of things that represent collections of more than one person) are _collective nouns_ and treated as a plural noun. "IBM provide..."  So either could be correct.  It seems that Node.js has standardized on US English. We seem to _favor_ US English rather than _favour_ UK English. So I would be mildly inclined to go with @orangemocha's suggestion. But again, either could be correct. 
rvagg		I think I've addressed all of the comments raised here so far, please review latest commit 
rvagg		Added a new section on community donations, listing all of the donations and donaters that we've had so far, including hardware that is not in active service. 
jbergstroem		Looking great, Rod. LGTM :shipit:  
rmg		LGTM 
joshgav		Representing Microsoft's view, I agree with @orangemocha that our company name should be Microsoft, not Microsoft Azure. Here's what we propose for the Microsoft line:  > **[Microsoft](https://www.microsoft.com/)** provides Windows-related test infrastructure on [Azure](https://azure.microsoft.com) for the Node.js CI system.  Could we take the logo from our [logo page](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/logo.aspx) (as below) for the image?   ![Microsoft Logo](https://c.s-microsoft.com/en-us/CMSImages/imgOne.jpg?version=2089bd70-681c-1e6e-0b1a-680002dd31d6)  ---  Somewhat relatedly, do we have a quantifiable differentiation between "1st tier" and "2nd tier"? It's something that might cause controversy so would be nice to be crisp. Also, perhaps "gold" and "silver" or any sequence that doesn't number sponsors might be more politic :) 
mikeal		Some companies are particular about how/where/when their logo is used. If we get a complaint please forward it to me right away and I'll figure out what the foundation need to do in order to get approval. 
mhdawson		lgtm 
mhdawson		One thing we discussed in the last WG meeting was to define a concrete definition for what contributes makes Tier 1 (# cpus, memory etc.) 
rvagg		A concrete definition is going to be hard for two main reasons: 1. Some providers don't want us to publicise how much we are using their stuff, for various reasons 2. Making a comparison between resources of the various providers is nearly impossible due to huge variations in what resources are available and used.  The definition for now is based on a rough calculation of **$$** spent if we were paying for these, and by far DigitalOcean and Rackspace lead everyone else in how much they are going out of their way to support the project. So IMO, putting them in a special category at the top is both appropriate and deserved. That said, we have contenders that have the potential to reach up in to Tier 1 for the same reason. SoftLayer, Joyent and Azure could be ramped up to reach similar levels.  I haven't done the spend-analysis for a few months, the Foundation Board asked for it a while back to understand the risks associated with having in-kind offerings. My suggestion would be for the core Build group to privately maintain this analysis, updated with latest usage across providers and when we have the numbers is should be very clear who is in Tier 1, _or_ whether the blurring between the tiers is enough to justify adding another or removing the designations all together.  Objections from Tier 2 providers about having the tiered designation suggests to me that it works as an incentive. I'm more than happy to have providers compete to be in the top tier and be thanked more loudly than others, this whole PR is largely about incentive construction. 
rvagg		I've just uploaded the spreadsheet I made back in August into Google Docs and shared it with @jbergstroem, @joaocgreis, @orangemocha and @mhdawson. It's very out of date and needs a lot of work to become accurate again. Let's try and keep this maintained (but the numbers private to respect the wishes of the various companies please). 
rvagg		If only companies offered downloadable versions of their logos while telling people _how_ to use their logos ... @joshgav can you point me to a higher res version of the Microsoft logos that I can use? https://c.s-microsoft.com/en-us/CMSImages/imgOne.jpg?version=2089bd70-681c-1e6e-0b1a-680002dd31d6 is not terrible but surely there's a vector or high-res version of individual files instead of something I have to cut up manually? 
rmg		http://news.microsoft.com/microsoft-news-center-photos/msft_logo_rgb_c-gray/ should do the trick.. 
rvagg		thanks @rmg, although that brand use page says to use it on white or blue, this is grey! 
rvagg		oh, nmind, that grey must be the transparency colour 
rvagg		Updated the Microsoft logo and text, latest can be seen @ https://github.com/nodejs/build/blob/234e3dadd28cafffef395e2dab37d776d18ef67e/README.md  How firm are the objections to the tiers? Enough to hold this up or can we go ahead with this and fix it up later? 
jbergstroem		Regarding tiers: whatever way we currently define it, digitalocean and rackspace are most used. I'm happy to merge without a clear definition of tiers and improve as we go. 
orangemocha		> I've just uploaded the spreadsheet I made back in August into Google Docs  Thanks @rvagg , I will update the spreadsheet with the Azure spend.  It's not a problem to merge as is and refine things later. Once we do, I think we might want to set a time period over which to compute a sponsor's contribution (in $). 1 month is probably too short as these numbers can fluctuate. If we were to consider contributions from all time, and even before the Foundation, Joyent and Microsoft would definitely have to be in the top tier. I would suggest to use a "last 12 month" period and define the tiers in terms of $ thresholds. 
mhdawson		I'm fine with merging as is.  
joshgav		LGTM. Thanks! 
rvagg		> If we were to consider contributions from all time, and even before the Foundation, Joyent and Microsoft would definitely have to be in the top tier.  Maybe, you have to consider that the resources we are using across the board are on a totally different scale than were ever maintained for jenkins.nodejs.org. I'd not be surprised if total spend in the last 12 months completely eclipsed any previous total spend on infra by the project.  Personally I'd rather time-limit it or even make it a snapshot thing cause calculating spend over spans of time is way more difficult and would require more dillegence than any of us can afford. So perhaps it's a quarterly, or 1/2 yearly thing we do.  Let's aim to have the spreadsheet updated by the next Build WG meeting we have and we'll come up with a strategy that properly recognises the major contributors and puts incentives in place that encourage new contributing providers and existing providers to level-up how they contribute. 
jbergstroem		Yeeeees :tada:  
mhdawson		Kicked off 2 runs to make sure it still recreates against head. 
mhdawson		Seems to have gotten further on test-osuosl-aix61-ppc64_be-1  
mhdawson		Got further on  test-osuosl-aix61-ppc64_be-2 as well. 
mhdawson		Runs on both machines seems to be ok 
mhdawson		Launched build to match the one that was failing.  It also failed.  They key difference seems to be that the GIT_ORIGIN_SCHEME as git versus https.  That does jive with the failure reported. 
mhdawson		Local checkout with git seemed out.  Looking at config for jenkins agent it looks like this:  export LIBPATH=/opt/freeware/lib:$LIBPATH;  is what causes the problem. 
mhdawson		Ok as a summary the issue was due to dependencies between openssl/openssh/git.    1) We had installed git from bull freeware and it required export LIBPATH=/opt/freeware/lib to find the version of the libcrypto.a library that it needed.  2) ssh which is only used when we build from the private repo, however, however, could not run with the version of libcrypto.a in /opt/freeware/lib.    Jenkins however uses both 1) and 2) when building from the private repo so there was no good way to make them both happy (despite my looking).  The resolution was to drop the bull freeware of git, install a version directly from IBM and to also update a number of dependencies to the latest so that all the pieces work together.    I've Updated the PR which covers the AIX setup setups to reflect these changes.  I've also update both test machines and the release machine and seen passes of both types (public and private repo) on the test machines as well as the release being succesfully built so I'm going to close this issue.  
kenperkins		This is getting close, but I'm unable to find `promote_nightly.sh` in source control. @rvagg are you able to assist on this? should I just take it from the current production webserver? 
kenperkins		Additional open questions: - What is `/root/mkdldtxt.sh`? - What is `/root/countdldtxt.sh` - Who is responsible to create `/home/dist/public` and `/home/dist/public-test` directories? - What is `/home/dist/dist-indexer/ls-types.js`? - What is `/home/dist/dist-indexer/transform-filename.js`? - What are all of the files in `/home/staging/promote`? - Who creates `/home/staging/staging` directory? 
kenperkins		Reminder @rvagg that I'm blocked on finishing this without some input. 
rvagg		Really sorry @kenperkins, I'm a terrible bottle-neck for these things, way too overloaded. Trying to get on top of some of these things now.  ### What is /root/mkdldtxt.sh?  An initial version of what's in `/home/iojs/update-download-stats.sh`, it's redundant now. It only ever was meant to be a temporary "let's see what these numbers look like" thing but that script is now run via cron and exposed via https://iojs.org/download-stats.json because ppl asked for it.  ### What is /root/countdldtxt.sh  Part of the above, redundant now too. There were two scripts to do the job that one is now doing.  ## Who is responsible to create /home/dist/public and /home/dist/public-test directories?  The promote script(s) are responsible for `/home/dist/public/`. There is a `mkdir -p` in there but the root directories should be created by the script I think since they are served by nginx. Currently there needs to be `next-nightly`, `nightly`, `release` (aliased ad https://iojs.org/dist/) and there is also a `test` directory but that's not controlled by any tool, it's been a place to share some test builds, manually, so unimportant.  ### What is /home/dist/dist-indexer/ls-types.js?  ### What is /home/dist/dist-indexer/transform-filename.js?  Parts of dist-indexer https://github.com/iojs/build/tree/master/tools/dist/dist-indexer, _but_ also: 1. There seems to be an incomplete refactor going on there and I'm wondering if a commit or two got lost somewhere, transform-filename.js should be used by dist-indexer.js as well as ls-types.js but it's only used by the latter with some duplicated code in dist-indexer.js. 2. ls-types.js is used by the scripts in `/usr/local/bin`, you'll see there are 3 `dist-*` scripts there now and they are used for the release process so that authorised release people can run (tools/release.sh](https://github.com/iojs/io.js/blob/v1.x/tools/release.sh) to promote and sign a release without needing to log in to the server. Those files need to become part of a scripted setup of the server too.  ### What are all of the files in /home/staging/promote?  I've put them in a PR including a README, see https://github.com/iojs/build/pull/76 and note that they are also invoked by scripts in `/usr/local/bin` which also need to be part of this setup  ### Who creates /home/staging/staging directory?  It should be created by server setup. Each of the release build slaves has a jenkins config which makes it `ssh` in to the server as staging and `mkdir -p` and then `scp` in the build artifacts, but as with `/home/dist/` we should probably set the basics of it up including the `next-nightly`, `nightly` and `release` subdirectories. 
kenperkins		Wow, it's like Christmas morning! Thanks @rvagg I'll try to resurrect this as soon as I can find some engineering time. 
jbergstroem		This unfortunately looks stale. Going to close it but feel free to reopen with a rebased version seeing how a lot of the changes has gone in one way or the other. 
cjihrig		Also cc: @nodejs/build 
joaocgreis		I checked, ci-release was having trouble with the job queue. It should be fixed, there should be nightlies again tonight.
refack		Confirmed for 2017-08-24 - https://nodejs.org/download/nightly/v9.0.0-nightly20170824abced13e29/ (2017-08-25 seems to still be ongoing, once it finishes I'll close the issue).  Thank you @joaocgreis 
refack		~We're missing the ARM64 builds, but I assume that is related to the miniNodes machines being down.~ So this issue has been resolved (also audited [v9.0.0-nightly201708274d893e093a](https://nodejs.org/download/nightly/v9.0.0-nightly201708274d893e093a/) ‚úîÔ∏è).
rvagg		FYI ARM64 comes from a release-packetnet-centos7 machine, have not had any major problems with it before
gibfahn		Please cc me as well, I'd like to help out with this.
mhdawson		@gibfahn sent terms to you.
mhdawson		Terms were acknowledged a while back.  Initial wip PR for adding configuration through ansible. https://github.com/nodejs/build/pull/796
mhdawson		Machines have been added a while back, closing.
joaocgreis		Both the MSI and the EXE of 5.1 are signed by Node.js Foundation. Did we miss something? 
rvagg		I'm pretty sure @joaocgreis reprovisioned the Windows release machines and they only have the Node.js Foundation certs on them now, which installers are you looking at @mikeal? 
jbergstroem		Just following up here. Is there anything else we need to do? 
rvagg		nada 
jbergstroem		We're using (almost) the same infra to build both, so the stack is there. I guess the question is rather why we possibly shouldn't do it. @rvagg? 
mscdex		Having an xz tarball for the source code would be good too. 
rvagg		The initial reason for not doing it was to get as close to parity with previous 0.10 and 0.12 builds as possible. Beyond that, there's the awkwardness of providing them 1/2 way through a major line, any tool that uses `xz` would either need to hard-wire a check for the exact version when we turned it on or check for existence. Perhaps that's not a problem. I don't actually have a very strong opinion on this atm.  So far during December, 9.985% of `.tar.?z` downloads of v4 and v5 have been for `.tar.xz`. That's not huge but it does suggest growing demand.  @ljharb is nvm using the .xz downloads? 
ljharb		Yes, @jbergstroem added support for them in https://github.com/creationix/nvm/pull/823. It hasn't been in use in travis-ci for long.  I suspect usage will grow very quickly if https://github.com/creationix/nvm/pull/823/files#diff-e1e8e036966ad5aaf7709712cd5d8ee3R2235 can be expanded to include more versions, and that change hits travis. 
jbergstroem		I reckon everyone should use `tar.xz` if possible (most packagers seems to) but I can see how most people will make the assumption that `tar.xz` is available throughout all versions if we started doing it halfway through a major. 
jbergstroem		Fixed! 
jbergstroem		Investigating. Looks like the test runner bails:  ``` /home/iojs/bin/python tools/test.py -p tap --logfile test.tap --mode=release message parallel sequential make: *** [test-ci] Error 1 Build step 'Conditional steps (multiple)' marked build as failure ```  The annoying part is that the tap collector seems to pick up an empty tap file and seems happy to. It should not be run at all at that point. 
jbergstroem		`-p tap` just doesn't seem to produce output (but tests are being run successfully). I'll see what I can do. 
rvagg		eek, do we need to do something like `test.py ... || echo "not ok >> test.tap` to make sure the failure bubbles up? 
jbergstroem		@rvagg We can move the tap file creation further down or just remove the file if `test.py` exits abnormally . First I just want to figure out why `logging` behaves differently on the python versions we're running on centos. I'll have a look in an hour. 
jbergstroem		@rvagg actually - as long as we make sure output is written to the tap-file, it should stay regardless so we know what's going on. 
jbergstroem		Fixed with nodejs/io.js@8606793999979829df002c2ab69235a05605fc4d. 
mhdawson		Have not seen these for a while so closing
benedictify		~ :> nvm ls  ->       system node -> stable (-> N/A) (default) iojs -> N/A (default) lts/* -> lts/boron (-> N/A) lts/argon -> v4.8.4 (-> N/A) lts/boron -> v6.11.3 (-> N/A)   and  ~ :> nvm --version 0.33.4
ljharb		I think this issue belongs on https://github.com/creationix/nvm instead. Please file it there, and fill out the issue template, and I'll be happy to help you later tonight :-)
benedictify		Oops, I meant to put it there! Thanks
ljharb		‚Üí https://github.com/creationix/nvm/issues/1615
jbergstroem		I can join tomorrow but won't be able to handle recording because of a crappy internet connection.  Most of my "work" has been about iojs build pr's/issues and jenkins stuff. I'm also preparing a pr of refactoring our ansible setup 
orangemocha		I don't have much to report, and given today's hectic schedule, I will not attend. @joaocgreis is working on parallelizing the test runs and I am working on the final tweaks to the test jobs (especially auto-rebase) before we can roll them out. 
jbergstroem		I'm suggesting we push forward one week then. 
mhdawson		Ok talk to you all next week 
jbergstroem		Cool. Please update the readme for automating the install if need-be as well. Finally, if you could install the latest available python 2.7.xx that includes pip it would be awesome. 
mhdawson		OS updates we should handle through a different issue as I've had many dependency issues so installing the latest may not be as straight forward as would be nice.  We would need your help to do the resize on the release machine since I can't log in to take it of lf line.  You going to be around the next 30 mins ?  
jbergstroem		Since we will be requiring pip to install tap2junit at some stage (junit xml output from test runners) I hope the install won't have any issues. According to http://www.aixtools.net/index.php/python, recent releases should have `pip` installed? 
jbergstroem		I will be available on IRC in about 30 minutes from now. 
mhdawson		In terms of what we are doing now, it is resizing the physical resources assigned to the machine so outside of the OS and does not affect the install instructions 
jbergstroem		Oh, gotcha. 
mhdawson		In terms of dependencies its more that if there are any shared dependencies it can have a ripple effect, last time I had a lot of work (many hours) to get to a state that worked with a new package.  Hopefully it will be smoother this time.    I'll open a new issue to install a python with pip.  I assume it should still be in the 2.7 line ?  
jbergstroem		Latest python 2.x! 
mhdawson		test machines resized, back on-online  and builds when through ok on test machines.  release machine resized, back on-line and custom test release built ok.  
mhdawson		@nodejs/build please chime in if you can make it.
Trott		I'll be missing this one.
jbergstroem		It'll be clutch for me -- if it's on I will be a few mins late.
mhdawson		Give the response lets make it next week instead.  @nodejs/build please chime in if you can make Feb 7 at 3 EST. 
Trott		The Feb 7 time is also bad for me (at least as of now) but hopefully it works for others....
mhdawson		Ok so lets do it today, I'll set up the hangout 
mhdawson		@nodejs/build updated with hangouts info for meeting today.
piccoloaiutante		@mhdawson can you add me to the hangout?
mhdawson		Link to new hangout https://hangouts.google.com/hangouts/_/ytl/oefFhZpJxiAGtWAMfxo8i_v2SgNTVM7T6cwLkWeAX1M=?eid=100598160817214911030&hl=en_US&authuser=0  @piccoloaiutante you should be able to join the meeting is open
mhdawson		Link for those that just want to watch http://youtu.be/iQzSe3aaL0U
mhdawson		PR for minutes https://github.com/nodejs/build/pull/620
gibfahn		Minutes landed.
mhdawson		All of our machines seem to be on-line so closing.
MylesBorins		could we make a generic skip script that writes a tap file and exit 0?  On Fri, Dec 16, 2016, 2:08 PM Johan Bergstr√∂m <notifications@github.com> wrote:  > Refs: nodejs/node#9618 <https://github.com/nodejs/node/pull/9618> > > Also, there doesn't seem to be a great way of doing this in jenkins since > a "return 0" would later fail on tests not being run, followed by > conditionals assuming tap files exist. > > ‚Äî > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > <https://github.com/nodejs/build/issues/570>, or mute the thread > <https://github.com/notifications/unsubscribe-auth/AAecV3TXQiYpqEYr3e0R7pMnnJTzAeOyks5rIuGwgaJpZM4LPgFW> > . > 
jbergstroem		It'd be nice if we could skip out of the build entirely, not just the step. If we were using pipelines (hi there, @rvagg) we could probably just bail out since each step in a pipeline usually triggers next.
maclover7		ping -- what is the status here?
maclover7		Closing due to inactivity, please reopen if needed
mhdawson		In further discussion he also mentioned macstadium, but I think that I remember @rvagg had already been talking to them.  
mhdawson		@rvagg ping ?  
mhdawson		@nodejs/build any objections ?  Otherwise I'll approach them to see if they might be an option.
gibfahn		SGTM. Probably worth reaching out to macstadium as well, just in case anything's changed since last time.
piccoloaiutante		SGTM too. I would go ahead to see if there is anyone that could support us..
jbergstroem		We have an open connection with macstadium; it just depends on us figuring out the way we can better show the support that is provided to us. @rvagg wanna chip in here?
mhdawson		Ok, I'll look to contact MacInCloud next week to see where that goes.
refack		Has anybody tried to contact Apple to see if they can help/allow us to run macOS as VM/Container? Since I have no corporate affiliation, I'll be happy to try.
mhdawson		Sent a follow up to MacInCloud  @refack I believe that @rvagg and @jbergstroem had both tried with Apple on several occasions without success.
refack		> @refack I believe that @rvagg and @jbergstroem had both tried with Apple on several occasions without success.  Good. Shame of Apple üòû 
maclover7		ping @mhdawson -- is still needed?
mhdawson		MacCloud was open to donating but we've not gotten through setting up the other OSX donations we received. I'd still like multiple providers though so probably makes sense to keep open],
rvagg		Been discussing this in #node-build fwiw  I pulled out the SSD that the Pi's use via NFS and this seems to have had a big hit on their perf, way more than I expected. It's possible that it'll fix itself out if left to their own devices but probably not.  I've been rearranging the network over here, replacing some key components with some (exciting and fun) new networking gear (my own gear, not specifically for the ARM cluster but should improve things for the cluster a great deal), and the SSD is going back but in a new host so this was supposed to be a single step in a bunch of steps to reconfigure. I might just pull arm-fanned out of test-pull-request for now until this is done, it might be another 24 hours till it's complete but I'll keep folks updated here.
rvagg		External access to the ARM cluster is disabled for the moment, but the machines on that network are all back online, Pi's are not activated in test-commit though still. I have a new gateway and parent switch in place that's letting me do some fancier config, and it should mean faster throughput internally too.
refack		I wanted to ask if it's worth taking `node-test-commit-arm-fanned` out of `nose-test-commit` but I see that's already been done üëç   jinx  
rvagg		Okie dokie! We have progress. I've moved to a new gateway/router and parent switch, along with a dedicated NAS that now holds the SSD that we were using for the Pi's. And, it's all much faster and more profesh all round. We're up to NFSv4, I don't know if that makes much difference but there is chatter that suggests that it _should_ be faster.  I've put node-test-commit-arm-fanned back in to node-test-commit so it's back in play.  I've also disconnected the switch that the Pi 3's were all attached to, it was the most ancient piece of hardware in the stack (an old Dell thing that I've had in my pile for _many_ years) and updated and reconnected all of them. There are enough ports in the other two switches, so they are now sharing the same switch as the Pi 2's. They are now back in node-test-commit-arm-fanned and appear to be working exactly as planned without the git / slowdown problems we were seeing before.  SSH access back in to the cluster (including the macOS machine) is active again, folks with access to nodejs_build_test should be able to get back in again if you have the config setup right for the jump host and all of the internal hostnames (one change being that test-requireio_arm-ubuntu1404-arm64_xgene-1 is now on 192.168.2.4 rather than 192.168.2.1, would someone mind updating that in the ansible config if it's there?).  It's not quite as simple as turning it all back on though (of course!), we have a couple of things to clean up, both of which can be seen in console output of test runs (e.g. https://ci.nodejs.org/job/node-test-binary-arm/8907/RUN_SUBSET=5,label=pi3-raspbian-jessie/console):   * `parallel/test-fs-readdir-ucs2` is failing on all of the Pi's: `Error: EINVAL: invalid argument, open '/home/iojs/build/workspace/node-test-binary-arm/test/tmp.0/=ÔøΩÔøΩ'`, this is likely to be to do with it being on an NFS share exported from a ZFS volume. I'm going to have to defer to @nodejs/testing to figure out how to handle that special case. Let me know if there's something I should be doing for that mount to enable UCS2 (no promises that it's achievable on my end tho).  * Because of the failure, the error message is getting into the tap output file and it's not parsable. `org.tap4j.parser.ParserException: Error parsing TAP Stream: Error parsing YAML` coming from `Caused by: unacceptable character '' (0x4) special characters are not allowed`. This will obviously go away when we fix up test-fs-readdir-ucs2 but suggests that we might need to do something to protect against this in future.  In addition, these machines are out of action, I'm going to turn them off completely until I get back from vacation, I know at least one of them has a corrupt SD card that I need to replace: test-requireio_bengl-debian7-arm_pi1p-1, test-requireio_joeyvandijk-debian7-arm_pi2-1, test-requireio_ceejbot-debian7-arm_pi2-1 (bad card), test-requireio_kahwee-debian8-arm_pi3-1.
refack		> Because of the failure, the error message is getting into the tap output file and it's not parsable. org.tap4j.parser.ParserException: Error parsing TAP Stream: Error parsing YAML coming from Caused by: unacceptable character 'ÔøΩ' (0x4) special characters are not allowed. This will obviously go away when we fix up test-fs-readdir-ucs2 but suggests that we might need to do something to protect against this in future.  +1 Do you think a fix in core's `tools/test.py` is needed? Something about encoding like: https://github.com/nodejs/node/issues/12786 or https://github.com/nodejs/node-gyp/pull/1203?
rmg		If I recall correctly, the reason was that they need a tty. We may need to take that into account when creating the init scripts.  
rvagg		yes, the tty was the blocker, _but_ this was pre-1.0.0 and things may have changed since then and even if they haven't perhaps we should make them change--failures were to do with width & height of the tty being `0` (or close to), but they are already like that for the dockerised tests and they don't fail on that any more 
jbergstroem		Which tests were failing without a TTY? 
rvagg		I think this may have actually been more of a libuv problem than an io.js problem, testing the width and height of the tty was one problem I recall 
jbergstroem		Should we perhaps start adding init scripts then?  
rvagg		we should do that .. I really need a solution for the raspberry pi machines at a minimum because they are so flaky and I'm regularly having to restart the slaves on them manually.  probably need the full gamut of init, upstart and systemd to cover what we're running 
jbergstroem		There's enough examples of jenkins launched from init, upstart and systemd for us to get something working. I don't think I have access to any of the stock linux boxes but can play around on local vm's and do PR's. 
rvagg		FYI I've just put `monit` on all the raspberry pi servers because I'm sick of restarting `java` on them, not worth putting in the ansible setup yet because it'd be nice to have init scripts take care of most of it but this is pretty much a replacement because it'll also work if they are rebooted.  /etc/monit/conf.d/jenkins:  ``` check process jenkins   matching slave.jar   start program = "/bin/su iojs -c 'cd /home/iojs && /home/iojs/start.sh'"   stop program = "/bin/su iojs -c 'pkill jenkins'" ``` 
jbergstroem		We can just replace start stop with the init equivalent when due. 
rvagg		I just hacked together an upstart script for the armv8 slave that's running ubuntu 14.04 because the slave was dead and instead of just restating it I figured I should do it properly.  This machine isn't ansibilised (yet).  ``` start on runlevel [2345] stop on runlevel [!2345]  respawn respawn limit 4 240  setuid iojs  env USER=iojs env SHELL=/bin/bash env HOME=/home/iojs env PATH=/usr/lib/ccache:/usr/bin:/bin env NODE_COMMON_PIPE=/home/iojs/test.pipe env OSTYPE=linux-gnu env ARCH=arm64 env DESTCPU=arm64  exec /usr/bin/java \           -jar /home/iojs/slave.jar \           -jnlpUrl https://jenkins-iojs.nodesource.com/computer/iojs-linaro-armv8-ubuntu1404/slave-agent.jnlp \           -secret secretz ``` 
jbergstroem		We still lack support in some ubuntu flavours, but I still think we should close this and (if needed) open something more specific. 
rvagg		great, tho I'm wondering if we shouldn't `@` that or something to prevent the "ERROR" text from showing up even when it's working fine, a quick scan or a ctrl-f will show up that text and may be easy to misread. 
joaocgreis		Added `ECHO OFF/ON` instead to make it clear that something is happening behind the scenes. - master: https://ci.nodejs.org/job/reis-extrafiles/11/nodes=win-vs2015/console - https://github.com/nodejs/node/pull/5369 : https://ci.nodejs.org/job/reis-extrafiles/12/nodes=win-vs2015/console 
joaocgreis		Added to job. 
joaocgreis		Tests in `node-stress-single-test` are run directly with node, without using `test.py`. Hence timeouts are not enforced at all. I just changed it to print the approximate duration of the test, that was easy to do.  To do this properly, however, would be to change `test.py` to run some test a number of times, optionally running all other tests as it does now or skipping them. I'll keep this in my work list, but don't have the time right now. Perhaps this is a good first task for the testing WG? Ping me if you need more changes in the jenkins job, that I can do fairly quickly. 
jbergstroem		fwiw, I saw a pretty long running test on the rpi's (3500 iterations done after ~6 hours) the other day. 
rvagg		I killed one that was about that many iterations yesterday cause it was the only thing holding up a jenkins restart 
mhdawson		Happened again today
mhdawson		And again today.  @rvagg which machine is it that the website runs on and does the rsync from the benchmarking machine.  Next time this happens I'd like to look on that machine to see if that gives me any more information as to what is going on.
mhdawson		It seems to be related to something that is running at around 6:25  From one of the syslog files, and saw the same pattern in another one as well.  ``` Oct  2 05:17:06 nodejs-data2 CRON[367]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly) Oct  2 06:17:07 nodejs-data2 CRON[2380]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly) Oct  2 06:25:03 nodejs-data2 CRON[2606]: (root) CMD (test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily )) Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357360] rsync invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0 Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357367] rsync cpuset=/ mems_allowed=0 Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357372] CPU: 0 PID: 29303 Comm: rsync Not tainted 3.13.0-65-generic #105-Ubuntu Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357375]  0000000000000000 ffff880008a07a98 ffffffff81723f80 ffff88000206c800 Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357379]  ffff880008a07b20 ffffffff8171e587 ffffffff81067a16 ffff880008a07af8 Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357381]  ffffffff810c79dc 00000000000000a9 ffff88003fe1ee28 0000000000000000 Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357384] Call Trace: Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357395]  [<ffffffff81723f80>] dump_stack+0x45/0x56 Oct  2 06:27:25 nodejs-data2 kernel: [1203087.357399]  [<ffffffff8171e587>] dump_header+0x7f/0x1f1  ```  and after that we can see the oom killer trying to free up memory and showing lots of rsync processes.
mhdawson		I was thinking it could be something started by anacron but I don't see a config file:  ``` root@nodejs-data2:/etc/cron.daily# cat /etc/anacrontab cat: /etc/anacrontab: No such file or directory ```
rvagg		it's all done on the web server, there ~are~ were two `rsync` commands in /etc/crontab, one that grabs charts and the other that grabs coverage data. They are both run every 15 minutes.  The problem here is the coverage data! It's now up at 21G (!! should we trim that?) and apparently it has trouble doing a full scan of that in 15 minutes so they start to overlap and over time they slow each other down and then just lock the server up.  So what I've done, aside from a `killall rsync` on the benchmarking server, is implement a lock-file for the rsync so it won't be run if there's an existing one being run. Checkout #904 and review it for me, that's what's on the server now.
mhdawson		Just reading this now.  Trimming the coverage data probably makes sense, although maybe we can just stop the rsync for the older directories, unless its causing space issues on the web server.  Today I don't seem to be able to get into the machine at all. Trying to reboot ...
mhdawson		Once I get back in I'll move the older coverage data to a different directory as a short term solution.
mhdawson		#904 looks good and I've manually trimmed the data for now.  Will think a bit more about what makes sense in terms of how long we keep the data available.
rvagg		@mhdawson we could remove the `--delete` from the rsync and then just keep it trimmed on the benchmark server but keep historical on the web server. We then can at least manage space issues in one place and won't have the overhead for rsync. Maybe a cron job on the benchmark server to prune it to only the last couple of days.
mhdawson		LGTM 
MylesBorins		LGTM 
jbergstroem		LGTM 
Trott		LGTM 
joaocgreis		LGTM 
MylesBorins		@nodejs/build can we give @gibfan access to jenkins while we wait for this to be voted? There is some work I could use his help on 
joaocgreis		Do we have or inherit a process for adding WG members from somewhere? Do we need a formal vote? The meetings are great to move stalled issues and discuss stuff, but we should be able to make decisions on GH issues. This has been open for 23 days without any objection, I do not see a reason not to move forward. 
gibfahn		So it looks like the people who haven't voted are: @rvagg @geek @phillipj @Starefossen  
Starefossen		LGTM üéâ 
phillipj		LGTM   > This has been open for 23 days without any objection, I do not see a reason not to move forward.  Completely agree! 
jbergstroem		@joaocgreis not really a formal process other than the tradition of attempting to vote on our meetings. I agree that this shouldn't hold up Getting Work Done and I believe we have consensus with the two last votes. Perhaps have all members add their vote post-addition as well.  @gibfahn can you ping me on irc? 
jbergstroem		Nit otherwise LGTM. Thanks for doing this so quickly! 
jbergstroem		Btw, I don't think you need to do `git commit -s`; it seems very specific to the linux foundation: http://developercertificate.org 
Starefossen		Thanks for the heads up about `-s` :-) 
mhdawson		LGTM 
orangemocha		LGTM 
mhdawson		Landed as 3795c65a9079de625a4949aa10290c1bf8613f49 
refack		Procedure for 2017-10-23 was:  * Set permissions as:   ![image](https://user-images.githubusercontent.com/15943089/31900722-5830ef06-b817-11e7-9769-ec84d20dc43a.png)  * Open an `incident` issue here and adding to `active` in project#1  * /CC <span>@</span>Collaborators   [refack: probably should also post a PSA on IRC]
bnoordhuis		@ryanstevens Is this still a thing? 
rvagg		pretty much good to go 
Starefossen		My preference would be towards a recurring meeting which can be canceled if there are no agenda items to discuss; so 2 or 3 would be my choices (in that order). 
orangemocha		Perhaps we could combine that with a meeting-agenda label to tag issues, so that the list can drive scheduling or cancelling the meeting. 
rmg		I like the idea of a recurring meeting that can be cancelled. I'm ok with moving the time.  I won't be able to attend today due to a lack of bandwidth.. had a major power outage in the area and I'm stuck with tethered internet off my phone. 
mhdawson		I like the idea of a recurring meeting that is cancelled when not needed as well 
jbergstroem		+1 to cancelling a scheduled meeting as long as we try to make it a day or two ahead.  
orangemocha		Ok, there seem to be consenus around having a pre-scheduled tentative meeting, that is confirmed or cancelled based on whether we have items to discuss. For the last part, I assume that using a "BuildWG-agenda" label is going to be uncontroversial....  What should be the cadence of the pre-scheduled tentative meeting? Doodle poll: http://doodle.com/poll/z86pau8scmt4pmaw 
orangemocha		It looks like we can use the `wg-agenda` label.  What should be the cadence of the pre-scheduled tentative meeting? Doodle poll: http://doodle.com/poll/z86pau8scmt4pmaw 
rvagg		sorry, filled in the doodle, looks like we're at least agreeing to not doing it every week  I'm easy whatever we do, we seem to be making good use of gitter and github, just as long as we make sure the entry-points for newcomers are obvious 
orangemocha		Ok, based on the poll everybody prefers the meeting to be tentatively on every 2 or 3 weeks. Given that more people liked the "every 4 weeks" option than the "every week" option, I went ahead and rescheduled the meeting for every 3 weeks, starting next Tuesday, September 29th. You should have received an updated calendar event.  I'll be sending an advance notice a few days before each occurrence (say on Thursday or Friday the week before) with a request for agenda items. If there are no agenda items by Monday, we can cancel the meeting occurrence. 
rvagg		@orangemocha or @joaocgreis can either of you take this one please? 
orangemocha		done! 
mhdawson		Created an ubuntu 14.04 machine quite a while ago but 64 bit compiles were failing.  Took a while to get back to looking at it but the problem seems to have been: - 32 bit is still the default for 14.04 for PPC (different from other platforms) - even compiling hello world failed - you need g++-multilib to get the 64 bit components (for x86 this would give you the 32 bit components instead)  After adding g++-multilib I have successfully build/run the tests and they all passed.  
mhdawson		I just ran citgm-all on one of our PPC BE RHEL 7.0 machines.  Nothing unexpected so seems like its reasonable to build on ubuntu 14.04 and get RHEL support as well. 
mhdawson		PR here for required core ansible updates: https://github.com/nodejs/build/pull/382 
mhdawson		Hooked up one ubu 14 be test machine and validated builds/tests ok under CI:  https://ci.nodejs.org/job/node-test-commit-plinux-mdawson/  Looks ok so next step is to switch over the existing be test machines from fedora to ubuntu  
mhdawson		building new test and release machine now 
mhdawson		second test machine moved over (test-osuosl-ubuntu14-ppc64_be-2), build to validate all is ok:  https://ci.nodejs.org/job/node-test-commit-plinux/2022/nodes=ppcbe-ubuntu1404/ 
mhdawson		Noticed I had not move all the config over from my earlier test job to force 64 bit on be, added that and new runs to validate: https://ci.nodejs.org/job/node-test-commit-plinux/2023/ https://ci.nodejs.org/job/node-test-commit-plinux/2024/ 
mhdawson		I've added a release machine https://ci-release.nodejs.org/computer/release-osuosl-ubuntu14-ppc64_be-1/ but left it offline for final clean/config by @jbergstroem since I don't have access to keys for release machines 
mhdawson		@rvagg,  @thealphanerd volunteered to do the next steps: - add release jobs/scripts etc - add to subsequent releases  Since he does have access to the release machines(which I'm assuming is necessary, at least for testing things out).  If that is something he can do could you confirm its ok for him to go ahead and point him at the PR for when le was added as I assume it will be a good template for adding be.   
MylesBorins		@mhdawson can you please assign this issue to me to follow up on 
mhdawson		@thealphanerd I tried assigning it to you but was not able for some reason 
MylesBorins		perhaps it is because I have not submitted a commit on this repo...  
jbergstroem		Ok, I've: - made sure we have hte right keyset for access - downloaded icu to `/home/iojs/node-icu` - added key pair so it can access [and upload] to `node-www` - Edit: enabled the slave through jenkins. 
mhdawson		@jbergstroem thanks, marked the step for setting up the release machine as complete. 
MylesBorins		Just clarifying the next steps that will likely need to be done - [x] Create temporary fork of release job - [x] enable `ppcbe-ubuntu1404-release-64` - [x] create successful build - [x] run tests to ensure that build works (with help from @mhdawson) - [x] enable `ppcbe-ubuntu1404-release-64` in production release job - [ ] in first release that uses updated production release job run tests to ensure that build works (with help from @mhdawson) - [ ] Update website to support new build  @rvagg does that sounds reasonable? @jbergstroem any complaints to me cloning the build job? I'd run the build exclusively as "test" and like only enable the single platform we are targeting 
mhdawson		Myles had create a test release job but there were issues in that it looks like it was running out of memory.  I investigated and with @jbergstroem figured out the issue was the concurrency setting (see https://github.com/nodejs/build/issues/385).  Binary was successfully built/published here:  https://nodejs.org/download/nightly/v5.11.1-nightly2016050299920480ae/  Next step is to validate the binaries look ok. 
mhdawson		Ran the core tests and citgm on the binaries.  1) CITGM and only modules marked flaky or the known eslint ppc test issue due to the phantomjs dependency in the eslint test had issues 2) The core tests passed except with a small number which fail in the same way on the corresponding x86 build.  I believe they are related to either running dev tests on release binary or being a 5.x nightly. 3) Install and test of the heapdump modules (which requires compilation as it is a native addon) was successful.  So the net is that the binary looks good. 
mhdawson		So I believe next step is to enable in the release job, see the nighties be generated and then included in next release.   Hoping we can do that as soon as current security releases go out. 
mhdawson		Looks like the security releases went out @rvagg @jbergstroem any objections to adding ppc be to the release job so we start to get regular nightlies ?  
jbergstroem		I'm ok with it. 
rvagg		ubuntu 14.04 makes me ok so +1 
jbergstroem		We've retired the fedora20 release slave. it remains to be deleted from our test environment though. 
mhdawson		Ok, added to regular release job, we'll track how it goes over the next few days 
mhdawson		Nightlies for 7 and 5 built ok last night, did a simple sanity check and they look ok. 
jbergstroem		Great! perhaps ping the website team to update links for upcoming releases? 
rvagg		fine by me 
mhdawson		@TheAlphaNerd had volunteered to put together the PR for the website, expect it soon 
mhdawson		Ok this is done, closing 
joaocgreis		The yellow ball of `dontcare` is more of a reminder that the test is still flaky, can't think of more reasons to keep `dontcare`. So, +1 
orangemocha		I am a bit concerned that we'd start losing visibility on the failing tests. What if a flaky test starts failing consistently, for a different reason?  I guess I am +0, but let some job (node-daily-master) not ignore flaky tests. 
rvagg		I want yellow balls to show up on flaky failures, I _really_ don't want the fact that we have this "flaky" thing in here lose visibility, I want collaborators OCD to kick in when they don't get all-green and make them go hunting and solving flakies, so I'm -1 if this means losing the yellow 
Fishrock123		If this makes show up as green: nope nope nope 
orangemocha		I also agree it's better to continue running them. 
jbergstroem		Ok, I believe we have somewhat of a consensus then. We'll keep running flaky tests.  ##### TL;DR  Pro: - avoid having flaky tests interfer with others - speed up test suite since flaky tests still wait for timeout  Con: - increased visibility of a fail - better test coverage - encourage fixing a fail instead of moving to next 
rvagg		@orangemocha can you check the calendar entry you created? It's coming up as 6am Tuesday for me and it should be 7am Wednesday, I'm not sure if this is lost in translation between calendaring systems or a simple mistake. 
orangemocha		Updated the event to be at 8pm UTC, every 3 weeks on Tuesday. Also updated the title (sorry for the double update).  I sent the new invitation starting November 10th, assuming tomorrow's instance would be canceled. There is still time to submit agenda items though. 
orangemocha		Since there are no items on the agenda, tomorrow's meeting is cancelled! The next occurrence is scheduled for November 10th. 
jbergstroem		Interesting! Question: does the reboot stuff really work? I've been fighting with ansible about waiting at reboots (for the host to come back up).
piccoloaiutante		@jbergstroem yes it worked on the two operating systems that I tested but I want to have a definitive response for @joaocgreis about Windows 2008 Server.  Here is the doc for the windows version. Have you tried to expand the `reboot_timeout_sec`? https://docs.ansible.com/ansible/win_reboot_module.html
jbergstroem		@piccoloaiutante no, i haven't tried windows rebooting at all -- its just a nightmare for !windows :\ 
joaocgreis		Landed in https://github.com/nodejs/build/commit/5cd0f41528bfa68885105e074f41afd9e31b0c62  I took the liberty to clean up a bit and add a small header (<50char) to the commit message, hope it's ok. I tested in Windows 2008R2, the reboot works beautifully. Thanks @piccoloaiutante !
piccoloaiutante		Excellent @joaocgreis now moving on generating .rdp and/or .remmina files.
gibfahn		Duplicate of https://github.com/nodejs/build/issues/687?
Trott		> Duplicate of #687?  Sure seems like it. I'll close.  
gibfahn		I'll take a look, also cc/ @gireeshpunathil  
jbergstroem		Just a note: I killed all lingering processes on both machines since the test runner was stalling.  
mhdawson		I don't see any failures today since @jbergstroem comment and I also had to restart the jenkins agent. Since then its seems to be be running ok.  We should probably check tomorrow to see if there are any jobs stacking up in the background.   
mhdawson		Looked today.  There were a small number of processes hanging around:  ```  iojs  8388860        1   0 18:59:37      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node /h ome/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/test/parallel/test-child-process-fork-dgram.js child     iojs 10289232        1   0   Sep 20      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node /h ome/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/test/parallel/test-child-process-fork-dgram.js child     iojs 10748052        1   0 16:24:13      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node /h ome/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/test/parallel/test-child-process-fork-dgram.js child     iojs 11141252        1   0 04:16:07      -  0:00 /home/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/out/Release/node /h ome/iojs/build/workspace/node-test-commit-aix/nodes/aix61-ppc64/test/parallel/test-child-process-fork-dgram.js child   Which are related to this issue:  https://github.com/nodejs/node/issues/8271 ``` 
gibfahn		@mhdawson That's interesting, I hadn't realised that the child processes weren't being killed in that test failure. So if the test times out, I assume the test runner kills the parent but not any children? 
jbergstroem		@mhdawson if you check my above paste (well, Rich's) `test/sequential/test-child-process-pass-fd.js` seems affected too. 
mhdawson		I've been looking at the fork-dgram failure, will just about to test proposed fix no AIX now.  The other one must be much less frequent. 
mhdawson		PR to address fork-dgram failures https://github.com/nodejs/node/pull/8697 
jbergstroem		@mhdawson, @gibfahn: log into -1 now and have a look. lots of stuff stalling. Here's a job that's been going for two hours as well (probably as a result of multiple test runners still being active): https://ci.nodejs.org/job/node-test-commit-aix/1040/nodes=aix61-ppc64/console 
gibfahn		@jbergstroem I'm not sure whether it's because someone's cleaned up the machine (or because `ps -ef` isn't the right command), but I'm only seeing a couple of test-dgram processes on that machine.  ![image](https://cloud.githubusercontent.com/assets/15943089/18779395/cb7c6afa-8170-11e6-8a63-623705381c7f.png)  Also when I first clicked your link I saw the still-running job (and I still have it open in a tab):  ![image](https://cloud.githubusercontent.com/assets/15943089/18778959/d33e01f6-816e-11e6-8e00-62f4e3b05b7d.png) ![image](https://cloud.githubusercontent.com/assets/15943089/18778965/d9651c68-816e-11e6-96ca-a60f2be060f1.png)  But now when I click through I don't:  ![image](https://cloud.githubusercontent.com/assets/15943089/18778929/b9e8a5f8-816e-11e6-96d1-dec94c15f9bf.png)  ![image](https://cloud.githubusercontent.com/assets/15943089/18778997/f40ef21e-816e-11e6-80dc-bba27ba97a5d.png)  ![image](https://cloud.githubusercontent.com/assets/15943089/18779013/069a566c-816f-11e6-9beb-e97f817dbc1a.png) 
jbergstroem		@gibfahn you also have a few python processes:  ```  ps -ef | grep python     iojs 4784192 2818506   0   Sep 21      -  0:04 /usr/bin/python tools/test.py -p tap --logfile test.tap --mode=release --flaky-tests=dontcare addons doctool inspector known_issues message parallel pseudo-tty sequential     iojs 2490818 1901104   0 17:54:10      -  0:04 /usr/bin/python tools/test.py -p tap --logfile test.tap --mode=release --flaky-tests=dontcare addons doctool inspector known_issues message parallel pseudo-tty sequential     iojs 3408274 3211782   0   Sep 20      -  0:04 /usr/bin/python tools/test.py -p tap --logfile test.tap --mode=release --flaky-tests=dontcare addons doctool inspector known_issues message parallel pseudo-tty sequential ```  ..and a few gmake processes:  ```  ps -ef | grep make     iojs 4128770       1   0 17:43:33      -  0:00 gmake run-ci -j 5     iojs 2621908       1   0   Sep 20      -  0:00 gmake run-ci -j 5     iojs 2818506 2818606   0   Sep 21      -  0:00 gmake test-ci     iojs 1901104 4128770   0 17:53:15      -  0:00 gmake test-ci     iojs 2818606       1   0   Sep 21      -  0:00 gmake run-ci -j 5     iojs 3211782 2621908   0   Sep 20      -  0:00 gmake test-ci ```  This is probably not good seeing how each test.py makes the assumption that they will be run exclusively (tmpdir, whatnot).  
jbergstroem		You can also see the `wait` invoked as well as as few defunct processes.  
gibfahn		@jbergstroem I'm not seeing those on `test-osuosl-aix61-ppc64_be-1` (a.k.a. `power8-nodejs2.osuosl.org`). Are you looking at `-2`?  **EDIT:** I do see them on `-2` 
jbergstroem		@gibfahn if I am then there's a naming thing. I'm talking about `140.211.9.101`. 
gibfahn		I might have them the wrong way round in my `.ssh/config`, I have:  ![image](https://cloud.githubusercontent.com/assets/15943089/18786446/d686e4c8-8195-11e6-95ff-2474d2cfac91.png)  Anyway, I'll take a look at why the processes are being left behind. 
mhdawson		Ok, the test that was causing processes to still be running after the test run was fixed so I think we can close this.  Please re-open if you feel differently.  
mikeal		This is a good time to figure out what service we want to pay for to get OSX builds. Budget is already allocated for it by the board.
jbergstroem		I don't think downtime is a good time to talk about future plans, but I'll bite. We already have the initiatives (#724, #741 or the classic #367) to get mac resources in place. To me, it seems we lack man hours. As you can see, we think the main issue is lack of sponsorship visibility which could be seen as a marketing effort. Suggestions on how to improve and speed up?
rvagg		Not unreasonable @mikeal, we've been in a limbo situation with our osx resources for a bit too long. We decided on some concrete steps we need to take ASAP and we've also looped in @mhdawson to the process so we're not reliant on fragile and unreliable dependencies (me). Best to take discussion on specifics to email at this stage though. 
jasnell		@rvagg ... For the arm cluster, have you written up a detailed description of what it would take to set up an equivalent mirror? If not, that would be quite helpful. 
mikeal		@jbergstroem we have budget allocated to pay a provider. I'm not saying we shouldn't pursue donors but we should get setup on a paid provider if only for the reliability if a donor goes down.
jasnell		@rvagg ... It would be helpful to have a post mortem write up of this (and all outages really) to the CTC/TSC once it's resolved. We need to have more visibility into these kinds of things.
rvagg		:thumbsup: @jasnell, FYI we're tracking to a point where the only difficult to duplicate resource is armv6 and we already have a precedent of allowing releases to go out without those. OSX is about to be sorted (just got good news via email on that front @mikeal) and armv8 has two new providers stepping up to take the load off the noisy boxes in my garage (I might decommission those entirely when we're redundant). I'll provide more detail when we're over this current hump tho, I know a number of people are concerned about our resilience at the moment. 
rvagg		We've just successfully kicked off a relationship with MacStadium, initially with a 6 month relationship with a review after that to see how it's working. Still working on setting up some resources but this will nix the weakest point in our infra at the moment! Will give more details soon but thought the good news is worth sharing.
jbergstroem		Just want to add that I'm also involved in both setting up the new MacOS cluster as well as offloading ARM to new sponsors. Downtimes like these shouldn't have to be a driver for improvement but usually ends up being anyway (remember digital ocean issues?).
rvagg		all working again, I also took the opportunity to do some minor maintenance across the Pi's
refack		No mining cryptocurrency with the spare cycles? https://www.bleepingcomputer.com/news/security/linux-malware-mines-for-cryptocurrency-using-raspberry-pi-devices/
rvagg		OK so the pi cluster is back to "healthy" again, my guess is simply that it's poor nfs performance to blame, because I cleared out workspaces after restarting everything it had to start from scratch but that leads to all of these machines simultaneously reading and writing over the network.  The disk can handle it, the server should be able to handle it, the individual machines should be able to handle it too, it's either the network topology or hardware that sucks or simply NFS that sucks. I've been assuming the latter and will continue to do so without better insight. Anyone with the nodejs_build_test key should be able to get in to these machines to do diagnosis so if someone thinks they have the skills to dig then you're welcome to.  Perhaps I should be exploring alternatives to NFS? Why hasn't NFS matured more than it currently is, are there better solutions? Should I be using cifs (windows/samba), sshfs, or something else?
refack		For my experience CIFS is not better than NFS
rvagg		@jasnell, @mhdawson, @trott, @nodejs/build I did a write-up of the outage here: https://github.com/nodejs/build/wiki/Service-disruption-post-mortems#2017-06-07-ci-infrastructure-partial-outages  Includes:  * Details  * Impact  * Resolution  * Weaknesses exposed  * Action items post-outage  Plus also links to various issues related.  I'd like us to produce similar write-ups on the same wiki page for future outages. It'd be a good habit for us to build, it keeps us accountable and it gives us a single place to record and share the info instead of scattering it across GitHub, IRC & email.  @jasnell, @mhdawson & @trott can I get you to weigh in on whether this needs to be shared more widely?
jasnell		Thank you @rvagg. I think making it available on the wiki or even as repo issues somewhere is sufficient.
mhdawson		I think putting these in a directory in the repo is probably good enough.  Maybe something like doc/service-disruptions-post-mortems  (or something shorter if somebody has a better name).  On the PPC/AIX front I agree with your write up that its not a high priority to get additional redundancy as the uptime for those systems has been quite good. I can't remember the last time they were down, just unfortunate timing of an uplanned power outage as OSUOSL this time. 
rvagg		lgtm  Ansible doesn't complain on use of "remote_user"? I recall having to switch to using the "become" stuff because of complaints. Or maybe it was "sudo" that was being deprecated. I can't keep up. 
jbergstroem		`sudo` is deprecated, replaced by `become`. Since we pass user through the config in the refactor I think we should just avoid writing extra logic. I guess the question is what hosts running ubuntu16 uses non-root ssh usernames? 
jbergstroem		I recall @rvagg mentioned that he cleared 113G logs less than a week ago; can't be filling up that quickly. Guessing it might be something else? 
thefourtheye		I remember seeing a permissions error while removing a directory in Ubuntu 15.04 64 box 
jbergstroem		@thefourtheye that might be a git issue. we're looking at installing a newer git version. 
saghul		@Fishrock123 I'm trying to trigger a libuv build and nothing happens, it just silently does nothing. Is this the observed behavior? 
joaocgreis		@saghul yes, that's it 
saghul		Thanks, I'll keep an eye on this issue then! 
orangemocha		@rvagg @jbergstroem @wblankenship @ryanstevens CI is blocked on this issue. You are the only ones who have access to the master machine. 
rvagg		``` # ls -alh total 119G drwxr-xr-x  2 jenkins jenkins 4.0K Aug  2 06:48 . drwxrwxr-x 12 root    syslog  4.0K Aug  2 06:49 .. -rw-r-----  1 jenkins jenkins 111G Aug  3 07:36 jenkins.log -rw-r-----  1 jenkins jenkins 8.1G Aug  2 06:49 jenkins.log.1 ```  restarted, it'll come back online soon but this is just going to happen again unless I can find a solution 
rvagg		@orangemocha lots of these types of errors, I think this might be a config problem on your end:  ``` WARNING: Caught exception evaluating: browser.getChangeSetLink(cs) in /job/node-test-commit-windows/81/. Reason: java.net.MalformedURLException: no protocol: ``` 
saghul		@rvagg thanks for bringing it up! 
orangemocha		> @orangemocha lots of these types of errors, I think this might be a config problem on your end: > WARNING: Caught exception evaluating: browser.getChangeSetLink(cs) in /job/node-test-commit-windows/8  I will take a closer looks. At first glance, I think it's from the github repo browser not having a URL configured, which was intentional. If the exception is indeed caught then we can ignore this warning. 
rvagg		we can ignore it, but it's filling up disk so .. 
orangemocha		I will investigate. 
orangemocha		> @orangemocha lots of these types of errors, I think this might be a config problem on your end: > WARNING: Caught exception evaluating: browser.getChangeSetLink(cs) in /job/node-test-commit-windows/8  That issue should be fixed now.  For the record, the repository browser adds a link next to every change  in the Jenkins job pages. Unfortunately it doesn't work with parametric repos, and any attempt to leave it blank will cause the warning in the log. It's silly, because the other two links work and I wish we could just remove the third link. Anyway, I hardcoded the root url to https://github.com/nodejs/node. 
jbergstroem		How often do we read the master log? Perhaps turn it off?  @rvagg do I have ssh access to the master machine? 
rvagg		no, just me and @wblankenship cause it's got NodeSource certs on it .. that'll change very soon when @mikeal gets the new certs paid for by the foundation, right @mikeal? 
orangemocha		@jbergstroem rather than turning the log off, we could try to limit its size by only keep the more recent content. A quick search seems to indicate that this is possible with the logrotate utility: http://jenkins-ci.361315.n4.nabble.com/Huge-hudson-log-td3304780.html 
orangemocha		Also, regarding artifacts, there is an option in Jenkins to only keep artifacts for the N most recent builds. 
jbergstroem		1. The main issue was DO backup jobs -- now disabled 2. We were running into memory issues (eden size and old gc size). I've reshuffled the ratios and increased the heap sizes. Our load has gone from ~10 on active runs to <1.  I still think we should household artifacts, but that doesn't seem related to this. Closing. 
MylesBorins		I recall this bug coming up when people were upgrading node versions  Perhaps this is from the testing we were doing on v0.10 or v0.12? 
MylesBorins		Looks like resources were not being cleaned between build (specifically those in the smoker folder)  This is now being cleaned and reinitialized at the beginning of each job and citgm is operational again 
jbergstroem		@gibfahn you mean like setting up a squid/proxy/similar? In order for it to be effective I'm thinking we would want to do that per-continent. The arm setup at @rvagg's seems like a good first setup; second could be at DO us east?
gibfahn		cc/ @mhdawson 
bnoordhuis		@jbergstroem In the case of openssl-fips-2.0.9.tar.gz, it could be hosted on local disk or on the local network.  That tarball was released 2 years ago and hasn't been the current one for 1.5 years.
jbergstroem		@bnoordhuis ok; similar to how we treat ICU I guess. 
mhdawson		The prior discusion/agreement was that we could host them on the main ci machine, we just never made progress on doing that.
bnoordhuis		Backref to https://github.com/nodejs/node/issues/13219 which was about the fips tarball download failing.
joaocgreis		While there's no better solution, this is what the ChakraCore Linux jobs use now:  <details> <summary>Bash function</summary>  ```bash function cache_download {   if [ -z "$1" ] || [ -z "$2" ] || [ -z "$3" ]; then     echo "Missing argument"     return 1   fi   URL=$1   EXPECTED_SHA=$2   DEST=$3    mkdir -p "$HOME/cache_files"   CACHE_FILE="$HOME/cache_files/$EXPECTED_SHA"    CACHE_SHA="X"   if [ -f "$CACHE_FILE" ]; then     CACHE_SHA=`shasum -a 256 "$CACHE_FILE" | cut '-d ' -f1`   fi    if [ "X${CACHE_SHA}" != "X${EXPECTED_SHA}" ]; then     rm "$CACHE_FILE" || true     curl -L "$URL" -o "$CACHE_FILE"     DOWNLOADED_SHA=`shasum -a 256 "$CACHE_FILE" | cut '-d ' -f1`     if [ "X${DOWNLOADED_SHA}" != "X${EXPECTED_SHA}" ]; then       echo Downloaded SHA256 mismatch.       rm -v "$CACHE_FILE"       return 1     fi   fi    cp -fv "$CACHE_FILE" "$DEST" } ```  </details><br>  
Trott		Closing due to long period of inactivity. Feel free to re-open if this is a thing. I'm just trying to close stuff that has been ignored for sufficiently long that it seems likely it's not something we're going to get to.
joaocgreis		This is already included for the Windows machines: https://github.com/janeasystems/build/blob/aded718cdd1b826d0dfe508cc47b8d20a09df58e/setup/windows/ansible-playbook.yaml#L69-L88 
jbergstroem		Here's a slightly improved version I have as part of my "ansible 2.0 refactor":  Edit: updated with 57.1.  ``` yaml --- # Downloads ICU and verifies checksums # @requires dest=/tmp  - name: Download ICU   get_url:     - url: https://ssl.icu-project.org/files/icu4c/{{ item.version }}/icu4c-{{ item.version|replace('.','_') }}-src.tgz     - dest: {{ dest }}     - checksum: md5:{{ item.md5 }}   with_items:     - { version: '54.1', md5: 'e844caed8f2ca24c088505b0d6271bc0' }     - { version: '55.1', md5: 'e2d523df79d6cb7855c2fbe284f4db29' }     - { version: '56.1', md5: 'c4a2d71ff56aec5ebfab2a3f059be99d' }     - { version: '57.1', md5: 'f797503ecaebf1d38920013dc7893066' } ``` 
Trott		Is this still an issue? Should it remain open?
richardlau		I don't believe this is an issue ever since Node.js started bundling and compiling against a stripped down ICU (icu-small, https://github.com/nodejs/node/pull/6088). Now that Node.js v4.x is EOL I believe all LTS and current releases build this way and do not require an external ICU. 
gireeshpunathil		/cc @gibfahn 
mhdawson		Was wondering if you still needed this given that the CI was green, but understand that was because the test was excluded.  I think @gireeshpunathil was going to work with you and he's had access to the machines in the past so maybe re-enabling that access for him makes sense.
ezequielgarcia		Well, if I can get access to the machines I could bite the bullet and take a look. Maybe some rainy sunday or so :-)
mhdawson		Our standard process is outlined here: https://github.com/nodejs/build/blob/master/doc/process/special_access_to_build_resources.md  As part of that these are the questions we look to answer before giving access:  * Does the scope and size of the need justify providing access. * Is the invidual a collaborator ? * Length and consistency of involvement with Node.js working groups   and/or community ? * Consequences to the individual in case of mis-behavior. For example,   would they potentially lose their job if they were reported as   mis-behaving to their employer ? Would being banned from involvement   in the Node.js community negatively affect them personally in some other   way ? * Are there collaborators who work with the individual and can vouch for   them?  
refack		I can vouch for @ezequielgarcia. <sub>(I know him virtually. I know of his work, and have been in touch with his regarding https://github.com/nodejs/node/pull/12794)</sub> P.S. @ezequielgarcia we usually give limited time access, that is usually from the point you say you're on it, till the specific problem is resolved. So if we get the necessary OKs, we can still put it on hold, and you would ping me to add your key if and when you're available.
gibfahn		+1 to giving @ezequielgarcia access, though I would suggest working with someone else if you haven't used AIX before, it's UNIX but not exactly Linux. Feel free to ping in here if you want to look at it, and hopefully someone can pair-program on it.  On which note I think @jbarz was going to look at this.
ezequielgarcia		- Does the scope and size of the need justify providing access. _Yes. Given I'm not familiar with AIX and don't have access to AIX hardware, I need it to find out what's the deal with the added dlopen test. Same thing happened with OSX, it wasn't until I got myself a OSX platform that I could find the linker flags for the test to pass._  - Is the invidual a collaborator ? _I'm a contributor._  - Length and consistency of involvement with Node.js working groups and/or community ? _My involvement with Node.js started with https://github.com/nodejs/node/pull/12794._  - Consequences to the individual in case of mis-behavior. _I'm not sure I would get fired. However, I'm proud of my FLOSS activities so I wouldn't do anything to harm my reputation._  - Are there collaborators who work with the individual and can vouch for them? _@refack has helped me recently. @gireeshpunathil offered to help on AIX too._
jBarz		I was able to find a fix I think. I will submit a PR
ezequielgarcia		I'm closing the issue, given @jBarz has taken the lead. Thanks a lot!
refack		@ezequielgarcia thanks for willing to follow up. You're kind of dedication is rare. Hats off üé©
rvagg		fwiw the lack of a git mirror isn't essential and should just be a quiet fail, while it'll slow it down if it's not in the right place it still should be fine.  I'm still messing with this machine, some weird network problems, can't even update the git mirror or clone a new one using `--mirror` or `--bare` although I have no idea if that's related! I've tried updating the machine, rebooting, rebooting jenkins master, deleting and recreating the node config on jenkins master, rebooting network hardware the machine is plugged in to. All to no avail.  I would try updating Java but a recent VMWare Fusion upgrade on that machine messed up access to the consoles, I only have black screens and have to use SSH to do everything like everyone else.  I'm out for 12+ hours but will look back at this soon. I'm not going to leave this machine offline because failures are better than a big queue in jenkins. If anyone else wants to tinker on the machine please do so.
rvagg		@nodejs/build did anyone install a new jenkins plugin recently?
rvagg		ok, really heading off now, my remaining theory is that a plugin was installed recently that's inteferring. If someone installed one in the last week or so, I'd appreciate you either drop a note in here or experiment with removing it and running node-test-commit-osx.
gibfahn		![image](https://user-images.githubusercontent.com/15943089/29998424-8c2b4d50-9022-11e7-989e-a6ece57ad498.png)  The fact that there are only two updates available suggests that the plugins were updated recently, so it might have been an update that changed things.  I haven't installed anything since the rebuild plugin (which was apparently in July https://github.com/nodejs/build/issues/672).
Trott		The node-test-commit-osx jobs seem to be trying to build on test-packetnet-ubuntu1604-x64-1 which seems to minimally be a labeling error, but maybe a full-on larger problem? See https://ci.nodejs.org/job/node-test-commit-osx/12132/ for an example.
gibfahn		@Trott is that a problem? The [parent job](https://ci.nodejs.org/job/node-test-commit-osx/12132/) is running there because it has the `jenkins-workspace`, but that just runs the initial git clone. The [actual job](https://ci.nodejs.org/job/node-test-commit-osx/12132/nodes=osx1010/) is running on [test-requireio-osx1010-x64-1](https://ci.nodejs.org/computer/test-requireio-osx1010-x64-1/).
Trott		> @Trott is that a problem? The parent job is running there because it has the jenkins-workspace, but that just runs the initial git clone. The actual job is running on test-requireio-osx1010-x64-1.  Nope, not a problem if it works. :-D
rvagg		@gibfahn I updated the plugins while trying to sort this out, which is why they look like that. Didn't help.  Interestingly there have been a few greens today and the last run only disconnected as it was already into the test running.  I'll see if I can figure out how to determine which plugins were most recently installed and start removing them to see if that makes a difference. Another alternative might be to try and get a different JVM installed but as I've already mentioned that's going to be tricky without a GUI!
rvagg		some more weird https errors on my local infra, this time on pi1's building binaries: https://ci-release.nodejs.org/job/iojs+release/1990/nodes=pi1-raspbian-wheezy/console (only accessible to people with release access)  `hudson.plugins.git.GitException: Failed to fetch from https://github.com/nodejs/node.git`  I haven't experienced anything similar on my own use of this network (I'm on it now) but this is very similar to what I was experiencing on the macOS box on the same network! I've done a cleanup on those two release machines and will keep an eye on those builds, I could conceive of a way this might be local to those machines but it's pretty suspicious on the face of it.  Re macOS, I've downloaded the latest Oracle JRE macOS tarball and unpacked it into /usr/local/lib with a /usr/local/bin/java symlink and changed the slave.jar startup to use that binary and it's running now. It's going @ https://ci.nodejs.org/job/node-test-commit-osx/12159/nodes=osx1010/console but taking a very long time to even get started which makes me still suspect a problematic plugin.  Will keep an eye on all of this and continue to work out what on earth is up. Again, any help or suggestions would be appreciated!
gibfahn		@rvagg is there anything in the master jenkins logs? If it's to do with the git clone it might be an issue moving clones around. Not sure what else could be going on here, haven't been seeing this on our internal Jenkins instance at all.
rvagg		Hm, yeah, a bunch of these just now for that host only `java.util.concurrent.TimeoutException` when trying to do the regular monitoring.  Maybe I just need to and shut everything down, including the internet connection and start it all up again slowly.
refack		> Another alternative might be to try and get a different JVM installed but as I've already mentioned that's going to be tricky without a GUI!  RE GUI, did you try to VNC to the VM? I have _some_ success with that...
rvagg		@refack got some command-line-fu to enable screen sharing? it's not enabled on the VMs unfortunately.
rvagg		@joaocgreis can you look at the shared binary repo and see if you can make it prune any more aggressively? It's at 3.2G at the moment and may be contributing somewhat to network problems (not likely the cause of this particular problem though).
refack		I think I used https://content.pivotal.io/blog/enabling-os-x-screen-sharing-from-the-command-line Which Is based off https://apple.stackexchange.com/questions/30238/how-to-enable-os-x-screen-sharing-vnc-through-ssh
rvagg		‚úã beaut! that worked nicely, I'm on the console now.
rvagg		https://ci.nodejs.org/job/node-test-commit-osx/12160/nodes=osx1010/console  More of the same, those errors are in jenkins.log in the master so I'm assuming they come from there rather than being reported from the slave. So, on the slave we have new java (same on master and slave), restarted (a few times), completely cycled all network hardware (none of the Pi's are even running as I'm writing this).  Other than rogue plugins, the _only_ other thing I can think of that might coincide with this is the updated SSL cert on master. The timing might explain this and the errors are SSL related. But why just this host? Not even the OSX host on ci-release is behaving this way.
rvagg		I've tried removing, reconfiguring, resetting network hardware in vmware for that VM. I've tried using NAT on the host instead of bridged direct onto the network. I've tried making new nodes on Jenkins and pointing it to them (test-requireio-osx1010-x64-2, test-requireio-osx1010-x64-3) just in case there's latent config garbage on the master.  Same.  ![argh](https://user-images.githubusercontent.com/495647/30043307-fb9ceeaa-9239-11e7-8d5b-252c67ad8252.gif) 
rvagg		Seeing similar errors now on almost all of the machines hosted on the same network, most of the Pi's are offline at the moment due to this and are labelled as "Ping response time is too long or timed out.". I did tweak the ping and timeout to lengthen it but apparently this hasn't helped matters, maybe even made it worse!?
rvagg		okie dokie, making progress _finally_  Installed an entirely new VM, from scratch, OSX 10.10 and we now have a string of greens on https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx1010/ with no disconnects (yet). Crossing fingers that the problems were isolated to that VM and will go away now ü§û. I'm seeing similar disconnection messages from other hosts that are similar now to the ones I was worried about in my last message so I think it's just a new form of Jenkins message I'm not used to seeing, so for now I'll avoid assuming there's something bigger at play on the local network.  While installing the VM I came up with a new _script_ that could be ansiblised. I avoided the UI as much as possible for this. These instructions will need to be used (and maybe improved) by whoever gets far enough with the MacStadium hosts that replace this single machine. (Note for people not in the _know_: the MacStadium setup is super complicated, vSphere stuff that we're having to level-up on, it's stalled because none of us have had the time to spend the extra time needed to get further than we have. I believe IBM may be assigning some staff to help with this.)  Steps listed below, I need to put these into the new doc/process/non-ansible-configuration-notes.md doc (or someone else is welcome to).  * (UI) Set up machine with an Apple ID (rvagg / rvagg@iojs.org, can share creds with nodejs_build_infra folks if helpful). Although I'm not sure this is strictly required, maybe just jump straight to the `iojs` user. * Gave this user `NOPASSWD: ALL` in /etc/sudoers for setup convenience * (UI) Create an `iojs` user, unprivileged * Install Homebrew `/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"`, which has a nice side effect of installing the xcode command-line tools so no separate step needed for that * `brew install git` (updated version) * `brew install ccache`, then set up with `(cd /usr/local/bin/ && for i in c++ cc clang g++ gcc; do ln -sf ccache $i; done)` (there may be a nicer way to do this, maybe with brew, or maybe just use `PATH` in the start script) * `brew install caskroom/cask/java` (üëè üëè) * As user `iojs` `mkdir /Users/iojs/build && mkdir /Users/iojs/git` * As user `iojs` `git clone --mirror https://github.com/nodejs/node.git /Users/iojs/git/io.js.reference` * As user `iojs` `git clone https://chromium.googlesource.com/external/gyp /Users/iojs/gyp` * Create executable script `/Users/iojs/start.sh` for user `iojs` with contents below * Create `/Library/LaunchDaemons/org.nodejs.osx.jenkins.plist` as `root:wheel` with contents below * `sudo launchctl load /Library/LaunchDaemons/org.nodejs.osx.jenkins.plist` * `sudo launchctl start org.nodejs.osx.jenkins`  _(edit: these two were run later as a result of @refack's comment below)_ * `sudo launchctl unload -w /System/Library/LaunchAgents/com.apple.ReportCrash.plist` * `sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.ReportCrash.Root.plist`   **start.sh**  ``` #!/bin/sh  curl -O https://ci.nodejs.org/jnlpJars/slave.jar  SECRET=<SECRET HERE> SERVER_ID=<NODE NAME HERE> CI_SERVER=ci.nodejs.org  export PATH="/usr/local/bin:$PATH" export JOBS=2 export NODE_COMMON_PIPE=/Users/iojs/test.pipe export OSTYPE=osx export ARCH=x64 export DESTCPU=x64  java -jar slave.jar -jnlpUrl https://${CI_SERVER}/computer/${SERVER_ID}/slave-agent.jnlp -secret $SECRET ```  **/Library/LaunchDaemons/org.nodejs.osx.jenkins.plist**  ``` <plist version="1.0">         <dict>                 <key>Label</key>                 <string>org.nodejs.osx.jenkins</string>                  <key>UserName</key>                 <string>iojs</string>                  <key>WorkingDirectory</key>                 <string>/Users/iojs</string>                  <key>Program</key>                 <string>/Users/iojs/start.sh</string>                  <key>RunAtLoad</key>                 <true/>                  <key>KeepAlive</key>                 <true/>                  <key>StandardErrorPath</key>                 <string>/Users/iojs/jenkins_err.log</string>                  <key>StandardOutPath</key>                 <string>/Users/iojs/jenkins_out.log</string>         </dict> </plist> ```
refack		An old favorite is raising it's head: ``` not ok 75 async-hooks/test-callback-error   ---   duration_ms: 15.545   severity: fail   stack: |-     start case 1     end case 1: 109.402ms     start case 2     end case 2: 99.464ms     start case 3     end case 3: 15.766ms     Error: test_callback_abort         at ActivityCollector.initHooks.oninit.common.mustCall (/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/test/async-hooks/test-callback-error.js:36:45)         at ActivityCollector.oninit (/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/test/common/index.js:509:15)         at ActivityCollector._init (/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/test/async-hooks/init-hooks.js:182:10)         at emitInitNative (async_hooks.js:446:43)         at Object.emitInitScript [as emitInit] (async_hooks.js:349:3)         at Object.<anonymous> (/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/test/async-hooks/test-callback-error.js:38:17)         at Module._compile (module.js:549:30)         at Object.Module._extensions..js (module.js:560:10)         at Module.load (module.js:483:32)         at tryModuleLoad (module.js:446:12)      1: node::Abort() [/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/out/Release/node]      2: node::Chdir(v8::FunctionCallbackInfo<v8::Value> const&) [/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/out/Release/node]      3: v8::internal::FunctionCallbackArguments::Call(void (*)(v8::FunctionCallbackInfo<v8::Value> const&)) [/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/out/Release/node]      4: v8::internal::MaybeHandle<v8::internal::Object> v8::internal::(anonymous namespace)::HandleApiCallHelper<false>(v8::internal::Isolate*, v8::internal::Handle<v8::internal::HeapObject>, v8::internal::Handle<v8::internal::HeapObject>, v8::internal::Handle<v8::internal::FunctionTemplateInfo>, v8::internal::Handle<v8::internal::Object>, v8::internal::BuiltinArguments) [/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/out/Release/node]      5: v8::internal::Builtin_Impl_HandleApiCall(v8::internal::BuiltinArguments, v8::internal::Isolate*) [/Users/iojs/build/workspace/node-test-commit-osx/nodes/osx1010/out/Release/node]      6: 0x16a6e9e846fd ``` Need to disable the crash reporter https://github.com/nodejs/node/issues/13527#issuecomment-311672640 > This has been fixed since @rvagg ran these two commands on the host: > > ```console > launchctl unload -w /System/Library/LaunchAgents/com.apple.ReportCrash.plist > sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.ReportCrash.Root.plist > ``` > > Note that they need to be run from a GUI login or else they error out.  > > There's an issue open to get this into the provisioning for OS X machines. But for now, the problem is resolved, so I'm going to close this.
refack		P.S. it's rejecting the test key: ``` debug2: service_accept: ssh-userauth debug1: SSH2_MSG_SERVICE_ACCEPT received debug1: Authentications that can continue: publickey,keyboard-interactive debug1: Next authentication method: publickey debug1: Trying private key: ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ debug2: we sent a publickey packet, wait for reply debug1: Authentications that can continue: publickey,keyboard-interactive debug2: we did not send a packet, disable method debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug2: we sent a keyboard-interactive packet, wait for reply debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: Killed by signal 2. ```
joaocgreis		@rvagg I cleaned the binary repo (can only do manually, because there's no way to tell if a branch is in use or not). Many branches were left behind when jobs were aborted, so I took a look and was able to change the way branches are deleted in `arm-fanned` and `windows-fanned` to run even if the job is aborted. 
rvagg		@refack thanks, just ran those commands via ssh (no security complaints, didn't need to do it via UI!), this re-run of a previously failed job (due to that error) has an unrelated failure: https://ci.nodejs.org/job/node-test-commit-osx/12210/nodes=osx1010/console, so I think we might be good now. Will watch today's daily jobs to see if we get more green. I've updated my scripted list above to include these. ü§û 
refack		1 green job https://ci.nodejs.org/job/node-test-commit-osx/12211/ Which IMO is a very possitive sign üèÜ 
refack		2nd green job in a row https://ci.nodejs.org/job/node-test-commit-osx/12212/ By Jove I think she got it
Trott		P.S. Let's get those commands added to ansible or whatever, yeah?
rvagg		<img width="173" alt="screenshot 2017-09-07 16 54 38" src="https://user-images.githubusercontent.com/495647/30150053-48713bdc-93ed-11e7-8372-e8c6d3bae5ec.png">  üëç  
rvagg		<img width="986" alt="screenshot 2017-09-11 20 20 46" src="https://user-images.githubusercontent.com/495647/30270114-c88bfe08-972e-11e7-9867-d34b6c0bc4db.png">  Something's still up. The 3 failures in that list of most recent builds from today are disconnects during compile. Not super-early like before but still the same kind of disconnect.  Still mostly green but regular disconnects it seems. I haven't looked any further back at the failures other than today's, I only just caught the machine offline so went for a quick dig. If anyone else wants to, go to https://ci.nodejs.org/computer/test-requireio-osx1010-x64-1/builds, wait for it to load and click through to each of them and look at the console output to see if it's a test failure or disconnection.  I don't have an answer here except that we need to get on with our mac IaaS work and have more of these for redundancy and hopefully stability.
refack		@rvagg @joaocgreis  `test-requireio-osx1010-x64-1` is still rejecting the test key for `iojs`
wolfeidau		At ninjablocks we are using the a couple of odroid XU boxes (8 core Arm A15 processor) as our build agents, each has 2gig of ram and flash based storage. These systems cost about $200 each.  Build time for NodeJS is approx. 5 minutes, which is better than the 45 minutes on either beagle bone or RaspberryPi...  In our case we run Ubuntu LTS on all our ARM based systems as they are now providing security updates in line with the x86 distribution, this was added when they released 14.04.  We found native builds where way more reliable and many times faster than QEMU arm emulation offered by Amazon, last time I checked performance was worse than Raspberry Pi.  Hosted ARM is currently pretty wild west, with very few commercial services, hopefully this will change once AMDs new ARM based Opterons go on sale later this year.  Look forward to tested and supported ARM NodeJS builds! 
mmalecki		So my idea here was to connect a few A20 boards (dual core, 1 GB RAM) over distcc and create a build cluster. So instead of having a single beefy machine as @wolfeidau does, cluster commodity hardware.  We don't have any data on how long such a cluster will take to compile node, or even how many machines this cluster will exactly have, but we're working on figuring that out. 
rvagg		@mmalecki I don't know much about distcc but is it going to give us a standard build as if you were to compile on a single machine? 
mmalecki		@rvagg yup! distcc was build to do exactly that, so it takes a bunch of precautions to make sure that's true, like not relaying on local header files.  Here's an excerpt from distcc overview:  > distcc is a program to distribute builds of C, C++, Objective C or Objective C++ code across several machines on a network. distcc should always generate the same results as a local build, is simple to install and use, and is usually much faster than a local compile. >  > distcc does not require all machines to share a filesystem, have synchronized clocks, or to have the same libraries or header files installed. They can even have different processors or operating systems, if cross-compilers are installed. 
andreas-marschke		In my mind another option to get or find hardware would be asking Linux distributions that are already deploying/building their complete stack and packages on arm.   See for example [Debian](https://www.debian.org/ports/) who support an extensive amount of "ports" of the debian distribution on different hardware. They also use donated or hosted hardware from community members and maintainers.   As luck would have it they already support nodejs on a lot of "non-standard" arches:  https://packages.debian.org/search?keywords=nodejs (as of this writing):  - amd64 - armel  - armhf  - i386  - kfreebsd-amd64  - kfreebsd-i386  - mipsel  So if there was a supportive developer or even the package maintainers themselves we could ask for assistance that would certainly help this task along.  Any opinions? 
andrewlow		Debian Node builds today are useful (as they exist) but they are built in a non-standard way if we compare it to binaries created by Joyent.  They dynamically link in the openssl libraries (probably a good idea) and also libv8 (a less good idea given some of the patches Node has made to V8 in the 0.10.x stream).  So you'd really be asking them to do an entirely different build than when they do today.  There is also the gcc compile farm https://gcc.gnu.org/wiki/CompileFarm  
andreas-marschke		@andrewlow Thanks for pointing out gcc.gnu.org. Haven't had  it on my radar.  And as you were pointing out the differences between Debians build of node vs. joyent:     Wouldn't it help the nodejs project to get some queues from the package maintainers of the larger distributions? Asking them what they see is a pain, where they'd like things improved. What goes and doesn't go for their distribution as in upstream changes etc. etc. I feel like it's not for nothing that nodejs  on Debian is  lagging behind 4/5(!) versions behind the most recent stable releases.  The situation on CentOS however is largely different. Having Red Hat as strong contributor is a good thing. [as of this writing] Comparatively they're only a merely version behind most recent stable.  I don't know how the rest of the core team or community sees this but being an administrator for a long time now trying to support and plan a larg-ish node infrastructure seems very much of a hassle unless theres is some support from the distribution or upstream. 
dshaw		@andreas-marschke Convincing the Debian maintainers to "do it the Node way" is challenging at best. They have a very specific way that they like to do things. If you're interested in the history, search for "debian" on the Node.js mailing list.  Many Linux admins choose to use Chris Lea's PPA which he's moved over to NodeSource. https://github.com/nodesource/distributions 
wolfeidau		Just a suggestion but may want to ping and chat with this startup http://labs.online.net/ we have started using them for build agents, they are in beta and by all accounts the product performs well.  Saves the need for hosting your own ARM machines.  Need to verify but they may also support docker, we have it running on our much smaller ARM platform, so builds for debian will be easier and in line with your existing tooling. 
bnoordhuis		Closing, we have ARM machines (including arm64 - woo!) hooked up. 
wolfeidau		:+1: awesome effort! 
gibfahn		+1 to giving Miroslav access, he works at IBM, @rsam, @mhdawson and I can vouch for him.
gibfahn		@bajtos Is it just Windows that's failing (and that you need debugging access to)?
sam-github		+1, @bnoordhuis worked with Miroslav as well if you need more CTC members who know him.
rvagg		Fine by me. @bajtos how much access do you need? Since we're not well set up (yet) to revoke the keys that we share it's usually preferable to give out per-machine access. How many machines are we looking? If you can provide a list then one of us could manage providing access and later revocation. Also, you'll have to specify a pubkey, you have a bunch in https://github.com/bajtos.keys
rvagg		ah, just saw on IRC that this is just about the ability to just interact with Jenkins as @bajtos is not a collaborator. That's even easier, I've manually added him to the Jenkins security matrix, this can be revoked at any time via https://ci.nodejs.org/configureSecurity/ by a jenkins-admins user.  @bajtos you can go to https://ci.nodejs.org/ and trigger node-test-pull-request or any of the sub-jobs. You should only need to provide the pull request in node-test-pull-request and tick the "certify safe" box, or provide the commit and fork details in one of the node-test-commit(-*) jobs if you want to run them individually. Give a shout out on IRC if you need help, #node-dev is probably easiest since it has more active people and all the collaborators have access already.
bajtos		Hello, thank you very much for your help! The ability to trigger a new CI build is all I need for now. I have had some success with reproducing the failing tests locally using a VM, I'll try this route some more to see how far I can get, and also to save CI resources.
bajtos		I had successfully triggered a new CI job: https://ci.nodejs.org/job/node-test-pull-request/9667  Let's close this issue as done. I'll reopen it in case I'll need a direct access to build machines.  Thank you again for your help üôá 
gibfahn		I've removed @bajtos from the security matrix.  I think this is now done, @rvagg could you double-check?
bajtos		@gibfahn oh, could you please keep me in the security matrix? My work on nodejs/node#13870 is not done yet.  Sorry if my last comment made the impression that I don't need to trigger CI builds anymore.
bajtos		For posterity, @gibfahn has restored my permissions, I can trigger new CI builds again üëç 
gibfahn		Sorry @bajtos, I misread your previous comment.  So we should keep this issue open while you need access, because that way the open issue reminds us to revoke the access afterwards.
gibfahn		@bajtos am I right in thinking this is no longer needed?
bajtos		Yes, I no longer need access to Node.js's CI.
gibfahn		Thanks, revoked.
gibfahn		Previously discussed in https://github.com/nodejs/build/issues/761#issuecomment-315940357
gibfahn		This seems relevant:  >Your Jenkins data directory "/var/lib/jenkins" (AKA JENKINS_HOME) is almost full. You should act on it before it gets completely full.
rvagg		Yeah, full disk, working on it, we bumped into not long ago and it turned out to be a couple of very large jobs. We're going to need to go full block device on this machine, perhaps I'll try and do that now eh?
gibfahn		>We're going to need to go full block device on this machine, perhaps I'll try and do that now eh?  Sounds reasonable, I don't think there was any objection when it was discussed before.  So how does access to the infra machines work? Everyone in build has access to `test`, the release WG (and some other people?) have access to `release`, and some subset of people have access to `infra`?  It'd be great to document this somewhere, maybe the build README.
refack		I'm assuming it's probably not the same "disk full issue" but goes under "CI flakiness" title, the `node-test-commit-arm-fanned` is stuck https://ci.nodejs.org/computer/node-msft-cross-compiler-1/ ![image](https://user-images.githubusercontent.com/96947/28313850-54f78076-6b86-11e7-8b40-cc21d93d041c.png) 
rvagg		So I think I figured it out, the backup job that also does the cleanup wasn't doing a full cleanup. It uses `find` to do the job and uses the following roots `/var/lib/jenkins/*/builds/` and `/var/lib/jenkins/*/configurations/axis-nodes/*/builds/` _but_ the axis jobs have names other than 'axis-nodes' including 'axis-MACHINE', 'axis-label', 'axis-v8test' and a bunch of others. Basically any multi-axis job has a name corresponding to the axis selected. What's more, the build directories are pretty deep within some of these due to the way they are configured.  So, I've changed the `find` to effectively do this: `find /var/lib/jenkins/ -depth -type d -regex '/var/lib/jenkins/.*/builds/[0-9]+' -mtime +7 -exec rm -rf '{}' \;`. So it now matches any depth of build directory and finds the job subdirectories underneath that are older than 7 days.  Back to 7 days and we're now at 86% disk usage on the machine by including these extra build directories.  @jbergstroem and @joaocgreis you might want to check my work on that .. particularly the axis thing @joaocgreis since that's your wheelhouse.  Regarding block storage, it turns out the reason we haven't done this yet is that this machine is in DigitalOcean's SFO1 which doesn't do block storage, so we'd have to redeploy the machine in a new datacenter to get this functionality unfortunately.  Regarding access, some of these key machines are accessible only by the "infra" group which is myself, @jbergstroem, @mhdawson and @joaocgreis. ci-release, www, backup(s) and a few others are reserved for this group.  Regarding further flakiness since my last message, as per @refack's comment, that's my fault as I inadvertantly upgraded Jenkins when I shouldn't have and got us into the Java 8 requirement territory as @gibfahn outlined in https://github.com/nodejs/build/issues/775 and a large chunk of the machines couldn't properly connect! I've downgraded it again and it seems to be back to normal but we may need a bit of time to flush out current work.
mhdawson		Looks like builds have been good since added so closing 
jbergstroem		LGTM 
mhdawson		Do we need any more reviewers before pulling this in ?  
rvagg		no, go for it, lgtm if that helps 
mhdawson		Fixed up to follow standard procedure as opposed to using merge button  Landed as 8fce5b90feaa1b59ce457268ae95c532ef2e0524 
orangemocha		More suggested agenda items: - The serialport daily test run in CI. We have recently re-enabled it. I wanted to make sure that everyone is ok with running this as part of Node's Jenkins or whether it belongs somewhere else. - We could have a quick discussion about GitHub team names for CI. Related: https://github.com/nodejs/CI/issues/7 
orangemocha		Meeting confirmed for today. Hangout links updated. 
joaocgreis		I won't be able to attend today, I'll read the minutes tomorrow. I made two changes to the ARM cross compilation and binary test jobs: [fixed cross compilation for ICU](https://github.com/nodejs/node/pull/6088#issuecomment-213795849) and [testing armv7 binaries on the Pi2 and Pi3](https://github.com/nodejs/build/issues/355#issuecomment-214716307). 
rvagg		Hey folks, sorry to be a downer but I'm going to have to bail on these meetings in this slot for now. I have to cut down on my early meetings and with DST these fall in at 6am for me now and I already have CTC & TSC each week. 
orangemocha		This meeting was had. 
mhdawson		@jbergstroem addressed all of the comments. Next time, for anything new that I have to do I'll look to see if there are ansible modules :) 
mhdawson		@jbergstroem updated to extract the larger drive name 
jbergstroem		LGTM 
mhdawson		landed as https://github.com/nodejs/build/commit/ae07514c43f079215a6d68c8373cddf52ad8d857 
gibfahn		Going to be on a plane for this one.
bnoordhuis		Must be a serious agenda if you're going to fly out for it.  (I'll get my coat.)
jbergstroem		Just got a meeting :( I will update this issue with my activity.  [edit]  My work:  - [wip] handling jenkins security  - looking into java 8 update options for older test operating systems (required by jenkins 2.6)  - done some work on standardizing the debian router for macos and hopefully the rpi cluster  - minor pr/review stuff
mhdawson		I'll try to be there.
piccoloaiutante		if @mhdawson can't make it, @joaocgreis do you have access for streaming through official account?
joaocgreis		No, I don't have access to the official account.  There are no issues for the meeting (https://github.com/nodejs/build/labels/wg-agenda), so I don't know if we need to have the meeting today. Is there something else to discuss?
piccoloaiutante		To me it's just standup.
mhdawson		I'll be there now that security releases are out.
mhdawson		Link for participants: https://hangouts.google.com/hangouts/_/ytl/sIbFbCb9B53dNNs-VMRAwklq7gSR36H7VcCbgV8oXHg=?eid=100598160817214911030
piccoloaiutante		@mhdawson do you want me to pr the minutes?
gibfahn		Minutes landed in https://github.com/nodejs/build/pull/810
EloB		I've tried to reinstall node and now I can't work because if this :(
SimenB		It usually completes in the end, but not fast enough for CI
jbergstroem		I'll have a look and see if something is up with cloudflare/digitalocean.
EloB		@jbergstroem https://www.cloudflarestatus.com/ they have problem.
SimenB		Either this or nodejs/nodejs.org#1061 should probably be closed, to keep it in one place üòÑ 
targos		Not sure it's a cloudflare issue. This is the error I get:  ![screenshot from 2016-12-09 14-44-53](https://cloud.githubusercontent.com/assets/2352663/21050965/2a1fdc92-be1e-11e6-8355-98caa7a3af73.png) 
SimenB		I did get some TLS errors initially, before it just plain timed out
PeterDaveHello		I wonder if there is any chance that we can build/provide mirror service in case we have some familiar problem again?
jbergstroem		Traffic is within 5% of what we can expect around this time of day. I didn't see any out of the ordinary errors in our nginx error log and our ssl certs are valid.
agsmorodin		Issue still exists.  Here is my speed now, Amsterdam location: ``` wget https://nodejs.org/dist/v0.12.2/node-v0.12.2-linux-x64.tar.gz --2016-12-09 13:43:57--  https://nodejs.org/dist/v0.12.2/node-v0.12.2-linux-x64.tar.gz Resolving nodejs.org (nodejs.org)... 104.20.22.46, 104.20.23.46, 2400:cb00:2048:1::6814:162e, ... Connecting to nodejs.org (nodejs.org)|104.20.22.46|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 9487774 (9.0M) [application/gzip] Saving to: 'node-v0.12.2-linux-x64.tar.gz.5   6% [=========>                                                                                                                                                         ] 638,731     14.6KB/s   in 3m 5s    2016-12-09 13:47:11 (3.36 KB/s) - Connection closed at byte 638731. Retrying.  --2016-12-09 13:47:12--  (try: 2)  https://nodejs.org/dist/v0.12.2/node-v0.12.2-linux-x64.tar.gz Connecting to nodejs.org (nodejs.org)|104.20.22.46|:443... connected. HTTP request sent, awaiting response... 206 Partial Content Length: 9487774 (9.0M), 8849043 (8.4M) remaining [application/gzip] Saving to: 'node-v0.12.2-linux-x64.tar.gz.5 1,539,008   5.44KB/s  eta 43m 48s^C ```
jbergstroem		(I'm not saying we're not experiencing issues, just taking notes from what I can measure)
malaire		Same problem when downloading from Finland -- all larger (few MB or more) downloads fail with Partial Content.
jbergstroem		I'm getting pretty awful packet loss to the origin host; investigating.
mhdawson		I can confirm that even just trying to load benchmarking.nodejs.org is taking a really long time from Ottawa/Canada
jbergstroem		Definitely something going on at our host. I've gotten in touch with support but will try to debug as much as possible (which as you know is a great experience over ssh with packet loss).  Update:  - root disk io: just fine  - block storage io: all good  - networking in/out: broken  - cpu/ram: nothing out of the ordinary (except higher interrupts waiting for network)  - open tcp connections: low enough to at least rule out starvation attempts  We have rsync jobs almost two hours old (backup, benchmarking, rsync dist) which would point to network degradation at least at that point. Bandwidth history seems to be pretty consistent though.. 
bennyn		I saw that Travis is downloading Node.js versions via `nvm`. Even if they are pre-installed on the VM.  Here is what is preinstalled (Build system information):  ![preinstalled](https://cloud.githubusercontent.com/assets/469989/21053587/57173702-be2a-11e6-9b42-412a1eeff739.PNG)  Here is the log entry from `nvm`:  ![image](https://cloud.githubusercontent.com/assets/469989/21053664/c2bfb5ec-be2a-11e6-8c64-74092f01c40c.png)  I don't know why it's doing that (downloading Node.js v6.9.1 again) because we have set an explicit version (which should be pre-installed) in our `.travis.yml`:  ```yml language: node_js node_js:   - 6.9.1 ```  Build Log:  - https://travis-ci.org/wireapp/wire-webapp/builds/182614458
PeterDaveHello		@bennyn I think `nvm` shows the reason as the checksum not matched.
PeterDaveHello		@jbergstroem is there any chance to provide an alternative or mirror site so that we can tolerance the problem until it's fixed? Thanks.
jbergstroem		@PeterDaveHello you can try using http://unencrypted.nodejs.org. It is a mirror but shouldn't be treated as such when nodejs.org is unavailable (read: this is not the official mirror, just an unencrypted version for old http clients)
jbergstroem		Update: The issue is identified, I'm working with DO support to resolve it.
matthewp		There's a second issue I think should be addressed. If a `.travis.yml` has a Node version set as `4` or `7` or whatever else and the download fails, Travis shouldn't continue on defaulting to 0.10 as it currently is doing. It should fail immediately, as it can't build what the user intended.  EDIT: whoops, look like that's a new issue for that now. Awesome!
SimenB		@matthewp that's an issue with travis, or maybe nvm. Not nodejs.org
matthewp		Correct! Sorry for the noise.
PeterDaveHello		@jbergstroem thanks for helping.  I thought http://unencrypted.nodejs.org/ really looks like an official mirror since it's under nodejs.org, it'll be great if we could have rsync protocol so that people can easily create mirrors, especially inside enterprise or the area with limited network access.
glasser		unencrypted.nodejs.org doesn't appear to have 4.6.1 or 4.6.2.  Are there any mirrors?
PeterDaveHello		@jbergstroem btw, http://unencrypted.nodejs.org/ looks out-dated?
PeterDaveHello		@glasser if you need nodejs v4.6.2/4.7 linux binary build, I can help: https://cdn.peterdavehello.org/node-v4.7.0-linux-x64.tar.xz
jbergstroem		@PeterDaveHello ouch. I'm updating it now -- give it a few minutes.
PeterDaveHello		@jbergstroem is it possible to provide an official method to mirror it? 
jbergstroem		@PeterDaveHello unencrypted.nodejs.org is in preparations of being both a http and rsync mirror. We will officially announce it when done.
rvagg		What we need is a consistent way of recording host information, at the moment it's a little bit scattered, most of it is in README.md files in the various ansible directories but I'm not sure that's optimal either. Perhaps we need to focus on beefing up the Ansible Inventory as the single-point-of-truth? 
gibfahn		This is done, script is [write-ssh-config.yml](https://github.com/nodejs/build/blob/master/ansible/playbooks/write-ssh-config.yml), and inventory is [inventory.yml](https://github.com/nodejs/build/blob/master/ansible/inventory.yml).
joaocgreis		- [x] `grep -q ^github.com ~/.ssh/known_hosts || (ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts)`  EDIT: Part of the Jenkins script.
joaocgreis		@nodejs/build any objections to just landing this? This has been working well enough for 2 years and should be in master. There can probably be improvements, but those can be done in other PRs.
gibfahn		Let's just land it
joaocgreis		@seishun all of those seem to have the same root cause. These scripts were developed for Ansible 1, and worked well at the time. Ansible 2 seems to have changed things: https://github.com/ansible/ansible/commit/2e39661a26d881f1ff5991ae46e5cbf45b91cfe9  The variables are defined in `ansible-vars.yaml`. The solution seems simple, just change `with_items: packages` to `with_items: "{{packages}}"`. The steps for ssh keys can be removed because we now use a shared key (that is already on the machine).
bnoordhuis		I'd start with `make install` first and tackle the GUI installers later.  `make install` runs tools/install.py.  You would add an action to the files() function, something like:  ``` python def files(action):   # ...   if action == install:     # concatenate files and install     action(...)   elif action == uninstall:     # just uninstall     action(...) ```  The install and uninstall functions only know about copying around files, you probably want to teach install how to write a string to file. 
matthewloring		Pull request opened at nodejs/node#3032. 
matthewloring		nodejs/node#3032 landed. Closing this. 
phillipj		@maclover7 thanks! Pushed a commit adding a comment with a friendly reminder.
maclover7		@phillipj This should be ready to go, right? Would you be able to merge this, and check and see if it helps at all with https://github.com/nodejs/node/pull/17151#issuecomment-346528636?
phillipj		@maclover7 I'm not able to immediately, but I'll at least have a look tomorrow.  I'm unsure if it would help any issues ATM, if anything probably make it worse? Currently it will happily respond to requests from *anywhere*, having this whitelist activated could if anything, make it worse? Or have I misunderstood the issue in the PR you referenced?
rvagg		LGTM, although it should be possible to insert some python in plugins/inventory/nodejs_yaml.py to pull out the IP addresses with the `jenkins-workspace` alias and insert them into a new context variable that you can then use in setup/github-bot/resources/environment-file. If you can't figure out how to do this then this change is a good first step. There's a checklist on the README in this ansible directory where you could add a TODO for doing this work.  But as I've mentioned in https://github.com/nodejs/node/pull/17151#issuecomment-346528636 I think the effect of this on non-pipeline jobs is that it's going to stop status reporting completely because the `curl`ing is happening on worker machines, not the workspace ones that are exclusively used by Jenkins.  @phillipj do you have access to update the bot machine with this change?
phillipj		> @phillipj do you have access to update the bot machine with this change?  Yepp. I'll merge this and update the env variables on the machine out shortly.
phillipj		@maclover7 this is live in production now.
rvagg		uhh, sorry, didn't mean to merge this one! got my local branches messed up and didn't bother cleaning up and a push to master to update hosts must have merged this. Hope nobody had problems with this though! 
ghost		what about the file size? will it go up? i mean, this is _electron_ we're talking about here 
Fishrock123		@ore Probably but it could act like a sort of version installer. Could have an option of which version from in it. Also it doesn't make the binaries go up in size at all, probably.  Do consider how many problems we've had which the current installer programs... they are just a mess all around, especially for OS X. 
ghost		@Fishrock123 yeah, that was my only concern pretty much. a multi-version installer sounds nice and reduces efforts on the website! 
MylesBorins		```      osx - 117.3 mb   linux32 -  60.3 mb   linux64 -  55.2 mb  win ia32 -  47.8 mb   win x64 -  66.2 mb ```  This is the application size for various deploy targets taken from https://github.com/electron/electron/issues/2003#issuecomment-135592382  I do think @Fishrock123's point is a good one though... this could be a one time installer and updater as opposed to what we have now 
ghost		just imagine! we'll be the first project of this size to only have _one_ download button on our website! 
mikeal		We could also end the install with onboarding into documentation on how to use Node.js.  Also, we could include a version switcher, that would make things really nice. 
MylesBorins		in theory the docs could be embedded in the application 
evanlucas		The biggest issue I've seen with OS X installers seems to be dependent on whether node/npm are owned by root. Can you get administrator access via electron on OS X? 
MylesBorins		Looks like there is a discussion about this alread  https://github.com/electron-userland/electron-packager/issues/33  /cc @maxogden 
MylesBorins		I'm not 100% this is what we want, but --> https://github.com/electron-userland/electron-builder  Seems to be capable of making a `.dmg`. It should be able to make a `.msi` but there is currently a bug they are fixing 
eljefedelrodeodeljefe		I'd be in support for @mikeal's point of the version manager. Also: installing headers (`process.realease.headersUrl`) for a respective version in a normalized place would be helpful on all the platforms. 
eljefedelrodeodeljefe		From https://github.com/nodejs/node/issues/7371: should include clean uninstallers. 
MylesBorins		it is happening https://github.com/nodejs/installer
feriancek		RSS ftw
mhart		I use this atm: https://nodejs.org/en/feed/blog.xml
mhart		(would love emails though tbh)
rrijnberk		I think it would be nice to have an update channel with issue numbers and tags so it would be possible to create hooks for updates that are personally relevant
jasnell		There's always this: https://github.com/nodejs/node/releases.atom
rvagg		@MylesBorins given that we have the RSS for the releases on the blog, can you be more specific on what you'd like to see that isn't available there? Perhaps a more specific format containing the items in parsable form? Also the use-case would be helpful here too.  Email is an interesting one, we could trigger that from the server whenever a new blog post goes live, pull out the data from the post and email that. We'd have to allow signups from arbitrary people though so either we need to write something new or use an existing service. We have mail capabilities via rackspace but nothing beyond basic there. Suggestions welcome!
MylesBorins		I was unaware of the RSS for the Blog... my thought was a feed directly tied to the release promotion that included the meta data I mentioned above. The use case would be to enable others to automate their build infra that does stuff when we release e.g. docker-node, IBM sdk, google sdk
chorrell		Being able to automate the docker-node updates would be great!
MylesBorins		ping regarding release feed
jbergstroem		Done & done. 
rvagg		they should be set up to reconnect but those two machines in particular seem to have trouble with this and I don't know why! 
jbergstroem		did the nfs disconnect or was it something else? 
rvagg		I _think_ it was when I rebooted the nfs master and the rest of them auto reconnected but these two are really stubborn. 
jbergstroem		Lets close this for now then? If you see it again, perhaps put dmesg output in here so we can troubleshoot. 
rvagg		no, I'd like to leave it open, these two machines are consistently problematic with nfs and something needs to be fixed. They usually take a very long time to boot because they can't get their mounts connected and then come up without them mounted and I have to do it manually. 
maclover7		ping @jbergstroem @rvagg -- should this stay open?
rvagg		nfs has been really solid since my last refactor, I'm pretty happy with it now
rvagg		oh, and the problem really was about boot-time connections, most/all of the time it failed and needed manual intervention so it required me remembering to check regularly. We've solved that with some unfortunate hackery but it works much better than all of the official ways (see setup/raspberry-pi if you want gory details)
mhdawson		Seems to be a persistent problem: https://ci.nodejs.org/job/node-test-commit-arm/12078/nodes=ubuntu1604-arm64_odroid_c2/console  @rvagg can you take a look ?
bnoordhuis		Still happening FWIW: https://ci.nodejs.org/job/node-test-commit-arm/12177/nodes=ubuntu1604-arm64_odroid_c2/console  Can it be taken offline until it's fixed?
gibfahn		Happy to take it offline if it's one machine, but if they're all having this problem then that's not really an option.   The one @bnoordhuis posted was `test-mininodes-ubuntu1604-arm64_odroid_c2-2`. I've taken it offline, we'll see if the issue crops up again.  ping @rvagg 
bnoordhuis		I've seen `g++: internal compiler error: Killed (program cc1plus)` a few times recently.  Probably the same OOM issue.  https://ci.nodejs.org/job/node-test-commit-arm/12880/nodes=ubuntu1604-arm64_odroid_c2/console
rvagg		I've updated and rebooted those C2 machines. They've been running for a lot longer than they normally do, we tend to have a lot of trouble with them. There were some `<defunct>` processes on there but nothing taking up much memory that would explain this.
MylesBorins		was getting the internal compiler error messages today and it was quite confusing while doing the release, should we take these boxes offline again?  https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu1604-arm64_odroid_c2/13015/
maclover7		Moving to more general odroid_c2 issue: #1093
richardlau		Related #419 (`--without-intl`)
sam-github		Related https://github.com/nodejs/node/pull/11747
gibfahn		@jbergstroem @mhdawson could one of you clone one of the build jobs (node-test-commit-linux I guess) so I can try to get this set up?
mhdawson		I assume you will cut it down to run on a single linux variant only, as we don't want to double the load and the one would probably be enough to catch compile problems.   Cloned one for you to work on here:  https://ci.nodejs.org/view/All/job/node-test-commit-linux-gibson/ 
rvagg		This came up the other day but I don't recall where. @mhdawson & @danbev, where was that, IRC?  Anyway, I tried one out, the config is all done and waiting to be enabled (by ticking the label "" to the list of node labels in the job) but it has a snag, perhaps the first of many? https://ci.nodejs.org/job/node-test-commit-linux-containered/3916/nodes=ubuntu1604_sharedlibs_withoutssl_x64/console (you'll have to scroll up a bit, above the addons)  `make test-ci` involves npm which tries to do an update check but fails because that loads crypto  ``` 09:36:53 Makefile:649: recipe for target 'tools/doc/node_modules/js-yaml/package.json' failed 09:36:53 make[2]: *** [tools/doc/node_modules/js-yaml/package.json] Error 1 09:36:53 Makefile:608: recipe for target 'doc-only' failed 09:36:53 make[1]: *** [doc-only] Error 2 ```  Before it can be turned on, we're going to need to skip this step for this type of build.
maclover7		This is a thing now: https://ci.nodejs.org/job/node-test-commit-linux-containered/nodes=ubuntu1604_sharedlibs_withoutintl_x64/
danbev		@maclover7 Awesome, thanks for that!
retrohacker		:heart: 
kenperkins		lgtm :+1:  
jbergstroem		Landed in 6a10343800bf47202d545fe492e91b3066138720. Thanks. 
retrohacker		I'm new to this workflow on GitHub. Instead of directly merging the pull request, what did you do to produce https://github.com/iojs/build/commit/6a10343800bf47202d545fe492e91b3066138720? 
jbergstroem		@wblankenship I wasn't entirely sure what was expected either, but the general pattern I've noticed on `iojs/io.js` is that you either merge someone else's PR (for instance https://github.com/iojs/io.js/commit/97b424365a883f3b8de18b3ec3f256307a92ad09) or just fast forward and merge your own pr branch with reviewers in commit message (https://github.com/iojs/io.js/commit/b9686233fc0be679d7ba1262b611711629ee334e). Since we haven't landed our own guidelines I followed their (which ours in progress are pretty similar to if I remember correctly). 
mikeal		Will we get a new picture of the cluster once it is in its new home?
rvagg		release machines are back online for now, armv6 and armv8, others are off, no ETA on proper connection yet
Trott		So with no ETA for the return of the Raspberry Pi cluster:  For the jobs that are stalled waiting for the Raspberry Pi farm, will they kick off tomorrow or whenever the farm comes back online? Or probably not and the jobs should just be canceled now?  Land stuff without Raspberry Pi test results in CI? Or wait for the Raspberry Pi cluster to come back?  /cc @nodejs/ctc 
joaocgreis		`armv7-ubuntu1404` and `armv8-ubuntu1404` were removed by @rvagg from the `node-test-commit-arm` job but `node-test-commit-arm-fanned` was left in place, possibly forgotten. I think it's better to cancel. I'll look for a way to remove the whole job.  EDIT: Just disabling the job woked, it is properly skipped by `node-test-commit`. Also disabled `git-rpi-clean`.
rvagg		Sorry, I thought I removed node-test-commit-arm-fanned. There shouldn't be any queued jobs, if there are then I've messed up!
rvagg		Thursday the 9th is the date I've been given for finalising this internet connection. Apparently there are some technical challenges (also I think some administrative incompetence but that's to be expected when dealing with large telcos!).
rvagg		Bad news .. I've been notified there are network problems in the area (monopoly government-provided internet infrastructure, yay) and it's been deferfed for another week. If it goes through then it should be up on the 16th of this month.
thefourtheye		We have three releases and we might get RCs out soon. Should we hold them till this setup is back up? We cannot release binaries without testing them, right?
italoacasas		I have two questions: - something I(we) can do to help right now? - something we can prepare(plan) in the case that this happens again in the future, like for example a storm, etc.
mhdawson		The LTS releases are planned for Feb 21st so availability on the 16 may not affect those directly.  It may affect plan RC's, in that case the question would be if the changes going in that we wanted validation through the RC would be ARM only or can be adequately covered by use on other platforms.  In terms of testing for the Current release, I wonder if the binaries could be tested manually by somebody with access to the release machine logging in and running the tests.  That might take a while to run thought since it would be on the single machine instead of fanned like it is in the regular jobs.
Trott		I think it's OK to release RCs without testing in the ARM cluster in this situation. Maybe explain/apologize in the release announcement.  And actual release (as opposed to an RC) might be different....
Fishrock123		Same, RCs/Betas should be fine.
rvagg		AAAAND we're back up online again on a new stable connection that's quite a bit faster than the old one as a bonus. Working my way through everything but I'm pretty sure I've got most things in place already so it should be working as it used to before the move. Please let me know if you encounter anything that doesn't seem right.  Regarding RCs and nightlies, I think that it got screwed up after a reconnect of my temporary connection where a new dynamic IP got assigned which messed up the iptables rules on both Jenkins machines. They _were working_ just not connecting! Ooops!
joaocgreis		Jobs seem to be running well! There are still 3 slaves offline and the DNS for the jump host is not updated, but this is not urgent. However, we have some tests failing:  - `test-dgram-address` is failing consistently for master on RPi 1 and 2 (`master` test runs: [1](https://ci.nodejs.org/view/All/job/node-test-commit-arm-fanned/6879/), [2](https://ci.nodejs.org/view/All/job/node-test-commit-arm-fanned/6880/), [3](https://ci.nodejs.org/view/All/job/node-test-commit-arm-fanned/6881/)) - [`v7.x-staging`](https://ci.nodejs.org/view/All/job/node-test-commit-arm-fanned/6882/) seems to have the same problem plus `test-npm-install` on all 3 RPis - [`v6.x-staging`](https://ci.nodejs.org/view/All/job/node-test-commit-arm-fanned/6883/) and [`v4.x-staging`](https://ci.nodejs.org/view/All/job/node-test-commit-arm-fanned/6884/) ~are still running at this moment~ seem good
rvagg		Thanks to @Trott for jumping on test-dgram-address @ https://github.com/nodejs/node/pull/11432, looks like that'll be addressed soon. Full green run @ https://ci.nodejs.org/job/node-test-binary-arm/6241/  I've taken three Pi's offline, suspecting corrupted filesystems or dodgy SD cards, some of the failures were because of that. I'll address them as soon as I can and bring them back online.
rvagg		Failures on test-requireio_arm-ubuntu1404-arm64_xgene-2 are interesting, e.g. https://ci.nodejs.org/job/node-test-commit-arm/7806/nodes=armv8-ubuntu1404/ and correlate with disconnection notifications that we keep on getting for just this machine and they date back pretty far (prior to the move). I was tinkering on that box last night trying to understand it but I have no idea what's going on. There's nothing special about it, in fact it's the least special of the 3 XGene machines (one runs the NFS for the Pi's and does release builds, another serves as a jump host for SSH, this one just runs test builds and nothing else!). Something about Jenkins keeps on disconnecting and reconnecting, perhaps it's a Java problem..  Anyone got ideas for debugging this? @joaocgreis, @jbergstroem?
joaocgreis		@rvagg It's strange that it's just that one machine. I have no solution, but perhaps you can try a different ping interval from the slave side. This is used for Windows: https://github.com/nodejs/build/blob/d2d5dd2506951d0dfdd2ffa83aeba8b37b3e6cbc/setup/windows/resources/jenkins.bat#L5 (the main thing that clearly fixed Windows was the ping interval from the master side, but this was left in place in all Windows slaves so at least it doesn't hurt).
rvagg		https://ci.nodejs.org/computer/test-requireio_arm-ubuntu1404-arm64_xgene-2/builds  I tweaked the job slightly after posting the above and you can see that it's _mostly_ green since then. It now downloads slave.jar before starting, each time, under the theory that having an updated slave.jar would be good ... but tbh I don't know if that's been a problem at all.  Kernel logs are still full of:  ``` [511493.450658] init: jenkins main process (8821) terminated with status 255 [511493.450681] init: jenkins main process ended, respawning [511499.729117] init: jenkins main process (8852) terminated with status 255 [511499.729139] init: jenkins main process ended, respawning [511505.963897] init: jenkins main process (8883) terminated with status 255 [511505.963921] init: jenkins main process ended, respawning ```  But failures are less frequent now but they still happen. I've implemented the extended ping interval thing just now so let's see if that helps at all.
jbergstroem		@rvagg does the exits correlate with anything interesting in the logs?
rvagg		@jbergstroem well, when I look at the actual times, it would correlate with _anything_ that's happening on the machine:  ``` [Wed Feb 22 13:43:04 2017] init: jenkins main process (26312) terminated with status 255 [Wed Feb 22 13:43:04 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:10 2017] init: jenkins main process (26343) terminated with status 255 [Wed Feb 22 13:43:10 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:16 2017] init: jenkins main process (26374) terminated with status 255 [Wed Feb 22 13:43:16 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:23 2017] init: jenkins main process (26405) terminated with status 255 [Wed Feb 22 13:43:23 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:29 2017] init: jenkins main process (26436) terminated with status 255 [Wed Feb 22 13:43:29 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:35 2017] init: jenkins main process (26467) terminated with status 255 [Wed Feb 22 13:43:35 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:42 2017] init: jenkins main process (26498) terminated with status 255 [Wed Feb 22 13:43:42 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:53 2017] init: jenkins main process (26529) terminated with status 255 [Wed Feb 22 13:43:53 2017] init: jenkins main process ended, respawning [Wed Feb 22 13:43:59 2017] init: jenkins main process (26560) terminated with status 255 [Wed Feb 22 13:43:59 2017] init: jenkins main process ended, respawning ```  (who knew `dmesg` had a `-T` eh?)  basically it's constantly happening. Going to have to run this manually and see if I can get anything from it.
rvagg		captured _a_ failure, not sure if this is _the_ failure, relevant log portions after connect are here: https://gist.github.com/rvagg/8eeb20b0fe7cf289601593ebff5bb827  There's a problem with child processes not being cleaned up properly which seems to cause Jenkins grief (never seen this before elsewhere) and then when it tries to reconnect it gets the kind of error you get when a node is already connected and it keeps on looping from there, which is similr behaviour to what I'm seeing with it running under upstart.  I'm trying out disabling the process tree killer as per https://wiki.jenkins-ci.org/display/JENKINS/ProcessTreeKiller to see if that helps, perhaps this is an architecture thing (i.e. this thing is "native code").
maclover7		ping @rvagg -- can this be closed?
Fishrock123		Last I heard, someone was working on a CLI tool for this. That paired with pushing build statuses to GitHub should work just as good, without the arbitrary caveats.  (I do not want to use that CI job again.) 
Trott		> Last I heard, someone was working on a CLI tool for this.  CI tool for what exactly? 
orangemocha		Perhaps @Fishrock123 is referring to https://github.com/nodejs/build/issues/177? It was a way to launch the job by doing a git push. Essentially a CLI-friendly front-end for node-accept-pull-request.   I think we need to make sure that CI is _really_ stable before going down the route of node-accept-pull-request.   Things that I would consider as steps towards additional stability: 1. Rebooting slaves at the end of each run. @joaocgreis has done some experimentation but last we talked about it it still needed work. 2. Secondary Jenkins masters for two purposes: a) to test job/configuration updates before going live in production; b) for additional availability in case the main jenkins goes down (this is less of a blocker, as currently if Jenkins goes down nobody can merge PRs anyway). 
orangemocha		- Adding more admins to Jenkins 
Trott		FYI, we've now had a _third_ PR land that breaks CI today: https://github.com/nodejs/node/pull/4741 This would not have happened with node-accept-pull-request because it would have rebased against master and caught the issue.  As we add more and more contributors, this is likely to happen more and more (on the assumption that more contributors means more contributions means more wires crossed between two PRs as happened here). 
MylesBorins		An alternative could be having a hook to run CI when there is a push to master and notify the build group if things are broken. 
Trott		I agree that CI stability needs to be solid for this to work. Can we come up with a checklist of measurable things that would indicate that we're ready to propose moving forward?  For tests, I'd say that if we can get 20 runs in a row on CI that don't fail spuriously, we're likely there. Would it probably be the same for CI? Can we use https://ci.nodejs.org/job/node-daily-master/ as our benchmark? If it runs for 20 consecutive days with nothing but yellow and green, maybe it's time to propose a move?  Here's a tentative checklist: - [ ] Slaves reboot after each run - [ ] Secondary Jenkins master for testing new configs - [ ] Increase Jenkins admins to `???` (what's the right integer there?) - [ ] 20 days of consecutive node-daily-master runs without any spurious red  Anything else? 
Trott		@TheAlphaNerd wrote:  > An alternative could be having a hook to run CI when there is a push to master and notify the build group if things are broken.  That would be an improvement over the current situation. To me, it puts the pressure on the wrong people. And it's reactive instead of proactive.  I would prefer: - Step 1: Stable CI infrastructure with reliable tests - Step 2: No, you can't land your commit if it breaks CI - Step 3: Pizza party 
Trott		Would the node-accept-pull-request job were to be used by the CLI tool? It seems the answer is "yes".  Moreover, does it make sense to enable the node-accept-pull-request tool but not require its use? I would totally use it. That would help shake bugs out in preparation for a CLI tool that leveraged it or whatever. 
mhdawson		I wonder about the reboot after each run which seems pretty heavy handed.  Would some sort of cleanup including processes etc would work ?  In particular if we give people access to machines to debug issues, ongoing reboots would make this difficult (although in that specific case maybe the machine should be disabled but I don't think people generally have the capability to do that) 
orangemocha		The CLI tool doesn't need to be a blocker IMO, but others might disagree. Maybe @Fishrock123 can confirm whether that's what he was referring to, and if so we can reopen https://github.com/nodejs/build/issues/177. /cc @silverwind 
silverwind		Is it possible to integrate Jenkins into Github, like many projects already have in place with Travis?  I think our CI is fast enough now to not generate a significant backlog, and it would give instant feedback to every contributor. 
silverwind		Maybe https://wiki.jenkins-ci.org/display/JENKINS/GitHub+Plugin 
jbergstroem		@silverwind both me and @Starefossen has been looking at doing this. It will require us to build a third party tool to avoid credential leaking. See https://github.com/nodejs/build/issues/236. 
jbergstroem		@orangemocha I still think rebooting is too dramatic and think it might lead to other issues (esp how jenkins and windows has been relatively flakey in terms of connectivity); at least with @joaocgreis's redeployment work on rackspace we should be able to have enough redundancy to do a lot of reboots. 
Starefossen		Here is my take on build status from Jenkins: https://github.com/Starefossen/jenkins-github-status. It is currently posing builds to the [#node-test](http://webchat.freenode.net/?channels=node-test) IRC channel. 
jbergstroem		It's unfortunate that this happens and I can see how our previous flakiness has been making it hard for collaborators to distinguish between "real" green runs, yellow runs and red runs. I think it could be good to remind everyone to just have a second look before merging; there's no real gain in being trigger happy. The best case is obviously automation such as `node-accept-pull-request` but I don't think the trust is quite there yet. 
orangemocha		Re: rebooting, it is only a hunch of mine that it would help and I don't have any quantifiable proof that it would help. So a criterion on consecutive green/yellow runs is a more direct proof of reliability or what needs to be fixed to achieve reliability.  
silverwind		@jbergstroem nice progress, wasn't aware of these efforts. I see that a lot of caution is necessary there. For example, rate-limiting and cancelling builds after a push when CI is running on the same PR are probably necessary to prevent someone from hogging CI resources.  @Trott I feel your pain, but few collaborators (including me) don't really like being forced through Jenkins. It's just not reliable enough and some just prefer the CLI workflow. Old habits die hard, unfortunately. 
Trott		@silverwind If a CLI wrapper around CI/Jenkins is the secret sauce that gets us buy-in from collaborators, that's good by me! 
jbergstroem		Based on the progress of our collaborators, the testing WG and our build WG I think we are soon in a state where we can trust jenkins to take all deviations seriously. Next step should be improving tooling for 1: visibility/feedback (wip, #236), 2: landing commits (`node-accept` or a derivative) and 3: latency (jenkins, more ci, parallel tests). Albeit slow, I feel we're making progress and the pain points for all all collaborators should slowly fade away. CI is very important and I don't believe that the 15 minutes we might have to wait for a green light should be replaces with a post-commit strategy. 
silverwind		Such a CLI wrapper could actually be very useful for collaborators. I've had the idea of a tool that pre-fills commit meta data on the CLI, but haven't found an easy way to supply `git rebase` with the meta data. I think I'll open a issue regarding that, maybe some git guru has a better idea. After rebasing is automated, I think adding some sort of webhook to start a Jenkins job and sleep the process until it has a result would be quite trivial. 
jbergstroem		It's definitely possible; I've worked with automating setting jenkins slaves up through the api. Jenkins api is by no means complete, but libraries usually patch the missing parts with plain 'ol http posts. If someone wants to look at this, here's a good start: https://github.com/jansepar/node-jenkins-api 
joaocgreis		@silverwind for reference, this is the relevant part of the script that was used by CI (node-push-merge-commit):  ``` bash GIT_COMMIT=$(git rev-parse HEAD) git rev-parse refs/remotes/origin/$TARGET_BRANCH  rm -rf env.git_editor touch env.git_editor echo '#!/bin/bash' >> env.git_editor # These two lines will strip existing metadata from the commit message # echo "grep --ignore-case --invert-match '^Reviewed-By\|^PR-URL' \$1 > env.commit_message" >> env.git_editor # echo "cp env.commit_message \$1" >> env.git_editor if [ -n "${FIXES// }" ]; then   echo "echo 'Fixes: $FIXES' >> \$1" >> env.git_editor fi if [ -n "${FIXES2// }" ]; then   echo "echo 'Fixes: $FIXES2' >> \$1" >> env.git_editor fi if [ -n "${REF// }" ]; then   echo "echo 'Ref: $REF' >> \$1" >> env.git_editor fi if [ -n "${PR_URL// }" ]; then   echo "echo 'PR-URL: $PR_URL' >> \$1" >> env.git_editor fi if [ -n "${REVIEWED_BY// }" ] && [ "${REVIEWED_BY}" != "<empty>" ]; then   echo "echo 'Reviewed-By: $REVIEWED_BY' >> \$1" >> env.git_editor fi if [ -n "${REVIEWED_BY2// }" ] && [ "${REVIEWED_BY2}" != "<empty>" ]; then   echo "echo 'Reviewed-By: $REVIEWED_BY2' >> \$1" >> env.git_editor fi if [ -n "${REVIEWED_BY3// }" ] && [ "${REVIEWED_BY3}" != "<empty>" ]; then   echo "echo 'Reviewed-By: $REVIEWED_BY3' >> \$1" >> env.git_editor fi if [ -n "${REVIEWED_BY4// }" ] && [ "${REVIEWED_BY4}" != "<empty>" ]; then   echo "echo 'Reviewed-By: $REVIEWED_BY4' >> \$1" >> env.git_editor fi  cat env.git_editor chmod 755 env.git_editor export GIT_EDITOR=./env.git_editor  rm -rf env.rebase_sequence git checkout -f $GIT_COMMIT git rebase refs/remotes/origin/$TARGET_BRANCH echo $? git log --reverse --format="reword %h %s" refs/remotes/origin/$TARGET_BRANCH..HEAD > env.rebase_sequence cat env.rebase_sequence  rm -rf env.git_sequence_editor touch env.git_sequence_editor echo '#!/bin/bash' >> env.git_sequence_editor echo "cp env.rebase_sequence \$1" >> env.git_sequence_editor cat env.git_sequence_editor chmod 755 env.git_sequence_editor export GIT_SEQUENCE_EDITOR=./env.git_sequence_editor  git rebase -i refs/remotes/origin/$TARGET_BRANCH ``` 
silverwind		@joaocgreis thanks, that use of `GIT_EDITOR` and `GIT_SEQUENCE_EDITOR` is quite clever! 
orangemocha		Scrubbing my notes, other issues that we might want to address for re-enabling node-accept-pull-request: - [should have] Eliminate checkpoint at the end of tests in Jenkins (https://github.com/nodejs/build/issues/165) - [should have] Release feed without PR tags (https://github.com/nodejs/node/issues/2665) - [nice to have] Have Jenkins comment on the PR or email the committer when the job completes. 
orangemocha		Another PR that was landed without running CI on the final set of changes, breaking master: https://github.com/nodejs/node/pull/4892 
Trott		Maybe we can at least get `node-accept-pull-request` to the point where it's an option for people (like me) who would _want_ to use it?  Then we can maybe build a CLI tool based on that work? 
orangemocha		As it was designed, if someone else pushes to the same target branch while the job is in progress, the job will fail. If you are still ok to use it with that caveat, we can work on reviving it. 
Trott		@orangemocha wrote:  > As it was designed, if someone else pushes to the same target branch while the job is in progress, the job will fail.   That's a feature, not a bug. :-) 
Fishrock123		cc @nodejs/build  
phillipj		Thanks for reviewing! For some reason I'm struggling to get my local ansible test setup up-n-running, really want to figure out what's causing that and ofc do a test run locally before applying to any of our servers. 
phillipj		@Fishrock123 too reduce the time spent cloning the node repo, I pushed a change fetching only the last revision. Cloning the whole repo history didn't seem necessary to me. From the [ansible docs](http://docs.ansible.com/ansible/git_module.html#options):  > Create a shallow clone with a history truncated to the specified number or revisions. The minimum possible value is 1, otherwise ignored.  You okey with that? 
Fishrock123		@phillipj It needs access to any `vM.x-staging` staging branches currently in use. Technically it doesn't need `master`. 
jbergstroem		By accessing staging stuff I reckon you'll be touching most of master. Might as well do a full checkout seeing how shallow clones will have issues traversing history. 
phillipj		Thanks for your thoughts, decided to remove the shallow clone, and just clone the entire node repo.  This has been applied to the github-bot server in production. 
jbergstroem		FWIW, @rvagg is already busy deploying this on newer buildbots :+1:  
retrohacker		I've never used ccache but googling gave me this comforting description:  > The most important aspect of a compiler cache is to always produce exactly the same output that the real compiler would produce. This includes providing exactly the same object files and exactly the same compiler warnings that would be produced if you use the real compiler. The only way you should be able to tell that you are using ccache is the speed. >  > ccache has been coded very carefully to try to provide these guarantees. However, if you experience any bugs, please report them.  Assuming it produces the same output, I don't see a reason why we shouldn't do this. :+1: 
jbergstroem		We run ccache pretty much everywhere nowadays. I'll run through the bots and see what's left. I think the OS X one still doesn't use ccache and Windows is left out for lack of support. 
rvagg		does ccache support llvm & OSX? I can put it on the OSX machines but I didn't think it supported it. 
jbergstroem		@rvagg neither should be a problem (I'm running it on my desktop for iojs/nodejs):  ``` $ ccache -s cache directory                     /Users/jbergstroem/.ccache primary config                      /Users/jbergstroem/.ccache/ccache.conf secondary config      (readonly)    /usr/local/Cellar/ccache/3.2.1/etc/ccache.conf cache hit (direct)                  3759 cache hit (preprocessed)             227 cache miss                          3066 called for link                       50 called for preprocessing             486 compile failed                         1 couldn't find the compiler            23 unsupported source language          123 no input file                         96 files in cache                      4242 cache size                         879.3 MB max cache size                       1.0 GB ```  Install through homebrew or similar and add to path. Should "Just Work" (famous last words). 
jbergstroem		The centos machines seems to lack ccache as well. I'll see what I can do. 
jbergstroem		Mission accomplished 
joaocgreis		I cannot reproduce now, pages load fine here. If this is still happening, do you have a way to check if it happens on multiple computers / ISPs?
refack		Work for me (Comcast East Coast 65.96.x.x)
joaocgreis		I've cleared the Cloudflare cache, don't know if that will help.  Duplicate: https://github.com/nodejs/nodejs.org/issues/1391
knksmith57		tunneling through DO's SFO1 datacenter results in a `200`. Here in Austin, TX my edge is still receiving the `404`.  looks like we're just playing a waiting game now.  for others following along at home, static mirrors: * http://bit.ly/node-6-api-docs-mirror * http://bit.ly/node-8-api-docs-mirror  thanks for following up, @joaocgreis
erinspice		I'm still receiving 404s at DFW.
gibfahn		This seems like something we should contact Cloudflare about. @mhdawson @joaocgreis @jbergstroem @rvagg I'm not sure how we do this, is there someone we could cc in this issue?
refack		I commented on https://github.com/nodejs/nodejs.org/issues/1391#issuecomment-334970060, trying to figure out what CF is caching. If everything was OK or if there was a time-window with 404s? This sounds like a counterintuitive behaviour of CF, if we have "Always-on" it should actually behave in the opposite manner.
phillipj		Just chiming in to confirm the docs works as expected in the Norway/Oslo area. Sadly I don't have any knowledge about the Cloudflare setup or how to contact them :/
joaocgreis		We have some CI machines at DFW and I can confirm this is an issue only there.  As noted in https://github.com/nodejs/build/issues/676#issuecomment-296089378 , Cloudflare is configured to fall back to http://unencrypted.nodejs.org/download/  when the primary server is not accessible. I can access the main server by IP from the machines at DFW, I don't know why Cloudflare is not using it.  There were no `docs` folders in that server, there was a systemd timer that was not executing because it did not have the full path in `ExecStart` and a crontab entry that was doing the work but excluded docs. I left only the timer and fixed it (ref https://github.com/nodejs/build/issues/590). 
refack		Can we add those URLs (http://unencrypted.nodejs.org/download/release/latest-v6.x/docs/api/ and http://unencrypted.nodejs.org/download/release/latest-v8.x/docs/api/) to `download-test.sh`?
joaocgreis		This is still an issue for https://coverage.nodejs.org/ and https://nodejs.org/metrics/ at least. I don't have any experience with Cloudflare, but we should probably figure out why it's not using the main server. cc @jbergstroem @rvagg 
refack		/cc @cloudflare https://twitter.com/refack/status/917396135995637760 because internet
rvagg		I've made some tweaks to the SSL config on the backup server, _maybe_ that'll help.  Taking this to private email with CF along with @mhdawson for now, will report back if we hear anything helpful.
rvagg		Some progress, this is interesting so I thought I'd share. There is an [API](https://api.cloudflare.com/#load-balancer-pools-properties) for querying the status of load balancer pools on CF. Doing so, with our account (`curl https://api.cloudflare.com/client/v4/user/load_balancers/pools/POOL_ID/health -H "X-Auth-Email: AUTH_EMAIL" -H "X-Auth-Key: AUTH_KEY"`) shows that they are all healthy _except_ Dallas, TX:  ```       "Dallas, TX": {         "healthy": false,         "origins": [           {             "PRIMARY_SERVER_IP": {               "healthy": false,               "rtt": null,               "failure_reason": "TCP connection failed",               "response_code": null             }           },           {             "SECONDARY_SERVER_IP": {               "healthy": false,               "rtt": null,               "failure_reason": "TCP connection failed",               "response_code": null             }           }         ]       }, ```  (IP addresses edited out)  Same error on two servers hosted by completely different providers, ongoing for a few days now, so I'm assuming it's on their end. Waiting to hear more on this from them.  
rvagg		It looks like `Dallas, TX` has been taken out of the edge list, maybe to fix up a problem, hopefully that means that traffic is being sent somewhere nearby to a healthy edge and that this is "fixed". Would love to hear from people who were having trouble with it previously.
evanlucas		https://coverage.nodejs.org still isn‚Äôt working for me. It shows the websites homepage and redirects to https://coverage.nodejs.org/en
rvagg		@evanlucas clear your cache perhaps? I don't think that one's related to CF, there's some funky nginx redirect stuff I've been noticing related to http/https and this sounds similar.
knksmith57		@rvagg from Austin, TX I'm now seeing correct content for the first time in 5 days: ``` ‚ùØ curl -iIX GET https://nodejs.org/dist/latest-v6.x/docs/api/ HTTP/1.1 200 OK Date: Wed, 11 Oct 2017 03:30:29 GMT Content-Type: text/html Transfer-Encoding: chunked Connection: keep-alive Set-Cookie: __cfduid=d4b8d8ec2041e46aeaecb8d69a018c9691507692629; expires=Thu, 11-Oct-18 03:30:29 GMT; path=/; domain=.nodejs.org; HttpOnly Last-Modified: Tue, 03 Oct 2017 17:14:42 GMT CF-Cache-Status: HIT Expires: Wed, 11 Oct 2017 07:30:29 GMT Cache-Control: public, max-age=14400 Server: cloudflare-nginx CF-RAY: 3abebdb45a4957f5-DFW ```  Speculating here, but the `CF-RAY` header leads me to believe CF is now serving correct content from Dallas.
rvagg		Still not seeing `Dallas, TX` in the edge list from their API but I did remove the backup server and then put it back in in our config in the hopes that would give them all a jolt, I guess there's a chance that's helped. Will wait to hear back from them.
prototypeme		From Dallas, TX, just wanted to confirm the doc links are working fine now. Thanks for fixing.
refack		Double confirmation so closing (DFW area users please comment if wrong)
evanlucas		the docs work for me, but https://coverage.nodejs.org still redirects to https://coverage.nodejs.org/en.
rvagg		Hey @evanlucas, this is precisely what's on the server for coverage.nodejs.org: https://github.com/nodejs/build/blob/master/setup/www/resources/config/coverage.nodejs.org, I don't see anything in there that would be causing a redirect. Do you still get it after clearing cache? Can you replicate the behaviour with another browser or `curl`?  There's a small possibility this is an SNI thing but I don't imagine you're running a browser old enough to be bothered by the fact that we're serving multiple https hosts off the same host. If it continues to happen I could see how switching on CloudFlare serving for that host goes, maybe that'll make an impact.  Status update from CloudFlare on the Dallas issue: last I heard from them they said that the edge server team was looking into it. I believe we're essentially solved on the problem and anything outstanding is on their end so üëç I suppose.
evanlucas		This is finally working for me now. I had cleared dns cache and all that, but even curl was still responding with a 301. Sorry for all the noise and thanks for following up @rvagg :]
maclover7		Would be a nice to have, but it doesn't seem like we have time to work on this right now, and isn't super pressing. Closing, but please reopen if someone would like to work on this.
MylesBorins		LGTM 
jbergstroem		LGTM 
mhdawson		LGTM 
bnoordhuis		> Debian unstable & testing  debian/testing seems fine (I think that's what most Debian desktop users run - I do, at least) but sid is frequently broken.  It might cause a lot of false negatives.  >  SmartOS 13.4.2 / SmartOS 14.2.0  Is anyone outside of Joyent running SmartOS?  The number of (non-SmartOS) Solaris users is a rounding error, I know both of them personally.  > Need variations for VS 2012 and VS 2013  There was a post on the v8-users mailing list this morning that [VS 2012 support is being phased out](https://groups.google.com/d/msg/v8-users/1DZPKQlIW7A/n9kakZ7ZqvMJ).  Node.js is a few V8 releases behind, of course. 
andrewlow		Please seriously consider supporting 32bit versions of the Node binaries. The build infrastructure can be 64bit versions of the OS, but only offering 64bit binaries means we're going to run V8 in 64bit mode.  For most Node applications this will be a waste of memory since they don't need to access more than 4G address space and V8 today doesn't do anything beyond a little bit of header compression in the 64bit environment.  Also for what it's worth - Node can probably be built once on a 64bit Linux machine, then packaged as a .dep, rpm, tgz for the various distros. I know from experience that building on RHEL 6.5 produces a binary that runs just fine on Ubuntu 10 and 12. Let's make it a goal to have as few unique binary builds as we can (while allowing for installer packaging wrappers to support the various OSes) 
rmg		From a never-looked-at-v8-internals POV, I would expect the extra registers from 64-bit mode would be quite useful for a VM. @andrewlow does the memory bloat outweigh the benefits of doubling the number of registers available? 
andrewlow		Intel benefits from having more registers in 64bit mode (and more instructions) but it's not enough to overcome the object size bloat. If you do the trick Java did with compressed references http://lowtek.ca/roo/2008/java-performance-in-64bit-land/ then you do get an overall win, but if you've done that you're really using 32bit objects.  PowerPC gets wider registers, but no new instructions or registers. It's more of a challenge on that platform.  I'm not sure where ARM and MIPS sit with 64bit, I suspect they don't get as big as win as Intel does.  In the end, the memory is so slow relative to CPU speed that having to move more is bad for performance. 
rmg		@andrewlow thanks for the explanation :-) 
rvagg		/cc @chrislea 
chrislea		It's an interesting discussion to me. Before I continue, I will state for the record that I strongly suspect I have considerably _less_ knowledge about the register state / pointer size / performance issues than @andrewlow does.  What I've seen from experience is that convenience and familiarity tend to trump, essentially, everything else. In this case specifically, the end user being able to type `apt-get install nodejs` or `yum install nodejs` and having the tooling of their Linux distro of choice install Node for them would, I am _guessing_, be more useful for overall adoption rates than putting effort into shipping 32bit builds into 64bit machines for some extra bit of runtime speed. I guess this because I also assume that if people were that concerned about runtime speed they'd be developing in Java (or something else that's not Node). I would also guess that most of these apps people are writing in Node are generally very I/O bound, so the runtime speed isn't the bottleneck.  This is sort of tossing $0.02 into the air I know. And I could be dead wrong if there are enterprise customers who really care about benchmarks. But without additional info, I'd continue to ship arch specific builds simply for the reason that it's easier to make everything "just work" in familiar ways to the end user when they are installing and updating. 
saghul		> MinGW  I would very much like to see MinGW (it's 3 variants) as a primary. Lots of hours have gone into making them actually work, and many people rely on them. Proof if that is that every time we break some MinGW build I get a pull request with a fix, which I need to manually try.  Supporting these doesn't require new gear, the 3 toolchains can be installed on a Windows 7 or 8 machine. 
saghul		Obviously my previous comment was regarding libuv :-) 
rvagg		@saghul from a previous thread you listed: - MinGW32 - MinGW-w64 (32 bit) - MinGW-w64 (64bit)  Which I'm guessing are the ones you are concerned about. Are the build instructions for these included with libuv somewhere or does gyp do a neat job of sorting things out? 
saghul		@rvagg yes, those are the ones.  As for how to build them: I found that autoconf is the easiest way. GYP can also work, but it's not officially supported.  The MinGW32 installer comes with a graphical package manager allows you to install some packages, so once you have autoconf, automake and libtool you can just build an run with `./autogen.sh && ./configure && make && make check`.  For MinGW-w64, I'd suggest to use MSYS2. There is an installer to get things started. Then in the MSYS2 shell you can install the 2 MinGW-w64 toolchains using pacman (they ported the Arch Linux package manager) and also autoconf, automake and libtool. Building is done the same (running these commands on the appropriate shell): `./autogen.sh && ./configure && make && make check`.  I don't have much free time, but I can try to help if needed. 
rvagg		FYI I've been messing with Buildbot and am noticing that CentOS 5 is going to require some work to make the tests a little less brittle across both Node and libuv. Mostly things work but there are a couple of tests in both that persistently fail and a few that occasionally fail.  I know this is a pain but I've run into two _major_ US companies already that are deploying Node into RHEL5 still in production. Ultimately it's up to the TC to decide minimum supported operating systems but I'd like to encourage them to seriously consider supporting EL 5, within a reasonable timeframe (perhaps till the end of the year when 7 starts to gain acceptance, which means enterprise is more likely to be running on 6!).  I don't want to share a URL in here but if anyone's interested in taking a look at the failures then email me @ rvagg@nodesource.com and I'll hook you up and can even give Node & libuv core members access to a box to play around with if needed. 
rvagg		Next question: llvm vs gcc, is there anything worth testing there or are we assured that compiling with both leads to a _similar enough_ result? I've seen many opting for llvm compiles because of lldb goodness and quicker compiles I think. 
chrislea		I've talked to a few people including TJ about this.  The answer is "use gcc when on Linux", even though you can build Node with clang.  The reason is that C++ doesn't have a stable ABI (which I always manage to forget because it's 2014 and **really** !!??!?), and so if people are building binary modules and their system defaults to using gcc, which it will, it's possible that things just won't work unless you use the same compiler.  So, use gcc, and more specifically use the gcc that your target distro ships with.  Annoying, but no way around it. So there you go.  On Sun, Aug 10, 2014 at 9:18 PM, Rod Vagg notifications@github.com wrote:  > Next question: llvm vs gcc, is there anything worth testing there or are > we assured that compiling with both leads to a _similar enough_ result? > I've seen many opting for llvm compiles because of lldb goodness and > quicker compiles I think. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/node-forward/build/issues/1#issuecomment-51740735.  ##   __ https://chrislea.com http://about.me/chrislea 310-709-4021 
bnoordhuis		> Next question: llvm vs gcc, is there anything worth testing there or are we assured that compiling with both leads to a similar enough result?  @rvagg clang/llvm has, in my experience, a greater proclivity for exploiting undefined behavior and that has exposed bugs in the past.  On the other hand, I've hit a greater number of compiler bugs with clang.  It's not all roses and unicorns.  If it's trivial to set up a two compiler matrix, then by all means please do.  If it's a lot of work, then I think it's okay if we stick with gcc for now.  On a related subject, do people have opinions on what version (or versions) of gcc to support?  The current baseline is 4.2 but that's mostly because of OS X and FreeBSD and they have moved to clang now.  V8 is [switching to C++11 soon](https://groups.google.com/d/msg/v8-users/o1wffwdnUdU/EKzlsTDd-KgJ) so middle/long-term, a modern gcc will be mandatory.  That sucks for people on RHEL5 systems but back-ports of g++-4.7 and 4.8 are available.  > I've seen many opting for llvm compiles because of lldb goodness and quicker compiles I think.  The quicker compile times are something of a perpetuated myth from llvm/clang's early days.  It was faster back then because it emitted lousy code.  It's mostly a wash these days.  I don't see an appreciable difference on the projects I work on, at least.  > The reason is that C++ doesn't have a stable ABI (which I always manage to forget because it's 2014 and **really** !!??!?), and so if people are building binary modules and their system defaults to using gcc, which it will, it's possible that things just won't work unless you use the same compiler.  @chrislea You may be thinking of the situation on Windows where msvc and mingw produce incompatible code (clang's msvc ABI support is improving, though) but clang++ and g++ on Linux (and, I think, most Unices) use a common ABI:  ``` $ for CXX in clang++ g++ ; do $CXX -dM -x c++ -E - </dev/null | grep __GXX_ABI_VERSION ; done #define __GXX_ABI_VERSION 1002 #define __GXX_ABI_VERSION 1002 ```  Where __GXX_ABI_VERSION - 1000 = 2 is the actual ABI version.  Version 2 was introduced with the release of g++ 3.4.  There are newer versions but they're mainly bug fixes and you have to opt in with -fabi-version=n. 
chrislea		Well then, I stand corrected, as @bnoordhuis clearly knows much more about this than I do!  The only reason I'd heard of to not use clang was the possibility of ABI compatibility issues. If those aren't a real problem, then so be it. I've made builds with clang before and can easily do so again for the next release into the NodeSource repo if people like, I just have to uncomment one line in the build stuff.  Having done it before, I can tell you the compile times are certainly faster with clang. I don't know about absolute performance numbers, or lldb goodness.  Anyway, just let me know if we want to do this going forward for the Ubuntu / Debian builds. 
trevnorris		Because I'm a dork and wanted to see the difference. Here are clean builds using `gcc 4.8.2` and `clang 3.5`:  ``` gcc $ ./configure; /usr/bin/time make -j8 564.04user 36.31system 1:24.62elapsed 709%CPU (0avgtext+0avgdata 355800maxresident)k 26976inputs+367288outputs (162major+16529028minor)pagefaults 0swaps  clang $ ./configure; /usr/bin/time make -j8 406.75user 26.37system 1:03.04elapsed 687%CPU (0avgtext+0avgdata 128232maxresident)k 0inputs+196184outputs (0major+14644183minor)pagefaults 0swaps ```  So, saved me 20 seconds :P, but the memory usage is noticeable (though not like I care since I have 16GB RAM). 
saghul		Should we also test some libc other than glibc on Linux? Sabotage Linux is based on musl, which makes it a good candidate as buildbot if we want to go for that. Thoughts @bnoordhuis? 
bnoordhuis		I don't know.  I suspect that the number of Linux systems that don't run glibc/eglibc and are not Android is a really tiny fraction.  But I'm not the one maintaining the build matrix, I just provide input.  @rvagg et al. are more qualified to comment on the cost/benefit ratio. 
kkoopa		What about [X32](https://sourceware.org/glibc/wiki/x32)? That gives more registers while still using small pointers and longs. 
rvagg		I'd like to know if anybody is actually using it. Ultimately this is up to the libuv and Node core teams to dictate and they may have interactions with people compiling x32 .. or not. 
kkoopa		Probably nobody is using it yet. V8 only got x32 support in June this year, with the release of 3.28. https://code.google.com/p/v8/source/detail?r=21955  However, Node has now integrated v8 3.28 (and might move on to 3.29), so x32 should be highly on topic. 
bnoordhuis		I've started work on a x32 port, see libuv/libuv#1.  Patches for node-forward to follow once the necessary changes land in libuv.  x32 is still a rather niche arch.  I'm perfectly fine with leaving it unsupported for now. 
ForbesLindesay		I don't think this has been mentioned anywhere yet:  It would be really good to have tests on windows run both on local discs and on a network share.  It's really surprising how many things break when you have windows but with unix style path separators.  This applies to azure web sites. 
rvagg		Ugh, any suggestions on how to implement this on Rackspace or similar Windows cloud provider?  For now, just getting the Windows builds _green_ is enough of a job! 
ForbesLindesay		I'm not sure.  Running them on azure web sites would essentially give you that by default, and I would have thought Microsoft would be willing to donate the compute?  I would guess you could share a local folder over the network and then just cd into it via its network path rather than its local path.  That would probably work. 
pquerna		@rvagg: On Rackspace you could do this by running two instances, probably in an isolated network (aka "Cloud Networks" in Rackspace terms, which is an isolated L2 domain) for security, one acting as a server, the other mounting it.   
qbit		What can I do to get an OpenBSD build env going? 
bnoordhuis		@rvagg Getting some BSD test coverage besides FreeBSD would be good for both libuv and io.js but I defer to you on how much effort it is to set up integration and whether that's worth it.  I would restrict it to the latest release, currently 5.6, and maybe just amd64?  I honestly don't know how many people run i386 openbsd these days.  @qbit Can you commit to anything when it comes to support?  Failures on openbsd wouldn't block a release but it would be nice if we could ping you to help investigate (and get a reply within a certain time frame; that's the key point, really.) 
qbit		I can commit. IMO the target for OpenBSD should be snapshots as that is the target every developer is using. That said.. release is better than nothing at all :) 
jbergstroem		I know this is probably out of scope for most of you but since this is critical and is currently running nightly it'd be good to have an eye or two for review/questions, @nodejs/build. 
thefourtheye		Just some minor nits. 
jbergstroem		First weekly finished; here's the current size/state:  ``` 70G     ./periodic/daily.0 636M    ./periodic/daily.1 715M    ./periodic/daily.2 22G     ./periodic/daily.3 102M    ./periodic/daily.4 114M    ./periodic/daily.5 2.9G    ./periodic/daily.6 181G    ./periodic/weekly.0 2.3G    ./archive/nodejs_logs 191G    ./archive/jenkins-before-job-deletion-plugin 118G    ./static/dist 6.5K    ./static/benchmark 604G    . ```  (`archive/jenkins-before-job-deletion-plugin` will be deleted shortly) 
thefourtheye		Instead of `daily.\d` would it be better to have the date? 
jbergstroem		@thefourtheye can't change that -- its how rsnapshot keeps track of rotation numbers (see `retain` sections in `rsnapshot.conf`). 
joaocgreis		Rubber-stamp LGTM. The bash scripts look good, but I have no experience with rsnapshot.  What should be inside of `.jenkins_credentials`? Github token? 
mhdawson		LGTM but again I'm not experienced with rsnapshot.    Could you send me a copy of the benchmarking backup file, I'll load it up to validate we are capturing correctly. 
jbergstroem		@joaocgreis yeah; something like "jbergstroem:token".  @mhdawson just mailed you. 
thefourtheye		LGTM 
mhdawson		Except for the possible issue of getting a log of emails, why not do both ? 
rvagg		Lets make a new email alias for this, we can easily put it @ iojs.org for now and migrate to nodejs.org when we get around to. Easy to do it here: https://github.com/nodejs/email/blob/master/iojs.org/aliases.json  Re slave status, there is prior-art here: https://github.com/nodejs/build/blob/master/tools/jenkins-status/jenkins-status.js 
bnoordhuis		Would a hook for #io.js on freenode be possible?  Most committers, yours truly included, don't hang out in the build channel. 
Starefossen		@bnoordhuis yes, that would absolutely be possible. Does you know if freenode provide some kind of webhook interface, or do we have to connect to the room using some sort of irc client? 
bnoordhuis		Freenode offers no webhooks that I'm aware of but connecting should be easy with node-irc. 
jbergstroem		+1 IRC as alert endpoint. I feel we would benefit from finding software that helps us here though ("if, then, else") -- getting an alert every 5 minutes is never a fun thing. Can't say I've used too many commercial monitoring websites, but perhaps theres room for a ~$30/mo thing somewhere? (lowest requirements would be being able to parse json responses) 
jbergstroem		@Starefossen ping! can we get an alert to #node-build? 
jbergstroem		This has been successfully running for a while. Closing. 
mikeal		The `versions.json` file we're using only has io.js releases in it. It would be good to know what the unified json file will look like so that we can test it.  https://github.com/nodejs/new.nodejs.org/blob/master/source/versions.json 
rvagg		doing that _right now_! 
mikeal		awesome :) 
rvagg		:boom: all done 
mhdawson		Added linuxOne as rhel72-s390x, job ran ok here:   https://ci.nodejs.org/job/node-stress-single-test/876/ 
jbergstroem		LGTM -- I guess we're waiting on the new osuosl setup to land so we can redeploy be/le stuff before adding it to stress? 
mhdawson		LE is already there, and BE I was just about to add, we should be able to get the new stuff going while making sure we always have a good set of machines. 
mhdawson		BE added, successful run: https://ci.nodejs.org/job/node-stress-single-test/877/nodes=ppcbe-ubuntu1404/ 
jbergstroem		Thanks for this. Looks great. In the above workflow I kind of miss having tests being run against a PR before reviewers start crunching their bits. That way, we can pass linting and testing feedback straight to author before reviewers have to point anything out. 
Qard		Perhaps there could be a separate linting process that runs automatically on each new commit to a PR? The final CI step would still do its own linting, but the per-commit ones could just provide some quick feedback.  I would also suggest splitting CI runs from release. We've been avoiding automatic CI just for security, but it would be nice to be able to just do a quick sanity check and send to CI without needing the full review.  Not sure how possible it is, but it'd be cool if you could use some command to tell it to try a release and it'd count LGTMs, check lint, and check CI results since the last commit in the PR. If those checks don't pass, it complains. 
jbergstroem		@Qard automatically running PR's from collaborators/approved users could be a start. 
jbergstroem		Also, homu doesn't seem to support jenkins. 
orangemocha		I have done something similar on jenkins.nodejs.org with node-accept-pullrequest. It does this part:  > homu runs tests and lint and lands on success.  It also automatically rewords all the commit to include the PR url, and the Reviewed-By. The content for Reviewed-By needs to be entered manually on the form. It shouldn't be too difficult to parse the PR comments and recognize the LGTMs (or another syntax we choose).  It doesn't do any of the highfive / assigning reviewers stuff.  
Fishrock123		So, just a thought.. This will choke if the PR has conflicts, right? Sometime's there will be a PR that is valid, and we'll still want to merge, but the author has gone AWOL, what then? Open a new PR? 
orangemocha		If you need to modify the source for the PR to merge cleanly, then yes, you'll need a new PR unless you can push to the original branch.  
bnoordhuis		> Also, homu doesn't seem to support jenkins.  It doesn't but I don't think that's a big obstacle.  It's pretty agnostic when it comes to CI systems, adding Jenkins support shouldn't be difficult.  I'd be happy to do the work on that. 
mhdawson		In relation to the comment by  @jbergstroem giving PR authors the ability to run tests across the platforms, at least in some specific cases,  is likely to be needed.  If the author only has access to a subset of the platforms and the tests fail on one they don't have access to we likely need to give them access to the failing platform to investigate/resolve and once they believe they have resolved confirm tests pass across the core platforms.   
indutny		We may use https://github.com/indutny/caine, and modify it a bit to support LGTM and CI. Should be very easy! 
retrohacker		This sounds fantastic! :+1:  Great suggestion @bnoordhuis  
brendanashworth		I'd like to try and take on the reviewer assigning / highfive / caine, is that okay?  > although it's an open question how to handle multiple LGTMs  As a suggestion, the golang project uses a +1 and +2 system, where +1 = LGTM but needs another sign-off, +2 = LGTM and can land as-is without more review. 
bnoordhuis		@brendanashworth You have my blessing and encouragement.  The +1/+2 approach sounds good to me. 
jbergstroem		sounds like a plan 
orangemocha		@brendanashworth that sounds good. How would that be implemented? FYI, there's also a Jenkins plugin that can be configured to trigger jobs based on comments given in pull request comments: https://wiki.jenkins-ci.org/display/JENKINS/GitHub+pull+request+builder+plugin 
orangemocha		We now have [merging of pull requests via CI](https://github.com/nodejs/node/wiki/Merging-pull-requests-with-Jenkins). Not automated though, in the sense that someone needs to trigger the job from Jenkins.  Do we want to keep this issue open? To track triggering the test jobs via comments? Or anything else?  FYI I also logged an idea at https://github.com/nodejs/build/issues/176 to potentially simplify the selection of reviewers based on LGTM. 
bnoordhuis		> To track triggering the test jobs via comments?  That would be a great time saver.  I'm alright with opening a separate, more focused issue for that and closing this one. 
rvagg		- Probably should discuss how to include node-gyp and NAN in Jenkins, we've had requests for both projects and it makes sense, particularly for NAN which has a big test suite (node-gyp has a ... smaller test suite). - I've been pondering our ARM hardware and would like to discuss how we might go about ensuring that we have hardware test coverage that is as close as possible to what is being used in the wild for Node‚Äîfor IoT, hobbyist, etc. users. Consider how newer platforms like Pi Zero have the potential to change the landscape. Also think of MIPS and how we have zero coverage there. It might be something we can defer to the Hardware WG, maybe they can do some surveys or perhaps they have existing data. 
jbergstroem		re arm/mips: I have a tessel 2 at my office. Not doing too much at the moment since running tests on it is painful. So far I'm looking into their build system as well as compile testing. 
orangemocha		The meeting is confirmed for tomorrow (Tuesday).  Hangout on air for participants: https://plus.google.com/events/ck9tfcvpo3aq2sc5c2k3dr0lqpc To watch: http://www.youtube.com/watch?v=9aVNXVazrVw 
MylesBorins		It seems like the link is to view the meeting not join 
orangemocha		https://plus.google.com/hangouts/_/hoaevent/AP36tYc0wOZKwtL2oFj2VAJl8ifMRPTtwXhHAZZRZXdFpEu0Gp4e_w?hl=en&authuser=0 
orangemocha		Meeting minutes are here: https://docs.google.com/document/d/1zFMmIYUP1tA_YS_sx7P0-vZQ3zy3sOYGSG73otRvoSc/edit 
jbergstroem		Meeting was had. 
Starefossen		I have been making progress on smoke testing popular Node.JS drivers which has external dependencies outside of Node.JS in nodejs/smoke-test#3. This is using a Docker setup where dependencies are spun as separate containers and linked into the container driving the driver's test suite. Would be interesting to discuss with @jasnell if/how this can be integrated with citgm.  As a observation; test coverage of io.js version for the drivers I have been integrating has been very spotty so far. 
rvagg		I've started https://github.com/nodejs/secrets as an experiment in sharing secrets. Certificates and keys are pretty tricky to share in a Google doc and we also need different levels of access privileges .. so I'd like to try this. It uses https://github.com/ConradIrwin/dotgpg which is just a wrapper around GPG, I'll need GPG keys of @jbergstroem, @kenperkins, @mhdawson, @orangemocha, @misterdjules and @joaocgreis to add access and replace that shared spreadsheet (armor please, `gpg --export --armor <email>`). I think we should also add @Starefossen who has said he is able to help with Linux server admin and is already on the Build WG. Also, anyone else on the Build WG that has time to help with this kind of thing, please open an issue or pipe up here and we can discuss. We can probably do a quick signoff of people that have the different levels of access during the meeting. 
mhdawson		I like to add discussion of  machine for benchmarking to the agenda 
orangemocha		I'd like to have a discussion on how we can make the CI runs more robust, in particular have more redundancy for the slaves and for people who are able to manage them. 
Starefossen		Yes, I can absolutely help out managing the Linux servers. Here is [my Keybase PGP key](https://keybase.io/starefossen/key.asc) ([proof](https://gist.github.com/56dad17439b5be4e73ca)) with the fingerprint of `2E95 E0BE A257 E127 628E 1CF4 8525 B593 4114 1813`. 
jbergstroem		I can't join this week either. Sorry (btw I'm CEST for the coming two weeks). 
mhdawson		My key   <PRE> -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2 mQINBFW3rsMBEAC5TTTGaVn8NS/Qb769FvlFktbV8W5K4D2gkEd+JQ5AENqQys7y tk8hGmk+E38CW8dqYqH6vREy8KMLRbjSNrYbschcH4pTJ5UPrX2SRpHf49XnFLeJ +cQrJ26PqtO+sdeyFqmEPZK1NJbjZSb8IkeWevQQ72Ns5j8w4UEfR1Y3Sd68SsyE SQMX9XyL++dcaL5MNNmDmkwEbIAc98FFxXXqz4ODSj8vAg21s/XPU+u72D919U5z Y38zxEVRfVoHtIJu5ubbmzN9eFvFnTBNaZpWVdbgJern4+z65HnbOvWud0DWjQAT nJnAgFeCiyEcVk7MW+UWau8vUHp1OXMnv6V/GCCWAKhaWVaRoXHCM3UCwV/Aos03 Sv+4GZlSn46kfOlifYtjrDrGQupQv5hLHzCoQ3rEnWrbonlgYeQfL7oOMH+le4ER UJzmR8c8z+rhFEoC8KhAV8lyTuEhI1IYtIQW6n1S6wcUTexDwuUexfpwkEv4a8VN kdmCAR5jaYFbWS1DIP7QuxhQO7qPYylefcLHapJ1nRWKBLB3Q241Gyy3FZk+g8nX SIYwbfo+ca3rHCFSlhf3kOkylIvO4jKEpat+R9mmJ6eBLXVutWXLH2iCQPVs7RQx t/8lhth7vvOp/Q9l5gbs3VAUoo6ttllLqRBkkAPs2Otd+k49TLf+URf5AwARAQAB tCpNaWNoYWVsIERhd3NvbiA8bWljaGFlbF9kYXdzb25AY2EuaWJtLmNvbT6JAj8E EwECACkFAlW3rsMCGwMFCQlmAYAHCwkIBwMCAQYVCAIJCgsEFgIDAQIeAQIXgAAK CRDidvkQpBQZ8PxGD/9fj9osdDG9cZRNMnH6shu791v3T5vMtYUZpVHPgi2YmxxG r67p1+vUiajryPtQk50ncxlMB5fKaLTWyzZD01T+8BZb0X2J6944AQVjc+tV8QfQ cPcdhubxGmrj5a+f0w/byijZovne1XKgIpJFFcAfgpz5UGb7ZKSw3/kvTodtcEeP Gwbkga/kpB1YwrooXm3F63lXBHfM9EKZVyapB6FWAXe9WDI6gIO0ewN+8jM/6Qv1 rTtBcW8JI5lK3AQgVyOf/UaSzxiTIrq4obvJELXl5npvepNyWIN64XfvQsCDYYZs ODun1IgPF+JmLgsfMr3pC0athWz8hg6m5Q6cJk0akvgyHA8C/Rlp8xnBm7z7tCcB 9qTGV2sw/01AUMjdnd51HYAN1teaWL5ubS0OgFKqko30dJdQSWoYEfi0cqkUQs3/ K2eQgZvox9s65WWbwrlcjAhSCOQ3lBsVsys8aZrM1C1u4RK2CLvoIur3z58bgVlp /Uiu1THv3KZRalPEr3mto0Mw5ziu1k5M16Mnh7COgLmPcpNFggbGZunqxB/kgnLK pX1amAgfjqMuuDDSK58Y0sui3qMBeoIUSUZTMBxpMfhFN+TEBdlxy4I+tZfidpGv yLp5fWc5e709DsEu4zBIP/d368ma7ljPlFX9n3Z/lOGbAsSflIBH1SQMzhaxZbkC DQRVt67DARAAu8DQcecW/HIF5WnbRHu0Kes/I0ESUtTY5vHiwPFEonsZDN39AssM HXgJjugz/A8wSlRuFlXBeBW/MxXA2Z+GsbPs9Rg7PEx/uMbdKq4JbNpEkjSazEQz 3lfM1gRfxbIGtQNoiFQiTQPWHAtanaHKxDz9IJ3EYCiAbXA2VZrgmJ6R2LUA+llb sWQfJIxzqkbKV5RhadBaTIYOQCesisoZ/AlJzcFcug3DzuNEYfN7lOFtZIE7nC/R C5V3xRCDLSxSPPKyu1+m6Vs5owUbEGayHjhDehaEZS6tFEURLmvzygMlKnOhoc3Y gZiiRhBMvvzsL78+b5UAul/KlDtrdqGcJhKscYlR8QV6kdP3J3bZ+nlVU1NJBhcj Z6RIG8nRXOFtmvFImFeJsCiubaU745LDIJyyMAxEpoGA5IhrxxmNCqAAxU5F+aJP HuNDfvS9lzXX8dQ5qyk0IRiYxvN4hqbMoqGrckrmwUmJd3zIEqWBvX8sQLzEkQE1 637XW7qrBxUYOIxaT1qHQSwF8buxDjjPYO24YskVugoKDrpr0fCNxBrYbcXXThPW JPFsrCbYQCLfu8BrKQd+2LC8IZBJndPhZDhCNsXXuIVQ343akhwoTL6Mjq0Ly+Do JPWVF5fggi3z6h6Zicu2lBlqioh0iNf8F8scxNfeTIdC73nwL1oKc/cAEQEAAYkC JQQYAQIADwUCVbeuwwIbDAUJCWYBgAAKCRDidvkQpBQZ8AsOEACjqyhcfjK5opBj 6hVsbxQXIuUVABQUz1sdXcGi/fQnJ4CaUeLWrvhuVplRx1Sg8JxP2BZafbDHnkG4 d4IEszKoDyDezsA2OU5dammiEqexkMkX/gmkKz28ngtXQL3hmPo0YCu2ZQmUFOet 4xcjlgmP5xx2Kk+435wrLj54TPy1zSh7TXoW1ylpTlPmaLIrI/H5bsq72lyitR6I VhFlDcn2ApOh0Kfkuy8vKtQEqPyPEKTqw4prRjRkjjVGX8RUqZ+N7Vje5nvrhDsx O9BI+u+rqYjVerlajG2hEJUWk6xhZwKiRrgj+4Oa6cXVRmMs4sy4r0nKKIY2zB9u 6NSgdKIvDBz9FGzOukxOT4s6lBo/NY495k8WyMPlaMS7YJWFvKtKnY3DZMSOxmxi vZty2jiCGGAk6LAdq21SAKWVGZg/fYH0LWX0Nyx9+Jz5i1l+W0E8JPpRVQP7N1tQ 3dtLyx7Jv+d5No4nV4surqqkLKRpA+A8ow99NyeP8QZeV+/8E/jYjRu75n77hqpi BpeMDWf+M20hJbRfgWOtXevQZ/kG4yhhSOXJHGdApERnEGr2pl183G2RpzQloMpf qV/o8JSEPdXxUGKVfLB7w19br47xdGWx81G5mTE9kkEdh2FmKSMI0SozIeb/XIow djcmJ5aEyEjn5G0wmxUytc9dhkYZ/Q== =93us -----END PGP PUBLIC KEY BLOCK----- </PRE>  fingerprint 2EBA B097 42A4 DBC5 E9CB  487D E276 F910 A414 19F0 
rvagg		@mhdawson & @Starefossen: I've added you both, pull the latest from the secrets repo and try something like `cd build && dotgpg cat windows-servers.txt`, or you can also just use `gpg --decrypt windows-servers.txt` to show the contents too. 
joaocgreis		@rvagg Here is my key. Thanks!  ``` -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v1  mQENBFW4stUBCADLUsZS8lS8KdX9/oCACXdXWiuroevCsrowBMzz0vS4hg2X1Ztj 9Rhyz6IGPinuKtrCnNBNUWU95cGCBROVP0g0KOdMXQjHGAUKvvJ7FNlUwvBxPUDQ 960QxvJw/mEi90T4bRa5Pzv1WcwvW2IMaAgZXe1C6ePVuYjE3CzFoxxO7xQnnJI9 L3OXqmprwrvU68hrveZS7u9Yu9U27AuUgBgpwAvI2ANZsw7jK8eI61zjomAhH7CY VeSlr+yOO1mEDRwhYjDEC5zFPNDp+hMcFZ/BytY/T2RzFd6tkzA06VuF55Yw3Svb SQyJ9qTt61HRdFR4xjx6Zt3m4tzBSXmCLXhNABEBAAG0K0pvw6NvIFJlaXMgKGRv dGdwZykgPHJlaXNAamFuZWFzeXN0ZW1zLmNvbT6JATgEEwECACIFAlW4stUCGy8G CwkIBwMCBhUIAgkKCwQWAgMBAh4BAheAAAoJEGCbjO+rq/1tJCMH/iJ9sR7BlrOU kOIkMSKSq97bHJM+dAjM93qaFRElOANbOiDCF63RrCqt6794fenlPWP5rJJLlqYD pbWjL3Hge/ZuqaqZGoRAFvF4/UfTEn8R9N7tYYBr4J8UfVG3yCX0mn83a9Z260T8 terUc9lPsGaaSTo2nBkD3Uvwfd0/zBzbER6MDgoK8x4d1VFzoYyrsbaxkXfXjxLT JTwcloxf4iX9MWKKTE3/tsEkR1n1dsKzXf45Mzyxq2zzs3yUGHcdPH2ILypxQAIG pId92bScexzhEgEUGR9xaG9TPnl0zjdOIiX0AYo7OUEEBGq30lbUbjagsmW/c3ou szKPnomEGnu5AQ0EVbiy1QEIALk8uCJnFhPhc01IqSpYiINxNXR+9J7OvpQVy98e WbY0Rl0Y59tGXuDznwqYk7oX6EJexgmhYpvD/5iDEGk6uDsVzdomXg9808SZ4w6x A5WI98Sx42sc/Jg8FDCPZ/c1hPValjcNwozRgHp1yWBLuZLbjmVqOko7ixq4TyzR TQ/cVus7DhMgMDQxCLRTv6xHplVFt3NiWg3Ppd/LCyYNocxDxF5CMD+LbO5HL52g XJl9IIMIVYD2M47ygmUbZUqjy6iqhWAoKWzNVoewz9CFycO2YcSpDdgJBtngcMxm knModYH4L1h35oIU/TZkm0nlppNBpp7M6pF/jz6Jk7z1lxcAEQEAAYkCPgQYAQIA CQUCVbiy1QIbLgEpCRBgm4zvq6v9bcBdIAQZAQIABgUCVbiy1QAKCRCjxGBAV+aY tUN3CACoDawWkGg/dFnv3CWo5xLnUajQwN1fDNLuxD8vMfPVV5wKw2WWVv7q+C3y HMwm+LUibxoHcSkNBuXbzKjSi3dTYi3a57NKZVETnQ7wZA5l0Nvk4/cdmR+VYti9 huYA78opxxNVGbIIt2lje0HbKb6KK7CIVDvnXqasrP1B8vZ37g9WCXVCj3Hnes6z K1NaW+HzTGM9d4Tsy9lj7ArQ+Ajvvqt7MLQw/QP2pPTqU/L10Am2QXPLJKH6xi13 l21BsRdv38v+kF9PmC03tYlsHIlyTvXh+HFy02rCRHcosnmY0VcQigWAzMfQUGci ZJcRIbs7aeY5TnfaNvXxGJOPKDJdewcH/2yKl7gly7/Aegwe+p9FVi5BdGDexjaX hCp23d2KzO1AqI09jQZaq0PXf1JIah35tC1jAAXCfHY74RJcgXw6BpSnSjm+0rlX 4RqG1ZWMFIxabyr3jOssAXyHVggHMaJP9nHZLnfYbkcuXaZgm+JKvxGNN7s9hiWk vjLIXFJJ2fZfbglRchn1NFiUQJOlStQ3grK36dAeNDdiKpmehw4ACMXV+ZEKnVg/ TwEWh9V2T9MZ1KAvLuY9clg70i8TfrRvWZe+bbcdFklrN9XSEf6TOEskplr8WsFo P64FoA7buZcY41VNuxH/8TTnfEStUSsNoDQhWhLkcEM0Q9oxBfU7nI0= =/eTd -----END PGP PUBLIC KEY BLOCK----- ```  Fingerprint: `1BD6 A351 554A 3622 5EB8  7621 609B 8CEF ABAB FD6D` 
orangemocha		My key:  ``` -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2  mQENBFW43qABCADTsCgDvL3RuCgsQ1jZSxAbHnbeVtBS1kCoxHHpeWdnP6SAOF+u 9wxbTV3+YTLRQjlw4+6guJKLBdEA6z2vnVQZi0qiVM9ABQDWAzhHZN6n8ZHfr9eb elVXi372aCOzr3wJTeqV5F8Sozxo22Qrv9ioZsCLsUU7iotyfcmj9R0IFkx/CiVR GuAx9y7w9c8mxfm0Y5CshuzGEbIL4Y8HakFoUNcFcgdODCBdzm0v6vHPQi8HTq3p R/U5ptMJBD6VrULF1iWP9DXFtvhlyDMAepioHe+Cu7/uo61ZxskgZ3GvrmBUOR1w jIP2RlVaFteRZG8duzSkWHmaGC3CBz2mh3lbABEBAAG0KkFsZXhpcyBDYW1wYWls bGEgPGFsZXhpc0BqYW5lYXN5c3RlbXMuY29tPokBOQQTAQgAIwUCVbjeoAIbIwcL CQgHAwIBBhUIAgkKCwQWAgMBAh4BAheAAAoJEIxBbdF36Uir/RsH/jDtPeCkdWjv l7VpooXjWzO8TSUT5GpPNz1382JOSQGNnI2Y+4QMI0gxhBcOYU2JkaL/32oSPg8X eemYjowZvlpm2uN4WKf/fqy9ZQ5nda/mYTUZ6dt6r6hqqEuVmsALzUUf/FSfmq6V 94a0pahKIhXHsY4bFnr9cBy7QJot4h8gZ2pm7n4cpQiIICNzaMVnGEEF/Pd/ksfX Le6DumU7ZYexCXl/i04wdatDJ4WPdwH9qq6/Dhc5Ev43RJNyzYXVhhx8KNabyYHL I6+kh5CF4045lsQ2EJCrSj5/H8pxi5skQuEA8bNd6NQrqZsBqPTmN0kFsU6UgQ0p phbqDVT2l2m5AQ0EVbjeoAEIAN7Ev+nJgufzaVq21ZmPKxOaiTitGATIwElFtnnN s4JdwejcIyJSqoGhfmqN663eqdeNsAuu73Hm4JkxK04C09+ElVBSDNkeDlLkgAkw grpNIx1ZdepfGLZm33hu4VGRzUZ/Aqa5RUm9YVBXc11bHFsPJnXKNkl31PPTDYHK V6nyQbAjSLkARUilhKTP5pFaXUCMz2CSbjHUPJLRFu7j2wvK1W4RtPk8R3I92UXD 4lz1Xmra5+OOCYHTzHvf1Csy5jOIAxSORBwp1wqRX8Q7dPVU/ybxy2+zdxBxZAKq jhOtHrg2WoRIM9NBYHTECBWVMPnnGiv55JdAub0G1cMOueMAEQEAAYkBHwQYAQgA CQUCVbjeoAIbDAAKCRCMQW3Rd+lIq4DQCACYocpvmiFOJX+YjiNikOntptU2u3K5 JGKCXl8dfgIWRuKQe94Irat4+ulIoLzeXsZMiMzze1ZW9iCR3Vv8fE2HXuIyq0x6 4ddiqPSB3R5P5/GsvJNe7fNDNYd98/huXsJOAcuyLM+D/4O/5bIKQio+lm1BPkuN p3UjqQB4xVjm3q5V9xJL9syN4QzWK9P2iWbkfnQrkTAgiZIttWeCN255ahDIJqVB 4Ycrm6Iwi8dpSDIm1eDu8mEAnrackSYLm773/MxG0rvXaLY7eulXHwytpiBhhs72 RhkLFyyNA4PJtuACQQOSZyhnqlGmuBx59Fv6VUxjkW9T2JPPsfXi6SU4 =+g6a -----END PGP PUBLIC KEY BLOCK----- ```  Fingerprint: `00D9C91CE126B50D2AE6F0F68C416DD177E948AB` 
jbergstroem		My key here: https://keybase.io/jbergstroem/key.asc 
rvagg		added y'all  also put @orangemocha and @jbergstroem to the build/release/ directory which will have to be a bit more restricted but we should discuss the exact policies around that at some point in the future 
gibfahn		cc/ @nodejs/tsc please review (more info at #926).
rvagg		love it, thanks @gibfahn, lgtm
rvagg		Agreement in today's TSC meeting on the proviso that we save current config where possible in git. So we should probably do a checkin of config.xmls here first then we can move ahead.
maclover7		ping @gibfahn 
jbergstroem		Sounds good to me. I can look at creating a new subfolder in the secrets repo with keys. Speaking of; who lives in `benchmark-admins`?
gibfahn		>Speaking of; who lives in benchmark-admins?  @jbergstroem Will be decided in https://github.com/nodejs/build/issues/641
jbergstroem		@gibfahn: ok; i'll wait until that lands before creating the group then!
Trott		Seems like this can be closed given its long dormancy, but by all means re-open and/or comment if I'm wrong about that.
refack		Superseded by https://github.com/nodejs/build/issues/1665
rvagg		lgtm 
orangemocha		Folks, I'll be at the VM summit at the time of this meeting next Tuesday, and I think @rvagg , @mhdawson will also be attending (in person or remotely).   I don't think that should let the rest of the team stop from meeting, but it would be up to the team to decide. Suggested options: 1. Have the meeting with the current schedule (facilitator needed) 2. Reschedule for the following week (and shift the recurring schedule of one week) 3. Skip this occurrence 
mhdawson		I'll be a the vm summit as well.  
jbergstroem		I can facilitate, but we'd be a pretty small gathering. Perhaps do a hands up so we know? (‚úãüèª) 
jbergstroem		Unless the summit people find time I suggest we postpone this meeting. 
orangemocha		I take it this meeting was not had. 
rvagg		Yes, this has been part of the original _grand plan_ bandied around and I even mentioned it on the last NodeUp.  Unfortunately, the challenges this involves make it significantly more complicated than just building and testing Node itself for two reasons: 1. Trust: it's easy to isolate builds of arbitrary code on Linux but this is the least interesting platform for providing pre-compiled addons. For Windows and OSX we'd have to use throw-away instances to safely isolate the build process for _untrusted_ code. The easiest path to dealing with this initially would be to have some kind of vetting process for packages to be included on the build list just as a protective step so we don't let just anybody compile on the machines we provide. As an aside, I have no idea how AppVeyor deal with this problem but it might be worth investigating to see if they have something we can duplicate. 2. Dependencies: many addons require additional libraries to be included on the system because often they are simply exposing those libraries to Node. The most egregious example of this in popular usage is node-canvas which needs Cairo. The simple-path here would be to not even allow complex addons into the build system which would provide an additional benefit of encouraging addon authors to simplify their distributions (by including libraries as source in their distributions for instance).  I'd love more discussion on this though, being able to provide a top-quality binary provision service with easy hooks to `npm install` would be amazing. 
voodootikigod		@rvagg - not meant as a "go implement" but a start discussion. Apologies if conveyed any way other than that. I think it would be a great thing to whiteboard out and definitely agree with the security concerns. Is the grand plan outlined somewhere that includes this? The read doesn't have it (worried it may have been left behind in a transfer) Would love to use it as a starter for discussions.  Cheers  _Chris Williams_  @voodootikigod http://twitter.com/voodootikigod | GitHub http://github.com/voodootikigod  _The things I make that you should check out:_ SaferAging http://www.saferaging.com/ | JSConf http://jsconf.com/ | RobotsConf http://robotsconf.com/ | RobotsWeekly http://robotsweekly.com/  Help me end the negativity on the internet, share this http://jsconf.eu/2011/an_end_to_negativity.html.  On Tue, Nov 11, 2014 at 3:03 PM, Rod Vagg notifications@github.com wrote:  > Yes, this has been part of the original _grand plan_ bandied around and I > even mentioned it on the last NodeUp. >  > Unfortunately, the challenges this involves make it significantly more > complicated than just building and testing Node itself for two reasons: > 1. Trust: it's easy to isolate builds of arbitrary code on Linux but >    this is the least interesting platform for providing pre-compiled addons. >    For Windows and OSX we'd have to use throw-away instances to safely isolate >    the build process for _untrusted_ code. The easiest path to dealing >    with this initially would be to have some kind of vetting process for >    packages to be included on the build list just as a protective step so we >    don't let just anybody compile on the machines we provide. As an aside, I >    have no idea how AppVeyor deal with this problem but it might be worth >    investigating to see if they have something we can duplicate. > 2. Dependencies: many addons require additional libraries to be >    included on the system because often they are simply exposing those >    libraries to Node. The most egregious example of this in popular usage is >    node-canvas which needs Cairo. The simple-path here would be to not even >    allow complex addons into the build system which would provide an >    additional benefit of encouraging addon authors to simplify their >    distributions (by including libraries as source in their distributions for >    instance). >  > I'd love more discussion on this though, being able to provide a > top-quality binary provision service with easy hooks to npm install would > be amazing. >  > ‚Äî > Reply to this email directly or view it on GitHub > https://github.com/node-forward/build/issues/12#issuecomment-62611544. 
jbergstroem		I think we're still too far away from this, but once we have the bits and bobs in place to handle a wider circle of trust this is very feasible. Closing for now since the issues mentioned travels wider than native modules. 
mhdawson		@nodejs/build please indicate if you can make it by adding your thumbs up or down above.
gibfahn		We currently have no open issues, if people have stuff that they want to discuss please mark/raise things appropriately.
rvagg		I'll either be in transit or very jetlagged for this one, probably won't participate.
piccoloaiutante		This is a little bit too late for me so I'll skip it. @mhdawson in section "When" there is still 4pm EST, you might want to change it to 6pm EST.
gibfahn		>in section "When" there is still 4pm EST, you might want to change it to 6pm EST.  That was my bad, fixed (in future feel free to change it yourself!)  If Rod can't make this meeting, is there a chance we could move this to the eariler slot so EU people can attend? Is there anyone who was relying on the later timezone?
mhdawson		I'm ok with moving up. @jbergstroem does that work for you ?
jbergstroem		I can make it but happy to move. I have some work I'd like to finish regarding mac setup so I don't have to drag my tail into the agenda.
mhdawson		Ok lets move to 4 EST.  Even if we don't have agenda items I think going through the updates on what we are working on is good to give people outside the build WG insight into what we are actively working is good.  So it might be a short meeting but lets still have it. 
mhdawson		Updated foundation calendar and youtube event and this issue to show 4 EST.
gibfahn		cc/ @piccoloaiutante @joaocgreis time might be more manageable for you guys now.
piccoloaiutante		Oh yes, for me it's better and i can join
jbergstroem		Bad news -- sucked into a meeting. I will make a summary here.
mhdawson		Link for participants - https://hangouts.google.com/hangouts/_/ytl/y4OQMSvMpVpL1xSac4fdeA7i_bTFUWvXpDeUUN9HKfA=?eid=100598160817214911030 
gibfahn		Minutes: https://github.com/nodejs/build/pull/767
Starefossen		LGTM with Johan's nit picks. 
mhdawson		@jbergstroem addressed all of your comments.  Let me know if it looks ok now. 
jbergstroem		LGTM 
mhdawson		landed as https://github.com/nodejs/build/commit/1e4941d1b29c0c370fd289ca3bd289f51694c5b1 
jbergstroem		(I'm still super keen) 
MylesBorins		o/ 
joaocgreis		Count me in. 
Starefossen		Still keen üòÖ 
phillipj		I'm keen! ü§ò  On Thursday, 8 September 2016, Hans Kristian Flaatten < notifications@github.com> wrote:  > Still keen üòÖ >  > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > https://github.com/nodejs/build/issues/489#issuecomment-245492794, or mute > the thread > https://github.com/notifications/unsubscribe-auth/ABLLE--__SNCZQKDMAdqgeWNO2bNKnR4ks5qn5O_gaJpZM4J3Y4n > . 
orangemocha		I probably won't be active. If that changes, hopefully I can reapply for membership :smile:  
rmg		I haven't been as active as I'd hoped, and I don't imagine that will be changing in the near future. Probably best to remove me as well. :-(  I'll still be "around" if anyone needs me, though :-) 
ghostbar		I haven't been active doing stuff here directly, but on alpine and keeping nodejs external apks for compatibility. If that's compatible in any way with what needs to be done here, let me know. Otherwise, I should be removed. 
geek		I am ready to be an active participant! 
mhdawson		I'm still active 
rvagg		you have my sword 
jbergstroem		Lets keep this going another week to get travellers/busy people a chance to respond. I'll make an attempt to sum up and file a PR as a result. Note: this will also force us to redeploy keys and change passwords (which is a good thing!). 
gibfahn		Not a member of the build team, but I'm definitely planning to be active! 
piccoloaiutante		I have no contribution at the moment but i'm willing to contribute :-) 
jbergstroem		So, summing up -- here's the replies from our current team:  Remaining: - @jbergstroem - @TheAlphaNerd  - @joaocgreis  - @Starefossen  - @phillipj  - @geek  - @mhdawson  - @rvagg  Leaving:  - @orangemocha - @rmg  - @ghostbar   No reply (implied leave): - @kenperkins  - @othiym23   I'll keep this open for another few days giving people not having replied yet a chance to do so.  To those who are leaving: thank you for both your efforts and deciding to step down. You're obviously welcome back at any time should you find the time to help out! One of the main reasons I wanted to get this initiative going is to make sure we actively maintain and protect our infrastructure, including secrets. I will be re-rolling our shared test keys in as a result of closing this issue seeing how we haven't done so in almost a year.  Finally, one thing I had in mind was perhaps creating a document that outlines responsibilities/areas of interest -- @Trott recently mentioned that he'd like to know how to generally ping if host $x goes down or host $y are experiencing problems. A list that for instance mentions pinging @mhdawson or @jbergstroem should ppc hosts  have issues would benefit all collaborators and likely improve the feedback when things go bad. I'll create another issue about this if people think its a good idea.  Also, feedback on this procedure is also greatly appreciated!  
rmg		Reminder: when/before this gets closed, the actual @nodejs/build group members list should be updated to reflect the results :-)  **edit**: I just removed myself so there's one less click for whoever does the updating. 
jbergstroem		@rmg yep, thought I'd follow up with a PR past! Thanks for reminding me. 
jbergstroem		I've now updated all memberships and teams. Key updates in progress. To those taking a leave: thank you for all your work - wouldn't be here today without it. 
MylesBorins		LGTM 
gibfahn		LGTM 
jbergstroem		Sounds good to me. Will you handle his pubkey and show him the ropes? (copy repo, take bot out of action, etc) 
joaocgreis		@mhdawson sounds good to me as well! 
mhdawson		I'll add his public key to one of the AIX machines and co-ordinate with respect to other interaction required. Since the AIX builds only run once a night the co-ordination with existing builds should be limited. 
gireeshpunathil		@mhdawson - the purpose of this temporary access is achieved, and the access can now be revoked. Thanks to the approvers and endorsers.
maclover7		@mhdawson Can you confirm access has been revoked?
mhdawson		Deleted the user I had setup for @gireeshpunathil 
gibfahn		Landed in https://github.com/nodejs/build/commit/16b2bb8a6e2b87c22da0e7625b6f36f151900bd6  Thanks again @nschonni ! If you're looking for something to work on let us know.
rvagg		Whoa, not great, must be because I replaced the ARMv7 build machine and the original config must have this wrong.  @Fishrock123 I've copied those files back in to staging and renamed them properly, could you run a promote again to get them copied in and signed? Then, after 1/2 a day or so we'll run it again and remove the `armv7` versions but in between we'll have both. 
rvagg		fixed in setup @ 77c98f2 
Fishrock123		Re-promoted. 
phillipj		@rvagg @Fishrock123 when you remove the `armv7` file from /dist/, please remember to revert my quickfix https://github.com/nodejs/new.nodejs.org/commit/800748034e68720e361fc0cf17bf24e336db8954 
phillipj		.. + altering the filename in the shasums list 
rvagg		thinking now that we could just leave them there, at least until the next release comes out 
orangemocha		Meeting confirmed for tomorrow, added hangout link. 
jbergstroem		Not a issue just yet but I'd like to discuss: - how do we manage and share tokens tied to jenkins/github? 
orangemocha		Minutes from today's meeting: https://docs.google.com/document/d/1hP5CmYFc8OkEk83gdnEo3J7W5nhDpVBKSL2uGso0ta0/edit?usp=sharing 
jbergstroem		I'll remove the `wg-agenda` tag from #291 until I have a proposal in place. 
Starefossen		I removed `wg-agenda` from #75. 
maclover7		ping -- does this need to stay open?
rvagg		no! fixed this completely by introducing the 2 jenkins-worker hosts!
phillipj		@nodejs/build any thoughts about merging this? These variables has been present in prod for a while now..
rvagg		merged and deployed, thanks @rnsloan  
rvagg		lgtm! thanks @joaocgreis  
jbergstroem		Thanks. LGTM. 
joaocgreis		Thanks, landed in https://github.com/nodejs/build/commit/39b63a0dd0ad28c2778eed57ddcca4ede19ad9fe 
joaocgreis		Hi! This is unrelated to "build only if scm changes" message. There's a space in the target repository of that job, so git tries to fetch `https://github.com/ h0tw1r3/libuv.git/` and fails. Here is another run with the same parameters: https://ci.nodejs.org/view/libuv/job/libuv-test-commit/78/  About the "build only if scm changes", that is an issue with Jenkins. We had to enable it because a plugin update inverted the logic. So it says that the feature is enabled but it is actually disabled. If we disabled it, it would only build when it detected changes. 
saghul		Doh, sorry for the noise and thanks for the explanation Joao! 
gibfahn		This seems like a general Jenkins question. Is this related to the Node.js Jenkins build farm (https://ci.nodejs.org/)? Assuming it's not, I'll close this. Let me know if I'm mistaken.  For general Jenkins questions, you should probably try [stackoverflow](http://stackoverflow.com/questions/tagged/jenkins), which has a lot of Jenkins questions already. 
rvagg		jenkins proces on the server gave up completely; I've restarted it (and performed the usual Windows updates & reboot in the process) and asked for a new build of this  https://jenkins-node-forward.nodesource.com/job/node-forward+libuv+v1.x+multi/nodes=node-forward-rackspace-win2012r2-msvs2013/lastBuild/console  Still a failure being reported; possibly an intermittent one because it has been passing, e.g.: https://jenkins-node-forward.nodesource.com/job/node-forward+libuv+v1.x+multi/nodes=node-forward-rackspace-win2012r2-msvs2013/45/console  @piscisaureus let me know if think it'd be helpful to have access to the machine itself or Jenkins admin access to trigger builds outside of performing commits (currently we don't have tooling set up to request it via API or anything). 
saghul		@rvagg FWIW I  would be interested in being able to manually trigger builds of a given branch. 
gdams		updated PTAL
mhdawson		Started building the CI job, noted a few issues in earlier comments.  What I think would also be useful is if it could take an option like "latest" which would figure out the latest for a give release line.  That we won't have to update the job as new releases come out.  Generally I'm thinking we'll only need to be testing the latest.
mhdawson		still getting this ``` cat: nodejs.org/dist/v8.1.4/SHASUMS256.txt: No such file or directory ./build/tools/download-test.sh: line 49: ${TAP}: ambiguous redirect ```
gdams		@mhdawson should be fixed now if you want to re-run?
gdams		CI job here for reference: https://ci.nodejs.org/view/All/job/validate-downloads/
gireeshpunathil		I think the robot.txt is not allowing recursive download. We may need to parse the index.html to get links individually and then download them. ``` bash-4.1$ wget -r -d  http://nodejs.org/dist/v8.1.4/ ... Deciding whether to enqueue "http://nodejs.org/dist/v8.1.4/node-v8.1.4-x86.msi". Rejecting path dist/v8.1.4/node-v8.1.4-x86.msi because of rule ‚Äúdist/‚Äù. Not following http://nodejs.org/dist/v8.1.4/node-v8.1.4-x86.msi because robots.txt forbids it. Decided NOT to load it. ```  ``` bash-4.1$ cat nodejs.org/robots.txt  User-Agent: * Disallow: /dist/ Disallow: /docs/ Allow: /dist/latest/ Allow: /dist/latest/docs/api/ Allow: /api/ bash-4.1$  ```
gireeshpunathil		ok, the latest script line `wget -r --no-parent --execute="robots = off" http://nodejs.org/dist/v${NODE_VERSION}/` seem to have addressed the robot block.
mhdawson		Ok, so seems to run but I am seeing this   ./build/tools/download-test.sh: line 36: **[: !=: unary operator expected**  @gdams I'd also like to be able to be able to specify something like 8.latest and have it figure out the latest version.
rvagg		Worth noting that unencrypted.nodejs.org is currently on a 15 minute cron to update its binaries from the main server, that may need to be taken into account if this tool is for automating something close to release. The 15 minute number is arbitrary and could be changed if needed fwiw, it's even possible that we could use a flag mechanism to trigger a sync when a release is promoted so it's near instant, although that's a bit more complicated obviously.
gibfahn		@gdams when I said tools in https://github.com/nodejs/build/issues/513#issuecomment-316216447 I linked to the `tools/` directory in `nodejs/node`. I don't think it makes sense to start keeping scripts in this repo. so could you move this PR over there?
gdams		@gibfahn I was asked by @mhdawson  to move to over to `nodejs/build` https://github.com/nodejs/node/pull/14363
gibfahn		@gdams ahh, don't know how I missed that PR. I'll comment over there.
mhdawson		Now that I'm back I'd like to move forward on this.  @gdams have you had time to look at my suggestions for it being able to automatically run using the latest release ? 
gdams		@mhdawson @gibfahn updated PTAL. You can now either pass in a full node version (v6.11.2) or just (v6) and it will fetch the latest 6 build. You can also export `DOWNLOAD_LOCATION=nightly` and fetch the latest nightly builds
mhdawson		@rvagg @jbergstroem any input on whether you think this best fits in the build or core node repo ?   My feeling is the build repo but I could be convinced as @gibfahn suggests that it go into the core node repo.
rvagg		Seems like a reasonable case could be made either way. If it's in nodejs/node then others could run it independently to confirm.  @mhdawson can you explain how the CI job runs? I can't quite work it out. It runs on a schedule but does that mean it's grabbing the latest version every hour to confirm? Also, you may want to make it run on `jenkins-workspace` because those machines are perfect for this kind of work.
rvagg		just saw the description in https://github.com/nodejs/build/issues/513 about how the script works, fine by me.  I've changed restrict nodes from `ubuntu1404-64 ||  benchmark` to `jenkins-workspace` which is serviced by two packet.net boxes with big 'ol disk.
rvagg		lgtm, I don't mind where it lands, someone else can have a strong opinion cause I don't
rvagg		Also, well done @gdams, good work! I didn't see the utility of this initially but I _get it_ now.
mhdawson		@rvagg thanks for the change to where it runs, I was wondering what the best machines would be.
gibfahn		>Seems like a reasonable case could be made either way. If it's in nodejs/node then others could run it independently to confirm.  Yeah it's a tricky one. Would be good to get opinions from more people in @nodejs/build.
joaocgreis		My opinion on this is that `nodejs/node` is **not** the place for this. When we checkout a commit from `nodejs/node`, everything there should relate to that exact version, and be depended on by something there.  The way I would have done this would be directly in a Jenkins job script. We have backups of the server, so it would be safe. However, it would not be public and easily reviewable, but that again links to the discussion about reviewing the jobs source. Since this is already done as a script, I don't see why not land it here.
mhdawson		@gibfahn I thinking from your +1 on @joaocgreis' comment that you are now ok with landing this this repo ? If so I assume we just need @gdams to address your latest review comments and we'll be good to land ? 
gibfahn		@joaocgreis   >My opinion on this is that nodejs/node is not the place for this. When we checkout a commit from nodejs/node, everything there should relate to that exact version, and be depended on by something there.  I don't think this applies to a lot of things in [nodejs/node:tools/](https://github.com/nodejs/node/tree/master/tools).   >The way I would have done this would be directly in a Jenkins job script. We have backups of the server, so it would be safe. However, it would not be public and easily reviewable, but that again links to the discussion about reviewing the jobs source.  Yeah I think putting things in a Jenkins script is the worst possible option (we can't review, there may be backups but in my experience they aren't as easy to version control as git, it's hidden by default).  @mhdawson   I'm -0 on it, yes. If we're going to do this I'd like us to decide where we're going to put these kinds of scripts, and maybe move more out of Jenkins configs and into this repo. IDK if `tools/` is the best name for this folder in that case, but no strong opinions.  Given that we have a meeting on Tuesday, we could probably talk about this there (in particular the general question of where files like this should go). I'll add it to the agenda.
joaocgreis		> I don't think this applies to a lot of things in [nodejs/node:tools/](https://github.com/nodejs/node/tree/master/tools).   In my opinion, it should. But I have no strong feelings about this.  Let's discuss at the meeting.
gibfahn		@gdams consensus in the meeting (minutes: https://github.com/nodejs/build/pull/845) was to create a `nodejs/build:/jenkins/` folder to put this in, as we might want to move more scripts in here going forwards.  If you could fix up my suggestions and change the folder, this should be good to go.
gdams		@gibfahn updated PTAL
gdams		@gibfahn updated PTAL
gibfahn		@nodejs/build any other issues? If not I'll land this later today.
gdams		remember that the download testing job needs to be updated with the new path
gibfahn		I think this is a duplicate of https://github.com/nodejs/node-gyp/issues/569#issuecomment-259421050 (also see https://github.com/nodejs/node-gyp/pull/1057). Basically the xcodebuild tool is only available in the full Xcode version, but it doesn't actually seem to be doing anything in node=gyp.  So the proper fix would be to submit a version of https://github.com/nodejs/node-gyp/pull/1057 to gyp, but in the meantime the errors can be ignored.
bnoordhuis		I don't remember seeing them before though.  Did something change on the OS X buildbot?
thefourtheye		> xcodebuild tool is only available in the full Xcode version  Actually `xcodebuild` expects XCode to be available, but it found just Command line tools. I recently uninstalled XCode while helping someone (https://twitter.com/dFourthi/status/828154632077668352), and I started getting this message after that. It doesn't affect the build though.
bnoordhuis		Okay, if it isn't really a bug I'll go ahead and close.  Thanks for the replies.
saraahmad1114		I have Xcode installed and that was one of the very first things I installed but I am still getting this error, I have a whole project running on xcode, so I don't know what else to do 
gibfahn		>I have Xcode installed and that was one of the very first things I installed but I am still getting this error, I have a whole project running on xcode, so I don't know what else to do  Unless your build fails, I wouldn't worry about it.
gibfahn		Yep, not sure what's going on here:  ### Waiting for executor:  ![image](https://user-images.githubusercontent.com/15943089/28964139-7c05f38e-7903-11e7-8cce-364a930de3f1.png)  ### Idle executor:  ![image](https://user-images.githubusercontent.com/15943089/28964148-81687de2-7903-11e7-846f-079f18900b06.png) 
gareth-ellis		Just to point out too, it doesnt seem to just be benchmarking stuff, there's a lot of stuff in the queue waiting for machines that are idle: ![image](https://user-images.githubusercontent.com/14981026/28964318-93530820-790c-11e7-838e-8e69068f8270.png) 
refack		Is this something @nodejs/jenkins-admins can resolve?
refack		AFAICT the first job in the queue is a `libuv` job so maybe cancelling it will kick start Jenkins ü§∑‚Äç‚ôÇÔ∏è  /cc @libuv @saghul
rvagg		I cancelled most things in the queue and it seems to have kicked it into gear again. I'm not entirely sure what them problem is but it seemed to be stuck on a windows restart job, one of the expected servers wasn't available for that one to complete. Also a libuv job was having trouble. I think that one may be due to the armv7-ubuntu1404 machines being unavailable. I've had to take them out completely because they all have problems (power supply problems I'm guessing). So I've removed armv7-ubuntu1404 and the Pi's from libuv test commit for now.   :zzz:
gareth-ellis		Can probably close this now...
mscdex		I was getting that error before as well, but I'm now getting [this](https://ci.nodejs.org/job/node-test-commit-osx/12749/nodes=osx1010/console):  ``` fatal error: error in backend: IO failure on output stream. ```
BridgeAR		@mscdex the issue should be the same. The build before yours reports:  https://ci.nodejs.org/job/node-test-commit-osx/12748/nodes=osx1010/console ``` Caused by: hudson.plugins.git.GitException: Command "git config remote.origin.url https://github.com/nodejs/node.git" returned status code 255: stdout:  stderr: error: could not lock config file .git/config: No space left on device ```
refack		Has been resolved.
lpinca		Nice catch, will update shortly.
gibfahn		This seems fine, but it'd be great to update the new [`ansible/`](https://github.com/nodejs/build/tree/master/ansible) scripts too, IDK if an equivalent for this exists there yet (cc/ @jbergstroem ).
rvagg		:+1: deployed, please test and confirm
rvagg		Working on it! Just having to push some things aside to make time to finish this up. :soon: 
rvagg		## Status update  We're rolling our own CI system using Node, it's frankly too hard _not to_! It will be open sourced ASAP so we can get more people on board.  However, given the recent apparent uptick in progress towards node-forward/node we've set ourselves a deadline early next week that if we can't have something at least basic to hook up we're just going to fire up Jenkins with some basic slaves to get going so we can at least have something that works.  In parallel we can continue to work towards a more solid Jenkins replacement with a wider set of build slaves that's better suited to our needs. It needs to be something that our community can **own**, extend and put to use in all sorts of creative ways (e.g. one goal is to have tooling that we can put to use generating suites of binaries for native addons for distribution via node-pre-gyp, i.e. the holy grail).  My personal goal is to have _something_ to say at the next TC meeting, whatever that takes. 
jbergstroem		Rod hooked it up! 
rvagg		lgtm, I'm liking the .j2 templates for these.  Also, I've been putting a download of slave.jar in my recent startup scripts (currently all the ARM servers do this but I haven't finished the changes so they are not in this repo yet), that way we can keep it updated. Not important now but something to keep in mind for future iterations. 
jbergstroem		@rvagg in the "duke nukem" refactor I have locally I made a task that updates all slave.jar's. 
mhdawson		LGTM 
rvagg		Added to @nodejs/build, lgtm and welcome on board @iWuzHere! 
imran-iq		Thanks guys! 
jbergstroem		Looks good. Thanks for adding the rpi installation instructions.  Nitpick: could you also rename the xgene's? 
rvagg		@jbergstroem yes I can get around to renaming them sometime, but so could you now! 
jbergstroem		Ok, merge this and I'll rename them separately! 
phillipj		Good question. I'll do a test run locally when the node repo is not present.
phillipj		```bash iojs@jessie:~/repos/node$ git status On branch master Your branch is up-to-date with 'origin/master'. nothing to commit, working directory clean ```  Repo still seems to be completely cloned initially. That is actually overkill since the staging branches are the interesting ones, which PR patches are automatically landed against, but that's more of a fun fact tho..
refack		ping @nodejs/build (specifically infra team)
refack		@mhdawson @jbergstroem Hoping you're online
rvagg		Sorry, as I said on IRC, I started working on this a couple of hours ago but then went and had breakfast and forgot all about it. It is Saturday after all!  CI has been restarted and seems to be responsive. I did some poking around and: CPU was reasonable although Java was spiking way up and then way down again‚Äîalthough I don't believe this is abnormal for Jenkins. Disk space is great (thanks to jenkins-workspace machines fixing that!). Without anything further to go on my guess is simply that it's Jenkins fatigue for not being restarted in a while. It does so much work that the JVM memory space is _massive_ and it seems to get bogged down after a while and a restart does it wonders. It's either that or maybe an underlying problem with the hardware, maybe disk I/O issues. Will monitor further.  Will be working on ensuring all the nodes reconnect properly over the next 30 mins to an hour. Should all be good to use if you want to submit new jobs now though.
jasnell		Thanks @rvagg, I appreciate it.   I think it's past due time for me to dig in to Jenkins so you don't have to spend your Saturday mornings chasing that down. 
rvagg		@nodejs/build I've taken the Odroid C2's out of node-test-commit-arm, they are all down. The second one has known disk problems and a card needs to be replaced, the other two are unresponsive. I've reached out to David @ miniNodes to look into it, hopefully it's just a hard reboot for them.  Which actually makes me suspect that this might be part of the cause of the problems with Jenkins today‚Äîif machines are down and we get a backup of queued jobs for that type of node then Jenkins can go a little bit crazy holding on to them. So look at the queue, see if there's any class that's fully offline and work back from there. I've forced stopped the queued C2 jobs already.
jasnell		Grrr Jenkins. 
refack		Same circumstances as https://github.com/nodejs/build/issues/872 (ramdrive full). Cleared ramdrive on both machines. Now CitGM finishes - https://ci.nodejs.org/view/Node.js-citgm/job/citgm-smoker-nobuild/MACHINE=aix61-ppc64/190/console
gibfahn		Okay, so I guess we should have CitGM clean up after itself.
refack		Two extra pieces of information. * One CitGM run consumes ~9% of the ramdisk   ```   /dev/ramdisk0   10485760   9575536    9%      279     1% /ramdisk0   ``` * `/ramdisk0` is owned by `root` and is mod 755, so Jenkins can't create `/ramdisk0/citgm`   ```   drwxr-xr-x    4 root     system          256 Sep 22 15:07 .   ```   hence cleanup should be done with `rm -Rf /ramdisk0/citgm/*`
rvagg		rebuilding nightlies doesn't cause problems, at least nobody has ever complained, so feel free to reuse the same commit & date to test nightly rebuilds
Paul-Ivinson		@gibfahn Gibson - can you take a look at this PR please? The nginx access.log records for win 7z and zip downloads were being skipped. Also the IBM platform file names were not parsed to populate the os and arch fields in the metrics csv files.
richardlau		@Paul-Ivinson Did you mean to close this?
Paul-Ivinson		Doh - no, I didn't. Thanks for spotting that.
joaocgreis		The changes look good (if `m[10]` starts with `win-` and does not include everything under the `win-*` folders). I have no experience with this, is there an easy way to do a test run?  cc @rvagg @jbergstroem 
Paul-Ivinson		Don't have access to the logs so I took a log file from my own nginx server and mocked up some [test data](https://github.com/nodejs/build/files/858358/test.txt). This highlighted a problem with parsing ppc64le - it needs to be handled before ppc64.   That's now fixed. It would be good if this change could be run against the real logs @jbergstroem   
rvagg		good to go! I've merged a single-commit version of this and am currently reprocessing all of 2017 logs, we could go back further than that but this'll do for now.
gibfahn		For posterity, this landed as https://github.com/nodejs/build/commit/ae39829c4f3a8e70c0e5e6f96ed7439bb7521a6a
ljharb		also iojs.org (https://twitter.com/ljharb/status/737456640140574721) 
ljharb		Seems to be fixed now? 
rvagg		üôã I wear the blame for this, sorry all, I was running ansible against a test host, or so I thought, but the combination of /etc/hosts, .ssh/config and ansible-inventory got me working against the real host and there's a couple of things in our setup that don't quite match what we do on the live server‚Äîbut hey, at least we know that now and I can fix them! 
chorrell		Would pinning the OpenSSH package to a version prior to 7.5 be viable?  Something like:  ``` apk add 'openssh<7.5' ```  That should, in theory, install the latest available OpenSSH package that's less than version 7.5. Any `apk upgrade` after that should stick to the same version constraint. I'm more familiar with apt-pinning, so I don't know how well this works in practice:  https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management#Holding_a_specific_package_back
misterdjules		@rvagg Unfortunately I don't have the bandwidth to assist you with this right now :(
cjihrig		I've opened an internal support ticket. I'll update here if I receive any news.
rvagg		wew. Thanks for chiming in @chorrell, I saw you in that original ticket but noticed you're not at Joyent anymore so didn't want to bother you by pinging you here. Great to see you're still with us! @misterdjules no problems, thanks for letting us know, I hope you're doing well though. @cjihrig I forgot you wee Joyent now. I'll keep that in mind for future support needs.
cjihrig		We also have @geek on the build team üòÑ 
refack		Are there and security issues with sshd<7.5? Otherwise pinning seems like the simplest solution.
geek		@rvagg I'll plan to take a look tomorrow morning.
rvagg		`apk add 'openssh<7.5'` worked, hosts up and running and I'm [trying them out now](https://ci.nodejs.org/job/node-test-commit-linux/13954/). Added another commit here.  I had to make a change in jenkins because it was forcing use of `$(getconf _NPROCESSORS_ONLN)` for `JOBS` even if you have `JOBS` set. Which means that in these containers it's defaulting to the host CPU count which is 48 on Joyent. So I've now got it using `JOBS` if that exists and falling back to `$(getconf _NPROCESSORS_ONLN)` if it doesn't. In Ansible I've allowed `server_jobs` to be included in inventory.yml so you don't have to know to put it in the host_vars file for all alpine jobs.  Unfortunately one of our Alpine 3.4 hosts is uncontactable via ssh. My guess is that it's the same problem as this OpenSSH upgrade. It's still connected to Jenkins and able to run jobs but we can't perform maintenance. This will likely mean that as soon as it becomes a problem we'll just have to yank Alpine 3.4 from the list completely.
rvagg		So, it seems that we have a very firm limit on CPU usage in these containers on Joyent. They get part way through build & test and then just freeze up. I can log in to the machine and check do stuff on it myself but the processes doing the work are sitting idle.  @jbergstroem we never fully enabled the alpine34 Joyent containers in CI replacing the single DO Docker alpine34 host we have going. What was the blocker to getting that done, was it this same issue I'm seeing here? Is this even going to be possible or do we need to go full Docker ourselves for these? I just deleted my WIP standalone Docker/alpine Ansible script PR from this repo, maybe I need to open that back up and go that route again?
rvagg		@geek perhaps you could have a look at these, it's just weird, I've left them hanging and they are in the middle of a run, just at the start of test: https://ci.nodejs.org/job/node-test-commit-linux/13958/nodes=alpine34-x64/console & https://ci.nodejs.org/job/node-test-commit-linux/13958/nodes=alpine35-x64/console, it's as if they are just paused but they are still responsive and you can log in and mess around. You should have access to the build/test key to get into them I think. If it helps, the former is infrastructure container UUID 8b68e6f8-587f-4936-8b61-c45fa22c8cf1 and the latter is UUID 1b909000-11d6-69b2-f145-b97b6fdc4092.   
rvagg		Just noticed this in the ansible/README.md:  ``` - [ ] remove native alpine34 vm's on joyent since the joyent host       is not mature enough to provide linux emulation. use docker instead. ```  I think that answers my question so perhaps this PR can sit idle or be closed.  Joyent folks: if our work here is helpful at all to you we can proceed to tinker, otherwise we'll just move on to running our own Docker hosts (PR incoming for that).
rvagg		Docker approach active according to this: #992
mhart		How does 3.6 support look? That's been out for a while now
mhart		Ah, nevermind, I see that's referenced in #992
geek		@rvagg it looks like you were able to make progress in the #992 PR. Do you need anything from me for 989?
rvagg		yep, I'll consider this closed and power down the joyent containers, perhaps we can revisit this in the future
rvagg		centos7 arm64 hosts suffering from the same problem, I've run ansible across all of `*centos7*` now. Need to follow-up a bit later to see if it's being used (using a simple `du -s /home/iojs/.ccache` across all the worker hosts to check).
gibfahn		Probably makes sense to cc/ @nodejs/release  and @nodejs/tsc  as well. I don't think we've had new releasers in a while, so the process won't have been very thoroughly tested. We should probably write up an access document and put it in [doc/process/](https://github.com/nodejs/build/tree/master/doc/process).  +1 from me anyway (as an @nodejs/build member).  I think @mhdawson has a list of questions we normally ask for `build` access, I assume we could do something similar for `release` access.  Also, we don't seem to have an `access` label, thoughts on adding one?
bnoordhuis		+1.  Hard making releases without access to the release CI job.
thefourtheye		+1
jbergstroem		LGTM as well. Someone with elevated github access needs to add him to the proper group.
evanlucas		I went ahead and added to the releases team
jbergstroem		@italoacasas can you confirm having access to ci-release.nodejs.org?
italoacasas		@jbergstroem confirmed!
jbergstroem		Awesome! Closing.
italoacasas		Thanks everyone.
rvagg		git is at 1.7.10.4 on all of these machines, there's no update available on top of that, this is Raspbian fwiw 
rvagg		running this while I head off to bed to upgrade git from source: https://github.com/nodejs/build/pull/160  
rvagg		they are all running 2.5.0 now 
targos		there is a new issue: https://jenkins-iojs.nodesource.com/job/node-test-commit-arm/253/nodes=pi1-raspbian-wheezy/console http://stackoverflow.com/questions/8329485/git-clone-fatal-unable-to-find-remote-helper-for-https 
rvagg		trying recompiling 2.5.0 with libcurl4-openssl-dev installed now 
jbergstroem		LGTM 
mhdawson		Landed as 6e37152c7addea9a101699f3e3f09b8fb5649e0b 
mhdawson		@jbergstroem, @rvagg, @orangemocha   any concerns or is it ok to go ahead ? 
mhdawson		One thing that probably needs to be addressed is to ensure it does not run for 0.10.X or 0.12.X or io.js release prior to 4.X.  I did trim the NODES_SUBSET to that it only includes io.js, pure_doc_changes and all_nodes but I expect we may need something more. @orangemocha would you have a suggestion of how we handle this. 
orangemocha		I reviewed the job configuration and it generally looks good to me.   Regarding NODES_SUBSET, your combination filter looks correct to ensure that it won't run on v0.x. :+1:   In the NODES_SUBSET choice parameter definition however, I would include 0.10 and 0.12. I am almost sure that it won't fail if they are not there (when invoked by node-test-commit on v0.x), but it's easier to add them than to test it. It will still be a no-op on those versions because of the combination filter.  I noticed that the last build took 25 minutes. This puts it close to our current longest pole. Is ccache set up on these machines?  +1 for adding this to node-test-commit. I would suggest notifying the collaborators when you do so, to let them know about the failing tests. 
mhdawson		@orangemocha  thanks, will add back the 0.10 and 0.12 to the NODES_SUBSET.  I know I should probably know this already but whats the standard way to notifying the collaborators.  It looks like ccache should be installed as part of the standard ansible setup which I used,  @jbergstroem is there any other setup on the machine that I should have to do  ? 
orangemocha		You can mention @ nodejs/collaborators, without the space. 
mhdawson		On the ccache front, looking at the history for the last 3 runs they are <10 mins.  I'm guessing the first 2 runs at around 25mins were warming up the cache on our 2 machines for the new job.  Unless anybody objects by tomorrow morning my time I'll go ahead and add the link to node-test-commit and send out the heads up to the collaborators. 
jbergstroem		@mhdawson no more setup for `ccache` is needed. What you're looking at when it takes time to build is a new v8 (or no cache at all). If it doesn't improve the path to `ccache` might differ but in this case you should be all good. 
mhdawson		@nodejs/collaborators as an FYI I just added node-test-commit-plinux as a sub-job for node-test-commit so that we run regression tests on the plinux platforms.   plinux LE is currently green while the following 2 tests fail for plinux BE  not ok 711 test-tick-processor.js not ok 45 test-child-process-fork-regr-gh-2847.js  There are issues opened for these 2 and people are working on them.  
rvagg		I'm going to take no further discussion here as an implicit vote of confidence in this approach! 
jbergstroem		Post-lgtm 
orangemocha		Possibly due to a dangling node instance, though I am not 100% sure.  It happened again on the same slave on the next run: https://ci.nodejs.org/job/node-test-binary-arm/74/RUN_SUBSET=6,nodes=pi1-raspbian-wheezy/console  I marked the slave as temporary offline so that CI can continue to operate until @rvagg is able to investigate or reboot (as he is the only one with access to that machine). 
rvagg		rebooted, cleaned workspace and back online 
evanlucas		Thanks! 
seishun		I'm not an expert, but I do use Windows and care about its support. So you can count me in as someone who would be interested in diving deep to figure out a Windows-specific issue. 
bnoordhuis		> There has been some discussion of moving to clang at some point but apparently it's not quite "ready", is anybody following this? Do we have any clue about our ability to even start testing with it?  I follow clang development quite closely but I haven't actually tried to build node with it yet.  I speculate that clang HEAD is compatible enough with MSVC now that a build would need only minor tweaks.  The biggest barrier is having to build clang from source.  @seishun If you're up for adventure, you could give that a try. 
seishun		@bnoordhuis LLVM already provides a [binary](http://llvm.org/releases/download.html) for Windows, so no need to build it from source. Building node on Windows using clang would certainly be a challenge, but what's the purpose? Surely you're not meaning to drop Visual Studio build support? 
Qard		The challenge is that, with C++11 features in V8, the support versions of Visual Studio is limited. Many developers still use older versions of Visual Studio and don't want to upgrade. More still don't even have it installed, and getting it set up properly is pretty awkward for someone not familiar with it. (Needs extra stuff for x64, etc)  By building with clang, we don't have to care what version is installed, or that they even _have_ Visual Studio. I believe the need to build from source is just to catch the latest Windows support features. Support for Windows is pretty new--I'm not familiar enough with it to know if current releases lag behind some important features, but it seems like that could be the case. 
seishun		> Many developers still use older versions of Visual Studio and don't want to upgrade.  Why should we cater to such developers? It's not like they can't use node without building it from source. 
Qard		It's native modules where clang is really needed. Some total newbie to node probably isn't going to be building node from source, but there's a good possibility of them trying to install something with npm that has native code in it and it blowing up at them for not having the right Visual Studio with everything in the right place.  Presently, native modules are built on the user end, which means a compiler must be available. There has been discussion about npm building and delivering built binaries in the future, but that'd take very significant effort. 
bnoordhuis		> Building node on Windows using clang would certainly be a challenge, but what's the purpose?   Several reasons: 1. We wouldn't have to work around VS bugs anymore.  Of the three major compilers we support, VS is the buggiest one by a wide margin; it's certainly the least compliant when it comes to C++11. 2. Like @Qard said, it would help immensely with native add-ons.  In order to build add-ons with clang, node needs to build with clang (or at least, its public headers should.) 3. Longer term, I expect Chromium (and therefore V8) to switch over to clang.  I'm not sure if they would drop VS support completely (maybe not for development) but it probably won't have tier 1 support either.  Apart from IDE support, and assuming good enough ABI compatibility, are there any downsides to moving to clang? 
seishun		> We wouldn't have to work around VS bugs anymore. Of the three major compilers we support, VS is the buggiest one by a wide margin; it's certainly the least compliant when it comes to C++11.  It's going to improve. VS2015 isn't far away and it's going to get significantly closer to C++11 compliance. Also, which VS bugs and missing C++11 features are you concerned about, besides constexpr and extended sizeof?  > Like @Qard said, it would help immensely with native add-ons. In order to build add-ons with clang, node needs to build with clang (or at least, its public headers should.)  Now that's a valid issue. But what prevents addons built with clang from working with node built with VS?  > Longer term, I expect Chromium (and therefore V8) to switch over to clang. I'm not sure if they would drop VS support completely (maybe not for development) but it probably won't have tier 1 support either.  If that does happen then I'll agree with you. But I don't see any reason to drop VS support before V8 does.  > Apart from IDE support, and assuming good enough ABI compatibility, are there any downsides to moving to clang?  I imagine most Windows developers are likely to have VS installed, not so with clang and whatever build system that would be used. Also, the convenience of IDE should not be taken lightly. Personally, I would be quite disappointed if I could no longer use Visual Studio to work with node's code. 
Qard		Devs that write native code would probably have VS installed. But there's a huge number of frontend devs, using node for tooling, that don't write native code and thus probably don't have VS installed.  There are core devs hacking on node from Windows, so I doubt VS support would ever be removed, it'd just not be the default.  Also, I think the intent might be to ship clang, pre-built, alongside node in some installer. So you'd just run the installer and not have to concern yourself with installing a particular version of compiler to get native modules to work. 
seishun		> Devs that write native code would probably have VS installed. But there's a huge number of frontend devs, using node for tooling, that don't write native code and thus probably don't have VS installed.  Sure, but we're talking about building node itself. Front-end devs are unlikely to want to do that.  > There are core devs hacking on node from Windows, so I doubt VS support would ever be removed, it'd just not be the default.  Supporting two build systems on Windows sounds like a huge pain.  > Also, I think the intent might be to ship clang, pre-built, alongside node in some installer. So you'd just run the installer and not have to concern yourself with installing a particular version of compiler to get native modules to work.  That would be great. So it boils down to whether we can get clang-built addons to work with VS-built node. 
smikes		> names of people who can legitimately help with making Windows a first-class citizen  I've done devel work on windows and would like to help.  > That would be great. So it boils down to whether we can get clang-built addons to work with VS-built node.  I can look into this.  Offhand I would guess that insofar as C++11 requires new name mangling, they will be hard to reconcile unless there is explicit collaboration between the compiler author groups. 
springmeyer		@rvagg please count me in for helping / happy to get mentioned anytime I might be of use (My email is dane@mapbox.com too). Other thoughts: - Stoked to see C++11 discussion here: it is critical for me because nearly all c++ addons I maintain need C++11. - I would recommend aiming to use Visual Studio 2015. I've encountered no bugs in Visual Studio 2014 CTP 4 (basically the same as the 2015 preview as far as C++11). It has worked great. I found 2013 to be too buggy to be usable in terms of C++11. - I've successfully been building Node v0.10.33 against Visual Studio 2014 CTP 4 with minor patches and plan to soon provide these upstream to joyent/node and iojs/node: https://github.com/mapbox/node/pull/1 - At Mapbox we maintain a CI setup that uses travis + aws to automate building C++11 node binaries on ubuntu precise, osx, and windows that might be of interest in this effort: https://github.com/mapbox/node-cpp11. I would love to retire this eventually if node/iojs could provide Visual Studio 2015 binaries. 
rvagg		We'll certainly rope you in for help @springmeyer since you've had so much experience with this, we'd also like to get you involved in #12 when we start thinking about that (perhaps not for a while yet!).  VS2015 was next on my list for Windows in the test cluster but I wasn't in a hurry because it's not properly released yet. We're only doing VS2013 at the moment. 
bnoordhuis		I'm going to tentatively close this now that we have @iojs/platform-windows and are getting close to zero test failures on Windows.  @springmeyer Let me know if you're still interested in helping out and I'll add you to the Windows team. 
jbergstroem		We added this to solve cross-filesystem and disk size issues. Although it might not be needed for ubuntu16, I think consistency across the fleet is desired. We haven't rolled this out on all machines (since we don't redeploy on every change to playbooks), but it will likely happen sooner or later. We didn't bother creating the directory as part of a playbook since the nodejs test suite added it. Should we not just add the directory instead? 
mcollina		@jbergstroem I'm happy with adding the directory. I thought that given it was not on the full fleet, it was a mistake.  I'm happy with having the directory. The whole point is making sure that the directory exists if the env variable is there.
jbergstroem		@mcollina perhaps citgm can create the tmp folder if it doesn't exist, similarly to the node test suite? I can add the folder as part of the playbook in parallel.
mcollina		@jbergstroem it's not a problem of CITGM for that. It's in https://github.com/nodejs/readable-stream/blob/master/test/common.js#L45, which is lifted from core https://github.com/nodejs/node/blob/master/test/common.js#L35-L36. So, it is the node test suite.  I would rather prefer not to do a `readable-stream` release just to change `test/common.js`
piccoloaiutante		@mcollina @jbergstroem how about adding [here](https://github.com/nodejs/build/blob/master/ansible/roles/jenkins-worker/tasks/partials/tap2junit/ubuntu.yml) this:  ``` - name: Creates NODE_TEST_DIR directory   file: path=/home/{{ server_user }}/tmp state=directory ```  Instead of this pr change?
jbergstroem		@piccoloaiutante i'd suggest adding in a more generic role, like a java worker. The tap2junit python script is more related to the conversion of output; not so much just running a java worker. 
piccoloaiutante		@jbergstroem shouldn't be better to add it as part of an already existing role like`bootstrap`? We just only need to create the missing folder for ubuntu machine, so we could add it in ubuntu partials [here](https://github.com/nodejs/build/tree/master/ansible/roles/bootstrap/tasks/partials).
jbergstroem		@piccoloaiutante exactly. It should live with the jenkins client role.
piccoloaiutante		should we close this in favour of https://github.com/nodejs/build/pull/785  @mcollina  ?
mcollina		Closed!
gibfahn		>should we close this in favour of¬†#785¬†@mcollina¬†?  In general I'd say leave it open, and add the `Fixes:` line in #785, that way the issue stays open even if the PR doesn't land for some reason. Doesn't really matter either way though. 
orangemocha		@mhdawson : from the last meeting https://github.com/nodejs/build/pull/354 was due for another iteration. Shall we remove it from the agenda until then, or should it stay on? 
Starefossen		17th of May is the üá≥üá¥ Norwegian Constitution Day üéâ, so I will probably not be able to attend this one. Have a nice meeting üòÑ  > On 12. mai 2016, at 18:20, Alexis Campailla notifications@github.com wrote: >  > @mhdawson : from the last meeting #354 was due for another iteration. Shall we remove it from the agenda until then, or should it stay on? >  > ‚Äî > You are receiving this because you are on a team that was mentioned. > Reply to this email directly or view it on GitHub 
imran-iq		On request of @mhdawson I will be listening in! 
jbergstroem		There's really no procedure (I know of :); its mostly manual work. Seeing how he's part of @nodejs/release he should have been added as part of joining the group.
MylesBorins		we should likely make an on boarding guide 
jbergstroem		> @MylesBorins said: > we should likely make an on boarding guide  yeah; for all "levels" of participation. I attempted it for the build group membership with @Trott rott and @Starefossen ‚Äì should probably summarize that someplace.
joaocgreis		The Windows 2016 failure is because the machines only have 3.5 Gb of RAM. I'll check if it reproduces every time.  The Windows 10 failure is probably because the compile is outdated in those machines, I'll update.
refack		I'm not trying to cause trouble, just trying to stress a test ü§∑‚Äç‚ôÇÔ∏è  `master` X `win2016` - https://ci.nodejs.org/job/node-stress-single-test/1318/nodes=win2016/ ``` v8_libbase.lib(stack_trace_win.obj) : fatal error LNK1318: Unexpected PDB error; OK (0) '???' [c:\workspace\node-stress-single-test\nodes\win2016\node.vcxproj] Build step 'Conditional steps (multiple)' marked build as failure Notifying upstream projects of job completion Finished: FAILURE ```
bnoordhuis		It was made private to test the security release.  It should be public again though. 
saghul		Oh, I didn't know that. Thanks for the heads up! 
jbergstroem		Yep, set back to public as of ~12 hours ago. 
Starefossen		I am currently looking into it. Monitoring service is online but not sending out emails as it is supposed to.   Will keep you guys posted.  > On 22. juli 2016, at 23:46, Michael Dawson notifications@github.com wrote: >  > I don't seem to be getting emails when machines go offline. >  > Talking to Johan sounds like the same for him. >  > @Starefossen >  > ‚Äî > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. 
Starefossen		Sendgrid had marked outgoing emails from the monitor as spam, I have marked them as safe and emptied the queue and they should start getting through from now on own. Sorry for the missing emails. 
jbergstroem		Seems to be working now. Closing. 
rvagg		Some additional notes: - now that we are putting out installers for 0.10 and 0.12 from the new infra I'd like to know whether this is still a problem with 0.10.41, 0.12.8 and 0.12.9, from what you've said it sounds like it might be resolved, at least for current builds (not necessarily for custom builds as you've suggested) - we're about to shift to [Packages](http://s.sudre.free.fr/Software/Packages/about.html), see https://github.com/nodejs/node/pull/2571, the only thing remaining is some testing and signoff by the build team. I hoped to get this in v5.0.0 but it didn't make it. Perhaps there's an easy solution in there and perhaps it doesn't even suffer from the same problems even when the build machines are set up in the same way that has lead to the initial problem. This doesn't solve the issue for older release lines unless we backport the new installer of course. - sudo in the `Makefile` is kind of gross and would be unfortunate 
maclover7		v0.x is EOL, I believe -- can this issue be closed?
maclover7		Closing due to inactivity. Please reopen if this is still needed :)
gibfahn		>Maybe remove some of the empty links like Hangouts on Air, for participants  Good point, I missed that. Done.
maclover7		I don't think it was explicitly an agenda item, but improving interaction process with non-build WG members was discussed. Should a separate issue be opened to discuss this further?
gibfahn		>I don't think it was explicitly an agenda item, but improving interaction process with non-build WG members was discussed. Should a separate issue be opened to discuss this further?  Possibly, depends exactly who you mean by non-build WG members. https://github.com/nodejs/build/issues/941 is aimed at helping to make it easier for people to get involved.  Happy to open an issue, but not sure what to say in it.
refack		> Happy to open an issue, but not sure what to say in it.  I think a very short desc of what the Build WG is and does, and how the CI is set up. * permission groups * job scripts stuck in Jenkins * `/ansible/` is future * `/setup/` is past * bot / email / website / CI * CI test vs release
gibfahn		Opened https://github.com/nodejs/build/issues/950  @maclover7 feel free to comment there, or otherwise tell us what you feel is unclear and we'll fix it.
shigeki		Ref for original PR: https://github.com/nodejs/node/pull/5712 
bnoordhuis		I think it's not supported.  We put a lot of effort in the ARM builds but that was never back-ported to v0.10 and v0.12. That's also why there are no binaries for those release lines. 
shigeki		I've got it. I thought I had ever seen CI for arm running on v0.12 for sometime ago. But it was wrong.  
bnoordhuis		I think it's okay to give companies credit for the fact that they're paying people to work on io.js.  It also signals that it's not just a hobby project. 
rvagg		meh, these companies are spending good money on contributing these people's time. Part of my intention here is to demonstrate that recognition is one of the rewards for paying people to work on this stuff.  Remember companies are people too, you don't want to hurt their feelings.  Oh, and that list is kind of stale, I really need to fix the readme. 
junosuarez		How about listing email addresses like traditional open source projects, this way companies can get implicit recognition if contributors choose to use their company email addresses? 
rvagg		yeah, that's a way forward if this really bothers people 
mikeal		I don't know if the email address thing does the same thing, I pretty much never list my company address.  The more I think about this the more I think it should just be up to individuals. Maybe the default is to add them without affiliation but to tell them they can add it like the others if they wish to change it. It should be up to each person how they want to be identified. 
kenperkins		:+1: on being left up to individuals. 
retrohacker		:+1: on being left to individuals. Perhaps people pull request in their own attribution to the readme? 
kenperkins		~~~README or GOVERNANCE? @wblankenship has #49 which includes collaborators as well.~~~   _nothing to see here_ 
retrohacker		The attributions are in the REAME in #49 :smile: 
kenperkins		Hmm? :smile:  
erickbelfy		Do not cling to classifications, all the people who help, whether companies or individuals, are important for the life of the project. That's my opinion. :+1:  
bnoordhuis		Closing.  Looks like this has run its course and that most people are okay with company affiliations. 
jbergstroem		How about configuring core dump directory to /home/iojs/dump or smth together with a cron job then? (find -mtime -delete) 
jbergstroem		I'm not very well versed on `coreadm`, but If you have a suggested route wrt dumping to another directory I can update playbooks. 
misterdjules		> How about configuring core dump directory to /home/iojs/dump or smth together with a cron job then? (find -mtime -delete)  That's exactly what I'm suggesting, with the nit that I would choose a different name for this directory: `/home/iojs/cores` instead of `/home/iojs/dump`, but I don't have a strong opinion about it.  > I'm not very well versed on coreadm, but If you have a suggested route wrt dumping to another directory I can update playbooks.  Running `coreadm -g /home/iojs/cores/core.%f.%p -e global` will enable global core files and will store them in `/home/iojs/cores`. It will also persist across reboots.   We should also make sure that `ulimit -c` outputs `unlimited` for the user under which the tests run. 
jbergstroem		Cool. I'm actually in the process of setting up smartos15 and 16 hosts. I'll look at incorporating this in the playbook. 
jbergstroem		Sorry for the delay here -- i'm a bit swamped at the moment. If anyone wants to chip in with improving playbooks for smartos14..16  that would be appreciated. I can spin up test machines if required.  Also, protip from @chorrell -- we can disable the SmartLogin (solving shared key access boundaries) by doing something in style with: - remove/comment `PubKeyPlugin libsmartsshd.so` in `/etc/sshd/ssd_config`  - restart sshd (`svcadm restart sshd`) 
jbergstroem		So, I spent some time on smartos today. Looks like we're running into issues with your openjdk8:  ``` java # /opt/local/java/openjdk8/bin/java -Xmx128m -jar slave.jar -jnlpUrl https://ci.nodejs.org/computer/test-joyent-smartos15-x64-1/slave-agent.jnlp -secret foo Exception in thread "main" java.lang.Error: Error during hash calculation         at sun.security.ssl.HandshakeHash.getFinishedHash(HandshakeHash.java:249)         at sun.security.ssl.HandshakeMessage$Finished.getFinished(HandshakeMessage.java:1952)         at sun.security.ssl.HandshakeMessage$Finished.<init>(HandshakeMessage.java:1899)         at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1214)         at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134)         at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348)         at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979)         at sun.security.ssl.Handshaker.process_record(Handshaker.java:914)         at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062)         at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375)         at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403)         at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387)         at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)         at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)         at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153)         at hudson.remoting.Launcher.parseJnlpArguments(Launcher.java:269)         at hudson.remoting.Launcher.run(Launcher.java:219)         at hudson.remoting.Launcher.main(Launcher.java:192) Caused by: java.lang.RuntimeException: Could not clone digest         at sun.security.ssl.HandshakeHash.cloneDigest(HandshakeHash.java:194)         at sun.security.ssl.HandshakeHash.getFinishedHash(HandshakeHash.java:247)         ... 17 more Caused by: java.lang.CloneNotSupportedException: SHA-384         at sun.security.pkcs11.P11Digest.clone(P11Digest.java:316)         at java.security.MessageDigest$Delegate.clone(MessageDigest.java:560)         at sun.security.ssl.HandshakeHash.cloneDigest(HandshakeHash.java:191)         ... 18 more Caused by: sun.security.pkcs11.wrapper.PKCS11Exception: CKR_STATE_UNSAVEABLE         at sun.security.pkcs11.wrapper.PKCS11.C_GetOperationState(Native Method)         at sun.security.pkcs11.P11Digest.clone(P11Digest.java:311)         ... 20 more ```  I don't have more time to look into that, but an up to date playbook is available in my repo. 
jbergstroem		..same happens for smartos16. 
chorrell		That's odd. I use a base-64-lts 15.4.0 with jenkins for Joyent image builds and openjdk8 works fine on that node. Is there more than one version of Java install, like a `sun-jre` package? 
jbergstroem		@chorrell no, [just openjdk8](https://github.com/jbergstroem/build/blob/feature/refactor-the-world/ansible/roles/java-base/vars/main.yml) 
chorrell		This might be relevant: https://www.illumos.org/issues/7227 
chorrell		So maybe:  ``` sun.security.pkcs11.enable-solaris=false ``` 
jbergstroem		@chorrell: `"-Dsun.security.pkcs11.enable-solaris=false" is not a valid option` 
jbergstroem		@chorrell sorry, it does work -- I just messed up ordering. 
jbergstroem		First run with 15,16 here: https://ci.nodejs.org/job/node-test-commit-smartos/4584/ 
jbergstroem		This has been implemented on all hosts and are available in the [playbooks in my refactor](https://github.com/jbergstroem/build/blob/feature/refactor-the-world/ansible/roles/baselayout/tasks/main.yml#L23..L25). 
misterdjules		@jbergstroem Thank you very much for your work! Should this issue be closed?
jbergstroem		I guess we could, but seeing how we still need to land my PR it would be slightly misleading?
misterdjules		> seeing how we still need to land my PR it would be slightly misleading  What PR are you referring to?
gibfahn		I assume this one: https://github.com/nodejs/build/pull/606
misterdjules		@gibfahn Thanks for the context!  Let's not close this issue until that PR is merged then.
misterdjules		#606 was merged a while ago, so closing. Thank you very much @jbergstroem!
indutny		cc @nodejs/build 
gibfahn		SGTM  Would you _**(EDIT: also)**_ need a CI job to test with? I'm thinking that it might be easier to just use Travis and Appveyor to start with.
indutny		Not sure if it is feasible, but remote desktop would be more helpful. I'll have to run it many many times, and CI will slow me down significantly.
gibfahn		Sorry, forgot to write also. Direct access with Remote Desktop SGTM. I was wondering if you'd also need a CI job.
indutny		No problem. I don't need CI at this point, just need to make it work.
Trott		SGTM
refack		@indutny need help with a live person in front of a windows machine?
indutny		@refack this is going to be "trial and error" until I will succeed :) Better get my hands on machine if we have one.
indutny		@refack but thanks for the offer!
refack		Also I have a some free credit, and a nice machine image on Azure if the org can't provide anything else. In parallel to you getting RDP, I have the next two days free, give me an entry point and I'll dig. I'm very comfortable on Windows... üòâ
joaocgreis		@indutny sent you an email with credentials. Let us know when you no longer need access.
joaocgreis		@indutny please let us know when you no longer need access, or when we can connect the machine back to CI even if you'd like to keep access. Thanks!
joaocgreis		@indutny the machine is back in the CI pool, let me know if you want to have it back exclusively.
maclover7		ping -- is access still needed?
jbergstroem		I think part of the reason it never got updated was because parts of this text was in the governance PR. Anyhow, LGTM if you need it. 
rvagg		yeah, that too, I might take over that PR from Will cause I know he's busy with school atm 
mhdawson		Ok initial job in place, passed on platforms configured in the job that I cloned from citgm  Next step is to add additional platforms not in the citgm job including smartos and windows.  Not sure about arm.  @rvagg @jbergstroem do you see any issues with this running regularly on the arm machines ? I know they are more resource contained.
mhdawson		Ok finally got windows working with a small tweak.  Smartos is broken but its because the nightlies are not currently building.  
joaocgreis		I saw the failure on `win2012r2`, looks like npm is not properly sandboxed. A good way is to set the variable `NPM_CONFIG_USERCONFIG` before calling any npm commands, as the citgm-smoker job does (for Windows, there was no trouble on Unix yet but probably only because of luck). `NPM_CONFIG_CACHE` should also point to a clean or stable location. Furthermore, the `win2012r2` label has machines with VCBT where `npm config set msvs_version 2015` is needed, I think it should be ok to do for all. Let me know if you have further trouble with Windows!
richardlau		@gibfahn / @mhdawson Would it be possible to stop the jobs trying to run on AIX and s390x when running against Node v4 since there are no Node.js binaries for those platforms?   e.g. https://ci.nodejs.org/view/post-mortem/job/nodereport-continuous-integration/93/MACHINE=aix61-ppc64/console
gibfahn		@richardlau Yes, I'll put a check in.
mhdawson		Believe this is done
